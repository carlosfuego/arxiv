[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.27641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27641v1",
                "updated": "2025-10-31T17:12:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    34,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:12:34Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    34,
                    4,
                    304,
                    0
                ],
                "title": "SpecAttn: Speculating Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecAttn: Speculating Sparse Attention"
                },
                "summary": "Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation."
                },
                "authors": [
                    {
                        "name": "Harsh Shah"
                    }
                ],
                "author_detail": {
                    "name": "Harsh Shah"
                },
                "author": "Harsh Shah",
                "arxiv_comment": "Accepted to NeurIPS 2025 Workshop on Structured Probabilistic\n  Inference & Generative Modeling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27617v1",
                "updated": "2025-10-31T16:40:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    40,
                    58,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T16:40:58Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    40,
                    58,
                    4,
                    304,
                    0
                ],
                "title": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation"
                },
                "summary": "Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training."
                },
                "authors": [
                    {
                        "name": "Heng Ping"
                    },
                    {
                        "name": "Arijit Bhattacharjee"
                    },
                    {
                        "name": "Peiyu Zhang"
                    },
                    {
                        "name": "Shixuan Li"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Anzhe Cheng"
                    },
                    {
                        "name": "Xiaole Zhang"
                    },
                    {
                        "name": "Jesse Thomason"
                    },
                    {
                        "name": "Ali Jannesari"
                    },
                    {
                        "name": "Nesreen Ahmed"
                    },
                    {
                        "name": "Paul Bogdan"
                    }
                ],
                "author_detail": {
                    "name": "Paul Bogdan"
                },
                "author": "Paul Bogdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04525v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04525v2",
                "updated": "2025-10-31T05:31:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    5,
                    31,
                    58,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-06T06:30:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion"
                },
                "summary": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers."
                },
                "authors": [
                    {
                        "name": "Satoshi Hayakawa"
                    },
                    {
                        "name": "Yuhta Takida"
                    },
                    {
                        "name": "Masaaki Imaizumi"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "23 pages, fixed cleveref-related issue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04525v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04525v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27171v1",
                "updated": "2025-10-31T04:47:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    47,
                    14,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T04:47:14Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    47,
                    14,
                    4,
                    304,
                    0
                ],
                "title": "H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance\n  Acceleration of Generative Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H2-Cache: A Novel Hierarchical Dual-Stage Cache for High-Performance\n  Acceleration of Generative Diffusion Models"
                },
                "summary": "Diffusion models have emerged as state-of-the-art in image generation, but\ntheir practical deployment is hindered by the significant computational cost of\ntheir iterative denoising process. While existing caching techniques can\naccelerate inference, they often create a challenging trade-off between speed\nand fidelity, suffering from quality degradation and high computational\noverhead. To address these limitations, we introduce H2-Cache, a novel\nhierarchical caching mechanism designed for modern generative diffusion model\narchitectures. Our method is founded on the key insight that the denoising\nprocess can be functionally separated into a structure-defining stage and a\ndetail-refining stage. H2-cache leverages this by employing a dual-threshold\nsystem, using independent thresholds to selectively cache each stage. To ensure\nthe efficiency of our dual-check approach, we introduce pooled feature\nsummarization (PFS), a lightweight technique for robust and fast similarity\nestimation. Extensive experiments on the Flux architecture demonstrate that\nH2-cache achieves significant acceleration (up to 5.08x) while maintaining\nimage quality nearly identical to the baseline, quantitatively and\nqualitatively outperforming existing caching methods. Our work presents a\nrobust and practical solution that effectively resolves the speed-quality\ndilemma, significantly lowering the barrier for the real-world application of\nhigh-fidelity diffusion models. Source code is available at\nhttps://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as state-of-the-art in image generation, but\ntheir practical deployment is hindered by the significant computational cost of\ntheir iterative denoising process. While existing caching techniques can\naccelerate inference, they often create a challenging trade-off between speed\nand fidelity, suffering from quality degradation and high computational\noverhead. To address these limitations, we introduce H2-Cache, a novel\nhierarchical caching mechanism designed for modern generative diffusion model\narchitectures. Our method is founded on the key insight that the denoising\nprocess can be functionally separated into a structure-defining stage and a\ndetail-refining stage. H2-cache leverages this by employing a dual-threshold\nsystem, using independent thresholds to selectively cache each stage. To ensure\nthe efficiency of our dual-check approach, we introduce pooled feature\nsummarization (PFS), a lightweight technique for robust and fast similarity\nestimation. Extensive experiments on the Flux architecture demonstrate that\nH2-cache achieves significant acceleration (up to 5.08x) while maintaining\nimage quality nearly identical to the baseline, quantitatively and\nqualitatively outperforming existing caching methods. Our work presents a\nrobust and practical solution that effectively resolves the speed-quality\ndilemma, significantly lowering the barrier for the real-world application of\nhigh-fidelity diffusion models. Source code is available at\nhttps://github.com/Bluear7878/H2-cache-A-Hierarchical-Dual-Stage-Cache."
                },
                "authors": [
                    {
                        "name": "Mingyu Sung"
                    },
                    {
                        "name": "Il-Min Kim"
                    },
                    {
                        "name": "Sangseok Yun"
                    },
                    {
                        "name": "Jae-Mo Kang"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Mo Kang"
                },
                "author": "Jae-Mo Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18586v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18586v2",
                "updated": "2025-10-31T04:17:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    4,
                    17,
                    5,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-21T12:39:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    12,
                    39,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokencake: A KV-Cache-centric Serving Framework for LLM-based\n  Multi-Agent Applications"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in complex multi-agent\napplications that use external function calls. This workload creates severe\nperformance challenges for the KV Cache: space contention leads to the eviction\nof critical agents' caches and time underutilization leaves the cache of agents\nstalled on long-running tool calls idling in GPU memory. We present Tokencake,\na KV-Cache-centric serving framework that co-optimizes scheduling and memory\nmanagement with an agent-aware design. Tokencake's Space Scheduler uses dynamic\nmemory partitioning to shield critical agents from contention, while its Time\nScheduler employs a proactive offload and predictive upload mechanism to\nrepurpose GPU memory during function call stalls. Our evaluation on\nrepresentative multi-agent benchmarks shows that Tokencake can reduce\nend-to-end latency by over 47.06%, improve effective GPU memory utilization by\nup to 16.9% compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Zhuohang Bian"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Youwei Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Youwei Zhuo"
                },
                "author": "Youwei Zhuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18586v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18586v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25977v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25977v2",
                "updated": "2025-10-31T01:52:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    1,
                    52,
                    13,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-29T21:22:08Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    22,
                    8,
                    2,
                    302,
                    0
                ],
                "title": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on\n  AWS Trainium",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuronMM: High-Performance Matrix Multiplication for LLM Inference on\n  AWS Trainium"
                },
                "summary": "AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI accelerators, customized to AI workloads, provide cost-effective and\nhigh-performance solutions for training and inference. Trainium, an AI\naccelerator recently developed by Amazon Web Services (AWS), provides an\nattractive option for LLM training and inference through its heterogeneous\narchitecture. However, leveraging Trainium architecture for high performance\ncan be challenging because of its systolic array architecture and special\nrequirement on data layout. In this paper, we design high-performance matrix\nmultiplication (matmul), a critical compute kernel, for LLM inference on\nTrainium. We introduce a series of techniques customized to Trainium based on\nkernel fusion and novel caching strategies to reduce data movement across the\nsoftware-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive\nmatrix transpose. Evaluating with nine datasets and four recent LLMs, we show\nthat our system largely outperforms the state-of-the-art matmul implemented by\nAWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x\nspeedup (up to 2.22x), which translates to an average 1.66x speedup (up to\n2.49x) for end-to-end LLM inference."
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Jierui Xu"
                    },
                    {
                        "name": "Weichu Yang"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_comment": "12 pages, 8 figures, submitted to the Proceedings of the Twenty-First\n  European Conference on Computer Systems (EuroSys'26)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25977v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27070v1",
                "updated": "2025-10-31T00:39:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    0,
                    39,
                    27,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T00:39:27Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    0,
                    39,
                    27,
                    4,
                    304,
                    0
                ],
                "title": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Descriptor-Based Object-Aware Memory Systems: A Comprehensive Review"
                },
                "summary": "The security and efficiency of modern computing systems are fundamentally\nundermined by the absence of a native architectural mechanism to propagate\nhigh-level program semantics, such as object identity, bounds, and lifetime,\nacross the hardware/software interface. This paper presents a comprehensive\nsurvey of the architectural paradigm designed to bridge this semantic gap:\ndescriptor-based, object-aware memory systems. By elevating the descriptor to a\nfirst-class architectural abstraction, this paradigm enables hardware to\ndynamically acquire and enforce the rich semantics of software-defined objects.\nThis survey systematically charts the evolution and current landscape of this\napproach. We establish the foundational concepts of memory objects and\ndescriptors and introduce a novel taxonomy of descriptor addressing modes,\nproviding a structured framework for analyzing and comparing diverse\nimplementations. Our unified analysis reveals how this paradigm holistically\naddresses the intertwined challenges of memory protection, management, and\nprocessing. As a culminating case study, we re-examine the CentroID model,\ndemonstrating how its hybrid tagged-pointer encoding and descriptor processing\nmechanisms embody the path toward practical and efficient object-aware designs.\nFinally, we outline how the explicit cross-layer communication of object\nsemantics provides a foundational research direction for next-generation cache\nhierarchies, unified virtual memory, and even 128-bit architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The security and efficiency of modern computing systems are fundamentally\nundermined by the absence of a native architectural mechanism to propagate\nhigh-level program semantics, such as object identity, bounds, and lifetime,\nacross the hardware/software interface. This paper presents a comprehensive\nsurvey of the architectural paradigm designed to bridge this semantic gap:\ndescriptor-based, object-aware memory systems. By elevating the descriptor to a\nfirst-class architectural abstraction, this paradigm enables hardware to\ndynamically acquire and enforce the rich semantics of software-defined objects.\nThis survey systematically charts the evolution and current landscape of this\napproach. We establish the foundational concepts of memory objects and\ndescriptors and introduce a novel taxonomy of descriptor addressing modes,\nproviding a structured framework for analyzing and comparing diverse\nimplementations. Our unified analysis reveals how this paradigm holistically\naddresses the intertwined challenges of memory protection, management, and\nprocessing. As a culminating case study, we re-examine the CentroID model,\ndemonstrating how its hybrid tagged-pointer encoding and descriptor processing\nmechanisms embody the path toward practical and efficient object-aware designs.\nFinally, we outline how the explicit cross-layer communication of object\nsemantics provides a foundational research direction for next-generation cache\nhierarchies, unified virtual memory, and even 128-bit architectures."
                },
                "authors": [
                    {
                        "name": "Dong Tong"
                    }
                ],
                "author_detail": {
                    "name": "Dong Tong"
                },
                "author": "Dong Tong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00413v2",
                "updated": "2025-10-30T21:11:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    21,
                    11,
                    33,
                    3,
                    303,
                    0
                ],
                "published": "2025-05-31T06:10:10Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    6,
                    10,
                    10,
                    5,
                    151,
                    0
                ],
                "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding"
                },
                "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks."
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    },
                    {
                        "name": "Aditya Grover"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Grover"
                },
                "author": "Aditya Grover",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26944v1",
                "updated": "2025-10-30T18:58:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    18,
                    58,
                    2,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T18:58:02Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    18,
                    58,
                    2,
                    3,
                    303,
                    0
                ],
                "title": "Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache\n  Hierarchies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Choreographer: A Full-System Framework for Fine-Grained Tasks in Cache\n  Hierarchies"
                },
                "summary": "In this paper, we introduce Choreographer, a simulation framework that\nenables a holistic system-level evaluation of fine-grained accelerators\ndesigned for latency-sensitive tasks. Unlike existing frameworks, Choreographer\ncaptures all hardware and software overheads in core-accelerator and\ncache-accelerator interactions, integrating a detailed gem5-based hardware\nstack featuring an AMBA coherent hub interface (CHI) mesh network and a\ncomplete Linux-based software stack. To facilitate rapid prototyping, it offers\na C++ application programming interface and modular configuration options. Our\ndetailed cache model provides accurate insights into performance variations\ncaused by cache configurations, which are not captured by other frameworks. The\nframework is demonstrated through two case studies: a data-aware prefetcher for\ngraph analytics workloads, and a quicksort accelerator. Our evaluation shows\nthat the prefetcher achieves speedups between 1.08x and 1.88x by reducing\nmemory access latency, while the quicksort accelerator delivers more than 2x\nspeedup with minimal address translation overhead. These findings underscore\nthe ability of Choreographer to model complex hardware-software interactions\nand optimize performance in small task offloading scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Choreographer, a simulation framework that\nenables a holistic system-level evaluation of fine-grained accelerators\ndesigned for latency-sensitive tasks. Unlike existing frameworks, Choreographer\ncaptures all hardware and software overheads in core-accelerator and\ncache-accelerator interactions, integrating a detailed gem5-based hardware\nstack featuring an AMBA coherent hub interface (CHI) mesh network and a\ncomplete Linux-based software stack. To facilitate rapid prototyping, it offers\na C++ application programming interface and modular configuration options. Our\ndetailed cache model provides accurate insights into performance variations\ncaused by cache configurations, which are not captured by other frameworks. The\nframework is demonstrated through two case studies: a data-aware prefetcher for\ngraph analytics workloads, and a quicksort accelerator. Our evaluation shows\nthat the prefetcher achieves speedups between 1.08x and 1.88x by reducing\nmemory access latency, while the quicksort accelerator delivers more than 2x\nspeedup with minimal address translation overhead. These findings underscore\nthe ability of Choreographer to model complex hardware-software interactions\nand optimize performance in small task offloading scenarios."
                },
                "authors": [
                    {
                        "name": "Hoa Nguyen"
                    },
                    {
                        "name": "Pongstorn Maidee"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    },
                    {
                        "name": "Alireza Kaviani"
                    }
                ],
                "author_detail": {
                    "name": "Alireza Kaviani"
                },
                "author": "Alireza Kaviani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26730v1",
                "updated": "2025-10-30T17:29:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    29,
                    27,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T17:29:27Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    29,
                    27,
                    3,
                    303,
                    0
                ],
                "title": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for\n  Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertFlow: Adaptive Expert Scheduling and Memory Coordination for\n  Efficient MoE Inference"
                },
                "summary": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models is increasingly limited by the\nconstrained memory capacity of modern GPUs. To mitigate this,\nMixture-of-Experts (MoE) architectures activate only a small portion of\nparameters during inference, significantly lowering both memory demand and\ncomputational overhead. However, conventional MoE inference approaches, which\nselect active experts independently at each layer, often introduce considerable\nlatency because of frequent parameter transfers between host and GPU memory. In\naddition, current cross-layer prediction strategies, which are typically based\non fixed steps, lack adaptability across different hardware platforms and\nworkloads, thereby reducing their robustness and effectiveness.\n  To address these challenges, we present ExpertFlow, a runtime system for MoE\ninference that combines adaptive expert prefetching and cache-aware routing.\nExpertFlow continuously adjusts its prediction horizon for expert activation by\nleveraging runtime statistics such as transfer bandwidth, parameter\ndimensionality, and model feedback signals. Furthermore, it incorporates a\nhybrid cross-layer prediction scheme that fuses pregating information with\nintermediate computational states to anticipate future expert needs. By\nadaptively refining prefetching decisions and aligning them with actual usage\nbehavior, ExpertFlow effectively decreases cache misses and removes latency\ncaused by expert swap-ins. Our evaluation demonstrates that ExpertFlow reduces\nmodel stall time to less than 0.1% of the baseline, highlighting its capability\nto optimize MoE inference under stringent memory constraints."
                },
                "authors": [
                    {
                        "name": "Zixu Shen"
                    },
                    {
                        "name": "Kexin Chu"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Dawei Xiang"
                    },
                    {
                        "name": "Runxin Wu"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "12 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26692v1",
                "updated": "2025-10-30T16:59:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    59,
                    43,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T16:59:43Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    16,
                    59,
                    43,
                    3,
                    303,
                    0
                ],
                "title": "Kimi Linear: An Expressive, Efficient Attention Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimi Linear: An Expressive, Efficient Attention Architecture"
                },
                "summary": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Kimi Linear, a hybrid linear attention architecture that, for\nthe first time, outperforms full attention under fair comparisons across\nvarious scenarios -- including short-context, long-context, and reinforcement\nlearning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an\nexpressive linear attention module that extends Gated DeltaNet with a\nfiner-grained gating mechanism, enabling more effective use of limited\nfinite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware\nefficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR)\ntransition matrices, which substantially reduces computation compared to the\ngeneral DPLR formulation while remaining more consistent with the classical\ndelta rule.\n  We pretrain a Kimi Linear model with 3B activated parameters and 48B total\nparameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention\n(MLA). Our experiments show that with an identical training recipe, Kimi Linear\noutperforms full MLA with a sizeable margin across all evaluated tasks, while\nreducing KV cache usage by up to 75% and achieving up to 6 times decoding\nthroughput for a 1M context. These results demonstrate that Kimi Linear can be\na drop-in replacement for full attention architectures with superior\nperformance and efficiency, including tasks with longer input and output\nlengths.\n  To support further research, we open-source the KDA kernel and vLLM\nimplementations, and release the pre-trained and instruction-tuned model\ncheckpoints."
                },
                "authors": [
                    {
                        "name": "Kimi Team"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Zongyu Lin"
                    },
                    {
                        "name": "Xingcheng Yao"
                    },
                    {
                        "name": "Jiaxi Hu"
                    },
                    {
                        "name": "Fanqing Meng"
                    },
                    {
                        "name": "Chengyin Liu"
                    },
                    {
                        "name": "Xin Men"
                    },
                    {
                        "name": "Songlin Yang"
                    },
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Wentao Li"
                    },
                    {
                        "name": "Enzhe Lu"
                    },
                    {
                        "name": "Weizhou Liu"
                    },
                    {
                        "name": "Yanru Chen"
                    },
                    {
                        "name": "Weixin Xu"
                    },
                    {
                        "name": "Longhui Yu"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Yu Fan"
                    },
                    {
                        "name": "Longguang Zhong"
                    },
                    {
                        "name": "Enming Yuan"
                    },
                    {
                        "name": "Dehao Zhang"
                    },
                    {
                        "name": "Yizhi Zhang"
                    },
                    {
                        "name": "T. Y. Liu"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Shengjun Fang"
                    },
                    {
                        "name": "Weiran He"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Jianlin Su"
                    },
                    {
                        "name": "Jiezhong Qiu"
                    },
                    {
                        "name": "Bo Pang"
                    },
                    {
                        "name": "Junjie Yan"
                    },
                    {
                        "name": "Zhejun Jiang"
                    },
                    {
                        "name": "Weixiao Huang"
                    },
                    {
                        "name": "Bohong Yin"
                    },
                    {
                        "name": "Jiacheng You"
                    },
                    {
                        "name": "Chu Wei"
                    },
                    {
                        "name": "Zhengtao Wang"
                    },
                    {
                        "name": "Chao Hong"
                    },
                    {
                        "name": "Yutian Chen"
                    },
                    {
                        "name": "Guanduo Chen"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Huabin Zheng"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Mengnan Dong"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Siyuan Pan"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Yuhao Wu"
                    },
                    {
                        "name": "Longyu Guan"
                    },
                    {
                        "name": "Jiawen Tao"
                    },
                    {
                        "name": "Guohong Fu"
                    },
                    {
                        "name": "Xinran Xu"
                    },
                    {
                        "name": "Yuzhi Wang"
                    },
                    {
                        "name": "Guokun Lai"
                    },
                    {
                        "name": "Yuxin Wu"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Zhilin Yang"
                    },
                    {
                        "name": "Yulun Du"
                    }
                ],
                "author_detail": {
                    "name": "Yulun Du"
                },
                "author": "Yulun Du",
                "arxiv_comment": "Kimi Linear tech report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20499v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20499v2",
                "updated": "2025-10-30T13:43:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    43,
                    31,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-23T12:39:59Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    39,
                    59,
                    3,
                    296,
                    0
                ],
                "title": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-Accelerated Primal Heuristics for Mixed Integer Programming"
                },
                "summary": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a fusion of GPU accelerated primal heuristics for Mixed Integer\nProgramming. Leveraging GPU acceleration enables exploration of larger search\nregions and faster iterations. A GPU-accelerated PDLP serves as an approximate\nLP solver, while a new probing cache facilitates rapid roundings and early\ninfeasibility detection. Several state-of-the-art heuristics, including\nFeasibility Pump, Feasibility Jump, and Fix-and-Propagate, are further\naccelerated and enhanced. The combined approach of these GPU-driven algorithms\nyields significant improvements over existing methods, both in the number of\nfeasible solutions and the quality of objectives by achieving 221 feasible\nsolutions and 22% objective gap in the MIPLIB2017 benchmark on a presolved\ndataset."
                },
                "authors": [
                    {
                        "name": "Akif rdk"
                    },
                    {
                        "name": "Piotr Sielski"
                    },
                    {
                        "name": "Alice Boucher"
                    },
                    {
                        "name": "Kumar Aatish"
                    }
                ],
                "author_detail": {
                    "name": "Kumar Aatish"
                },
                "author": "Kumar Aatish",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20499v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20499v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26486v1",
                "updated": "2025-10-30T13:39:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    39,
                    8,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T13:39:08Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    13,
                    39,
                    8,
                    3,
                    303,
                    0
                ],
                "title": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LINK-KG: LLM-Driven Coreference-Resolved Knowledge Graphs for Human\n  Smuggling Networks"
                },
                "summary": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human smuggling networks are complex and constantly evolving, making them\ndifficult to analyze comprehensively. Legal case documents offer rich factual\nand procedural insights into these networks but are often long, unstructured,\nand filled with ambiguous or shifting references, posing significant challenges\nfor automated knowledge graph (KG) construction. Existing methods either\noverlook coreference resolution or fail to scale beyond short text spans,\nleading to fragmented graphs and inconsistent entity linking. We propose\nLINK-KG, a modular framework that integrates a three-stage, LLM-guided\ncoreference resolution pipeline with downstream KG extraction. At the core of\nour approach is a type-specific Prompt Cache, which consistently tracks and\nresolves references across document chunks, enabling clean and disambiguated\nnarratives for structured knowledge graph construction from both short and long\nlegal texts. LINK-KG reduces average node duplication by 45.21% and noisy nodes\nby 32.22% compared to baseline methods, resulting in cleaner and more coherent\ngraph structures. These improvements establish LINK-KG as a strong foundation\nfor analyzing complex criminal networks."
                },
                "authors": [
                    {
                        "name": "Dipak Meher"
                    },
                    {
                        "name": "Carlotta Domeniconi"
                    },
                    {
                        "name": "Guadalupe Correa-Cabrera"
                    }
                ],
                "author_detail": {
                    "name": "Guadalupe Correa-Cabrera"
                },
                "author": "Guadalupe Correa-Cabrera",
                "arxiv_comment": "Accepted in ICKG 2025 Conference, 8 Pages, 2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25160v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25160v2",
                "updated": "2025-10-30T08:52:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    52,
                    17,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T04:29:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    29,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "Model-Document Protocol for AI Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Document Protocol for AI Search"
                },
                "summary": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI search depends on linking large language models (LLMs) with vast external\nknowledge sources. Yet web pages, PDF files, and other raw documents are not\ninherently LLM-ready: they are long, noisy, and unstructured. Conventional\nretrieval methods treat these documents as verbatim text and return raw\npassages, leaving the burden of fragment assembly and contextual reasoning to\nthe LLM. This gap underscores the need for a new retrieval paradigm that\nredefines how models interact with documents.\n  We introduce the Model-Document Protocol (MDP), a general framework that\nformalizes how raw text is bridged to LLMs through consumable knowledge\nrepresentations. Rather than treating retrieval as passage fetching, MDP\ndefines multiple pathways that transform unstructured documents into\ntask-specific, LLM-ready inputs. These include agentic reasoning, which curates\nraw evidence into coherent context; memory grounding, which accumulates\nreusable notes to enrich reasoning; and structured leveraging, which encodes\ndocuments into formal representations such as graphs or key-value caches. All\nthree pathways share the same goal: ensuring that what reaches the LLM is not\nraw fragments but compact, structured knowledge directly consumable for\nreasoning.\n  As an instantiation, we present MDP-Agent, which realizes the protocol\nthrough an agentic process: constructing document-level gist memories for\nglobal coverage, performing diffusion-based exploration with vertical\nexploitation to uncover layered dependencies, and applying map-reduce style\nsynthesis to integrate large-scale evidence into compact yet sufficient\ncontext. Experiments on information-seeking benchmarks demonstrate that\nMDP-Agent outperforms baselines, validating both the soundness of the MDP\nframework and the effectiveness of its agentic instantiation."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Liu"
                },
                "author": "Zheng Liu",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25160v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25160v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18480v2",
                "updated": "2025-10-30T08:46:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    46,
                    37,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-21T10:00:32Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    0,
                    32,
                    1,
                    294,
                    0
                ],
                "title": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Efficient Are Diffusion Language Models? A Critical Examination of\n  Efficiency Evaluation Practices"
                },
                "summary": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (DLMs) have emerged as a promising alternative to\nthe long-dominant autoregressive (AR) paradigm, offering a parallelable\ndecoding process that could yield greater efficiency. Yet, in practice, current\nopen-source DLMs often underperform their AR counterparts in speed, limiting\ntheir real-world utility. This work presents a systematic study of DLM\nefficiency, identifying key issues in prior evaluation methods. Through\nempirical benchmarking and a roofline-based theoretical analysis, we\ndemonstrate that AR models generally achieve higher throughput, while DLMs\nconsistently lag. We also investigate acceleration strategies, finding that\ntechniques like dual cache and parallel decoding mainly offer gains at small\nbatch sizes, with their benefits diminishing upon scaling. Our findings\nunderscore the necessity of robust evaluation methods and improved acceleration\nstrategies to advance research on DLMs."
                },
                "authors": [
                    {
                        "name": "Han Peng"
                    },
                    {
                        "name": "Peiyu Liu"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Daixuan Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Yiru Tang"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Wayne Xin Zhao"
                },
                "author": "Wayne Xin Zhao",
                "arxiv_comment": "Withdrawn by the authors to better delineate the related work from\n  the paper's original contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26234v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26234v1",
                "updated": "2025-10-30T08:12:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    12,
                    53,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T08:12:53Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    8,
                    12,
                    53,
                    3,
                    303,
                    0
                ],
                "title": "From req/res to pub/sub: Exploring Media over QUIC Transport for DNS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From req/res to pub/sub: Exploring Media over QUIC Transport for DNS"
                },
                "summary": "The DNS is a key component of the Internet. Originally designed to facilitate\nthe resolution of host names to IP addresses, its scope has continuously\nexpanded over the years, today covering use cases such as load balancing or\nservice discovery. While DNS was initially conceived as a rather static\ndirectory service in which resource records (RR) only change rarely, we have\nseen a number of use cases over the years where a DNS flavor that isn't purely\nbased upon requesting and caching RRs, but rather on an active distribution of\nupdates for all resolvers that showed interest in the respective records in the\npast, would be preferable. In this paper, we thus explore a publish-subscribe\nvariant of DNS based on the Media-over-QUIC architecture, where we devise a\nstrawman system and protocol proposal to enable pushing RR updates. We provide\na prototype implementation, finding that DNS can benefit from a\npublish-subscribe variant: next to limiting update traffic, it can considerably\nreduce the time it takes for a resolver to receive the latest version of a\nrecord, thereby supporting use cases such as load balancing in content\ndistribution networks. The publish-subscribe architecture also brings new\nchallenges to the DNS, including a higher overhead for endpoints due to\nadditional state management, and increased query latencies on first lookup, due\nto session establishment latencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS is a key component of the Internet. Originally designed to facilitate\nthe resolution of host names to IP addresses, its scope has continuously\nexpanded over the years, today covering use cases such as load balancing or\nservice discovery. While DNS was initially conceived as a rather static\ndirectory service in which resource records (RR) only change rarely, we have\nseen a number of use cases over the years where a DNS flavor that isn't purely\nbased upon requesting and caching RRs, but rather on an active distribution of\nupdates for all resolvers that showed interest in the respective records in the\npast, would be preferable. In this paper, we thus explore a publish-subscribe\nvariant of DNS based on the Media-over-QUIC architecture, where we devise a\nstrawman system and protocol proposal to enable pushing RR updates. We provide\na prototype implementation, finding that DNS can benefit from a\npublish-subscribe variant: next to limiting update traffic, it can considerably\nreduce the time it takes for a resolver to receive the latest version of a\nrecord, thereby supporting use cases such as load balancing in content\ndistribution networks. The publish-subscribe architecture also brings new\nchallenges to the DNS, including a higher overhead for endpoints due to\nadditional state management, and increased query latencies on first lookup, due\nto session establishment latencies."
                },
                "authors": [
                    {
                        "name": "Mathis Engelbart"
                    },
                    {
                        "name": "Mike Kosek"
                    },
                    {
                        "name": "Lars Eggert"
                    },
                    {
                        "name": "Jrg Ott"
                    }
                ],
                "author_detail": {
                    "name": "Jrg Ott"
                },
                "author": "Jrg Ott",
                "arxiv_doi": "10.1145/3772356.3772416",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772356.3772416",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.26234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26234v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "HotNets 2025",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25600v2",
                "updated": "2025-10-30T03:43:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    43,
                    2,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-29T15:10:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    10,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse\n  Attention for Vision-Language Large Models"
                },
                "summary": "Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Large Models (VLLMs) face significant efficiency challenges\nwhen processing high-resolution inputs. The quadratic complexity in attention\nand autoregressive generation, as well as the constantly growing key value (KV)\ncache size, severely hinder the prefilling and decoding stages. Recent efforts\nhave attempted to compress KV cache by identifying and pruning KV cache of less\nimportant tokens, but these methods typically rely on attention scores to\nestimate token importance, making them incompatible with efficient attention\nmechanisms such as FlashAttention and Sparse Attention, which do not explicitly\ncompute attention matrices. Moreover, existing methods overlook how sparse\nattention, while accelerating the prefilling stage, alters the information\nstructure of the KV cache, thereby compromising the effectiveness of downstream\nKV cache compression strategies. To address this issue, we propose PureKV, a\nplug-and-play framework for joint optimization of sparse attention and KV cache\ncompression. We first introduce a KV cache compression strategy that is fully\ncompatible with efficient attention accelerators. Our method utilizes lower\nlayer attention scores to estimate the importance of high layers' KV cache,\nenabling active pruning without compromising accuracy. In addition, we have\ndesigned a Spatial-Temporal Sparse Attention (ST-SpAttn) module specifically\ntailored for video KV cache compression algorithms. This module combines\nspatial and temporal attention sparsity to improve the compression efficiency\nof KV cache optimization algorithms by purifying spatial noise and temporal\nredundancy in KV cache. At the same time, ST-SpAttn also accelerated the\nprefilling stage of VLLMs. Extensive experiments on VLLMs (VideoLLaMA2,\nQwen2.5-VL) have shown that PureKV achieves 5.0 times KV cache compression and\n3.16 times prefill acceleration, with negligible quality degradation."
                },
                "authors": [
                    {
                        "name": "Zhonghua Jiang"
                    },
                    {
                        "name": "Kunxi Li"
                    },
                    {
                        "name": "Yiyun Zhou"
                    },
                    {
                        "name": "Sihao Liu"
                    },
                    {
                        "name": "Zhaode Wang"
                    },
                    {
                        "name": "Chengfei lv"
                    },
                    {
                        "name": "Shengyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengyu Zhang"
                },
                "author": "Shengyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26104v1",
                "updated": "2025-10-30T03:30:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    30,
                    12,
                    3,
                    303,
                    0
                ],
                "published": "2025-10-30T03:30:12Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    3,
                    30,
                    12,
                    3,
                    303,
                    0
                ],
                "title": "OneTrans: Unified Feature Interaction and Sequence Modeling with One\n  Transformer in Industrial Recommender",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OneTrans: Unified Feature Interaction and Sequence Modeling with One\n  Transformer in Industrial Recommender"
                },
                "summary": "In recommendation systems, scaling up feature-interaction modules (e.g.,\nWukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has\nachieved notable success. However, these efforts typically proceed on separate\ntracks, which not only hinders bidirectional information exchange but also\nprevents unified optimization and scaling. In this paper, we propose OneTrans,\na unified Transformer backbone that simultaneously performs user-behavior\nsequence modeling and feature interaction. OneTrans employs a unified tokenizer\nto convert both sequential and non-sequential attributes into a single token\nsequence. The stacked OneTrans blocks share parameters across similar\nsequential tokens while assigning token-specific parameters to non-sequential\ntokens. Through causal attention and cross-request KV caching, OneTrans enables\nprecomputation and caching of intermediate representations, significantly\nreducing computational costs during both training and inference. Experimental\nresults on industrial-scale datasets demonstrate that OneTrans scales\nefficiently with increasing parameters, consistently outperforms strong\nbaselines, and yields a 5.68% lift in per-user GMV in online A/B tests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recommendation systems, scaling up feature-interaction modules (e.g.,\nWukong, RankMixer) or user-behavior sequence modules (e.g., LONGER) has\nachieved notable success. However, these efforts typically proceed on separate\ntracks, which not only hinders bidirectional information exchange but also\nprevents unified optimization and scaling. In this paper, we propose OneTrans,\na unified Transformer backbone that simultaneously performs user-behavior\nsequence modeling and feature interaction. OneTrans employs a unified tokenizer\nto convert both sequential and non-sequential attributes into a single token\nsequence. The stacked OneTrans blocks share parameters across similar\nsequential tokens while assigning token-specific parameters to non-sequential\ntokens. Through causal attention and cross-request KV caching, OneTrans enables\nprecomputation and caching of intermediate representations, significantly\nreducing computational costs during both training and inference. Experimental\nresults on industrial-scale datasets demonstrate that OneTrans scales\nefficiently with increasing parameters, consistently outperforms strong\nbaselines, and yields a 5.68% lift in per-user GMV in online A/B tests."
                },
                "authors": [
                    {
                        "name": "Zhaoqi Zhang"
                    },
                    {
                        "name": "Haolei Pei"
                    },
                    {
                        "name": "Jun Guo"
                    },
                    {
                        "name": "Tianyu Wang"
                    },
                    {
                        "name": "Yufei Feng"
                    },
                    {
                        "name": "Hui Sun"
                    },
                    {
                        "name": "Shaowei Liu"
                    },
                    {
                        "name": "Aixin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Aixin Sun"
                },
                "author": "Aixin Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11507v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11507v2",
                "updated": "2025-10-29T21:56:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    56,
                    19,
                    2,
                    302,
                    0
                ],
                "published": "2025-07-15T17:23:22Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "title": "Oneiros: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oneiros: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving"
                },
                "summary": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce Oneiros, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that Oneiros significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM. Source code of Oneiros is available at\nhttps://github.com/UT-SysML/Oneiros/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce Oneiros, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that Oneiros significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM. Source code of Oneiros is available at\nhttps://github.com/UT-SysML/Oneiros/."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Shagnik Pal"
                    },
                    {
                        "name": "Vineeth Narayan Pullu"
                    },
                    {
                        "name": "Prasoon Sinha"
                    },
                    {
                        "name": "Jeeho Ryoo"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11507v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11507v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25979v1",
                "updated": "2025-10-29T21:26:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    26,
                    17,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T21:26:17Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    21,
                    26,
                    17,
                    2,
                    302,
                    0
                ],
                "title": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via\n  Attention Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttnCache: Accelerating Self-Attention Inference for LLM Prefill via\n  Attention Cache"
                },
                "summary": "Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used in generative applications such\nas chatting, code generation, and reasoning. However, many realworld workloads\nsuch as classification, question answering, recommendation, and text embedding\nrely solely on the prefill stage of inference, where the model encodes input\nsequences without performing autoregressive decoding. In these prefill only\nscenarios, the self-attention computation becomes the primary performance\nbottleneck due to its quadratic complexity with respect to sequence length. In\nthis paper, we observe that semantically different sentences often produce\nsimilar attention maps across layers and heads. Building on this insight, we\npropose AttnCache, a framework that accelerates the prefill stage of LLM\ninference by retrieving and reusing similar attention maps. Based on an\nattention map memorization database, AttnCache employs efficient caching and\nsimilarity search techniques to identify and reuse pre-cached attention maps\nduring inference, thereby reducing the computational overhead of\nself-attention. Experimental results show that AttnCache achieves an average of\n1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x\nattention speedup on GPU, with negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Dinghong Song"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Shangye Chen"
                    },
                    {
                        "name": "Cyril Guyot"
                    },
                    {
                        "name": "Filip Blagojevic"
                    },
                    {
                        "name": "Hyeran Jeon"
                    },
                    {
                        "name": "Pengfei Su"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "arxiv_affiliation": "University of California, Merced, USA",
                "author": "Dong Li",
                "arxiv_comment": "10 pages, 6 figures, submitted to Ninth Annual Conference on Machine\n  Learning and Systems (MLSys'26)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26835v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26835v1",
                "updated": "2025-10-29T19:59:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    19,
                    59,
                    45,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T19:59:45Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    19,
                    59,
                    45,
                    2,
                    302,
                    0
                ],
                "title": "Category-Aware Semantic Caching for Heterogeneous LLM Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Category-Aware Semantic Caching for Heterogeneous LLM Workloads"
                },
                "summary": "LLM serving systems process heterogeneous query workloads where different\ncategories exhibit different characteristics. Code queries cluster densely in\nembedding space while conversational queries distribute sparsely. Content\nstaleness varies from minutes (stock data) to months (code patterns). Query\nrepetition patterns range from power-law (code) to uniform (conversation),\nproducing long tail cache hit rate distributions: high-repetition categories\nachieve 40-60% hit rates while low-repetition or volatile categories achieve\n5-15% hit rates. Vector databases must exclude the long tail because remote\nsearch costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of\nproduction traffic uncached. Uniform cache policies compound this problem:\nfixed thresholds cause false positives in dense spaces and miss valid\nparaphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This\npaper presents category-aware semantic caching where similarity thresholds,\nTTLs, and quotas vary by query category. We present a hybrid architecture\nseparating in-memory HNSW search from external document storage, reducing miss\ncost from 30ms to 2ms. This reduction makes low-hit-rate categories\neconomically viable (break-even at 3-5% versus 15-20%), enabling cache coverage\nacross the entire workload distribution. Adaptive load-based policies extend\nthis framework to respond to downstream model load, dynamically adjusting\nthresholds and TTLs to reduce traffic to overloaded models by 9-17% in\ntheoretical projections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM serving systems process heterogeneous query workloads where different\ncategories exhibit different characteristics. Code queries cluster densely in\nembedding space while conversational queries distribute sparsely. Content\nstaleness varies from minutes (stock data) to months (code patterns). Query\nrepetition patterns range from power-law (code) to uniform (conversation),\nproducing long tail cache hit rate distributions: high-repetition categories\nachieve 40-60% hit rates while low-repetition or volatile categories achieve\n5-15% hit rates. Vector databases must exclude the long tail because remote\nsearch costs (30ms) require 15--20% hit rates to break even, leaving 20-30% of\nproduction traffic uncached. Uniform cache policies compound this problem:\nfixed thresholds cause false positives in dense spaces and miss valid\nparaphrases in sparse spaces; fixed TTLs waste memory or serve stale data. This\npaper presents category-aware semantic caching where similarity thresholds,\nTTLs, and quotas vary by query category. We present a hybrid architecture\nseparating in-memory HNSW search from external document storage, reducing miss\ncost from 30ms to 2ms. This reduction makes low-hit-rate categories\neconomically viable (break-even at 3-5% versus 15-20%), enabling cache coverage\nacross the entire workload distribution. Adaptive load-based policies extend\nthis framework to respond to downstream model load, dynamically adjusting\nthresholds and TTLs to reduce traffic to overloaded models by 9-17% in\ntheoretical projections."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Xunzhuo Liu"
                    },
                    {
                        "name": "Yue Zhu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Priya Nagpurkar"
                    },
                    {
                        "name": "Huamin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huamin Chen"
                },
                "author": "Huamin Chen",
                "arxiv_comment": "13 pages including reference, position paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26835v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26835v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25695v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25695v1",
                "updated": "2025-10-29T17:00:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    0,
                    16,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T17:00:16Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    17,
                    0,
                    16,
                    2,
                    302,
                    0
                ],
                "title": "Over 3 kV and Ultra-Low leakage Vertical (011) \\b{eta}-Ga2O3 Power\n  Diodes with Engineered Schottky Contact and High-permittivity Dielectric\n  Field Plate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over 3 kV and Ultra-Low leakage Vertical (011) \\b{eta}-Ga2O3 Power\n  Diodes with Engineered Schottky Contact and High-permittivity Dielectric\n  Field Plate"
                },
                "summary": "We report over 3 kV breakdown voltage and ultra-low leakage (011)\n\\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and\nhigh-permittivity (\\k{appa}) dielectric (ZrO2) field plate. The (011)\norientation of \\b{eta}-Ga2O3 enabled low background doping and thick drift\nlayers which are promising to support kV-class vertical \\b{eta}-Ga2O3 power\nswitches. The Schottky barrier engineering was performed with a composite Pt\ncap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse\nblocking capabilities enabled by PtOx while allowing low turn-on voltage by the\ninterfacing thin Pt layer. We also performed a systematic study using a\nco-processed Pt/(011) \\b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same\nwafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the\nfield-plate Pt/(011) \\b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage\nof 2.75 kV owing to the edge field management. Further enhancement of the\nbreakdown voltage was achieved by tunneling leakage management using composite\nPt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown\nvoltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt\n(1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011)\n\\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management\nby composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage,\nedge field reduction by high-\\k{appa} dielectric ZrO2 field plate, as well as\nthe advantageous material properties offered by (011) \\b{eta}-Ga2O3 demonstrate\na promising strategy for developing ultra-low leakage and multi-kV class\nvertical (011) \\b{eta}-Ga2O3 power devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report over 3 kV breakdown voltage and ultra-low leakage (011)\n\\b{eta}-Ga2O3 power devices utilizing Schottky barrier engineering and\nhigh-permittivity (\\k{appa}) dielectric (ZrO2) field plate. The (011)\norientation of \\b{eta}-Ga2O3 enabled low background doping and thick drift\nlayers which are promising to support kV-class vertical \\b{eta}-Ga2O3 power\nswitches. The Schottky barrier engineering was performed with a composite Pt\ncap/PtOx/Pt (1.5 nm) anode contact to take advantage of the enhanced reverse\nblocking capabilities enabled by PtOx while allowing low turn-on voltage by the\ninterfacing thin Pt layer. We also performed a systematic study using a\nco-processed Pt/(011) \\b{eta}-Ga2O3 Schottky barrier diodes (SBDs) on the same\nwafer. The bare SBDs revealed a breakdown voltage of ~1.5 kV, while the\nfield-plate Pt/(011) \\b{eta}-Ga2O3 SBDs achieved an increased breakdown voltage\nof 2.75 kV owing to the edge field management. Further enhancement of the\nbreakdown voltage was achieved by tunneling leakage management using composite\nPt cap/PtOx/Pt (1.5 nm) Schottky contacts that ultimately enabled breakdown\nvoltage of 3.7 kV for the field-plate diodes. Remarkably, the Pt cap/PtOx/Pt\n(1.5 nm) Schottky contacts maintained similar turn-on voltage as the Pt/(011)\n\\b{eta}-Ga2O3 SBDs. The combination of efficient tunneling leakage management\nby composite Pt cap/PtOx/Pt (1.5 nm) contacts with similar turn-on voltage,\nedge field reduction by high-\\k{appa} dielectric ZrO2 field plate, as well as\nthe advantageous material properties offered by (011) \\b{eta}-Ga2O3 demonstrate\na promising strategy for developing ultra-low leakage and multi-kV class\nvertical (011) \\b{eta}-Ga2O3 power devices."
                },
                "authors": [
                    {
                        "name": "Emerson J. Hollar"
                    },
                    {
                        "name": "Esmat Farzana"
                    }
                ],
                "author_detail": {
                    "name": "Esmat Farzana"
                },
                "author": "Esmat Farzana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25695v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25695v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25604v1",
                "updated": "2025-10-29T15:12:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    12,
                    35,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T15:12:35Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    15,
                    12,
                    35,
                    2,
                    302,
                    0
                ],
                "title": "Quickest Change Point Detection with Measurements over a Lossy Link",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quickest Change Point Detection with Measurements over a Lossy Link"
                },
                "summary": "Motivated by Industry 4.0 applications, we consider quickest change detection\n(QCD) of an abrupt change in a process when its measurements are transmitted by\na sensor over a lossy wireless link to a decision maker (DM). The sensor node\nsamples measurements using a Bernoulli sampling process, and places the\nmeasurement samples in the transmit queue of its transmitter. The transmitter\nuses a retransmit-until-success transmission strategy to deliver packets to the\nDM over the lossy link, in which the packet losses are modeled as a Bernoulli\nprocess, with different loss probabilities before and after the change. We pose\nthe QCD problem in the non-Bayesian setting under Lorden's framework, and\npropose a CUSUM algorithm. By defining a suitable Markov process, involving the\nDM measurements and the queue length process, we show that the problem reduces\nto QCD in a Markov process. Characterizing the information measure per\nmeasurement sample at the DM, we establish the asymptotic optimality of our\nalgorithm when the false alarm rate tends to zero. Further, when the DM\nreceives incomplete data due to channel loss, we present asymptotically optimal\nQCD algorithms by suitably modifying the CUSUM algorithm. We then explore the\nlast-come-first-served (LCFS) queuing discipline at the sensor transmit queue\nto lower detection delay in the non-asymptotic case. Next, we consider the case\nof multiple sensors, each with its own wireless transmitter queue, and show\nthat our analysis extends to the case of multiple homogeneous sensors. When the\nsensors are heterogeneous, we present a sensor scheduling algorithm that\nminimizes detection delay by balancing the trade-off between the age of the\nobservations and their information content. Numerical analysis demonstrate\ntrade-offs that can be used to optimize system design parameters in the\nnon-asymptotic regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by Industry 4.0 applications, we consider quickest change detection\n(QCD) of an abrupt change in a process when its measurements are transmitted by\na sensor over a lossy wireless link to a decision maker (DM). The sensor node\nsamples measurements using a Bernoulli sampling process, and places the\nmeasurement samples in the transmit queue of its transmitter. The transmitter\nuses a retransmit-until-success transmission strategy to deliver packets to the\nDM over the lossy link, in which the packet losses are modeled as a Bernoulli\nprocess, with different loss probabilities before and after the change. We pose\nthe QCD problem in the non-Bayesian setting under Lorden's framework, and\npropose a CUSUM algorithm. By defining a suitable Markov process, involving the\nDM measurements and the queue length process, we show that the problem reduces\nto QCD in a Markov process. Characterizing the information measure per\nmeasurement sample at the DM, we establish the asymptotic optimality of our\nalgorithm when the false alarm rate tends to zero. Further, when the DM\nreceives incomplete data due to channel loss, we present asymptotically optimal\nQCD algorithms by suitably modifying the CUSUM algorithm. We then explore the\nlast-come-first-served (LCFS) queuing discipline at the sensor transmit queue\nto lower detection delay in the non-asymptotic case. Next, we consider the case\nof multiple sensors, each with its own wireless transmitter queue, and show\nthat our analysis extends to the case of multiple homogeneous sensors. When the\nsensors are heterogeneous, we present a sensor scheduling algorithm that\nminimizes detection delay by balancing the trade-off between the age of the\nobservations and their information content. Numerical analysis demonstrate\ntrade-offs that can be used to optimize system design parameters in the\nnon-asymptotic regime."
                },
                "authors": [
                    {
                        "name": "Krishna Chaythanya KV"
                    },
                    {
                        "name": "Saqib Abbas Baba"
                    },
                    {
                        "name": "Anurag Kumar"
                    },
                    {
                        "name": "Arpan Chattopadhyay"
                    },
                    {
                        "name": "Rajesh Sundaresan"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Sundaresan"
                },
                "author": "Rajesh Sundaresan",
                "arxiv_comment": "17 pages, 6 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25590v1",
                "updated": "2025-10-29T14:58:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    58,
                    37,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T14:58:37Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    58,
                    37,
                    2,
                    302,
                    0
                ],
                "title": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RegionE: Adaptive Region-Aware Generation for Efficient Image Editing"
                },
                "summary": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, instruction-based image editing (IIE) has received widespread\nattention. In practice, IIE often modifies only specific regions of an image,\nwhile the remaining areas largely remain unchanged. Although these two types of\nregions differ significantly in generation difficulty and computational\nredundancy, existing IIE models do not account for this distinction, instead\napplying a uniform generation process across the entire image. This motivates\nus to propose RegionE, an adaptive, region-aware generation framework that\naccelerates IIE tasks without additional training. Specifically, the RegionE\nframework consists of three main components: 1) Adaptive Region Partition. We\nobserved that the trajectory of unedited regions is straight, allowing for\nmulti-step denoised predictions to be inferred in a single step. Therefore, in\nthe early denoising stages, we partition the image into edited and unedited\nregions based on the difference between the final estimated result and the\nreference image. 2) Region-Aware Generation. After distinguishing the regions,\nwe replace multi-step denoising with one-step prediction for unedited areas.\nFor edited regions, the trajectory is curved, requiring local iterative\ndenoising. To improve the efficiency and quality of local iterative generation,\nwe propose the Region-Instruction KV Cache, which reduces computational cost\nwhile incorporating global information. 3) Adaptive Velocity Decay Cache.\nObserving that adjacent timesteps in edited regions exhibit strong velocity\nsimilarity, we further propose an adaptive velocity decay cache to accelerate\nthe local denoising process. We applied RegionE to state-of-the-art IIE base\nmodels, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE\nachieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o\nconfirmed that semantic and perceptual fidelity were well preserved."
                },
                "authors": [
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Xianfang Zeng"
                    },
                    {
                        "name": "Maosen Zhao"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Peng Ye"
                    },
                    {
                        "name": "Bangyin Xiang"
                    },
                    {
                        "name": "Zhibo Wang"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Gang Yu"
                    },
                    {
                        "name": "Tao Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tao Chen"
                },
                "author": "Tao Chen",
                "arxiv_comment": "26 pages, 10 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.21710v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.21710v2",
                "updated": "2025-10-29T14:46:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    14,
                    46,
                    17,
                    2,
                    302,
                    0
                ],
                "published": "2025-06-26T18:51:04Z",
                "published_parsed": [
                    2025,
                    6,
                    26,
                    18,
                    51,
                    4,
                    3,
                    177,
                    0
                ],
                "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual\n  Question Answering"
                },
                "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and three types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute."
                },
                "authors": [
                    {
                        "name": "Liangyu Zhong"
                    },
                    {
                        "name": "Fabio Rosenthal"
                    },
                    {
                        "name": "Joachim Sicking"
                    },
                    {
                        "name": "Fabian Hger"
                    },
                    {
                        "name": "Thorsten Bagdonat"
                    },
                    {
                        "name": "Hanno Gottschalk"
                    },
                    {
                        "name": "Leo Schwinn"
                    }
                ],
                "author_detail": {
                    "name": "Leo Schwinn"
                },
                "author": "Leo Schwinn",
                "arxiv_comment": "Accepted by NeurIPS 2025 - main track. Project page:\n  https://focus-mllm-vqa.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.21710v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.21710v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25412v1",
                "updated": "2025-10-29T11:29:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T11:29:03Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    11,
                    29,
                    3,
                    2,
                    302,
                    0
                ],
                "title": "Serve Programs, Not Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serve Programs, Not Prompts"
                },
                "summary": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current large language model (LLM) serving systems, primarily designed for\ntext completion, are neither efficient nor adaptable for increasingly complex\nLLM applications due to their inflexible design. We propose a new LLM serving\nsystem architecture that serves programs instead of prompts to address this\nproblem. These programs, called LLM Inference Programs (LIPs), allow users to\ncustomize token prediction and KV cache management at runtime and to offload\nparts of their application logic, such as tool execution, to the server. We\ndescribe an example of this architecture through a system named Symphony, which\nfunctions as an operating system for LIPs. Symphony exposes LLM model\ncomputations via system calls and virtualizes KV cache with a dedicated file\nsystem, while ensuring GPU efficiency with a two-level process scheduling\nscheme. Symphony has the potential to open the door to a more efficient and\nextensible ecosystem for LLM applications."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3713082.3730398",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3713082.3730398",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.25412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotOS 2025. Follow-up implementation work (SOSP 2025) is available at\n  arXiv:2510.24051",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25152v1",
                "updated": "2025-10-29T04:09:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    9,
                    50,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T04:09:50Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    4,
                    9,
                    50,
                    2,
                    302,
                    0
                ],
                "title": "Off-Centered WoS-Type Solvers with Statistical Weighting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Off-Centered WoS-Type Solvers with Statistical Weighting"
                },
                "summary": "Stochastic PDE solvers have emerged as a powerful alternative to traditional\ndiscretization-based methods for solving partial differential equations (PDEs),\nespecially in geometry processing and graphics. While off-centered estimators\nenhance sample reuse in WoS-type Monte Carlo solvers, they introduce\ncorrelation artifacts and bias when Green's functions are approximated. In this\npaper, we propose a statistically weighted off-centered WoS-type estimator that\nleverages local similarity filtering to selectively combine samples across\nneighboring evaluation points. Our method balances bias and variance through a\nprincipled weighting strategy that suppresses unreliable estimators. We\ndemonstrate our approach's effectiveness on various PDEs,including screened\nPoisson equations and boundary conditions, achieving consistent improvements\nover existing solvers such as vanilla Walk on Spheres, mean value caching, and\nboundary value caching. Our method also naturally extends to gradient field\nestimation and mixed boundary problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic PDE solvers have emerged as a powerful alternative to traditional\ndiscretization-based methods for solving partial differential equations (PDEs),\nespecially in geometry processing and graphics. While off-centered estimators\nenhance sample reuse in WoS-type Monte Carlo solvers, they introduce\ncorrelation artifacts and bias when Green's functions are approximated. In this\npaper, we propose a statistically weighted off-centered WoS-type estimator that\nleverages local similarity filtering to selectively combine samples across\nneighboring evaluation points. Our method balances bias and variance through a\nprincipled weighting strategy that suppresses unreliable estimators. We\ndemonstrate our approach's effectiveness on various PDEs,including screened\nPoisson equations and boundary conditions, achieving consistent improvements\nover existing solvers such as vanilla Walk on Spheres, mean value caching, and\nboundary value caching. Our method also naturally extends to gradient field\nestimation and mixed boundary problems."
                },
                "authors": [
                    {
                        "name": "Anchang Bao"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Enya Shen"
                    },
                    {
                        "name": "Jianmin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianmin Wang"
                },
                "author": "Jianmin Wang",
                "arxiv_comment": "SIGGRAPH Asia 2025 conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25122v1",
                "updated": "2025-10-29T03:00:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    29,
                    3,
                    0,
                    36,
                    2,
                    302,
                    0
                ],
                "published": "2025-10-29T03:00:36Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    3,
                    0,
                    36,
                    2,
                    302,
                    0
                ],
                "title": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized\n  Generalist Robotic Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NanoVLA: Routing Decoupled Vision-Language Understanding for Nano-sized\n  Generalist Robotic Policies"
                },
                "summary": "Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by integrating vision-language models (VLMs), and action decoders\ninto a unified architecture. However, their deployment on resource-constrained\nedge devices, such as mobile robots or embedded systems (e.g., Jetson Orin\nNano), remains challenging due to high computational demands, especially in\nreal-world scenarios where power, latency, and computational resources are\ncritical. To close this gap, we introduce Nano-scale Vision-Language Action\n(NanoVLA), a family of lightweight VLA architectures that achieve high\nperformance with minimal resources. Our core innovations include: (1)\nvision-language decoupling that moves conventional early vision and language\ninputs fusion in VLM to late stage, achieving better performance while enabling\ncaching and reduce inference overhead and latency; (2) long-short action\nchunking to ensure smooth, coherent multi-step planning without sacrificing\nreal-time responsiveness; (3) dynamic routing that adaptively assigns\nlightweight or heavy backbones based on task complexity, further optimizing\ninference efficiency. Experimental results on several benchmarks, as well as\nreal-world deployments, demonstrate that NanoVLA achieves up to 52x faster\ninference on edge devices compared to previous state-of-the-art VLA models,\nwith 98% less parameters while maintaining or surpassing their task accuracy\nand generalization. Ablation studies confirm that our decoupling strategy\npreserves cross-task transferability, and the routing module enhances\ncost-performance trade-offs, enabling practical, high-precision robotic\nmanipulation on resource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action (VLA) models have significantly advanced robotic\nmanipulation by integrating vision-language models (VLMs), and action decoders\ninto a unified architecture. However, their deployment on resource-constrained\nedge devices, such as mobile robots or embedded systems (e.g., Jetson Orin\nNano), remains challenging due to high computational demands, especially in\nreal-world scenarios where power, latency, and computational resources are\ncritical. To close this gap, we introduce Nano-scale Vision-Language Action\n(NanoVLA), a family of lightweight VLA architectures that achieve high\nperformance with minimal resources. Our core innovations include: (1)\nvision-language decoupling that moves conventional early vision and language\ninputs fusion in VLM to late stage, achieving better performance while enabling\ncaching and reduce inference overhead and latency; (2) long-short action\nchunking to ensure smooth, coherent multi-step planning without sacrificing\nreal-time responsiveness; (3) dynamic routing that adaptively assigns\nlightweight or heavy backbones based on task complexity, further optimizing\ninference efficiency. Experimental results on several benchmarks, as well as\nreal-world deployments, demonstrate that NanoVLA achieves up to 52x faster\ninference on edge devices compared to previous state-of-the-art VLA models,\nwith 98% less parameters while maintaining or surpassing their task accuracy\nand generalization. Ablation studies confirm that our decoupling strategy\npreserves cross-task transferability, and the routing module enhances\ncost-performance trade-offs, enabling practical, high-precision robotic\nmanipulation on resource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Jiahong Chen"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Chuwei Cai"
                    },
                    {
                        "name": "Jinghui Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jinghui Lu"
                },
                "author": "Jinghui Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24824v1",
                "updated": "2025-10-28T15:35:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    35,
                    50,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T15:35:50Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    15,
                    35,
                    50,
                    1,
                    301,
                    0
                ],
                "title": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Loop Transformer for Efficient Test-Time Computation Scaling"
                },
                "summary": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are powerful but often too slow and costly for\nreal-world use during inference. Looped transformers save on parameters by\nreusing the same weights for multiple computational steps, or \"loops.\" However,\nthis approach has a major flaw: the loops run one after another, causing\ninference latency and memory requirements to increase with each added loop.\nThis makes them impractical for fast applications. To solve this problem, we\nintroduce the Parallel Loop Transformer (PLT). PLT is a new architecture that\ndelivers the performance benefits of a deep, looped model but with the low\nlatency of a standard, non-looped model. PLT works using two key techniques.\nFirst, Cross-Loop Parallelism (CLP) breaks the sequential dependency by\ncomputing different loops for different tokens at the same time, all within a\nsingle pass. Second, to prevent memory costs from growing, we use an Efficient\nRepresentation Enhancement strategy. This method shares the memory (KV cache)\nfrom the first loop with all other loops. It then uses a Gated Sliding-Window\nAttention (G-SWA) to combine this shared global information with local\ninformation, maintaining high accuracy. Our experiments show that PLT achieves\nthe high accuracy of a traditional looped model but with almost no extra\nlatency or memory cost compared to a standard transformer."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Xiang Luo"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Qifan Yu"
                    },
                    {
                        "name": "Fan Xia"
                    },
                    {
                        "name": "Tianqi Zhang"
                    },
                    {
                        "name": "Hongrui Zhan"
                    },
                    {
                        "name": "Zheng Zhong"
                    },
                    {
                        "name": "Xun Zhou"
                    },
                    {
                        "name": "Siyuan Qiao"
                    },
                    {
                        "name": "Xingyan Bin"
                    }
                ],
                "author_detail": {
                    "name": "Xingyan Bin"
                },
                "author": "Xingyan Bin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24359v1",
                "updated": "2025-10-28T12:28:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    28,
                    2,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T12:28:02Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    12,
                    28,
                    2,
                    1,
                    301,
                    0
                ],
                "title": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine"
                },
                "summary": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence in medicine is built to serve the average patient. By\nminimizing error across large datasets, most systems deliver strong aggregate\naccuracy yet falter at the margins: patients with rare variants,\nmultimorbidity, or underrepresented demographics. This average patient fallacy\nerodes both equity and trust. We propose a different design: a multi-agent\necosystem for N-of-1 decision support. In this environment, agents clustered by\norgan systems, patient populations, and analytic modalities draw on a shared\nlibrary of models and evidence synthesis tools. Their results converge in a\ncoordination layer that weighs reliability, uncertainty, and data density\nbefore presenting the clinician with a decision-support packet: risk estimates\nbounded by confidence ranges, outlier flags, and linked evidence. Validation\nshifts from population averages to individual reliability, measured by error in\nlow-density regions, calibration in the small, and risk--coverage trade-offs.\nAnticipated challenges include computational demands, automation bias, and\nregulatory fit, addressed through caching strategies, consensus checks, and\nadaptive trial frameworks. By moving from monolithic models to orchestrated\nintelligence, this approach seeks to align medical AI with the first principle\nof medicine: care that is transparent, equitable, and centered on the\nindividual."
                },
                "authors": [
                    {
                        "name": "Pedram Fard"
                    },
                    {
                        "name": "Alaleh Azhir"
                    },
                    {
                        "name": "Neguine Rezaii"
                    },
                    {
                        "name": "Jiazi Tian"
                    },
                    {
                        "name": "Hossein Estiri"
                    }
                ],
                "author_detail": {
                    "name": "Hossein Estiri"
                },
                "author": "Hossein Estiri",
                "arxiv_comment": "This study has been supported by grants from the National Institutes\n  of Health: The National Institute on Aging R01AG074372 and The National\n  Institute of Allergy and Infectious Diseases R01AI165535",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24273v1",
                "updated": "2025-10-28T10:32:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T10:32:52Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    10,
                    32,
                    52,
                    1,
                    301,
                    0
                ],
                "title": "SALS: Sparse Attention in Latent Space for KV cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALS: Sparse Attention in Latent Space for KV cache Compression"
                },
                "summary": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models capable of handling extended contexts are in high\ndemand, yet their inference remains challenging due to substantial Key-Value\ncache size and high memory bandwidth requirements. Previous research has\ndemonstrated that KV cache exhibits low-rank characteristics within the hidden\ndimension, suggesting the potential for effective compression. However, due to\nthe widely adopted Rotary Position Embedding mechanism in modern LLMs, naive\nlow-rank compression suffers severe accuracy degradation or creates a new speed\nbottleneck, as the low-rank cache must first be reconstructed in order to apply\nRoPE. In this paper, we introduce two key insights: first, the application of\nRoPE to the key vectors increases their variance, which in turn results in a\nhigher rank; second, after the key vectors are transformed into the latent\nspace, they largely maintain their representation across most layers. Based on\nthese insights, we propose the Sparse Attention in Latent Space framework. SALS\nprojects the KV cache into a compact latent space via low-rank projection, and\nperforms sparse token selection using RoPE-free query-key interactions in this\nspace. By reconstructing only a small subset of important tokens, it avoids the\noverhead of full KV cache reconstruction. We comprehensively evaluate SALS on\nvarious tasks using two large-scale models: LLaMA2-7b-chat and Mistral-7b, and\nadditionally verify its scalability on the RULER-128k benchmark with\nLLaMA3.1-8B-Instruct. Experimental results demonstrate that SALS achieves SOTA\nperformance by maintaining competitive accuracy. Under different settings, SALS\nachieves 6.4-fold KV cache compression and 5.7-fold speed-up in the attention\noperator compared to FlashAttention2 on the 4K sequence. For the end-to-end\nthroughput performance, we achieves 1.4-fold and 4.5-fold improvement compared\nto GPT-fast on 4k and 32K sequences, respectively."
                },
                "authors": [
                    {
                        "name": "Junlin Mu"
                    },
                    {
                        "name": "Hantao Huang"
                    },
                    {
                        "name": "Jihang Zhang"
                    },
                    {
                        "name": "Minghui Yu"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Yidong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yidong Li"
                },
                "author": "Yidong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24051v1",
                "updated": "2025-10-28T04:17:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    17,
                    55,
                    1,
                    301,
                    0
                ],
                "published": "2025-10-28T04:17:55Z",
                "published_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    17,
                    55,
                    1,
                    301,
                    0
                ],
                "title": "Pie: A Programmable Serving System for Emerging LLM Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pie: A Programmable Serving System for Emerging LLM Applications"
                },
                "summary": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging large language model (LLM) applications involve diverse reasoning\nstrategies and agentic workflows, straining the capabilities of existing\nserving systems built on a monolithic token generation loop. This paper\nintroduces Pie, a programmable LLM serving system designed for flexibility and\nefficiency. Pie decomposes the traditional generation loop into fine-grained\nservice handlers exposed via an API and delegates control of the generation\nprocess to user-provided programs, called inferlets. This enables applications\nto implement new KV cache strategies, bespoke generation logic, and seamlessly\nintegrate computation and I/O-entirely within the application, without\nrequiring modifications to the serving system. Pie executes inferlets using\nWebAssembly, benefiting from its lightweight sandboxing. Our evaluation shows\nPie matches state-of-the-art performance on standard tasks (3-12% latency\noverhead) while significantly improving latency and throughput (1.3x-3.4x\nhigher) on agentic workflows by enabling application-specific optimizations."
                },
                "authors": [
                    {
                        "name": "In Gim"
                    },
                    {
                        "name": "Zhiyao Ma"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Lin Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Lin Zhong"
                },
                "author": "Lin Zhong",
                "arxiv_doi": "10.1145/3731569.3764814",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731569.3764814",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.24051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SOSP 2025. Source code available at\n  https://github.com/pie-project/pie",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01068v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01068v4",
                "updated": "2025-10-28T04:00:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    28,
                    4,
                    0,
                    18,
                    1,
                    301,
                    0
                ],
                "published": "2025-02-03T05:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    25,
                    9,
                    0,
                    34,
                    0
                ],
                "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastKV: KV Cache Compression for Fast Long-Context Processing with\n  Token-Selective Propagation"
                },
                "summary": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) excel at handling long-context sequences,\nthey require substantial prefill computation and key-value (KV) cache, which\ncan heavily burden computational efficiency and memory usage in both prefill\nand decoding stages. Recent works that compress KV caches with prefill\nacceleration reduce this cost but inadvertently tie the prefill compute\nreduction to the decoding KV budget. This coupling arises from overlooking the\nlayer-dependent variation of critical context, often leading to accuracy\ndegradation. To address this issue, we introduce FastKV, a KV cache compression\nframework designed to reduce latency in both prefill and decoding by leveraging\nthe stabilization of token importance in later layers. FastKV performs\nfull-context computation until a Token-Selective Propagation (TSP) layer, which\nforwards only the most informative tokens to subsequent layers. From these\npropagated tokens, FastKV independently selects salient KV entries for caching,\nthereby decoupling KV budget from the prefill compute reduction based on the\nTSP decision. This independent control of the TSP rate and KV retention rate\nenables flexible optimization of efficiency and accuracy. Experimental results\nshow that FastKV achieves speedups of up to 1.82$\\times$ in prefill and\n2.87$\\times$ in decoding compared to the full-context baseline, while matching\nthe accuracy of the baselines that only accelerate the decoding stage. Our code\nis available at https://github.com/dongwonjo/FastKV."
                },
                "authors": [
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01068v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01068v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14969v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14969v2",
                "updated": "2025-10-27T21:48:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    21,
                    48,
                    48,
                    0,
                    300,
                    0
                ],
                "published": "2025-05-20T23:12:16Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    23,
                    12,
                    16,
                    1,
                    140,
                    0
                ],
                "title": "STree: Speculative Tree Decoding for Hybrid State-Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STree: Speculative Tree Decoding for Hybrid State-Space Models"
                },
                "summary": "Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a technique to leverage hardware concurrency in order\nto enable multiple steps of token generation in a single forward pass, thus\nimproving the efficiency of large-scale autoregressive (AR) Transformer models.\nState-space models (SSMs) are already more efficient than AR Transformers,\nsince their state summarizes all past data with no need to cache or re-process\ntokens in the sliding window context. However, their state can also comprise\nthousands of tokens; so, speculative decoding has recently been extended to\nSSMs. Existing approaches, however, do not leverage the tree-based verification\nmethods, since current SSMs lack the means to compute a token tree efficiently.\nWe propose the first scalable algorithm to perform tree-based speculative\ndecoding in state-space models (SSMs) and hybrid architectures of SSMs and\nTransformer layers. We exploit the structure of accumulated state transition\nmatrices to facilitate tree-based speculative decoding with minimal overhead\nrelative to current SSM implementations. Along with the algorithm, we describe\na hardware-aware implementation that improves naive application of AR\nTransformer tree-based speculative decoding methods to SSMs. Furthermore, we\noutperform vanilla speculative decoding with SSMs even with a baseline drafting\nmodel and tree structure on three different benchmarks, opening up\nopportunities for further speed up with SSM and hybrid model inference. Code\ncan be found at: https://github.com/wyc1997/stree."
                },
                "authors": [
                    {
                        "name": "Yangchao Wu"
                    },
                    {
                        "name": "Zongyue Qin"
                    },
                    {
                        "name": "Alex Wong"
                    },
                    {
                        "name": "Stefano Soatto"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Soatto"
                },
                "author": "Stefano Soatto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14969v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14969v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12362v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12362v2",
                "updated": "2025-10-27T17:31:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    17,
                    31,
                    15,
                    0,
                    300,
                    0
                ],
                "published": "2024-04-18T17:45:19Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    17,
                    45,
                    19,
                    3,
                    109,
                    0
                ],
                "title": "KV-weights are all you need for skipless transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-weights are all you need for skipless transformers"
                },
                "summary": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "He and Hofmann (arXiv:2311.01906) detailed a skipless transformer without the\nV and P (post-attention projection) linear layers, which reduces the total\nnumber of weights. However, this scheme is only applicable to MHA (multi-head\nattention), but not for MQA (multi-query attention) and GQA (grouped-query\nattention). The latter schemes are used by many popular LLMs such as Llama 2,\nMistral, Mixtral, PaLM, and Gemma. Therefore, this micro-paper proposes\nmathematically equivalent versions that are suitable for MQA and GQA. For\nexample, removing Q and P from a skipless version of Mistral-7B would remove\n15% of its weights (and thus reduce its compute and memory complexity). Watch\nour explainer video https://youtu.be/Tx_lMpphd2g and see\nhttps://github.com/OpenMachine-ai/transformer-tricks for code and more\ntransformer tricks."
                },
                "authors": [
                    {
                        "name": "Nils Graef"
                    }
                ],
                "author_detail": {
                    "name": "Nils Graef"
                },
                "author": "Nils Graef",
                "arxiv_comment": "6 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12362v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12362v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05530v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05530v3",
                "updated": "2025-10-27T16:20:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    16,
                    20,
                    28,
                    0,
                    300,
                    0
                ],
                "published": "2025-03-07T15:54:04Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    15,
                    54,
                    4,
                    4,
                    66,
                    0
                ],
                "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) improves the reliability of large\nlanguage model (LLM) answers by integrating external knowledge. However, RAG\nincreases the end-to-end inference time since looking for relevant documents\nfrom large vector databases is computationally expensive. To address this, we\nintroduce Proximity, an approximate key-value cache that optimizes the RAG\nworkflow by leveraging similarities in user queries. Instead of treating each\nquery independently, Proximity reuses previously retrieved documents when\nsimilar queries appear, substantially reducing the reliance on expensive vector\ndatabase lookups. To efficiently scale, Proximity employs a locality-sensitive\nhashing (LSH) scheme that enables fast cache lookups while preserving retrieval\naccuracy. We evaluate Proximity using the MMLU and MedRAG question-answering\nbenchmarks. Our experiments demonstrate that Proximity with our LSH scheme and\na realistically-skewed MedRAG workload reduces database calls by 77.2% while\nmaintaining database recall and test accuracy. We experiment with different\nsimilarity tolerances and cache capacities, and show that the time spent within\nthe Proximity cache remains low and constant (4.8 microseconds) even as the\ncache grows substantially in size. Our results demonstrate that approximate\ncaching is a practical and effective strategy for optimizing RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Shai Bergman"
                    },
                    {
                        "name": "Anne-Marie Kermarrec"
                    },
                    {
                        "name": "Diana Petrescu"
                    },
                    {
                        "name": "Rafael Pires"
                    },
                    {
                        "name": "Mathis Randl"
                    },
                    {
                        "name": "Martijn de Vos"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_doi": "10.1145/3721462.3770776",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721462.3770776",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.05530v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05530v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at Middleware '25",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08343v2",
                "updated": "2025-10-27T14:59:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    14,
                    59,
                    46,
                    0,
                    300,
                    0
                ],
                "published": "2025-08-11T10:47:35Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    10,
                    47,
                    35,
                    0,
                    223,
                    0
                ],
                "title": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Data-driven ML Approach for Maximizing Performance in LLM-Adapter\n  Serving"
                },
                "summary": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid adoption of Large Language Models (LLMs), LLM-adapters have\nbecome increasingly common, providing lightweight specialization of large-scale\nmodels. Serving hundreds or thousands of these adapters on a single GPU allows\nrequest aggregation, increasing throughput, but may also cause request\nstarvation if GPU memory limits are exceeded. To address this issue, this study\nfocuses on determining the joint configuration of concurrent and parallel\nadapters that maximizes GPU throughput without inducing starvation, given\nheterogeneous adapter and traffic properties. We propose a data-driven ML\napproach leveraging interpretable models to tackle this caching problem and\nintroduce the first Digital Twin capable of reproducing an LLM-adapter serving\nsystem, enabling efficient training data generation. Experiments with the vLLM\nframework and LoRA adapters show that the Digital Twin reproduces throughput\nwithin 5.1% of real results, while the ML approach predicts optimal numbers of\nconcurrent and parallel adapters with an error of at most 7.2% under\nheterogeneous, real-world workloads."
                },
                "authors": [
                    {
                        "name": "Ferran Agullo"
                    },
                    {
                        "name": "Joan Oliveras"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Alberto Gutierrez-Torre"
                    },
                    {
                        "name": "Olivier Tardieu"
                    },
                    {
                        "name": "Alaa Youssef"
                    },
                    {
                        "name": "Jordi Torres"
                    },
                    {
                        "name": "Josep Ll. Berral"
                    }
                ],
                "author_detail": {
                    "name": "Josep Ll. Berral"
                },
                "author": "Josep Ll. Berral",
                "arxiv_comment": "Accepted in a computer science workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01488v2",
                "updated": "2025-10-27T11:55:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    27,
                    11,
                    55,
                    7,
                    0,
                    300,
                    0
                ],
                "published": "2025-08-02T21:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "title": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective"
                },
                "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Alain Riou"
                    },
                    {
                        "name": "Bernardo Torres"
                    },
                    {
                        "name": "Ben Hayes"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Gatan Hadjeres"
                    },
                    {
                        "name": "Gal Richard"
                    },
                    {
                        "name": "Geoffroy Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Geoffroy Peeters"
                },
                "author": "Geoffroy Peeters",
                "arxiv_doi": "10.5334/TISMIR.251",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5334/TISMIR.251",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Transactions of the International Society for Music Information\n  Retrieval, 8(1): 334-352 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22876v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22876v1",
                "updated": "2025-10-26T23:59:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T23:59:23Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    23,
                    59,
                    23,
                    6,
                    299,
                    0
                ],
                "title": "Batch Speculative Decoding Done Right",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch Speculative Decoding Done Right"
                },
                "summary": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding speeds up LLM inference by using a small draft model to\npropose multiple tokens that a target model verifies in parallel. Extending\nthis idea to batches is essential for production serving, but it introduces the\nragged tensor problem: sequences in the same batch accept different numbers of\ndraft tokens, breaking right-alignment and corrupting position IDs, attention\nmasks, and KV-cache state. We show that several existing batch implementations\nviolate output equivalence-the fundamental requirement that speculative\ndecoding must produce identical token sequences to standard autoregressive\ngeneration. These violations occur precisely due to improper handling of the\nragged tensor problem. In response, we (1) characterize the synchronization\nrequirements that guarantee correctness, (2) present a correctness-first batch\nspeculative decoding EQSPEC that exposes realignment as consuming 40% of\noverhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences\nand dynamically forms same-length groups, to reduce the realignment overhead\nwhile preserving per-sequence speculative speedups. On the SpecBench dataset,\nacross Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our\napproach achieves up to 3$\\times$ throughput improvement at batch size 8\ncompared to batch size 1, with efficient scaling through batch size 8, while\nmaintaining 95% output equivalence. Our method requires no custom kernels and\nintegrates cleanly with existing inference stacks. Our code is available at\nhttps://github.com/eBay/spec_dec."
                },
                "authors": [
                    {
                        "name": "Ranran Haoran Zhang"
                    },
                    {
                        "name": "Soumik Dey"
                    },
                    {
                        "name": "Ashirbad Mishra"
                    },
                    {
                        "name": "Hansi Wu"
                    },
                    {
                        "name": "Binbin Li"
                    },
                    {
                        "name": "Rui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Zhang"
                },
                "author": "Rui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22876v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22876v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22765v1",
                "updated": "2025-10-26T17:28:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    17,
                    28,
                    5,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T17:28:05Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    17,
                    28,
                    5,
                    6,
                    299,
                    0
                ],
                "title": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jarvis: Towards Personalized AI Assistant via Personal KV-Cache\n  Retrieval"
                },
                "summary": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Vision-language models (VLMs) enables open-ended\nperception and reasoning. Recent works have started to investigate how to adapt\ngeneral-purpose VLMs into personalized assistants. Even commercial models such\nas ChatGPT now support model personalization by incorporating user-specific\ninformation. However, existing methods either learn a set of concept tokens or\ntrain a VLM to utilize user-specific information. However, both pipelines\nstruggle to generate accurate answers as personalized assistants. We introduce\nJarvis, an innovative framework for a personalized AI assistant through\npersonal KV-Cache retrieval, which stores user-specific information in the\nKV-Caches of both textual and visual tokens. The textual tokens are created by\nsummarizing user information into metadata, while the visual tokens are\nproduced by extracting distinct image patches from the user's images. When\nanswering a question, Jarvis first retrieves related KV-Caches from personal\nstorage and uses them to ensure accuracy in responses. We also introduce a\nfine-grained benchmark built with the same distinct image patch mining\npipeline, emphasizing accurate question answering based on fine-grained\nuser-specific information. Jarvis is capable of providing more accurate\nresponses, particularly when they depend on specific local details. Jarvis\nachieves state-of-the-art results in both visual question answering and\ntext-only tasks across multiple datasets, indicating a practical path toward\npersonalized AI assistants. The code and dataset will be released."
                },
                "authors": [
                    {
                        "name": "Binxiao Xu"
                    },
                    {
                        "name": "Junyu Feng"
                    },
                    {
                        "name": "Ruichuan An"
                    },
                    {
                        "name": "Yulin Luo"
                    },
                    {
                        "name": "Shilin Yan"
                    },
                    {
                        "name": "Hao Liang"
                    },
                    {
                        "name": "Ming Lu"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v3",
                "updated": "2025-10-26T13:31:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    13,
                    31,
                    41,
                    6,
                    299,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year and has been open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year and has been open-sourced."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22556v1",
                "updated": "2025-10-26T07:17:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    7,
                    17,
                    10,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T07:17:10Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    7,
                    17,
                    10,
                    6,
                    299,
                    0
                ],
                "title": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression\n  Block Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SABlock: Semantic-Aware KV Cache Eviction with Adaptive Compression\n  Block Size"
                },
                "summary": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory footprint of the Key-Value (KV) cache poses a severe\nscalability bottleneck for long-context Large Language Model (LLM) inference.\nWhile KV cache eviction has emerged as an effective solution by discarding less\ncritical tokens, existing token-, block-, and sentence-level compression\nmethods struggle to balance semantic coherence and memory efficiency. To this\nend, we introduce SABlock, a \\underline{s}emantic-aware KV cache eviction\nframework with \\underline{a}daptive \\underline{block} sizes. Specifically,\nSABlock first performs semantic segmentation to align compression boundaries\nwith linguistic structures, then applies segment-guided token scoring to refine\ntoken importance estimation. Finally, for each segment, a budget-driven search\nstrategy adaptively determines the optimal block size that preserves semantic\nintegrity while improving compression efficiency under a given cache budget.\nExtensive experiments on long-context benchmarks demonstrate that SABlock\nconsistently outperforms state-of-the-art baselines under the same memory\nbudgets. For instance, on Needle-in-a-Haystack (NIAH), SABlock achieves 99.9%\nretrieval accuracy with only 96 KV entries, nearly matching the performance of\nthe full-cache baseline that retains up to 8K entries. Under a fixed cache\nbudget of 1,024, SABlock further reduces peak memory usage by 46.28% and\nachieves up to 9.5x faster decoding on a 128K context length."
                },
                "authors": [
                    {
                        "name": "Jinhan Chen"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Xianjun Gao"
                    },
                    {
                        "name": "Shilong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shilong Wang"
                },
                "author": "Shilong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v3",
                "updated": "2025-10-26T04:25:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    4,
                    25,
                    10,
                    6,
                    299,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through static modeling of attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the temporal patterns in attention scores, resulting in\na noticeable degradation in LLM performance. To address this challenge, we\npropose AttentionPredictor, which is the first learning-based method to\ndirectly predict attention patterns for KV cache compression and critical token\nidentification. Specifically, AttentionPredictor learns a lightweight, unified\nconvolution model to dynamically capture spatiotemporal patterns and predict\nthe next-token attention scores. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score and shares the unified\nprediction model, which consumes negligible memory, among all transformer\nlayers. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n13$\\times$ KV cache compression and 5.6$\\times$ speedup in a cache offloading\nscenario with comparable LLM performance, significantly outperforming the\nstate-of-the-arts. The code is available at\nhttps://github.com/MIRALab-USTC/LLM-AttentionPredictor."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23657v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23657v1",
                "updated": "2025-10-26T01:25:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    1,
                    25,
                    24,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T01:25:24Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    1,
                    25,
                    24,
                    6,
                    299,
                    0
                ],
                "title": "A machine learning framework integrating seed traits and plasma\n  parameters for predicting germination uplift in crops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A machine learning framework integrating seed traits and plasma\n  parameters for predicting germination uplift in crops"
                },
                "summary": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold plasma (CP) is an eco-friendly method to enhance seed germination, yet\noutcomes remain difficult to predict due to complex seed--plasma--environment\ninteractions. This study introduces the first machine learning framework to\nforecast germination uplift in soybean, barley, sunflower, radish, and tomato\nunder dielectric barrier discharge (DBD) plasma. Among the models tested (GB,\nXGB, ET, and hybrids), Extra Trees (ET) performed best (R\\textsuperscript{2} =\n0.919; RMSE = 3.21; MAE = 2.62), improving to R\\textsuperscript{2} = 0.925\nafter feature reduction. Engineering analysis revealed a hormetic response:\nnegligible effects at $<$7 kV or $<$200 s, maximum germination at 7--15 kV for\n200--500 s, and reduced germination beyond 20 kV or prolonged exposures.\nDischarge power was also a dominant factor, with germination rate maximizing at\n$\\geq$100 W with low exposure time. Species and cultivar-level predictions\nshowed radish (MAE = 1.46) and soybean (MAE = 2.05) were modeled with high\nconsistency, while sunflower remained slightly higher variable (MAE = 3.80).\nAmong cultivars, Williams (MAE = 1.23) and Sari (1.33) were well predicted,\nwhile Arian (2.86) and Ny\\'{\\i}rs\\'{e}gi fekete (3.74) were comparatively\npoorly captured. This framework was also embedded into MLflow, providing a\ndecision-support tool for optimizing CP seed germination in precision\nagriculture."
                },
                "authors": [
                    {
                        "name": "Saklain Niam"
                    },
                    {
                        "name": "Tashfiqur Rahman"
                    },
                    {
                        "name": "Md. Amjad Patwary"
                    },
                    {
                        "name": "Mukarram Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Mukarram Hossain"
                },
                "author": "Mukarram Hossain",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23657v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23657v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22467v1",
                "updated": "2025-10-26T00:50:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    26,
                    0,
                    50,
                    12,
                    6,
                    299,
                    0
                ],
                "published": "2025-10-26T00:50:12Z",
                "published_parsed": [
                    2025,
                    10,
                    26,
                    0,
                    50,
                    12,
                    6,
                    299,
                    0
                ],
                "title": "Backward-Friendly Optimization: Training Large Language Models with\n  Approximate Gradients under Memory Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backward-Friendly Optimization: Training Large Language Models with\n  Approximate Gradients under Memory Constraints"
                },
                "summary": "Full fine-tuning of Large Language Models (LLMs) is notoriously\nmemory-intensive, primarily because conventional optimizers such as SGD or Adam\nassume access to exact gradients derived from cached activations. Existing\nsolutions either alter the model architecture (e.g., reversible networks) or\ntrade memory for computation (e.g., activation checkpointing), but the\noptimizer itself remains untouched. In this work, we introduce GradLite, a\nbackward-friendly optimizer that relaxes the requirement of exact gradients,\nenabling efficient training even when intermediate activations are aggressively\ndiscarded or approximated. GradLite leverages two key techniques: (i) low-rank\nJacobian approximation, which reduces the dimensionality of backpropagated\nerror signals, and (ii) error-feedback correction, which accumulates and\ncompensates approximation errors across iterations to preserve convergence\nguarantees. We provide a theoretical analysis showing that GradLite maintains\nunbiased gradient estimates with bounded variance, ensuring convergence rates\ncomparable to Adam. Empirically, GradLite reduces optimizer-state and\nactivation memory consumption by up to 50\\% without architectural changes, and\nachieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),\nmultilingual, and dialogue benchmarks compared to checkpointing and\noptimizer-centric baselines (LoMo, GaLore).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full fine-tuning of Large Language Models (LLMs) is notoriously\nmemory-intensive, primarily because conventional optimizers such as SGD or Adam\nassume access to exact gradients derived from cached activations. Existing\nsolutions either alter the model architecture (e.g., reversible networks) or\ntrade memory for computation (e.g., activation checkpointing), but the\noptimizer itself remains untouched. In this work, we introduce GradLite, a\nbackward-friendly optimizer that relaxes the requirement of exact gradients,\nenabling efficient training even when intermediate activations are aggressively\ndiscarded or approximated. GradLite leverages two key techniques: (i) low-rank\nJacobian approximation, which reduces the dimensionality of backpropagated\nerror signals, and (ii) error-feedback correction, which accumulates and\ncompensates approximation errors across iterations to preserve convergence\nguarantees. We provide a theoretical analysis showing that GradLite maintains\nunbiased gradient estimates with bounded variance, ensuring convergence rates\ncomparable to Adam. Empirically, GradLite reduces optimizer-state and\nactivation memory consumption by up to 50\\% without architectural changes, and\nachieves on-par or superior downstream performance on reasoning (MMLU, GSM8K),\nmultilingual, and dialogue benchmarks compared to checkpointing and\noptimizer-centric baselines (LoMo, GaLore)."
                },
                "authors": [
                    {
                        "name": "Jing Yang"
                    },
                    {
                        "name": "Kaitong Cai"
                    },
                    {
                        "name": "Yijia Fan"
                    },
                    {
                        "name": "Yufeng Yang"
                    },
                    {
                        "name": "Keze Wang"
                    }
                ],
                "author_detail": {
                    "name": "Keze Wang"
                },
                "author": "Keze Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v3",
                "updated": "2025-10-25T14:12:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    14,
                    12,
                    56,
                    5,
                    298,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to further decrease memory footprint.\nAcross model scales ranging from 135M to 1.7B parameters, MoR forms a new\nPareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to further decrease memory footprint.\nAcross model scales ranging from 135M to 1.7B parameters, MoR forms a new\nPareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "38 pages, 9 figures, 17 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23649v1",
                "updated": "2025-10-25T11:43:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "published": "2025-10-25T11:43:27Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "title": "Efficient Low Rank Attention for Long-Context Inference in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Low Rank Attention for Long-Context Inference in Large\n  Language Models"
                },
                "summary": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the length of input text grows, the key-value (KV) cache in LLMs imposes\nprohibitive GPU memory costs and limits long-context inference on resource\nconstrained devices. Existing approaches, such as KV quantization and pruning,\nreduce memory usage but suffer from numerical precision loss or suboptimal\nretention of key-value pairs. We introduce Low Rank Query and Key attention\n(LRQK), a two-stage framework that jointly decomposes the full-precision query\nand key matrices into compact rank-\\(r\\) factors during the prefill stage, and\nthen uses these low-dimensional projections to compute proxy attention scores\nin \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the\ntop-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed\nGPU-CPU cache with a hit-and-miss mechanism that transfers only missing\nfull-precision KV pairs, thereby preserving exact attention outputs while\nreducing CPU-GPU data movement. Extensive experiments on the RULER and\nLongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK\nmatches or surpasses leading sparse-attention methods in long context settings,\nwhile delivering significant memory savings with minimal loss in accuracy. Our\ncode is available at https://github.com/tenghuilee/LRQK."
                },
                "authors": [
                    {
                        "name": "Tenghui Li"
                    },
                    {
                        "name": "Guoxu Zhou"
                    },
                    {
                        "name": "Xuyang Zhao"
                    },
                    {
                        "name": "Yuning Qiu"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22145v1",
                "updated": "2025-10-25T03:34:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    3,
                    34,
                    34,
                    5,
                    298,
                    0
                ],
                "published": "2025-10-25T03:34:34Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    3,
                    34,
                    34,
                    5,
                    298,
                    0
                ],
                "title": "Fundamental Limits of Coded Caching with Fixed Subpacketization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamental Limits of Coded Caching with Fixed Subpacketization"
                },
                "summary": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to create coded multicast\nopportunities for cache-aided networks. By splitting each file into $F$ equal\npackets (i.e., the subpacketization level $F$) and letting each user cache a\nset of packets, the transmission load can be significantly reduced via coded\nmulticasting. It has been shown that a higher subpacketization level could\npotentially lead to a lower transmission load, as more packets can be combined\nfor efficient transmission. On the other hand, a larger $F$ indicates a higher\ncoding complexity and is problematic from a practical perspective when $F$ is\nextremely large. Despite many works attempting to design coded caching schemes\nwith low subpacketization levels, a fundamental problem remains open: What is\nthe minimum transmission load given any fixed subpacketization level? In this\npaper, we consider the classical cache-aided networks with identically uncoded\nplacement and one-shot delivery strategy, and investigate the fundamental\ntrade-off between the transmission load and the subpacketization level. We\npropose a \\emph{general} lower bound on the transmission load for any fixed\nsubpacketization by reformulating the centralized coded caching schemes via the\ncombinatorial structure of the corresponding placement delivery array. The\nlower bound also recovers existing optimality results for the bipartite graph\nscheme (including the well-known Maddah-Ali and Niesen (MN) scheme and the\nconjugate MN scheme) as well as the grouping bipartite graph scheme.\nFurthermore, by carefully exploiting the combinatorial structure and computing\nthe union size of sorted sets, we establish a new optimality result, i.e., the\npartition scheme can achieve the optimal rate-subpacketization trade-off."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Yifei Huang"
                    },
                    {
                        "name": "Youlong Wu"
                    },
                    {
                        "name": "Jinyan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinyan Wang"
                },
                "author": "Jinyan Wang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v3",
                "updated": "2025-10-25T02:29:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    2,
                    29,
                    47,
                    5,
                    298,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "accepted by ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v4",
                "updated": "2025-10-25T00:33:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    25,
                    0,
                    33,
                    14,
                    5,
                    298,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "arxiv_comment": "To appear on NeurIPS 2025 (spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.22049v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.22049v1",
                "updated": "2025-10-24T22:17:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T22:17:49Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    22,
                    17,
                    49,
                    4,
                    297,
                    0
                ],
                "title": "Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive Memorization with Hundreds of Trillions of Parameters for\n  Sequential Transducer Generative Recommenders"
                },
                "summary": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large-scale recommendation systems rely heavily on user interaction\nhistory sequences to enhance the model performance. The advent of large\nlanguage models and sequential modeling techniques, particularly\ntransformer-like architectures, has led to significant advancements recently\n(e.g., HSTU, SIM, and TWIN models). While scaling to ultra-long user histories\n(10k to 100k items) generally improves model performance, it also creates\nsignificant challenges on latency, queries per second (QPS) and GPU cost in\nindustry-scale recommendation systems. Existing models do not adequately\naddress these industrial scalability issues. In this paper, we propose a novel\ntwo-stage modeling framework, namely VIrtual Sequential Target Attention\n(VISTA), which decomposes traditional target attention from a candidate item to\nuser history items into two distinct stages: (1) user history summarization\ninto a few hundred tokens; followed by (2) candidate item attention to those\ntokens. These summarization token embeddings are then cached in storage system\nand then utilized as sequence features for downstream model training and\ninference. This novel design for scalability enables VISTA to scale to lifelong\nuser histories (up to one million items) while keeping downstream training and\ninference costs fixed, which is essential in industry. Our approach achieves\nsignificant improvements in offline and online metrics and has been\nsuccessfully deployed on an industry leading recommendation platform serving\nbillions of users."
                },
                "authors": [
                    {
                        "name": "Zhimin Chen"
                    },
                    {
                        "name": "Chenyu Zhao"
                    },
                    {
                        "name": "Ka Chun Mo"
                    },
                    {
                        "name": "Yunjiang Jiang"
                    },
                    {
                        "name": "Jane H. Lee"
                    },
                    {
                        "name": "Shouwei Chen"
                    },
                    {
                        "name": "Khushhall Chandra Mahajan"
                    },
                    {
                        "name": "Ning Jiang"
                    },
                    {
                        "name": "Kai Ren"
                    },
                    {
                        "name": "Jinhui Li"
                    },
                    {
                        "name": "Wen-Yun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Yun Yang"
                },
                "author": "Wen-Yun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.22049v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.22049v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21696v1",
                "updated": "2025-10-24T17:56:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T17:56:37Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    17,
                    56,
                    37,
                    4,
                    297,
                    0
                ],
                "title": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BachVid: Training-Free Video Generation with Consistent Background and\n  Character"
                },
                "summary": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have recently driven significant progress in\ntext-to-video (T2V) generation. However, generating multiple videos with\nconsistent characters and backgrounds remains a significant challenge. Existing\nmethods typically rely on reference images or extensive training, and often\nonly address character consistency, leaving background consistency to\nimage-to-video models. We introduce BachVid, the first training-free method\nthat achieves consistent video generation without needing any reference images.\nOur approach is based on a systematic analysis of DiT's attention mechanism and\nintermediate features, revealing its ability to extract foreground masks and\nidentify matching points during the denoising process. Our method leverages\nthis finding by first generating an identity video and caching the intermediate\nvariables, and then inject these cached variables into corresponding positions\nin newly generated videos, ensuring both foreground and background consistency\nacross multiple videos. Experimental results demonstrate that BachVid achieves\nrobust consistency in generated videos without requiring additional training,\noffering a novel and efficient solution for consistent video generation without\nrelying on reference images or additional training."
                },
                "authors": [
                    {
                        "name": "Han Yan"
                    },
                    {
                        "name": "Xibin Song"
                    },
                    {
                        "name": "Yifu Wang"
                    },
                    {
                        "name": "Hongdong Li"
                    },
                    {
                        "name": "Pan Ji"
                    },
                    {
                        "name": "Chao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chao Ma"
                },
                "author": "Chao Ma",
                "arxiv_comment": "Project page: https://wolfball.github.io/bachvid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20787v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20787v2",
                "updated": "2025-10-24T16:56:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    16,
                    56,
                    22,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-23T17:53:03Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    17,
                    53,
                    3,
                    3,
                    296,
                    0
                ],
                "title": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alleviating Forgetfulness of Linear Attention by Hybrid Sparse Attention\n  and Contextualized Learnable Token Eviction"
                },
                "summary": "Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear-attention models that compress the entire input sequence into a\nfixed-size recurrent state offer an efficient alternative to Transformers, but\ntheir finite memory induces forgetfulness that harms retrieval-intensive tasks.\nTo mitigate the issue, we explore a series of hybrid models that restore direct\naccess to past tokens. We interleave token mixers with intermediate time and\nspace complexity between linear and full attention, including sparse attention\nwith token eviction, and the query-aware native sparse attention. Particularly,\nwe propose a novel learnable token eviction approach. Combined with\nsliding-window attention, an end-to-end trainable lightweight CNN aggregates\ninformation from both past and future adjacent tokens to adaptively retain a\nlimited set of critical KV-pairs per head, maintaining linear attention's\nconstant time and space complexity. Efficient Triton kernels for the sparse\nattention mechanisms are provided. Empirical evaluations on retrieval-intensive\nbenchmarks support the effectiveness of our approaches."
                },
                "authors": [
                    {
                        "name": "Mutian He"
                    },
                    {
                        "name": "Philip N. Garner"
                    }
                ],
                "author_detail": {
                    "name": "Philip N. Garner"
                },
                "author": "Philip N. Garner",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20787v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17388v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17388v2",
                "updated": "2025-10-24T14:55:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    55,
                    42,
                    4,
                    297,
                    0
                ],
                "published": "2025-09-22T06:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory"
                },
                "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."
                },
                "authors": [
                    {
                        "name": "Manel Lurbe"
                    },
                    {
                        "name": "Miguel Avargues"
                    },
                    {
                        "name": "Salvador Petit"
                    },
                    {
                        "name": "Maria E. Gomez"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Guanhao Wang"
                    },
                    {
                        "name": "Julio Sahuquillo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Sahuquillo"
                },
                "author": "Julio Sahuquillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17388v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17388v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22922v2",
                "updated": "2025-10-24T11:53:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    53,
                    34,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-28T15:15:31Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    15,
                    15,
                    31,
                    5,
                    179,
                    0
                ],
                "title": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job\n  Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Global Predecessor Indexing: Avoiding Binary Search in Weighted Job\n  Scheduling"
                },
                "summary": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n\n\\log(n))$ time due to comparison-based sorting and per-job binary search, we\neliminate the binary search bottleneck. In its place, we introduce a novel\nmulti-phase preprocessing technique called \\emph{Global Predecessor Indexing\n(GPI)}, which computes the latest non-overlapping job (i.e., the predecessor)\nfor all jobs via a two-pointer linear-time pass after sorting. This yields a\ntime complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI\nenables direct use in the classical DP recurrence. When combined with\nlinear-time sorting, GPI yields a complete $O(n)$ solution. Even with\ncomparison-based sorting, GPI significantly outperforms the classical solution\nin practice by avoiding repeated binary searches in favor of the more\ncache-efficient extra sort and two-pointer pass.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an improved solution to the Weighted Job Scheduling (WJS) problem.\nWhile the classical dynamic programming (DP) solution for $n$ jobs runs in $O(n\n\\log(n))$ time due to comparison-based sorting and per-job binary search, we\neliminate the binary search bottleneck. In its place, we introduce a novel\nmulti-phase preprocessing technique called \\emph{Global Predecessor Indexing\n(GPI)}, which computes the latest non-overlapping job (i.e., the predecessor)\nfor all jobs via a two-pointer linear-time pass after sorting. This yields a\ntime complexity of $O(S(n) + n)$ where $S(n)$ is the time to sort all jobs. GPI\nenables direct use in the classical DP recurrence. When combined with\nlinear-time sorting, GPI yields a complete $O(n)$ solution. Even with\ncomparison-based sorting, GPI significantly outperforms the classical solution\nin practice by avoiding repeated binary searches in favor of the more\ncache-efficient extra sort and two-pointer pass."
                },
                "authors": [
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "6 pages, 9 figures including tables. Short theoretical and practical\n  paper on improved dynamic programming for weighted job scheduling with\n  linear-time preprocessing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21361v1",
                "updated": "2025-10-24T11:42:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-24T11:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    11,
                    42,
                    38,
                    4,
                    297,
                    0
                ],
                "title": "Compositional Monte Carlo Tree Diffusion for Extendable Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional Monte Carlo Tree Diffusion for Extendable Planning"
                },
                "summary": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured\ntree search to enable effective trajectory exploration through stepwise\nreasoning. However, MCTD remains fundamentally limited by training trajectory\nlengths. While periodic replanning allows plan concatenation for longer plan\ngeneration, the planning process remains locally confined, as MCTD searches\nwithin individual trajectories without access to global context. We propose\nCompositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates\nplanning from individual trajectory optimization to reasoning over complete\nplan compositions. C-MCTD introduces three complementary components: (1) Online\nComposer, which performs globally-aware planning by searching across entire\nplan compositions; (2) Distributed Composer, which reduces search complexity\nthrough parallel exploration from multiple starting points; and (3) Preplan\nComposer, which accelerates inference by leveraging cached plan graphs."
                },
                "authors": [
                    {
                        "name": "Jaesik Yoon"
                    },
                    {
                        "name": "Hyeonseo Cho"
                    },
                    {
                        "name": "Sungjin Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Ahn"
                },
                "author": "Sungjin Ahn",
                "arxiv_comment": "24 pages, 4 figures, NeurIPS 25 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v6",
                "updated": "2025-10-24T08:41:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    8,
                    41,
                    38,
                    4,
                    297,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce excessive computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025.\n  https://neurips.cc/virtual/2025/poster/116615",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19240v2",
                "updated": "2025-10-24T08:35:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    8,
                    35,
                    21,
                    4,
                    297,
                    0
                ],
                "published": "2025-10-22T04:48:41Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    4,
                    48,
                    41,
                    2,
                    295,
                    0
                ],
                "title": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Solution for the Implementation of CI/CD in Embedded Linux\n  Development"
                },
                "summary": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing use of embedded systems in various industries, the need for\nautomated platforms for the development and deployment of customized\nLinux-based operating systems has become more important. This research was\nconducted with the aim of designing and implementing an integrated and\nreproducible infrastructure for the development, building, and testing of a\nLinux-based operating system using the Yocto Project. The proposed structure\nwas implemented based on a three-layer architecture consisting of the main\nYocto repositories, a custom layer (meta-custom), and a coordinating manifest\nlayer to ensure version synchronization, scalability, and reproducibility.\nThree sample projects, including libhelloworld, helloworld, and the kernel\nmodule hello mod, were developed and integrated into the build process.\nContinuous Integration and Continuous Deployment pipelines were implemented\nwith GitLab CI and combined with an isolated Docker environment to automate and\nstreamline the build and testing workflows. Using a local cache server\ncontaining hashserv, downloads and sstate cache significantly reduced the build\ntime. The functionality and stability of the system were verified through six\nboot test scenarios in the QEMU simulator. The results show that the proposed\ndesign not only ensures reproducibility but also can be extended to advanced\napplications such as continuous deployment of real-time Linux versions. Future\nrecommendations include expanding automated tests, implementing system\nmonitoring with Prometheus and Grafana, using distributed builds, optimizing\nwith Docker multi-stage builds, and enabling continuous deployment of real-time\nLinux changes to provide a stable and scalable model for industrial and\nresearch projects in embedded systems with a rapid and reliable development\ncycle."
                },
                "authors": [
                    {
                        "name": "Behnam Agahi"
                    },
                    {
                        "name": "Hamed Farbeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Farbeh"
                },
                "author": "Hamed Farbeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15745v2",
                "updated": "2025-10-24T05:39:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    5,
                    39,
                    3,
                    4,
                    297,
                    0
                ],
                "published": "2025-06-18T02:22:14Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    2,
                    22,
                    14,
                    2,
                    169,
                    0
                ],
                "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video\n  Understanding"
                },
                "summary": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time-quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy-even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern multimodal large language models (MLLMs) can reason over hour-long\nvideo, yet their key-value (KV) cache grows linearly with time-quickly\nexceeding the fixed memory of phones, AR glasses, and edge robots. Prior\ncompression schemes either assume the whole video and user query are available\noffline or must first build the full cache, so memory still scales with stream\nlength. InfiniPot-V is the first training-free, query-agnostic framework that\nenforces a hard, length-independent memory cap for streaming video\nunderstanding. During video encoding it monitors the cache and, once a user-set\nthreshold is reached, runs a lightweight compression pass that (i) removes\ntemporally redundant tokens via Temporal-axis Redundancy (TaR) metric and (ii)\nkeeps semantically significant tokens via Value-Norm (VaN) ranking. Across four\nopen-source MLLMs and four long-video and streaming-video benchmarks,\nInfiniPot-V cuts peak GPU memory by up to 94%, sustains real-time generation,\nand matches or surpasses full-cache accuracy-even in multi-turn dialogues. By\ndissolving the KV cache bottleneck without retraining or query knowledge,\nInfiniPot-V closes the gap for on-device streaming video assistants."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Kyuhong Shim"
                    },
                    {
                        "name": "Jungwook Choi"
                    },
                    {
                        "name": "Simyung Chang"
                    }
                ],
                "author_detail": {
                    "name": "Simyung Chang"
                },
                "author": "Simyung Chang",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.13866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.13866v2",
                "updated": "2025-10-24T04:48:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    24,
                    4,
                    48,
                    6,
                    4,
                    297,
                    0
                ],
                "published": "2025-05-20T03:21:52Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    3,
                    21,
                    52,
                    1,
                    140,
                    0
                ],
                "title": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Path Compression: Compressing Generation Trajectories for\n  Efficient LLM Reasoning"
                },
                "summary": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and reduce throughput of\ntoken generation, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining cache entries that receive\nhigh importance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent reasoning-focused language models achieve high accuracy by generating\nlengthy intermediate reasoning paths before producing final answers. While this\napproach is effective in solving problems that require logical thinking, long\nreasoning paths significantly increase memory usage and reduce throughput of\ntoken generation, limiting the practical deployment of such models. We propose\nReasoning Path Compression (RPC), a training-free method that accelerates\ninference by leveraging the semantic sparsity of reasoning paths. RPC\nperiodically compresses the KV cache by retaining cache entries that receive\nhigh importance score, which are computed using a selector window composed of\nrecently generated queries. Experiments show that RPC improves generation\nthroughput of QwQ-32B by up to 1.60$\\times$ compared to the inference with full\nKV cache, with an accuracy drop of 1.2\\% on the AIME 2024 benchmark. Our\nfindings demonstrate that semantic sparsity in reasoning traces can be\neffectively exploited for compression, offering a practical path toward\nefficient deployment of reasoning LLMs. Our code is available at\nhttps://github.com/jiwonsong-dev/ReasoningPathCompression."
                },
                "authors": [
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Dongwon Jo"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.13866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.13866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v5",
                "updated": "2025-10-23T23:35:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    23,
                    35,
                    32,
                    3,
                    296,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, substantially shrinking the KV cache size\nat inference time. By factorizing these representations into contextual\nlow-rank components and seamlessly integrating with Rotary Position Embedding\n(RoPE), TPA achieves improved model quality alongside memory efficiency. Based\non TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation on\nlanguage modeling tasks, we demonstrate that T6 surpasses or matches the\nperformance of standard Transformer baselines including Multi-Head Attention\n(MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and\nMulti-Head Latent Attention (MLA) across various metrics, including perplexity\nand a range of established evaluation benchmarks. Notably, TPA's memory\nefficiency and computational efficiency at decoding stage enables processing\nlonger sequences under fixed resource constraints, addressing a critical\nscalability challenge in modern language models. Project Page:\nhttps://github.com/tensorgi/TPA."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "Published in NeurIPS 2025 (Spotlight); Project Page:\n  https://github.com/tensorgi/TPA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.16986v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.16986v2",
                "updated": "2025-10-23T21:31:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    21,
                    31,
                    35,
                    3,
                    296,
                    0
                ],
                "published": "2025-05-22T17:54:32Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    17,
                    54,
                    32,
                    3,
                    142,
                    0
                ],
                "title": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic\n  Planning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-weight and proprietary large language models. We\npresent results powered by T1-Agent, highlighting their ability to plan and\nreason in complex, tool-dependent scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities as\nintelligent agents capable of solving complex problems. However, effective\nplanning in scenarios involving dependencies between API or tool\ncalls-particularly in multi-turn conversations-remains a significant challenge.\nTo address this, we introduce T1, a tool-augmented, multi-domain, multi-turn\nconversational dataset specifically designed to capture and manage inter-tool\ndependencies across diverse domains. T1 enables rigorous evaluation of agents'\nability to coordinate tool use across nine distinct domains (4 single domain\nand 5 multi-domain) with the help of an integrated caching mechanism for both\nshort- and long-term memory, while supporting dynamic replanning-such as\ndeciding whether to recompute or reuse cached results. Beyond facilitating\nresearch on tool use and planning, T1 also serves as a benchmark for evaluating\nthe performance of open-weight and proprietary large language models. We\npresent results powered by T1-Agent, highlighting their ability to plan and\nreason in complex, tool-dependent scenarios."
                },
                "authors": [
                    {
                        "name": "Amartya Chakraborty"
                    },
                    {
                        "name": "Paresh Dashore"
                    },
                    {
                        "name": "Nadia Bathaee"
                    },
                    {
                        "name": "Anmol Jain"
                    },
                    {
                        "name": "Anirban Das"
                    },
                    {
                        "name": "Shi-Xiong Zhang"
                    },
                    {
                        "name": "Sambit Sahu"
                    },
                    {
                        "name": "Milind Naphade"
                    },
                    {
                        "name": "Genta Indra Winata"
                    }
                ],
                "author_detail": {
                    "name": "Genta Indra Winata"
                },
                "author": "Genta Indra Winata",
                "arxiv_comment": "Accepted by NeurIPS 2025 Datasets and Benchmarks Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.16986v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.16986v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00566v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00566v3",
                "updated": "2025-10-23T19:45:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    19,
                    45,
                    39,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-01T06:38:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "Panorama: Fast-Track Nearest Neighbors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panorama: Fast-Track Nearest Neighbors"
                },
                "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss."
                },
                "authors": [
                    {
                        "name": "Vansh Ramani"
                    },
                    {
                        "name": "Alexis Schlomer"
                    },
                    {
                        "name": "Akash Nayar"
                    },
                    {
                        "name": "Sayan Ranu"
                    },
                    {
                        "name": "Jignesh M. Patel"
                    },
                    {
                        "name": "Panagiotis Karras"
                    }
                ],
                "author_detail": {
                    "name": "Panagiotis Karras"
                },
                "author": "Panagiotis Karras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00566v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00566v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07254v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07254v3",
                "updated": "2025-10-23T18:52:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    18,
                    52,
                    25,
                    3,
                    296,
                    0
                ],
                "published": "2025-06-08T18:43:31Z",
                "published_parsed": [
                    2025,
                    6,
                    8,
                    18,
                    43,
                    31,
                    6,
                    159,
                    0
                ],
                "title": "A Stable Whitening Optimizer for Efficient Neural Network Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Stable Whitening Optimizer for Efficient Neural Network Training"
                },
                "summary": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44-58% of the gradient steps and 62-83% of the\nwallclock time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we take an experimentally grounded look at neural network\noptimization. Building on the Shampoo family of algorithms, we identify and\nalleviate three key issues, resulting in the proposed SPlus method. First, we\nfind that naive Shampoo is prone to divergence when matrix-inverses are cached\nfor long periods. We introduce an alternate bounded update combining a\nhistorical eigenbasis with instantaneous normalization, resulting in\nacross-the-board stability and significantly lower computational requirements.\nSecond, we adapt a shape-aware scaling to enable learning rate transfer across\nnetwork width. Third, we find that high learning rates result in large\nparameter noise, and propose a simple iterate-averaging scheme which unblocks\nfaster learning. To properly confirm these findings, we introduce a pointed\nTransformer training benchmark, considering three objectives (language\nmodelling, image classification, and diffusion modelling) across different\nstages of training. On average, SPlus is able to reach the validation\nperformance of Adam within 44-58% of the gradient steps and 62-83% of the\nwallclock time."
                },
                "authors": [
                    {
                        "name": "Kevin Frans"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07254v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07254v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20707v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20707v1",
                "updated": "2025-10-23T16:17:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    16,
                    17,
                    47,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T16:17:47Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    16,
                    17,
                    47,
                    3,
                    296,
                    0
                ],
                "title": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixing Importance with Diversity: Joint Optimization for KV Cache\n  Compression in Large Vision-Language Models"
                },
                "summary": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large vision-language models (LVLMs) demonstrate remarkable\ncapabilities in processing extended multi-modal sequences, yet the resulting\nkey-value (KV) cache expansion creates a critical memory bottleneck that\nfundamentally limits deployment scalability. While existing KV cache\ncompression methods focus on retaining high-importance KV pairs to minimize\nstorage, they often overlook the modality-specific semantic redundancy patterns\nthat emerge distinctively in multi-modal KV caches. In this work, we first\nanalyze how, beyond simple importance, the KV cache in LVLMs exhibits varying\nlevels of redundancy across attention heads. We show that relying solely on\nimportance can only cover a subset of the full KV cache information\ndistribution, leading to potential loss of semantic coverage. To address this,\nwe propose \\texttt{MixKV}, a novel method that mixes importance with diversity\nfor optimized KV cache compression in LVLMs. \\texttt{MixKV} adapts to head-wise\nsemantic redundancy, selectively balancing diversity and importance when\ncompressing KV pairs. Extensive experiments demonstrate that \\texttt{MixKV}\nconsistently enhances existing methods across multiple LVLMs. Under extreme\ncompression (budget=64), \\texttt{MixKV} improves baseline methods by an average\nof \\textbf{5.1\\%} across five multi-modal understanding benchmarks and achieves\nremarkable gains of \\textbf{8.0\\%} and \\textbf{9.0\\%} for SnapKV and AdaKV on\nGUI grounding tasks, all while maintaining comparable inference efficiency.\nFurthermore, \\texttt{MixKV} extends seamlessly to LLMs with comparable\nperformance gains. Our code is available at\n\\href{https://github.com/xuyang-liu16/MixKV}{\\textcolor{citeblue}{https://github.com/xuyang-liu16/MixKV}}."
                },
                "authors": [
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Xiyan Gui"
                    },
                    {
                        "name": "Yuchao Zhang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "Our code is available at https://github.com/xuyang-liu16/MixKV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20707v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20707v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16407v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16407v2",
                "updated": "2025-10-23T15:26:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    15,
                    26,
                    38,
                    3,
                    296,
                    0
                ],
                "published": "2025-09-19T20:31:38Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "title": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables"
                },
                "summary": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs."
                },
                "authors": [
                    {
                        "name": "Hunter McCoy"
                    },
                    {
                        "name": "Prashant Pandey"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Pandey"
                },
                "author": "Prashant Pandey",
                "arxiv_comment": "Accepted to ALENEX`26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16407v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16407v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v4",
                "updated": "2025-10-23T14:23:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    14,
                    23,
                    24,
                    3,
                    296,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "35 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20878v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20878v1",
                "updated": "2025-10-23T12:28:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    28,
                    58,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T12:28:58Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    12,
                    28,
                    58,
                    3,
                    296,
                    0
                ],
                "title": "HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data\n  Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HA-RAG: Hotness-Aware RAG Acceleration via Mixed Precision and Data\n  Placement"
                },
                "summary": "Retrieval-Augmented Generation (RAG) improves model output accuracy by\nleveraging external knowledge bases, serving as an effective solution to\naddress hallucination issues and knowledge-update delays in Large Language\nModels (LLMs). However, the introduction of external knowledge bases presents\nRAG with challenges in long-context processing, significantly increasing memory\nconsumption and inference latency. Existing research accelerates inference by\nprecomputing Key and Value (KV) of the knowledge base and loading them\non-demand during inference. Based on the access frequency of different KV\nchunks within the external knowledge base, this paper proposes a hotness-aware\nRAG (HA-RAG) inference optimization system. First, leveraging the numerical\ndistribution of KV chunks, we introduce a hotness-aware mixed-precision\ncompressing and loading method to reduce disk I/O and memory access overhead.\nSecond, we design a hotness-aware data placement strategy that prioritizes\nstoring frequently accessed KV chunks in high-speed memory to improve data\naccess efficiency. Experimental results demonstrate that, compared with\nTurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum\nspeedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) improves model output accuracy by\nleveraging external knowledge bases, serving as an effective solution to\naddress hallucination issues and knowledge-update delays in Large Language\nModels (LLMs). However, the introduction of external knowledge bases presents\nRAG with challenges in long-context processing, significantly increasing memory\nconsumption and inference latency. Existing research accelerates inference by\nprecomputing Key and Value (KV) of the knowledge base and loading them\non-demand during inference. Based on the access frequency of different KV\nchunks within the external knowledge base, this paper proposes a hotness-aware\nRAG (HA-RAG) inference optimization system. First, leveraging the numerical\ndistribution of KV chunks, we introduce a hotness-aware mixed-precision\ncompressing and loading method to reduce disk I/O and memory access overhead.\nSecond, we design a hotness-aware data placement strategy that prioritizes\nstoring frequently accessed KV chunks in high-speed memory to improve data\naccess efficiency. Experimental results demonstrate that, compared with\nTurboRAG, the proposed HA-RAG achieves an average speedup of 2.10x and maximum\nspeedup of 10.49x in Time-To-First-Token (TTFT) with negligible accuracy loss."
                },
                "authors": [
                    {
                        "name": "Danying Ge"
                    },
                    {
                        "name": "Jianhua Gao"
                    },
                    {
                        "name": "Yixue Yang"
                    },
                    {
                        "name": "Weixing Ji"
                    }
                ],
                "author_detail": {
                    "name": "Weixing Ji"
                },
                "author": "Weixing Ji",
                "arxiv_comment": "13 pages,16 figures,2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20878v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20878v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; E.4; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21865v1",
                "updated": "2025-10-23T10:35:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    35,
                    35,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T10:35:35Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    35,
                    35,
                    3,
                    296,
                    0
                ],
                "title": "Prefetching Cache Optimization Using Graph Neural Networks: A Modular\n  Framework and Conceptual Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching Cache Optimization Using Graph Neural Networks: A Modular\n  Framework and Conceptual Analysis"
                },
                "summary": "Caching and prefetching techniques are fundamental to modern computing,\nserving to bridge the growing performance gap between processors and memory.\nTraditional prefetching strategies are often limited by their reliance on\npredefined heuristics or simplified statistical models, which fail to capture\nthe complex, non-linear dependencies in modern data access patterns. This paper\nintroduces a modular framework leveraging Graph Neural Networks (GNNs) to model\nand predict access patterns within graph-structured data, focusing on web\nnavigation and hierarchical file systems. The toolchain consists of: a route\nmapper for extracting structural information, a graph constructor for creating\ngraph representations, a walk session generator for simulating user behaviors,\nand a gnn prefetch module for training and inference. We provide a detailed\nconceptual analysis showing how GNN-based approaches can outperform\nconventional methods by learning intricate dependencies. This work offers both\ntheoretical foundations and a practical, replicable pipeline for future\nresearch in graph-driven systems optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching and prefetching techniques are fundamental to modern computing,\nserving to bridge the growing performance gap between processors and memory.\nTraditional prefetching strategies are often limited by their reliance on\npredefined heuristics or simplified statistical models, which fail to capture\nthe complex, non-linear dependencies in modern data access patterns. This paper\nintroduces a modular framework leveraging Graph Neural Networks (GNNs) to model\nand predict access patterns within graph-structured data, focusing on web\nnavigation and hierarchical file systems. The toolchain consists of: a route\nmapper for extracting structural information, a graph constructor for creating\ngraph representations, a walk session generator for simulating user behaviors,\nand a gnn prefetch module for training and inference. We provide a detailed\nconceptual analysis showing how GNN-based approaches can outperform\nconventional methods by learning intricate dependencies. This work offers both\ntheoretical foundations and a practical, replicable pipeline for future\nresearch in graph-driven systems optimization."
                },
                "authors": [
                    {
                        "name": "F. I. Qowy"
                    }
                ],
                "author_detail": {
                    "name": "F. I. Qowy"
                },
                "author": "F. I. Qowy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20400v1",
                "updated": "2025-10-23T10:06:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    6,
                    48,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T10:06:48Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    10,
                    6,
                    48,
                    3,
                    296,
                    0
                ],
                "title": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism\n  on Dependency-Bound Kernels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squire: A General-Purpose Accelerator to Exploit Fine-Grain Parallelism\n  on Dependency-Bound Kernels"
                },
                "summary": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple HPC applications are often bottlenecked by compute-intensive kernels\nimplementing complex dependency patterns (data-dependency bound). Traditional\ngeneral-purpose accelerators struggle to effectively exploit fine-grain\nparallelism due to limitations in implementing convoluted data-dependency\npatterns (like SIMD) and overheads due to synchronization and data transfers\n(like GPGPUs). In contrast, custom FPGA and ASIC designs offer improved\nperformance and energy efficiency at a high cost in hardware design and\nprogramming complexity and often lack the flexibility to process different\nworkloads. We propose Squire, a general-purpose accelerator designed to exploit\nfine-grain parallelism effectively on dependency-bound kernels. Each Squire\naccelerator has a set of general-purpose low-power in-order cores that can\nrapidly communicate among themselves and directly access data from the L2\ncache. Our proposal integrates one Squire accelerator per core in a typical\nmulticore system, allowing the acceleration of dependency-bound kernels within\nparallel tasks with minimal software changes. As a case study, we evaluate\nSquire's effectiveness by accelerating five kernels that implement complex\ndependency patterns. We use three of these kernels to build an end-to-end\nread-mapping tool that will be used to evaluate Squire. Squire obtains speedups\nup to 7.64$\\times$ in dynamic programming kernels. Overall, Squire provides an\nacceleration for an end-to-end application of 3.66$\\times$. In addition, Squire\nreduces energy consumption by up to 56% with a minimal area overhead of 10.5%\ncompared to a Neoverse-N1 baseline."
                },
                "authors": [
                    {
                        "name": "Rubn Langarita"
                    },
                    {
                        "name": "Jess Alastruey-Bened"
                    },
                    {
                        "name": "Pablo Ibez-Marn"
                    },
                    {
                        "name": "Santiago Marco-Sola"
                    },
                    {
                        "name": "Miquel Moret"
                    },
                    {
                        "name": "Adri Armejach"
                    }
                ],
                "author_detail": {
                    "name": "Adri Armejach"
                },
                "author": "Adri Armejach",
                "arxiv_comment": "11 pages, 10 figures, 5 tables, 4 algorithms, accepted on PACT25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11843v2",
                "updated": "2025-10-23T09:55:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    9,
                    55,
                    50,
                    3,
                    296,
                    0
                ],
                "published": "2024-11-18T18:59:15Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    18,
                    59,
                    15,
                    0,
                    323,
                    0
                ],
                "title": "Bi-Mamba: Towards Accurate 1-Bit State Space Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bi-Mamba: Towards Accurate 1-Bit State Space Models"
                },
                "summary": "The typical Selective State-Space Model (SSM) used in Mamba addresses several\nlimitations of Transformers, such as the quadratic computational complexity\nwith respect to sequence length and the significant memory requirements during\ninference due to the key-value (KV) cache. However, the increasing size of\nMamba models continues to pose challenges for training and deployment,\nparticularly due to their substantial computational demands during both\ntraining and inference. In this work, we introduce $\\texttt{Bi-Mamba}$, a\nscalable and powerful 1-bit Mamba architecture designed to enable more\nefficient large language models (LLMs), with model sizes of 780M, 1.3B, and\n2.7B parameters. $\\texttt{Bi-Mamba}$ models are trained from scratch on a\nstandard LLM-scale dataset using an autoregressive distillation loss. Extensive\nexperiments on language modeling benchmarks demonstrate that\n$\\texttt{Bi-Mamba}$ achieves performance comparable to its full-precision (FP16\nor BF16) counterparts, while outperforming post-training binarization (PTB)\nMamba and binarization-aware training (BAT) Transformer baselines. Moreover,\n$\\texttt{Bi-Mamba}$ drastically reduces memory usage and computational cost\ncompared to the original Mamba. Our work pioneers a new line of\nlinear-complexity LLMs under low-bit representation and provides the way for\nthe design of specialized hardware optimized for efficient 1-bit Mamba-based\nmodels. Code and the pre-trained weights are available at\nhttps://github.com/Tangshengku/Bi-Mamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The typical Selective State-Space Model (SSM) used in Mamba addresses several\nlimitations of Transformers, such as the quadratic computational complexity\nwith respect to sequence length and the significant memory requirements during\ninference due to the key-value (KV) cache. However, the increasing size of\nMamba models continues to pose challenges for training and deployment,\nparticularly due to their substantial computational demands during both\ntraining and inference. In this work, we introduce $\\texttt{Bi-Mamba}$, a\nscalable and powerful 1-bit Mamba architecture designed to enable more\nefficient large language models (LLMs), with model sizes of 780M, 1.3B, and\n2.7B parameters. $\\texttt{Bi-Mamba}$ models are trained from scratch on a\nstandard LLM-scale dataset using an autoregressive distillation loss. Extensive\nexperiments on language modeling benchmarks demonstrate that\n$\\texttt{Bi-Mamba}$ achieves performance comparable to its full-precision (FP16\nor BF16) counterparts, while outperforming post-training binarization (PTB)\nMamba and binarization-aware training (BAT) Transformer baselines. Moreover,\n$\\texttt{Bi-Mamba}$ drastically reduces memory usage and computational cost\ncompared to the original Mamba. Our work pioneers a new line of\nlinear-complexity LLMs under low-bit representation and provides the way for\nthe design of specialized hardware optimized for efficient 1-bit Mamba-based\nmodels. Code and the pre-trained weights are available at\nhttps://github.com/Tangshengku/Bi-Mamba."
                },
                "authors": [
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Mingjie Sun"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Shen"
                },
                "author": "Zhiqiang Shen",
                "arxiv_comment": "Accepted in TMLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19755v2",
                "updated": "2025-10-23T09:09:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    9,
                    9,
                    15,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-22T16:46:05Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    16,
                    46,
                    5,
                    2,
                    295,
                    0
                ],
                "title": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Cache Methods in Diffusion Models: Toward Efficient\n  Multi-Modal Generation"
                },
                "summary": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Models have become a cornerstone of modern generative AI for their\nexceptional generation quality and controllability. However, their inherent\n\\textit{multi-step iterations} and \\textit{complex backbone networks} lead to\nprohibitive computational overhead and generation latency, forming a major\nbottleneck for real-time applications. Although existing acceleration\ntechniques have made progress, they still face challenges such as limited\napplicability, high training costs, or quality degradation.\n  Against this backdrop, \\textbf{Diffusion Caching} offers a promising\ntraining-free, architecture-agnostic, and efficient inference paradigm. Its\ncore mechanism identifies and reuses intrinsic computational redundancies in\nthe diffusion process. By enabling feature-level cross-step reuse and\ninter-layer scheduling, it reduces computation without modifying model\nparameters. This paper systematically reviews the theoretical foundations and\nevolution of Diffusion Caching and proposes a unified framework for its\nclassification and analysis.\n  Through comparative analysis of representative methods, we show that\nDiffusion Caching evolves from \\textit{static reuse} to \\textit{dynamic\nprediction}. This trend enhances caching flexibility across diverse tasks and\nenables integration with other acceleration techniques such as sampling\noptimization and model distillation, paving the way for a unified, efficient\ninference framework for future multimodal and interactive applications. We\nargue that this paradigm will become a key enabler of real-time and efficient\ngenerative AI, injecting new vitality into both theory and practice of\n\\textit{Efficient Generative Intelligence}."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Zhikai Wang"
                    },
                    {
                        "name": "Peiru Wang"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Zhengan Yan"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Zhengyi Shi"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "22 pages,2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16807v2",
                "updated": "2025-10-23T08:29:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    8,
                    29,
                    11,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-19T12:17:42Z",
                "published_parsed": [
                    2025,
                    10,
                    19,
                    12,
                    17,
                    42,
                    6,
                    292,
                    0
                ],
                "title": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Model Representation and Reducing KV Cache via Skip\n  Connections with First Value Heads"
                },
                "summary": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models have driven breakthroughs across various language tasks by\ntheir strong capability to learn rich contextual representations. Scaling them\nto improve representation, however, often demands substantial memory and\ncompute costs, such as the Key-Value (KV) cache used during auto-regressive\ndecoding. Skip connections offer a promising way to improve representation\nwithout bloating resource usage, yet most prior works either improve\nexpressivity while leaving KV costs unchanged, or reduce memory at the cost of\nweaker representation. In this work, we propose SkipV1Former, a Transformer\nvariant that uses skip connections from the first layer's Value heads to\nstrengthen model representation and reduce KV cache. Specifically, from the\nsecond block onward, each layer reuses half of its Value heads from the very\nfirst layer, while computing the other half as usual-cutting Value projections\nand V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed\nfirst-layer Values into deeper layers restores information lost to compression\nand accelerates the model's implicit mesa-optimization-a key pattern of\nTransformer in auto-regressive tasks. Empirically, across different model\nscales, SkipV1Former delivers consistent reductions of approximately 25 \\% in\nKV cache while improving perplexity relative to standard Multi-Head Attention\n(MHA) Transformers and some advanced variants. Moreover, we propose a recipe\nfor uptraining existing MHA Transformer checkpoints to SkipV1Former with only\n10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine\nadvanced methods like Group-Query Attention and Multi-Latent Attention to\nachieve further KV cache savings and performance improvement. When combined\nwith YOCO, it cuts KV cache size by nearly 50 \\% while still improving\nperformance."
                },
                "authors": [
                    {
                        "name": "Zhoutong Wu"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Yiming Dong"
                    },
                    {
                        "name": "Chenheng Zhang"
                    },
                    {
                        "name": "Cong Fang"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Zhouchen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouchen Lin"
                },
                "author": "Zhouchen Lin",
                "arxiv_comment": "The code is available at:\n  \\url{https://github.com/Zhoutong-Wu/SkipV1Former}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.20230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.20230v1",
                "updated": "2025-10-23T05:22:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    5,
                    22,
                    9,
                    3,
                    296,
                    0
                ],
                "published": "2025-10-23T05:22:09Z",
                "published_parsed": [
                    2025,
                    10,
                    23,
                    5,
                    22,
                    9,
                    3,
                    296,
                    0
                ],
                "title": "Soft Phonon Charge-Density Wave Formation in the Kagome Metal\n  KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Phonon Charge-Density Wave Formation in the Kagome Metal\n  KV$_3$Sb$_5$"
                },
                "summary": "A range of of unusual emergent behaviors have been reported in the\ncharge-density wave (CDW) state of the $A$V$_3$Sb$_5$ ($A=~$K, Rb, Cs) kagome\nmetals, including a CDW formation process without soft phonons, which points to\nan unconventional CDW mechanism. Here, we use inelastic x-ray scattering to\nshow that the CDW in KV$_3$Sb$_5$ forms via phonons that soften to zero energy\nat the CDW ordering vector ($L$-point) around $T_{\\rm CDW}=78$~K. These soft\nphonons exhibit a remarkable in-plane anisotropy, extending over a much larger\nmomentum range along $L$-$A$ relative to $L$-$H$, which leads to diffuse\nscattering common among $A$V$_3$Sb$_5$. Using first-principles calculations, we\nfind that the momentum-dependent electron-phonon coupling (EPC) is peaked at\n$L$ and exhibits the same in-plane anisotropy as the phonon softening.\nConversely, the electronic susceptibility is not peaked at $L$ and shows the\nopposite in-plane anisotropy. Our findings favor momentum-dependent EPC as the\ndriving mechanism of the CDW in KV$_3$Sb$_5$, with a CDW formation process\nsimilar to that of transition metal dichalcogenides.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of of unusual emergent behaviors have been reported in the\ncharge-density wave (CDW) state of the $A$V$_3$Sb$_5$ ($A=~$K, Rb, Cs) kagome\nmetals, including a CDW formation process without soft phonons, which points to\nan unconventional CDW mechanism. Here, we use inelastic x-ray scattering to\nshow that the CDW in KV$_3$Sb$_5$ forms via phonons that soften to zero energy\nat the CDW ordering vector ($L$-point) around $T_{\\rm CDW}=78$~K. These soft\nphonons exhibit a remarkable in-plane anisotropy, extending over a much larger\nmomentum range along $L$-$A$ relative to $L$-$H$, which leads to diffuse\nscattering common among $A$V$_3$Sb$_5$. Using first-principles calculations, we\nfind that the momentum-dependent electron-phonon coupling (EPC) is peaked at\n$L$ and exhibits the same in-plane anisotropy as the phonon softening.\nConversely, the electronic susceptibility is not peaked at $L$ and shows the\nopposite in-plane anisotropy. Our findings favor momentum-dependent EPC as the\ndriving mechanism of the CDW in KV$_3$Sb$_5$, with a CDW formation process\nsimilar to that of transition metal dichalcogenides."
                },
                "authors": [
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Chenchao Xu"
                    },
                    {
                        "name": "Zhimian Wu"
                    },
                    {
                        "name": "Huachen Rao"
                    },
                    {
                        "name": "Zhaoyang Shan"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Guanghan Cao"
                    },
                    {
                        "name": "Michael Smidman"
                    },
                    {
                        "name": "Ming Shi"
                    },
                    {
                        "name": "Huiqiu Yuan"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Xianhui Chen"
                    },
                    {
                        "name": "Chao Cao"
                    },
                    {
                        "name": "Yu Song"
                    }
                ],
                "author_detail": {
                    "name": "Yu Song"
                },
                "author": "Yu Song",
                "arxiv_comment": "submitted to journal in July 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.20230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.20230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19258v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19258v4",
                "updated": "2025-10-23T00:47:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    0,
                    47,
                    24,
                    3,
                    296,
                    0
                ],
                "published": "2024-10-25T02:22:00Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    2,
                    22,
                    0,
                    4,
                    299,
                    0
                ],
                "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Heads Matter: A Head-Level KV Cache Compression Method with\n  Integrated Retrieval and Reasoning"
                },
                "summary": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark. Codes are\navailable at https://github.com/FYYFU/HeadKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) caching is a common technique to enhance the computational\nefficiency of Large Language Models (LLMs), but its memory overhead grows\nrapidly with input length. Prior work has shown that not all tokens are equally\nimportant for text generation, proposing layer-level KV cache compression to\nselectively retain key information. Recognizing the distinct roles of attention\nheads in generation, we propose HeadKV, a head-level KV cache compression\nmethod, and HeadKV-R2, which leverages a novel contextual reasoning ability\nestimation for compression. Our approach operates at the level of individual\nheads, estimating their importance for contextual QA tasks that require both\nretrieval and reasoning capabilities. Extensive experiments across diverse\nbenchmarks (LongBench, LooGLE), model architectures (e.g., Llama-3-8B-Instruct,\nMistral-7B-Instruct), and long-context abilities tests demonstrate that our\nhead-level KV cache compression significantly outperforms strong baselines,\nparticularly in low-resource settings (KV size = 64 & 128). Notably, our method\nretains just 1.5% of the KV cache while achieving 97% of the performance of the\nfull KV cache on the contextual question answering benchmark. Codes are\navailable at https://github.com/FYYFU/HeadKV"
                },
                "authors": [
                    {
                        "name": "Yu Fu"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Abedelkadir Asi"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "arxiv_comment": "Accepted to ICLR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19258v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19258v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00744v2",
                "updated": "2025-10-23T00:40:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    23,
                    0,
                    40,
                    38,
                    3,
                    296,
                    0
                ],
                "published": "2025-05-31T23:16:53Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    23,
                    16,
                    53,
                    5,
                    151,
                    0
                ],
                "title": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear\n  Transformers"
                },
                "summary": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with fast weight memory through dynamic synaptic\nmodulation (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system, differing in how and when input\ninformation is delivered to each system, to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop hybrid memory architectures for general-purpose sequence\nprocessing neural networks, that combine key-value memory using softmax\nattention (KV-memory) with fast weight memory through dynamic synaptic\nmodulation (FW-memory) -- the core principles of quadratic and linear\ntransformers, respectively. These two memory systems have complementary but\nindividually limited properties: KV-memory offers precise retrieval but is\nconstrained by quadratic complexity in sequence length, while FW-memory\nsupports arbitrarily long sequences and enables more expressive computation but\nsacrifices precise recall. We propose and compare three methods to blend these\ntwo systems into a single memory system, differing in how and when input\ninformation is delivered to each system, to leverage the strengths of both. We\nconduct experiments on general language modeling and retrieval tasks by\ntraining 340M- and 1.3B-parameter models from scratch, as well as on synthetic\nalgorithmic tasks designed to precisely illustrate the benefits of certain\nhybrid methods over others. We also evaluate our hybrid memory systems on\nreinforcement learning in partially observable environments. Overall, we\ndemonstrate how a well-designed hybrid can overcome the limitations of its\nindividual components, offering new insights into the design principle of\nneural memory systems."
                },
                "authors": [
                    {
                        "name": "Kazuki Irie"
                    },
                    {
                        "name": "Morris Yau"
                    },
                    {
                        "name": "Samuel J. Gershman"
                    }
                ],
                "author_detail": {
                    "name": "Samuel J. Gershman"
                },
                "author": "Samuel J. Gershman",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03712v2",
                "updated": "2025-10-22T23:56:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    23,
                    56,
                    45,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-04T07:22:39Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    7,
                    22,
                    39,
                    5,
                    277,
                    0
                ],
                "title": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting and Preventing Latent Risk Accumulation in High-Performance\n  Software Systems"
                },
                "summary": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern distributed systems employ aggressive optimization strategies that\ncreate latent risks - hidden vulnerabilities where exceptional performance\nmasks catastrophic fragility when optimizations fail. Cache layers achieving\n99% hit rates can obscure database bottlenecks until cache failures trigger\n100x load amplification and cascading collapse. Current reliability engineering\nfocuses on reactive incident response rather than proactive detection of\noptimization-induced vulnerabilities. This paper presents the first\ncomprehensive framework for systematic latent risk detection, prevention, and\noptimization through integrated mathematical modeling, intelligent perturbation\ntesting, and risk-aware performance optimization. We introduce the Latent Risk\nIndex (LRI) that correlates strongly with incident severity (r=0.863, p<0.001),\nenabling predictive risk assessment. Our framework integrates three systems:\nHYDRA employing six optimization-aware perturbation strategies achieving 89.7%\nrisk discovery rates, RAVEN providing continuous production monitoring with\n92.9% precision and 93.8% recall across 1,748 scenarios, and APEX enabling\nrisk-aware optimization maintaining 96.6% baseline performance while reducing\nlatent risks by 59.2%. Evaluation across three testbed environments\ndemonstrates strong statistical validation with large effect sizes (Cohen\nd>2.0) and exceptional reproducibility (r>0.92). Production deployment over 24\nweeks shows 69.1% mean time to recovery reduction, 78.6% incident severity\nreduction, and 81 prevented incidents generating 1.44M USD average annual\nbenefits with 3.2-month ROI. Our approach transforms reliability engineering\nfrom reactive incident management to proactive risk-aware optimization."
                },
                "authors": [
                    {
                        "name": "Jahidul Arafat"
                    },
                    {
                        "name": "Kh. M. Moniruzzaman"
                    },
                    {
                        "name": "Shamim Hossain"
                    },
                    {
                        "name": "Fariha Tasmin"
                    }
                ],
                "author_detail": {
                    "name": "Fariha Tasmin"
                },
                "author": "Fariha Tasmin",
                "arxiv_comment": "26 pages, 12 tables, 4 figures. Academic-industry collaboration.\n  Framework (HYDRA, RAVEN, APEX) for optimization-induced vulnerabilities.\n  Evaluated: 2,160 configs, 12.7TB data, 1,748 scenarios",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68M15, 90B25, 68T05, 90C29",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; C.2.4; D.2.5; D.4.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19670v1",
                "updated": "2025-10-22T15:16:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T15:16:56Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    15,
                    16,
                    56,
                    2,
                    295,
                    0
                ],
                "title": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSense-LLM: Semantics at the Edge with Cost- and Uncertainty-Aware\n  Cloud-Edge Cooperation"
                },
                "summary": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CoSense-LLM, an edge-first framework that turns continuous\nmultimodal sensor streams (for example Wi-Fi CSI, IMU, audio, RFID, and\nlightweight vision) into compact, verifiable semantic tokens and coordinates\nwith large language models under explicit latency, energy, bandwidth, and\nprivacy constraints. CoSense-LLM has four parts: (i) SenseFusion, a lightweight\nencoder that aligns sensor embeddings with language and compresses them into\nshort discrete code sequences; (ii) Edge-RAG, a local hybrid retrieval layer\nthat grounds generation in site specific policies and notes; (iii)\nPromptRouter, a cost and uncertainty aware policy that selects edge only\ngeneration, edge plus retrieval, or compact cloud escalation; and (iv) Secure\nExecution, an auditable redaction path that enforces data minimization so raw\nwaveforms never leave the device. The system works with modern serving\noptimizations, including paged or streaming KV caches, FlashAttention style\nkernels, speculative decoding, and quantized LoRA adapters, and supports on\ndevice personalization and federated updates under non IID drift. Across home,\noffice, and clinic deployments, CoSense-LLM delivers grounded explanations\nwhile meeting tight service level objectives: it sustains sub second (p95) end\nto end latency on edge dominant paths, reduces inter tier token and bandwidth\ncosts by preferring local retrieval grounded responses, and preserves privacy\nby transmitting only discrete codes and redacted metadata. Ablations show that\nEdge-RAG improves factual consistency and reduces contradictions, calibrated\nuncertainty enables selective abstention and controlled escalations, and KV\nplus decoding accelerators lower energy per decision. The results support an\nedge first design that treats semantics, privacy, and predictable latency as co\nequal goals for large model deployments in interference prone environments."
                },
                "authors": [
                    {
                        "name": "Hasan Akgul"
                    },
                    {
                        "name": "Mari Eplik"
                    },
                    {
                        "name": "Javier Rojas"
                    },
                    {
                        "name": "Aina Binti Abdullah"
                    },
                    {
                        "name": "Pieter van der Merwe"
                    }
                ],
                "author_detail": {
                    "name": "Pieter van der Merwe"
                },
                "author": "Pieter van der Merwe",
                "arxiv_comment": "19 pages,8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; C.2.4; C.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08666v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08666v3",
                "updated": "2025-10-22T14:33:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    14,
                    33,
                    49,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-09T16:19:42Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    19,
                    42,
                    3,
                    282,
                    0
                ],
                "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dInfer: An Efficient Inference Framework for Diffusion Language Models"
                },
                "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer."
                },
                "authors": [
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Qian Xu"
                    },
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Guofeng Feng"
                    },
                    {
                        "name": "Guoshan Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Xiaojing Qi"
                    },
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Haibo Feng"
                    },
                    {
                        "name": "Ziyun Jiang"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Da Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Da Zheng"
                },
                "author": "Da Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08666v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08666v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19875v1",
                "updated": "2025-10-22T09:42:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    42,
                    29,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T09:42:29Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    9,
                    42,
                    29,
                    2,
                    295,
                    0
                ],
                "title": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs\n  via Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs\n  via Sparse Attention"
                },
                "summary": "As Large Language Models (LLMs) scale to million-token contexts, traditional\nMechanistic Interpretability techniques for analyzing attention scale\nquadratically with context length, demanding terabytes of memory beyond 100,000\ntokens. We introduce Sparse Tracing, a novel technique that leverages dynamic\nsparse attention to efficiently analyze long context attention patterns. We\npresent Stream, a compilable hierarchical pruning algorithm that estimates\nper-head sparse attention masks in near-linear time $O(T \\log T)$ and linear\nspace $O(T)$, enabling one-pass interpretability at scale. Stream performs a\nbinary-search-style refinement to retain only the top-$k$ key blocks per query\nwhile preserving the model's next-token behavior. We apply Stream to long\nchain-of-thought reasoning traces and identify thought anchors while pruning\n97-99\\% of token interactions. On the RULER benchmark, Stream preserves\ncritical retrieval paths while discarding 90-96\\% of interactions and exposes\nlayer-wise routes from the needle to output. Our method offers a practical\ndrop-in tool for analyzing attention patterns and tracing information flow\nwithout terabytes of caches. By making long context interpretability feasible\non consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.\nCode is available at https://anonymous.4open.science/r/stream-03B8/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) scale to million-token contexts, traditional\nMechanistic Interpretability techniques for analyzing attention scale\nquadratically with context length, demanding terabytes of memory beyond 100,000\ntokens. We introduce Sparse Tracing, a novel technique that leverages dynamic\nsparse attention to efficiently analyze long context attention patterns. We\npresent Stream, a compilable hierarchical pruning algorithm that estimates\nper-head sparse attention masks in near-linear time $O(T \\log T)$ and linear\nspace $O(T)$, enabling one-pass interpretability at scale. Stream performs a\nbinary-search-style refinement to retain only the top-$k$ key blocks per query\nwhile preserving the model's next-token behavior. We apply Stream to long\nchain-of-thought reasoning traces and identify thought anchors while pruning\n97-99\\% of token interactions. On the RULER benchmark, Stream preserves\ncritical retrieval paths while discarding 90-96\\% of interactions and exposes\nlayer-wise routes from the needle to output. Our method offers a practical\ndrop-in tool for analyzing attention patterns and tracing information flow\nwithout terabytes of caches. By making long context interpretability feasible\non consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.\nCode is available at https://anonymous.4open.science/r/stream-03B8/."
                },
                "authors": [
                    {
                        "name": "J Rosser"
                    },
                    {
                        "name": "Jos Luis Redondo Garca"
                    },
                    {
                        "name": "Gustavo Penha"
                    },
                    {
                        "name": "Konstantina Palla"
                    },
                    {
                        "name": "Hugues Bouchard"
                    }
                ],
                "author_detail": {
                    "name": "Hugues Bouchard"
                },
                "author": "Hugues Bouchard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T40",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.24761v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.24761v1",
                "updated": "2025-10-22T07:50:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    7,
                    50,
                    6,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T07:50:06Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    7,
                    50,
                    6,
                    2,
                    295,
                    0
                ],
                "title": "ODataX: A Progressive Evolution of the Open Data Protocol",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ODataX: A Progressive Evolution of the Open Data Protocol"
                },
                "summary": "The Open Data Protocol (OData) provides a standardized approach for building\nand consuming RESTful APIs with rich query capabilities. Despite its power and\nmaturity, OData adoption remains confined primarily to enterprise environments,\nparticularly within Microsoft and SAP ecosystems. This paper analyzes the key\nbarriers preventing wider OData adoption and introduces ODataX, an evolved\nversion of the protocol designed to address these limitations. ODataX maintains\nbackward compatibility with OData v4 while introducing progressive complexity\ndisclosure through simplified query syntax, built-in performance guardrails via\nquery cost estimation, and enhanced caching mechanisms. This work aims to\nbridge the gap between enterprise-grade query standardization and the\nsimplicity demanded by modern web development practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open Data Protocol (OData) provides a standardized approach for building\nand consuming RESTful APIs with rich query capabilities. Despite its power and\nmaturity, OData adoption remains confined primarily to enterprise environments,\nparticularly within Microsoft and SAP ecosystems. This paper analyzes the key\nbarriers preventing wider OData adoption and introduces ODataX, an evolved\nversion of the protocol designed to address these limitations. ODataX maintains\nbackward compatibility with OData v4 while introducing progressive complexity\ndisclosure through simplified query syntax, built-in performance guardrails via\nquery cost estimation, and enhanced caching mechanisms. This work aims to\nbridge the gap between enterprise-grade query standardization and the\nsimplicity demanded by modern web development practices."
                },
                "authors": [
                    {
                        "name": "Anirudh Ganesh"
                    },
                    {
                        "name": "Nitin Sood"
                    }
                ],
                "author_detail": {
                    "name": "Nitin Sood"
                },
                "author": "Nitin Sood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.24761v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.24761v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19264v1",
                "updated": "2025-10-22T05:47:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    5,
                    47,
                    41,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T05:47:41Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    5,
                    47,
                    41,
                    2,
                    295,
                    0
                ],
                "title": "LAPRAD: LLM-Assisted PRotocol Attack Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAPRAD: LLM-Assisted PRotocol Attack Discovery"
                },
                "summary": "With the goal of improving the security of Internet protocols, we seek\nfaster, semi-automatic methods to discover new vulnerabilities in protocols\nsuch as DNS, BGP, and others. To this end, we introduce the LLM-Assisted\nProtocol Attack Discovery (LAPRAD) methodology, enabling security researchers\nwith some DNS knowledge to efficiently uncover vulnerabilities that would\notherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM\n(GPT-o1) that has been trained on a broad corpus of DNS-related sources and\nprevious DDoS attacks to identify potential exploits. In the second stage, a\ndifferent LLM automatically constructs the corresponding attack configurations\nusing the ReACT approach implemented via LangChain (DNS zone file generation).\nFinally, in the third stage, we validate the attack's functionality and\neffectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and\nrediscovered two recently reported ones that were not included in the LLM's\ntraining data. The first new attack employs a bait-and-switch technique to\ntrick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving\ncapacity to as little as 6%. The second exploits large DNSSEC encryption\nalgorithms (RSA-4096) with multiple keys, thereby bypassing a recently\nimplemented default RRSet limit. The third leverages ANY-type responses to\nproduce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush,\ncircumvent existing patches, severely degrade resolver query capacity, and\nimpact the latest versions of major DNS resolver implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the goal of improving the security of Internet protocols, we seek\nfaster, semi-automatic methods to discover new vulnerabilities in protocols\nsuch as DNS, BGP, and others. To this end, we introduce the LLM-Assisted\nProtocol Attack Discovery (LAPRAD) methodology, enabling security researchers\nwith some DNS knowledge to efficiently uncover vulnerabilities that would\notherwise be hard to detect.\n  LAPRAD follows a three-stage process. In the first, we consult an LLM\n(GPT-o1) that has been trained on a broad corpus of DNS-related sources and\nprevious DDoS attacks to identify potential exploits. In the second stage, a\ndifferent LLM automatically constructs the corresponding attack configurations\nusing the ReACT approach implemented via LangChain (DNS zone file generation).\nFinally, in the third stage, we validate the attack's functionality and\neffectiveness.\n  Using LAPRAD, we uncovered three new DDoS attacks on the DNS protocol and\nrediscovered two recently reported ones that were not included in the LLM's\ntraining data. The first new attack employs a bait-and-switch technique to\ntrick resolvers into caching large, bogus DNSSEC RRSIGs, reducing their serving\ncapacity to as little as 6%. The second exploits large DNSSEC encryption\nalgorithms (RSA-4096) with multiple keys, thereby bypassing a recently\nimplemented default RRSet limit. The third leverages ANY-type responses to\nproduce a similar effect.\n  These variations of a cache-flushing DDoS attack, called SigCacheFlush,\ncircumvent existing patches, severely degrade resolver query capacity, and\nimpact the latest versions of major DNS resolver implementations."
                },
                "authors": [
                    {
                        "name": "R. Can Aygun"
                    },
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    },
                    {
                        "name": "Leonard Kleinrock"
                    }
                ],
                "author_detail": {
                    "name": "Leonard Kleinrock"
                },
                "arxiv_affiliation": "UCLA",
                "author": "Leonard Kleinrock",
                "arxiv_comment": "IFIP Networking 2025 Proceedings (Accepted on 05.05.2025)",
                "arxiv_journal_ref": "Published in IFIP Networking 2025 Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19183v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19183v1",
                "updated": "2025-10-22T02:41:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    41,
                    7,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T02:41:07Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    41,
                    7,
                    2,
                    295,
                    0
                ],
                "title": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models\n  through Adaptive KV Cache Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PruneHal: Reducing Hallucinations in Multi-modal Large Language Models\n  through Adaptive KV Cache Pruning"
                },
                "summary": "While multi-modal large language models (MLLMs) have made significant\nprogress in recent years, the issue of hallucinations remains a major\nchallenge. To mitigate this phenomenon, existing solutions either introduce\nadditional data for further training or incorporate external or internal\ninformation during inference. However, these approaches inevitably introduce\nextra computational costs. In this paper, we observe that hallucinations in\nMLLMs are strongly associated with insufficient attention allocated to visual\ntokens. In particular, the presence of redundant visual tokens disperses the\nmodel's attention, preventing it from focusing on the most informative ones. As\na result, critical visual cues are often under-attended, which in turn\nexacerbates the occurrence of hallucinations. Building on this observation, we\npropose \\textbf{PruneHal}, a training-free, simple yet effective method that\nleverages adaptive KV cache pruning to enhance the model's focus on critical\nvisual information, thereby mitigating hallucinations. To the best of our\nknowledge, we are the first to apply token pruning for hallucination mitigation\nin MLLMs. Notably, our method don't require additional training and incurs\nnearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be\nseamlessly integrated with different decoding strategies, including those\nspecifically designed for hallucination mitigation. We evaluate PruneHal on\nseveral widely used hallucination evaluation benchmarks using four mainstream\nMLLMs, achieving robust and outstanding results that highlight the\neffectiveness and superiority of our method. Our code will be publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While multi-modal large language models (MLLMs) have made significant\nprogress in recent years, the issue of hallucinations remains a major\nchallenge. To mitigate this phenomenon, existing solutions either introduce\nadditional data for further training or incorporate external or internal\ninformation during inference. However, these approaches inevitably introduce\nextra computational costs. In this paper, we observe that hallucinations in\nMLLMs are strongly associated with insufficient attention allocated to visual\ntokens. In particular, the presence of redundant visual tokens disperses the\nmodel's attention, preventing it from focusing on the most informative ones. As\na result, critical visual cues are often under-attended, which in turn\nexacerbates the occurrence of hallucinations. Building on this observation, we\npropose \\textbf{PruneHal}, a training-free, simple yet effective method that\nleverages adaptive KV cache pruning to enhance the model's focus on critical\nvisual information, thereby mitigating hallucinations. To the best of our\nknowledge, we are the first to apply token pruning for hallucination mitigation\nin MLLMs. Notably, our method don't require additional training and incurs\nnearly no extra inference cost. Moreover, PruneHal is model-agnostic and can be\nseamlessly integrated with different decoding strategies, including those\nspecifically designed for hallucination mitigation. We evaluate PruneHal on\nseveral widely used hallucination evaluation benchmarks using four mainstream\nMLLMs, achieving robust and outstanding results that highlight the\neffectiveness and superiority of our method. Our code will be publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Fengyuan Sun"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Xinhao Xu"
                    },
                    {
                        "name": "Dandan Zheng"
                    },
                    {
                        "name": "Jingdong Chen"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19183v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19183v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.19171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.19171v1",
                "updated": "2025-10-22T02:09:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    9,
                    23,
                    2,
                    295,
                    0
                ],
                "published": "2025-10-22T02:09:23Z",
                "published_parsed": [
                    2025,
                    10,
                    22,
                    2,
                    9,
                    23,
                    2,
                    295,
                    0
                ],
                "title": "Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop\n  RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think Straight, Stop Smart: Structured Reasoning for Efficient Multi-Hop\n  RAG"
                },
                "summary": "Multi-hop retrieval-augmented generation (RAG) is a promising strategy for\ncomplex reasoning, yet existing iterative prompting approaches remain\ninefficient. They often regenerate predictable token sequences at every step\nand rely on stochastic stopping, leading to excessive token usage and unstable\ntermination. We propose TSSS (Think Straight, Stop Smart), a structured\nmulti-hop RAG framework designed for efficiency. TSSS introduces (i) a\ntemplate-based reasoning that caches recurring prefixes and anchors sub-queries\nto the main question, reducing token generation cost while promoting stable\nreasoning, and (ii) a retriever-based terminator, which deterministically halts\nreasoning once additional sub-queries collapse into repetition. This separation\nof structured reasoning and termination control enables both faster inference\nand more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS\nachieves state-of-the-art accuracy and competitive efficiency among RAG-CoT\napproaches, highlighting its effectiveness in efficiency-constrained scenarios\nsuch as on-device inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop retrieval-augmented generation (RAG) is a promising strategy for\ncomplex reasoning, yet existing iterative prompting approaches remain\ninefficient. They often regenerate predictable token sequences at every step\nand rely on stochastic stopping, leading to excessive token usage and unstable\ntermination. We propose TSSS (Think Straight, Stop Smart), a structured\nmulti-hop RAG framework designed for efficiency. TSSS introduces (i) a\ntemplate-based reasoning that caches recurring prefixes and anchors sub-queries\nto the main question, reducing token generation cost while promoting stable\nreasoning, and (ii) a retriever-based terminator, which deterministically halts\nreasoning once additional sub-queries collapse into repetition. This separation\nof structured reasoning and termination control enables both faster inference\nand more reliable answers. On HotpotQA, 2WikiMultiHop, and MuSiQue, TSSS\nachieves state-of-the-art accuracy and competitive efficiency among RAG-CoT\napproaches, highlighting its effectiveness in efficiency-constrained scenarios\nsuch as on-device inference."
                },
                "authors": [
                    {
                        "name": "Jihwan Bang"
                    },
                    {
                        "name": "Juntae Lee"
                    },
                    {
                        "name": "Seunghan Yang"
                    },
                    {
                        "name": "Sungha Choi"
                    }
                ],
                "author_detail": {
                    "name": "Sungha Choi"
                },
                "author": "Sungha Choi",
                "arxiv_comment": "Accepted at NeurIPS 2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.19171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.19171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11857v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11857v3",
                "updated": "2025-10-21T22:37:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    22,
                    37,
                    11,
                    1,
                    294,
                    0
                ],
                "published": "2024-10-04T15:23:28Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    23,
                    28,
                    4,
                    278,
                    0
                ],
                "title": "LLMBridge: Reducing Costs to Access LLMs in a Prompt-Centric Internet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMBridge: Reducing Costs to Access LLMs in a Prompt-Centric Internet"
                },
                "summary": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality."
                },
                "authors": [
                    {
                        "name": "Noah Martin"
                    },
                    {
                        "name": "Abdullah Bin Faisal"
                    },
                    {
                        "name": "Hiba Eltigani"
                    },
                    {
                        "name": "Rukhshan Haroon"
                    },
                    {
                        "name": "Swaminathan Lamelas"
                    },
                    {
                        "name": "Fahad Dogar"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Dogar"
                },
                "author": "Fahad Dogar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11857v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11857v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18191v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18191v3",
                "updated": "2025-10-21T21:07:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    21,
                    7,
                    17,
                    1,
                    294,
                    0
                ],
                "published": "2025-03-23T20:18:16Z",
                "published_parsed": [
                    2025,
                    3,
                    23,
                    20,
                    18,
                    16,
                    6,
                    82,
                    0
                ],
                "title": "DFUSE: Strongly Consistent Write-Back Kernel Caching for Distributed\n  Userspace File Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DFUSE: Strongly Consistent Write-Back Kernel Caching for Distributed\n  Userspace File Systems"
                },
                "summary": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, a limitation that has kept FUSE distributed file systems out\nof write-intensive cloud workloads.\n  To this end, we present DFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DFUSE achieves this\nby offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DFUSE achieves\nup to 68.0% higher throughput and 40.4% lower latency than the existing\nwrite-through design of FUSE-based distributed file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud platforms host thousands of tenants that demand POSIX semantics, high\nthroughput, and rapid evolution from their storage layer. Kernel-native\ndistributed file systems supply raw speed, but their privileged code base\ncouples every release to the kernel, widens the blast radius of crashes, and\nslows innovation. FUSE-based distributed file systems flip those trade-offs:\nthey run in user space for fast deployment and strong fault isolation, yet the\nFUSE interface disables the kernel's write-back page cache whenever strong\nconsistency is required. Practitioners must therefore choose between (i) weak\nconsistency with fast write-back caching or (ii) strong consistency with slow\nwrite-through I/O, a limitation that has kept FUSE distributed file systems out\nof write-intensive cloud workloads.\n  To this end, we present DFUSE, the first distributed FUSE file system that\ndelivers write-back kernel caching and strong consistency. DFUSE achieves this\nby offloading userspace consistency control to the kernel driver, allowing\ncoordinated access to the kernel's page cache across nodes. This design\neliminates blind local cache updates and ensures cluster-wide strong\nconsistency without compromising performance. In our evaluation, DFUSE achieves\nup to 68.0% higher throughput and 40.4% lower latency than the existing\nwrite-through design of FUSE-based distributed file systems."
                },
                "authors": [
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Jingkai Fu"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Windsor Hsu"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "arxiv_doi": "10.1145/3772052.3772208",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772052.3772208",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18191v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18191v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This is the version accepted to ACM SoCC 2025. The title has been\n  updated to match the published version",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.15878v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.15878v2",
                "updated": "2025-10-21T16:32:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    16,
                    32,
                    50,
                    1,
                    294,
                    0
                ],
                "published": "2025-08-21T16:10:26Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    10,
                    26,
                    3,
                    233,
                    0
                ],
                "title": "Putting the Context back into Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Putting the Context back into Memory"
                },
                "summary": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Requests arriving at main memory are often different from what programmers\ncan observe or estimate by using CPU-based monitoring. Hardware cache\nprefetching, memory request scheduling and interleaving cause a loss of\nobservability that limits potential data movement and tiering optimizations. In\nresponse, memory-side telemetry hardware like page access heat map units (HMU)\nand page prefetchers were proposed to inform Operating Systems with accurate\nusage data. However, it is still hard to map memory activity to software\nprogram functions and objects because of the decoupled nature of host\nprocessors and memory devices. Valuable program context is stripped out from\nthe memory bus, leaving only commands, addresses and data. Programmers have\nexpert knowledge of future data accesses, priorities, and access to processor\nstate, which could be useful hints for runtime memory device optimization. This\npaper makes context visible at memory devices by encoding any user-visible\nstate as detectable packets in the memory read address stream, in a\nnondestructive manner without significant capacity overhead, drivers or special\naccess privileges. We prototyped an end-to-end system with metadata injection\nthat can be reliably detected and decoded from a memory address trace, either\nby a host processor, or a memory module. We illustrate a use case with precise\ncode execution markers and object address range tracking. In the future, real\ntime metadata decoding with near-memory computing (NMC) could provide\ncustomized telemetry and statistics to users, or act on application hints to\nperform functions like prioritizing requests, remapping data and reconfiguring\ndevices."
                },
                "authors": [
                    {
                        "name": "David A. Roberts"
                    }
                ],
                "author_detail": {
                    "name": "David A. Roberts"
                },
                "author": "David A. Roberts",
                "arxiv_comment": "Fixed errors in paragraph numbering",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.15878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.15878v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18716v1",
                "updated": "2025-10-21T15:17:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    17,
                    37,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T15:17:37Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    17,
                    37,
                    1,
                    294,
                    0
                ],
                "title": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SSD: Spatial-Semantic Head Decoupling for Efficient Autoregressive Image\n  Generation"
                },
                "summary": "Autoregressive image generation models like Janus-Pro produce high-quality\nimages, but at the significant cost of high memory and ever-growing\ncomputational demands due to the large number of visual tokens. While KV cache\ncompression has been extensively studied in language modeling, it still remains\nlargely unexplored for the image generation domain. In this work, we begin by\nidentifying a distinct and prominent attention phenomenon, which we term\nspatial locality and emergent semantic sink. To leverage this key insight, we\nintroduce a novel KV cache compression framework. Specifically, we compress the\nKV cache for all visual tokens by adaptively decoupling attention heads into\ntwo separate types: for spatial-locality heads, our method maintains a short\nrecent token window; for semantic-sink heads, it strategically preserves a\ncompact set of highly-attended tokens. Our extensive experiments demonstrate\nthat the proposed method achieves a 5$\\times$ reduction in memory usage and a\nnotable 6.6$\\times$ speedup in overall throughput with only minimal visual\nquality loss, thereby enabling highly efficient native autoregressive image\ngeneration on resource-constrained hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive image generation models like Janus-Pro produce high-quality\nimages, but at the significant cost of high memory and ever-growing\ncomputational demands due to the large number of visual tokens. While KV cache\ncompression has been extensively studied in language modeling, it still remains\nlargely unexplored for the image generation domain. In this work, we begin by\nidentifying a distinct and prominent attention phenomenon, which we term\nspatial locality and emergent semantic sink. To leverage this key insight, we\nintroduce a novel KV cache compression framework. Specifically, we compress the\nKV cache for all visual tokens by adaptively decoupling attention heads into\ntwo separate types: for spatial-locality heads, our method maintains a short\nrecent token window; for semantic-sink heads, it strategically preserves a\ncompact set of highly-attended tokens. Our extensive experiments demonstrate\nthat the proposed method achieves a 5$\\times$ reduction in memory usage and a\nnotable 6.6$\\times$ speedup in overall throughput with only minimal visual\nquality loss, thereby enabling highly efficient native autoregressive image\ngeneration on resource-constrained hardware."
                },
                "authors": [
                    {
                        "name": "Siyong Jian"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14576v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14576v3",
                "updated": "2025-10-21T15:13:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    15,
                    13,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2024-01-26T00:27:00Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    0,
                    27,
                    0,
                    4,
                    26,
                    0
                ],
                "title": "ParaLog: Consistent Host-side Logging for Parallel Checkpoints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ParaLog: Consistent Host-side Logging for Parallel Checkpoints"
                },
                "summary": "Output-intensive scientific applications are highly sensitive to low storage\nthroughput. While existing scientific application stacks are optimized for\ntraditional High-Performance Computing (HPC) environments with high remote\nstorage and network bandwidth, these assumptions often fail in modern settings\nlike cloud deployment. This is because the existing scientific application I/O\nstack fails to leverage the available resources. At the same time, scientific\napplications exhibit special synchronization and data output requirements that\nare difficult to satisfy using traditional approaches such as block-level or\nfilesystem-level caching. We introduce ParaLog, a distributed host-side logging\napproach designed to accelerate scientific applications transparently. ParaLog\nemphasizes deployability, enabling support for unmodified message passing\ninterface (MPI) applications and implementations while preserving crash\nconsistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC,\nlocal clusters, and hybrid environments, demonstrating its capability to reduce\nend-to-end execution time by 13-26% for popular scientific applications in\ncloud settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Output-intensive scientific applications are highly sensitive to low storage\nthroughput. While existing scientific application stacks are optimized for\ntraditional High-Performance Computing (HPC) environments with high remote\nstorage and network bandwidth, these assumptions often fail in modern settings\nlike cloud deployment. This is because the existing scientific application I/O\nstack fails to leverage the available resources. At the same time, scientific\napplications exhibit special synchronization and data output requirements that\nare difficult to satisfy using traditional approaches such as block-level or\nfilesystem-level caching. We introduce ParaLog, a distributed host-side logging\napproach designed to accelerate scientific applications transparently. ParaLog\nemphasizes deployability, enabling support for unmodified message passing\ninterface (MPI) applications and implementations while preserving crash\nconsistency semantics. We evaluate ParaLog across traditional HPC, cloud HPC,\nlocal clusters, and hybrid environments, demonstrating its capability to reduce\nend-to-end execution time by 13-26% for popular scientific applications in\ncloud settings."
                },
                "authors": [
                    {
                        "name": "Steven W. D. Chien"
                    },
                    {
                        "name": "Kento Sato"
                    },
                    {
                        "name": "Artur Podobas"
                    },
                    {
                        "name": "Niclas Jansson"
                    },
                    {
                        "name": "Stefano Markidis"
                    },
                    {
                        "name": "Michio Honda"
                    }
                ],
                "author_detail": {
                    "name": "Michio Honda"
                },
                "author": "Michio Honda",
                "arxiv_doi": "10.1145/3772052.3772212",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3772052.3772212",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.14576v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14576v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SoCC 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18672v1",
                "updated": "2025-10-21T14:25:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T14:25:51Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    14,
                    25,
                    51,
                    1,
                    294,
                    0
                ],
                "title": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Language Model Inference Serving Unveiled: An Empirical Study"
                },
                "summary": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning large language model (RLLM) has been proven competitive in\nsolving complex reasoning tasks such as mathematics, coding, compared to\ngeneral LLM. However, the serving performance and behavior of RLLM remains\nunexplored, which may undermine the deployment and utilization of RLLM in\nreal-world scenario. To close this gap, in this paper, we conduct a\ncomprehensive study of RLLM service. We first perform a pilot study on\ncomparing the serving performance between RLLM and traditional LLM and reveal\nthat there are several distinct differences regarding serving behavior: (1)\nsignificant memory usage and fluctuations; (2) straggler requests; (3) adaptive\nrunning time; (4) domain preference. Then we further investigate whether\nexisting inference optimization techniques are valid for RLLM. Our main\ntakeaways are that model quantization methods and speculative decoding can\nimprove service system efficiency with small compromise to RLLM accuracy, while\nprefix caching, KV cache quantization may even degrade accuracy or serving\nperformance for small RLLM. Lastly, we conduct evaluation under real world\nworkload modeled by Gamma distribution to verify our findings. Empirical\nresults of real world workload evaluation across different dataset are aligned\nwith our main findings regarding RLLM serving. We hope our work can provide the\nresearch community and industry with insights to advance RLLM inference\nserving."
                },
                "authors": [
                    {
                        "name": "Qi Li"
                    },
                    {
                        "name": "Junpan Wu"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Yuhan Chen"
                    },
                    {
                        "name": "Shaohuai Shi"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18546v1",
                "updated": "2025-10-21T11:52:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T11:52:44Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    11,
                    52,
                    44,
                    1,
                    294,
                    0
                ],
                "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation\n  Map Caching and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation\n  Map Caching and Retrieval"
                },
                "summary": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-goal navigation (ObjNav) tasks an agent with navigating to the\nlocation of a specific object in an unseen environment. Embodied agents\nequipped with large language models (LLMs) and online constructed navigation\nmaps can perform ObjNav in a zero-shot manner. However, existing agents heavily\nrely on giant LLMs on the cloud, e.g., GPT-4, while directly switching to small\nLLMs, e.g., LLaMA3.2-11b, suffer from significant success rate drops due to\nlimited model capacity for understanding complex navigation maps, which\nprevents deploying ObjNav on local devices. At the same time, the long prompt\nintroduced by the navigation map description will cause high planning latency\non local devices. In this paper, we propose EfficientNav to enable on-device\nefficient LLM-based zero-shot ObjNav. To help the smaller LLMs better\nunderstand the environment, we propose semantics-aware memory retrieval to\nprune redundant information in navigation maps. To reduce planning latency, we\npropose discrete memory caching and attention-based memory clustering to\nefficiently save and re-use the KV cache. Extensive experimental results\ndemonstrate that EfficientNav achieves 11.1% improvement in success rate on\nHM3D benchmark over GPT-4-based baselines, and demonstrates 6.7x real-time\nlatency reduction and 4.7x end-to-end latency reduction over GPT-4 planner. Our\ncode will be released soon."
                },
                "authors": [
                    {
                        "name": "Zebin Yang"
                    },
                    {
                        "name": "Sunjian Zheng"
                    },
                    {
                        "name": "Tong Xie"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Bo Yu"
                    },
                    {
                        "name": "Fan Wang"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Shaoshan Liu"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02175v2",
                "updated": "2025-10-21T10:33:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    33,
                    29,
                    1,
                    294,
                    0
                ],
                "published": "2025-02-04T09:48:14Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    48,
                    14,
                    1,
                    35,
                    0
                ],
                "title": "VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive\n  Token Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive\n  Token Caching"
                },
                "summary": "Vision-Language-Action (VLA) models have demonstrated strong multi-modal\nreasoning capabilities, enabling direct action generation from visual\nperception and language instructions in an end-to-end manner. However, their\nsubstantial computational cost poses a challenge for real-time robotic control,\nwhere rapid decision-making is essential. This paper introduces VLA-Cache, a\ntraining-free inference acceleration method that reduces computational overhead\nby adaptively caching and reusing static visual tokens across frames.\nExploiting the temporal continuity in robotic manipulation, VLA-Cache\nidentifies minimally changed tokens between adjacent frames and reuses their\ncached key-value representations, thereby circumventing redundant computations.\nAdditionally, to maintain action precision, VLA-Cache selectively re-computes\ntask-relevant tokens that are environmentally sensitive, ensuring the fidelity\nof critical visual information. To further optimize efficiency, we introduce a\nlayer adaptive token reusing strategy that dynamically adjusts the reuse ratio\nbased on attention concentration across decoder layers, prioritizing critical\ntokens for recomputation. Extensive experiments on two simulation platforms\n(LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache\nachieves up to 1.7x speedup in CUDA latency and a 15% increase in control\nfrequency, with negligible loss on task success rate. The code and videos can\nbe found at our project page: https://vla-cache.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have demonstrated strong multi-modal\nreasoning capabilities, enabling direct action generation from visual\nperception and language instructions in an end-to-end manner. However, their\nsubstantial computational cost poses a challenge for real-time robotic control,\nwhere rapid decision-making is essential. This paper introduces VLA-Cache, a\ntraining-free inference acceleration method that reduces computational overhead\nby adaptively caching and reusing static visual tokens across frames.\nExploiting the temporal continuity in robotic manipulation, VLA-Cache\nidentifies minimally changed tokens between adjacent frames and reuses their\ncached key-value representations, thereby circumventing redundant computations.\nAdditionally, to maintain action precision, VLA-Cache selectively re-computes\ntask-relevant tokens that are environmentally sensitive, ensuring the fidelity\nof critical visual information. To further optimize efficiency, we introduce a\nlayer adaptive token reusing strategy that dynamically adjusts the reuse ratio\nbased on attention concentration across decoder layers, prioritizing critical\ntokens for recomputation. Extensive experiments on two simulation platforms\n(LIBERO and SIMPLER) and a real-world robotic system demonstrate that VLA-Cache\nachieves up to 1.7x speedup in CUDA latency and a 15% increase in control\nfrequency, with negligible loss on task success rate. The code and videos can\nbe found at our project page: https://vla-cache.github.io."
                },
                "authors": [
                    {
                        "name": "Siyu Xu"
                    },
                    {
                        "name": "Yunke Wang"
                    },
                    {
                        "name": "Chenghao Xia"
                    },
                    {
                        "name": "Dihao Zhu"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06436v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06436v2",
                "updated": "2025-10-21T10:08:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    10,
                    8,
                    33,
                    1,
                    294,
                    0
                ],
                "published": "2025-09-08T08:34:02Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    8,
                    34,
                    2,
                    0,
                    251,
                    0
                ],
                "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree of Agents: Improving Long-Context Capabilities of Large Language\n  Models through Multi-Perspective Reasoning"
                },
                "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents."
                },
                "authors": [
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Xiaofei Xu"
                    },
                    {
                        "name": "Ke Deng"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Lin Tian"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tian"
                },
                "author": "Lin Tian",
                "arxiv_comment": "19 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06436v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06436v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18413v1",
                "updated": "2025-10-21T08:44:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    8,
                    44,
                    47,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T08:44:47Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    8,
                    44,
                    47,
                    1,
                    294,
                    0
                ],
                "title": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference"
                },
                "summary": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) now support context windows of hundreds of\nthousands to millions of tokens, enabling applications such as long-document\nsummarization, large-scale code synthesis, multi-document question answering\nand persistent multi-turn dialogue. However, such extended contexts exacerbate\nthe quadratic cost of self-attention, leading to severe latency in\nautoregressive decoding. Existing sparse attention methods alleviate these\ncosts but rely on heuristic patterns that struggle to recall critical key-value\n(KV) pairs for each query, resulting in accuracy degradation. We introduce\nAdamas, a lightweight yet highly accurate sparse attention mechanism designed\nfor long-context inference. Adamas applies the Hadamard transform,\nbucketization and 2-bit compression to produce compact representations, and\nleverages Manhattan-distance estimation for efficient top-k selections.\nExperiments show that Adamas matches the accuracy of full attention with only a\n64-token budget, achieves near-lossless performance at 128, and supports up to\n8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering\nup to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.\nRemarkably, Adamas attains comparable or even lower perplexity than full\nattention, underscoring its effectiveness in maintaining accuracy under\naggressive sparsity."
                },
                "authors": [
                    {
                        "name": "Siyuan Yan"
                    },
                    {
                        "name": "Guo-Qing Jiang"
                    },
                    {
                        "name": "Yuchen Zhang"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    },
                    {
                        "name": "Ran Zhu"
                    },
                    {
                        "name": "Chun Cao"
                    },
                    {
                        "name": "Jingwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jingwei Xu"
                },
                "author": "Jingwei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v5",
                "updated": "2025-10-21T06:47:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    6,
                    47,
                    29,
                    1,
                    294,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was first submitted for review on Sept. 5, 2024, and the\n  initial version was uploaded to Arxiv on Sept. 30, 2024. The latest version\n  has accepted for publication by IEEE Transactions on Information Forensics\n  and Security (TIFS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14374v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14374v3",
                "updated": "2025-10-21T06:30:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    6,
                    30,
                    21,
                    1,
                    294,
                    0
                ],
                "published": "2025-04-19T18:25:20Z",
                "published_parsed": [
                    2025,
                    4,
                    19,
                    18,
                    25,
                    20,
                    5,
                    109,
                    0
                ],
                "title": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fast MPI-based Distributed Hash-Table as Surrogate Model demonstrated\n  in a coupled reactive transport HPC simulation"
                },
                "summary": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surrogate models can play a pivotal role in enhancing performance in\ncontemporary High-Performance Computing applications. Cache-based surrogates\nuse already calculated simulation results to interpolate or extrapolate further\nsimulation output values. But this approach only pays off if the access time to\nretrieve the needed values is much faster than the actual simulation. While the\nmost existing key-value stores use a Client-Server architecture with dedicated\nstorage nodes, this is not the most suitable architecture for HPC applications.\nInstead, we propose a distributed architecture where the parallel processes\noffer a part of their available memory to build a shared distributed hash table\nbased on MPI. This paper presents three DHT approaches with the special\nrequirements of HPC applications in mind. The presented lock-free design\noutperforms both DHT versions which use explicit synchronization by\ncoarse-grained resp. fine-grained locking. The lock-free DHT shows very good\nscaling regarding read and write performance. The runtime of a coupled reactive\ntransport simulation was improved between 14% and 42% using the lock-free DHT\nas a surrogate model."
                },
                "authors": [
                    {
                        "name": "Max Lbke"
                    },
                    {
                        "name": "Marco De Lucia"
                    },
                    {
                        "name": "Steffen Christgau"
                    },
                    {
                        "name": "Stefan Petri"
                    },
                    {
                        "name": "Bettina Schnor"
                    }
                ],
                "author_detail": {
                    "name": "Bettina Schnor"
                },
                "author": "Bettina Schnor",
                "arxiv_doi": "10.1007/978-3-031-97635-3_28",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-97635-3_28",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.14374v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14374v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Long version, 15 pages, 6 figures; Short version (8 pages) included\n  in the proceedings of \"25th International Conference on Computational\n  Science\" (ICCS25)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.18269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.18269v1",
                "updated": "2025-10-21T03:39:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    21,
                    3,
                    39,
                    41,
                    1,
                    294,
                    0
                ],
                "published": "2025-10-21T03:39:41Z",
                "published_parsed": [
                    2025,
                    10,
                    21,
                    3,
                    39,
                    41,
                    1,
                    294,
                    0
                ],
                "title": "StreamingTOM: Streaming Token Compression for Efficient Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingTOM: Streaming Token Compression for Efficient Video\n  Understanding"
                },
                "summary": "Unlike offline processing, streaming video vision-language models face two\nfundamental constraints: causality and accumulation. Causality prevents access\nto future frames that offline methods exploit, while accumulation causes tokens\nto grow unbounded, creating efficiency bottlenecks. However, existing\napproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill\nunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage\nframework that addresses both pre-LLM and post-LLM bottlenecks with predictable\nlatency. Causal Temporal Reduction imposes a fixed per-frame budget and selects\ntokens based on adjacent-frame changes and token saliency, drastically reducing\nper-frame prefill cost by processing only a compact subset of visual tokens per\nframe instead of all visual tokens. Online Quantized Memory stores tokens in\n4-bit format, retrieves relevant groups on demand, and dequantizes them,\nkeeping the active kv-cache bounded regardless of stream length. Experiments\ndemonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$\nlower peak memory and $2\\times$ faster TTFT compared to prior SOTA.\nStreamingTOM maintains state-of-the-art accuracy among training-free methods\nwith an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS.\nThese results highlight the practical benefits of our two-stage approach for\nefficient streaming video understanding with bounded growth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike offline processing, streaming video vision-language models face two\nfundamental constraints: causality and accumulation. Causality prevents access\nto future frames that offline methods exploit, while accumulation causes tokens\nto grow unbounded, creating efficiency bottlenecks. However, existing\napproaches only regulate post-LLM kv-cache, leaving costly pre-LLM prefill\nunchanged. We introduce StreamingTOM, a training-free, plug-and-play two-stage\nframework that addresses both pre-LLM and post-LLM bottlenecks with predictable\nlatency. Causal Temporal Reduction imposes a fixed per-frame budget and selects\ntokens based on adjacent-frame changes and token saliency, drastically reducing\nper-frame prefill cost by processing only a compact subset of visual tokens per\nframe instead of all visual tokens. Online Quantized Memory stores tokens in\n4-bit format, retrieves relevant groups on demand, and dequantizes them,\nkeeping the active kv-cache bounded regardless of stream length. Experiments\ndemonstrate our method achieves $15.7\\times$ kv-cache compression, $1.2\\times$\nlower peak memory and $2\\times$ faster TTFT compared to prior SOTA.\nStreamingTOM maintains state-of-the-art accuracy among training-free methods\nwith an average of $63.8\\%$ on offline benchmarks and $55.8\\%/3.7$ on RVS.\nThese results highlight the practical benefits of our two-stage approach for\nefficient streaming video understanding with bounded growth."
                },
                "authors": [
                    {
                        "name": "Xueyi Chen"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Kele Shao"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.18269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.18269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17777v1",
                "updated": "2025-10-20T17:35:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    35,
                    47,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T17:35:47Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    17,
                    35,
                    47,
                    0,
                    293,
                    0
                ],
                "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference"
                },
                "summary": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and\ntextual reasoning, powering applications across high-resolution image\nunderstanding, long-video analysis, and multi-turn conversation. However, their\nscalability remains limited by the growing number of visual tokens that\ndominate inference latency. We present SparseVILA, a new paradigm for efficient\nVLM inference that decouples visual sparsity across the prefilling and decoding\nstages. SparseVILA distributes sparsity across stages by pruning redundant\nvisual tokens during prefill and retrieving only query-relevant tokens during\ndecoding. This decoupled design matches leading prefill pruning methods while\npreserving multi-turn fidelity by retaining most of the visual cache so that\nquery-aware tokens can be retrieved at each conversation round. Built on an\nAWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster\nprefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end\nspeedup on long-context video tasks -- while improving accuracy on\ndocument-understanding and reasoning tasks. By decoupling query-agnostic\npruning and query-aware retrieval, SparseVILA establishes a new direction for\nefficient multimodal inference, offering a training-free, architecture-agnostic\nframework for accelerating large VLMs without sacrificing capability."
                },
                "authors": [
                    {
                        "name": "Samir Khaki"
                    },
                    {
                        "name": "Junxian Guo"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Konstantinos N. Plataniotis"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Zhijian Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhijian Liu"
                },
                "author": "Zhijian Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17238v1",
                "updated": "2025-10-20T07:27:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T07:27:37Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    7,
                    27,
                    37,
                    0,
                    293,
                    0
                ],
                "title": "StreamingThinker: Large Language Models Can Think While Reading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingThinker: Large Language Models Can Think While Reading"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}"
                },
                "authors": [
                    {
                        "name": "Junlong Tong"
                    },
                    {
                        "name": "Yingqi Fan"
                    },
                    {
                        "name": "Anhao Zhao"
                    },
                    {
                        "name": "Yunpu Ma"
                    },
                    {
                        "name": "Xiaoyu Shen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyu Shen"
                },
                "author": "Xiaoyu Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17098v1",
                "updated": "2025-10-20T02:04:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    20,
                    2,
                    4,
                    18,
                    0,
                    293,
                    0
                ],
                "published": "2025-10-20T02:04:18Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    2,
                    4,
                    18,
                    0,
                    293,
                    0
                ],
                "title": "Can Transformer Memory Be Corrupted? Investigating Cache-Side\n  Vulnerabilities in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Transformer Memory Be Corrupted? Investigating Cache-Side\n  Vulnerabilities in Large Language Models"
                },
                "summary": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even when prompts and parameters are secured, transformer language models\nremain vulnerable because their key-value (KV) cache during inference\nconstitutes an overlooked attack surface. This paper introduces Malicious Token\nInjection (MTI), a modular framework that systematically perturbs cached key\nvectors at selected layers and timesteps through controlled magnitude and\nfrequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A\ntheoretical analysis quantifies how these perturbations propagate through\nattention, linking logit deviations to the Frobenius norm of corruption and\nsoftmax Lipschitz dynamics. Empirical results show that MTI significantly\nalters next-token distributions and downstream task performance across GPT-2\nand LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic\nreasoning pipelines. These findings identify cache integrity as a critical yet\nunderexplored vulnerability in current LLM deployments, positioning cache\ncorruption as a reproducible and theoretically grounded threat model for future\nrobustness and security research."
                },
                "authors": [
                    {
                        "name": "Elias Hossain"
                    },
                    {
                        "name": "Swayamjit Saha"
                    },
                    {
                        "name": "Somshubhra Roy"
                    },
                    {
                        "name": "Ravi Prasad"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Prasad"
                },
                "author": "Ravi Prasad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.27690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27690v1",
                "updated": "2025-10-31T17:59:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    59,
                    3,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:59:03Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    59,
                    3,
                    4,
                    304,
                    0
                ],
                "title": "Soft Gravitons, Hard Truths: Infrared Safety of Particle Processes in a\n  Gravitational-Wave Background",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft Gravitons, Hard Truths: Infrared Safety of Particle Processes in a\n  Gravitational-Wave Background"
                },
                "summary": "Gravitational waves are thought to propagate unattenuated through matter due\nto a cancellation between graviton absorption and stimulated emission inferred\nfrom leading-order soft-graviton arguments. We revisit this reasoning and show\nthat it fails for the converse problem: the effect of a gravitational-wave\nbackground on matter. For unstable particles, real graviton emission \\emph{and}\nabsorption appear to enhance decay rates. By extending the soft-graviton\nframework describing real and virtual processes in a gravitational wave\nbackground, and resumming them to all orders, we show that inclusive decay\nrates remain essentially unchanged. The mutual transparency between matter and\ngravitational radiation thus follows from infrared safety, and not from a\nfortuitous cancellation in the lowest-order approximation of exclusive rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational waves are thought to propagate unattenuated through matter due\nto a cancellation between graviton absorption and stimulated emission inferred\nfrom leading-order soft-graviton arguments. We revisit this reasoning and show\nthat it fails for the converse problem: the effect of a gravitational-wave\nbackground on matter. For unstable particles, real graviton emission \\emph{and}\nabsorption appear to enhance decay rates. By extending the soft-graviton\nframework describing real and virtual processes in a gravitational wave\nbackground, and resumming them to all orders, we show that inclusive decay\nrates remain essentially unchanged. The mutual transparency between matter and\ngravitational radiation thus follows from infrared safety, and not from a\nfortuitous cancellation in the lowest-order approximation of exclusive rates."
                },
                "authors": [
                    {
                        "name": "Wen-Yuan Ai"
                    },
                    {
                        "name": "Sebastian A. R. Ellis"
                    },
                    {
                        "name": "Josef Pradler"
                    }
                ],
                "author_detail": {
                    "name": "Josef Pradler"
                },
                "author": "Josef Pradler",
                "arxiv_comment": "8 pages, revtex format",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27688v1",
                "updated": "2025-10-31T17:58:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    58,
                    11,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:58:11Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    58,
                    11,
                    4,
                    304,
                    0
                ],
                "title": "Continuous Autoregressive Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Autoregressive Language Models"
                },
                "summary": "The efficiency of large language models (LLMs) is fundamentally limited by\ntheir sequential, token-by-token generation process. We argue that overcoming\nthis bottleneck requires a new design axis for LLM scaling: increasing the\nsemantic bandwidth of each generative step. To this end, we introduce\nContinuous Autoregressive Language Models (CALM), a paradigm shift from\ndiscrete next-token prediction to continuous next-vector prediction. CALM uses\na high-fidelity autoencoder to compress a chunk of K tokens into a single\ncontinuous vector, from which the original tokens can be reconstructed with\nover 99.9\\% accuracy. This allows us to model language as a sequence of\ncontinuous vectors instead of discrete tokens, which reduces the number of\ngenerative steps by a factor of K. The paradigm shift necessitates a new\nmodeling toolkit; therefore, we develop a comprehensive likelihood-free\nframework that enables robust training, evaluation, and controllable sampling\nin the continuous domain. Experiments show that CALM significantly improves the\nperformance-compute trade-off, achieving the performance of strong discrete\nbaselines at a significantly lower computational cost. More importantly, these\nfindings establish next-vector prediction as a powerful and scalable pathway\ntowards ultra-efficient language models. Code:\nhttps://github.com/shaochenze/calm. Project:\nhttps://shaochenze.github.io/blog/2025/CALM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large language models (LLMs) is fundamentally limited by\ntheir sequential, token-by-token generation process. We argue that overcoming\nthis bottleneck requires a new design axis for LLM scaling: increasing the\nsemantic bandwidth of each generative step. To this end, we introduce\nContinuous Autoregressive Language Models (CALM), a paradigm shift from\ndiscrete next-token prediction to continuous next-vector prediction. CALM uses\na high-fidelity autoencoder to compress a chunk of K tokens into a single\ncontinuous vector, from which the original tokens can be reconstructed with\nover 99.9\\% accuracy. This allows us to model language as a sequence of\ncontinuous vectors instead of discrete tokens, which reduces the number of\ngenerative steps by a factor of K. The paradigm shift necessitates a new\nmodeling toolkit; therefore, we develop a comprehensive likelihood-free\nframework that enables robust training, evaluation, and controllable sampling\nin the continuous domain. Experiments show that CALM significantly improves the\nperformance-compute trade-off, achieving the performance of strong discrete\nbaselines at a significantly lower computational cost. More importantly, these\nfindings establish next-vector prediction as a powerful and scalable pathway\ntowards ultra-efficient language models. Code:\nhttps://github.com/shaochenze/calm. Project:\nhttps://shaochenze.github.io/blog/2025/CALM."
                },
                "authors": [
                    {
                        "name": "Chenze Shao"
                    },
                    {
                        "name": "Darren Li"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27681v1",
                "updated": "2025-10-31T17:49:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    49,
                    50,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:49:50Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    49,
                    50,
                    4,
                    304,
                    0
                ],
                "title": "Personalized AI Scaffolds Synergistic Multi-Turn Collaboration in\n  Creative Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized AI Scaffolds Synergistic Multi-Turn Collaboration in\n  Creative Work"
                },
                "summary": "As AI becomes more deeply embedded in knowledge work, building assistants\nthat support human creativity and expertise becomes more important. Yet\nachieving synergy in human-AI collaboration is not easy. Providing AI with\ndetailed information about a user's demographics, psychological attributes,\ndivergent thinking, and domain expertise may improve performance by scaffolding\nmore effective multi-turn interactions. We implemented a personalized LLM-based\nassistant, informed by users' psychometric profiles and an AI-guided interview\nabout their work style, to help users complete a marketing task for a fictional\nstartup. We randomized 331 participants to work with AI that was either generic\n(n = 116), partially personalized (n = 114), or fully personalized (n=101).\nParticipants working with personalized AI produce marketing campaigns of\nsignificantly higher quality and creativity, beyond what AI alone could have\nproduced. Compared to generic AI, personalized AI leads to higher self-reported\nlevels of assistance and feedback, while also increasing participant trust and\nconfidence. Causal mediation analysis shows that personalization improves\nperformance indirectly by enhancing collective memory, attention, and reasoning\nin the human-AI interaction. These findings provide a theory-driven framework\nin which personalization functions as external scaffolding that builds common\nground and shared partner models, reducing uncertainty and enhancing joint\ncognition. This informs the design of future AI assistants that maximize\nsynergy and support human creative potential while limiting negative\nhomogenization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI becomes more deeply embedded in knowledge work, building assistants\nthat support human creativity and expertise becomes more important. Yet\nachieving synergy in human-AI collaboration is not easy. Providing AI with\ndetailed information about a user's demographics, psychological attributes,\ndivergent thinking, and domain expertise may improve performance by scaffolding\nmore effective multi-turn interactions. We implemented a personalized LLM-based\nassistant, informed by users' psychometric profiles and an AI-guided interview\nabout their work style, to help users complete a marketing task for a fictional\nstartup. We randomized 331 participants to work with AI that was either generic\n(n = 116), partially personalized (n = 114), or fully personalized (n=101).\nParticipants working with personalized AI produce marketing campaigns of\nsignificantly higher quality and creativity, beyond what AI alone could have\nproduced. Compared to generic AI, personalized AI leads to higher self-reported\nlevels of assistance and feedback, while also increasing participant trust and\nconfidence. Causal mediation analysis shows that personalization improves\nperformance indirectly by enhancing collective memory, attention, and reasoning\nin the human-AI interaction. These findings provide a theory-driven framework\nin which personalization functions as external scaffolding that builds common\nground and shared partner models, reducing uncertainty and enhancing joint\ncognition. This informs the design of future AI assistants that maximize\nsynergy and support human creative potential while limiting negative\nhomogenization."
                },
                "authors": [
                    {
                        "name": "Sean Kelley"
                    },
                    {
                        "name": "David De Cremer"
                    },
                    {
                        "name": "Christoph Riedl"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Riedl"
                },
                "author": "Christoph Riedl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27680v1",
                "updated": "2025-10-31T17:49:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    49,
                    1,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:49:01Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    49,
                    1,
                    4,
                    304,
                    0
                ],
                "title": "PETAR: Localized Findings Generation with Mask-Aware Vision-Language\n  Modeling for PET Automated Reporting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PETAR: Localized Findings Generation with Mask-Aware Vision-Language\n  Modeling for PET Automated Reporting"
                },
                "summary": "Recent advances in vision-language models (VLMs) have enabled impressive\nmultimodal reasoning, yet most medical applications remain limited to 2D\nimaging. In this work, we extend VLMs to 3D positron emission tomography and\ncomputed tomography (PET/CT), a domain characterized by large volumetric data,\nsmall and dispersed lesions, and lengthy radiology reports. We introduce a\nlarge-scale dataset comprising over 11,000 lesion-level descriptions paired\nwith 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid\nrule-based and large language model (LLM) pipeline. Building upon this dataset,\nwe propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET,\nCT, and lesion contours for spatially grounded report generation. PETAR bridges\nglobal contextual reasoning with fine-grained lesion awareness, producing\nclinically coherent and localized findings. Comprehensive automated and human\nevaluations demonstrate that PETAR substantially improves PET/CT report\ngeneration quality, advancing 3D medical vision-language understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language models (VLMs) have enabled impressive\nmultimodal reasoning, yet most medical applications remain limited to 2D\nimaging. In this work, we extend VLMs to 3D positron emission tomography and\ncomputed tomography (PET/CT), a domain characterized by large volumetric data,\nsmall and dispersed lesions, and lengthy radiology reports. We introduce a\nlarge-scale dataset comprising over 11,000 lesion-level descriptions paired\nwith 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid\nrule-based and large language model (LLM) pipeline. Building upon this dataset,\nwe propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET,\nCT, and lesion contours for spatially grounded report generation. PETAR bridges\nglobal contextual reasoning with fine-grained lesion awareness, producing\nclinically coherent and localized findings. Comprehensive automated and human\nevaluations demonstrate that PETAR substantially improves PET/CT report\ngeneration quality, advancing 3D medical vision-language understanding."
                },
                "authors": [
                    {
                        "name": "Danyal Maqbool"
                    },
                    {
                        "name": "Changhee Lee"
                    },
                    {
                        "name": "Zachary Huemann"
                    },
                    {
                        "name": "Samuel D. Church"
                    },
                    {
                        "name": "Matthew E. Larson"
                    },
                    {
                        "name": "Scott B. Perlman"
                    },
                    {
                        "name": "Tomas A. Romero"
                    },
                    {
                        "name": "Joshua D. Warner"
                    },
                    {
                        "name": "Meghan Lubner"
                    },
                    {
                        "name": "Xin Tie"
                    },
                    {
                        "name": "Jameson Merkow"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Steve Y. Cho"
                    },
                    {
                        "name": "Tyler J. Bradshaw"
                    }
                ],
                "author_detail": {
                    "name": "Tyler J. Bradshaw"
                },
                "author": "Tyler J. Bradshaw",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27675v1",
                "updated": "2025-10-31T17:41:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    41,
                    58,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:41:58Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    41,
                    58,
                    4,
                    304,
                    0
                ],
                "title": "On Selecting Few-Shot Examples for LLM-based Code Vulnerability\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Selecting Few-Shot Examples for LLM-based Code Vulnerability\n  Detection"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities for\nmany coding tasks, including summarization, translation, completion, and code\ngeneration. However, detecting code vulnerabilities remains a challenging task\nfor LLMs. An effective way to improve LLM performance is in-context learning\n(ICL) - providing few-shot examples similar to the query, along with correct\nanswers, can improve an LLM's ability to generate correct solutions. However,\nchoosing the few-shot examples appropriately is crucial to improving model\nperformance. In this paper, we explore two criteria for choosing few-shot\nexamples for ICL used in the code vulnerability detection task. The first\ncriterion considers if the LLM (consistently) makes a mistake or not on a\nsample with the intuition that LLM performance on a sample is informative about\nits usefulness as a few-shot example. The other criterion considers similarity\nof the examples with the program under query and chooses few-shot examples\nbased on the $k$-nearest neighbors to the given sample. We perform evaluations\nto determine the benefits of these criteria individually as well as under\nvarious combinations, using open-source models on multiple datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities for\nmany coding tasks, including summarization, translation, completion, and code\ngeneration. However, detecting code vulnerabilities remains a challenging task\nfor LLMs. An effective way to improve LLM performance is in-context learning\n(ICL) - providing few-shot examples similar to the query, along with correct\nanswers, can improve an LLM's ability to generate correct solutions. However,\nchoosing the few-shot examples appropriately is crucial to improving model\nperformance. In this paper, we explore two criteria for choosing few-shot\nexamples for ICL used in the code vulnerability detection task. The first\ncriterion considers if the LLM (consistently) makes a mistake or not on a\nsample with the intuition that LLM performance on a sample is informative about\nits usefulness as a few-shot example. The other criterion considers similarity\nof the examples with the program under query and chooses few-shot examples\nbased on the $k$-nearest neighbors to the given sample. We perform evaluations\nto determine the benefits of these criteria individually as well as under\nvarious combinations, using open-source models on multiple datasets."
                },
                "authors": [
                    {
                        "name": "Md Abdul Hannan"
                    },
                    {
                        "name": "Ronghao Ni"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Limin Jia"
                    },
                    {
                        "name": "Ravi Mangal"
                    },
                    {
                        "name": "Corina S. Pasareanu"
                    }
                ],
                "author_detail": {
                    "name": "Corina S. Pasareanu"
                },
                "author": "Corina S. Pasareanu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27672v1",
                "updated": "2025-10-31T17:37:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    37,
                    34,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:37:34Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    37,
                    34,
                    4,
                    304,
                    0
                ],
                "title": "Culture Cartography: Mapping the Landscape of Cultural Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culture Cartography: Mapping the Landscape of Cultural Knowledge"
                },
                "summary": "To serve global users safely and productively, LLMs need culture-specific\nknowledge that might not be learned during pre-training. How do we find such\nknowledge that is (1) salient to in-group users, but (2) unknown to LLMs? The\nmost common solutions are single-initiative: either researchers define\nchallenging questions that users passively answer (traditional annotation), or\nusers actively produce data that researchers structure as benchmarks (knowledge\nextraction). The process would benefit from mixed-initiative collaboration,\nwhere users guide the process to meaningfully reflect their cultures, and LLMs\nsteer the process towards more challenging questions that meet the researcher's\ngoals. We propose a mixed-initiative methodology called CultureCartography.\nHere, an LLM initializes annotation with questions for which it has\nlow-confidence answers, making explicit both its prior knowledge and the gaps\ntherein. This allows a human respondent to fill these gaps and steer the model\ntowards salient topics through direct edits. We implement this methodology as a\ntool called CultureExplorer. Compared to a baseline where humans answer\nLLM-proposed questions, we find that CultureExplorer more effectively produces\nknowledge that leading models like DeepSeek R1 and GPT-4o are missing, even\nwith web search. Fine-tuning on this data boosts the accuracy of Llama-3.1-8B\nby up to 19.2% on related culture benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To serve global users safely and productively, LLMs need culture-specific\nknowledge that might not be learned during pre-training. How do we find such\nknowledge that is (1) salient to in-group users, but (2) unknown to LLMs? The\nmost common solutions are single-initiative: either researchers define\nchallenging questions that users passively answer (traditional annotation), or\nusers actively produce data that researchers structure as benchmarks (knowledge\nextraction). The process would benefit from mixed-initiative collaboration,\nwhere users guide the process to meaningfully reflect their cultures, and LLMs\nsteer the process towards more challenging questions that meet the researcher's\ngoals. We propose a mixed-initiative methodology called CultureCartography.\nHere, an LLM initializes annotation with questions for which it has\nlow-confidence answers, making explicit both its prior knowledge and the gaps\ntherein. This allows a human respondent to fill these gaps and steer the model\ntowards salient topics through direct edits. We implement this methodology as a\ntool called CultureExplorer. Compared to a baseline where humans answer\nLLM-proposed questions, we find that CultureExplorer more effectively produces\nknowledge that leading models like DeepSeek R1 and GPT-4o are missing, even\nwith web search. Fine-tuning on this data boosts the accuracy of Llama-3.1-8B\nby up to 19.2% on related culture benchmarks."
                },
                "authors": [
                    {
                        "name": "Caleb Ziems"
                    },
                    {
                        "name": "William Held"
                    },
                    {
                        "name": "Jane Yu"
                    },
                    {
                        "name": "Amir Goldberg"
                    },
                    {
                        "name": "David Grusky"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26697v2",
                "updated": "2025-10-31T17:36:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    36,
                    35,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-30T17:01:43Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    1,
                    43,
                    3,
                    303,
                    0
                ],
                "title": "The End of Manual Decoding: Towards Truly End-to-End Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The End of Manual Decoding: Towards Truly End-to-End Language Models"
                },
                "summary": "The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a\nnon-differentiable decoding process that requires laborious, hand-tuning of\nhyperparameters like temperature and top-p. This paper introduces AutoDeco, a\nnovel architecture that enables truly \"end-to-end\" generation by learning to\ncontrol its own decoding strategy. We augment the standard transformer with\nlightweight heads that, at each step, dynamically predict context-specific\ntemperature and top-p values alongside the next-token logits. This approach\ntransforms decoding into a parametric, token-level process, allowing the model\nto self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that\nAutoDeco not only significantly outperforms default decoding strategies but\nalso achieves performance comparable to an oracle-tuned baseline derived from\n\"hacking the test set\"-a practical upper bound for any static method.\nCrucially, we uncover an emergent capability for instruction-based decoding\ncontrol: the model learns to interpret natural language commands (e.g.,\n\"generate with low randomness\") and adjusts its predicted temperature and top-p\non a token-by-token basis, opening a new paradigm for steerable and interactive\nLLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a\nnon-differentiable decoding process that requires laborious, hand-tuning of\nhyperparameters like temperature and top-p. This paper introduces AutoDeco, a\nnovel architecture that enables truly \"end-to-end\" generation by learning to\ncontrol its own decoding strategy. We augment the standard transformer with\nlightweight heads that, at each step, dynamically predict context-specific\ntemperature and top-p values alongside the next-token logits. This approach\ntransforms decoding into a parametric, token-level process, allowing the model\nto self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that\nAutoDeco not only significantly outperforms default decoding strategies but\nalso achieves performance comparable to an oracle-tuned baseline derived from\n\"hacking the test set\"-a practical upper bound for any static method.\nCrucially, we uncover an emergent capability for instruction-based decoding\ncontrol: the model learns to interpret natural language commands (e.g.,\n\"generate with low randomness\") and adjusts its predicted temperature and top-p\non a token-by-token basis, opening a new paradigm for steerable and interactive\nLLM decoding."
                },
                "authors": [
                    {
                        "name": "Zhichao Wang"
                    },
                    {
                        "name": "Dongyang Ma"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Xiaoying Tang"
                    },
                    {
                        "name": "Yan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Wang"
                },
                "author": "Yan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16886v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16886v4",
                "updated": "2025-10-31T17:29:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    29,
                    51,
                    4,
                    304,
                    0
                ],
                "published": "2024-08-29T20:19:10Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    20,
                    19,
                    10,
                    3,
                    242,
                    0
                ],
                "title": "LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation"
                },
                "summary": "While large models have achieved significant progress in computer vision,\nchallenges such as optimization complexity, the intricacy of transformer\narchitectures, computational constraints, and practical application demands\nhighlight the importance of simpler model designs in medical image\nsegmentation. This need is particularly pronounced in mobile medical devices,\nwhich require lightweight, deployable models with real-time performance.\nHowever, existing lightweight models often suffer from poor robustness across\ndatasets, limiting their widespread adoption. To address these challenges, this\npaper introduces LV-UNet, a lightweight and vanilla model that leverages\npre-trained MobileNetv3-Large backbones and incorporates fusible modules.\nLV-UNet employs an enhanced deep training strategy and switches to a deployment\nmode during inference by re-parametrization, significantly reducing parameter\ncount and computational overhead. Experimental results on ISIC 2016, BUSI,\nCVC-ClinicDB, CVC-ColonDB, and Kvair-SEG datasets demonstrate a better\ntrade-off between performance and the computational load. The code will be\nreleased at https://github.com/juntaoJianggavin/LV-UNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large models have achieved significant progress in computer vision,\nchallenges such as optimization complexity, the intricacy of transformer\narchitectures, computational constraints, and practical application demands\nhighlight the importance of simpler model designs in medical image\nsegmentation. This need is particularly pronounced in mobile medical devices,\nwhich require lightweight, deployable models with real-time performance.\nHowever, existing lightweight models often suffer from poor robustness across\ndatasets, limiting their widespread adoption. To address these challenges, this\npaper introduces LV-UNet, a lightweight and vanilla model that leverages\npre-trained MobileNetv3-Large backbones and incorporates fusible modules.\nLV-UNet employs an enhanced deep training strategy and switches to a deployment\nmode during inference by re-parametrization, significantly reducing parameter\ncount and computational overhead. Experimental results on ISIC 2016, BUSI,\nCVC-ClinicDB, CVC-ColonDB, and Kvair-SEG datasets demonstrate a better\ntrade-off between performance and the computational load. The code will be\nreleased at https://github.com/juntaoJianggavin/LV-UNet."
                },
                "authors": [
                    {
                        "name": "Juntao Jiang"
                    },
                    {
                        "name": "Mengmeng Wang"
                    },
                    {
                        "name": "Huizhong Tian"
                    },
                    {
                        "name": "Lingbo Cheng"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Accepted by IEEE BIBM2024 ML4BMI workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16886v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16886v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27656v1",
                "updated": "2025-10-31T17:28:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    28,
                    22,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:28:22Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    28,
                    22,
                    4,
                    304,
                    0
                ],
                "title": "RDMA Point-to-Point Communication for LLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RDMA Point-to-Point Communication for LLM Systems"
                },
                "summary": "Emerging Large Language Model (LLM) system patterns, such as disaggregated\ninference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement\nfine-tuning, require flexible point-to-point communication beyond simple\ncollectives. Existing implementations are locked to specific Network Interface\nControllers (NICs), hindering integration into inference engines and\nportability across hardware providers. We present TransferEngine, which bridges\nthe functionality of common NICs to expose a uniform interface. TransferEngine\nexposes one-sided WriteImm operations with a ImmCounter primitive for\ncompletion notification, without ordering assumptions of network transport,\ntransparently managing multiple NICs per GPU. We demonstrate peak throughput of\n400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We\nshowcase TransferEngine through three production systems: (1) KvCache transfer\nfor disaggregated inference with dynamic scaling, (2) RL weight updates\nachieving 1.3 seconds for trillion-parameter models, and (3) MoE\ndispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,\nwith the first viable latencies on EFA. We demonstrate that our portable\npoint-to-point communication complements collectives while avoiding lock-in.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) system patterns, such as disaggregated\ninference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement\nfine-tuning, require flexible point-to-point communication beyond simple\ncollectives. Existing implementations are locked to specific Network Interface\nControllers (NICs), hindering integration into inference engines and\nportability across hardware providers. We present TransferEngine, which bridges\nthe functionality of common NICs to expose a uniform interface. TransferEngine\nexposes one-sided WriteImm operations with a ImmCounter primitive for\ncompletion notification, without ordering assumptions of network transport,\ntransparently managing multiple NICs per GPU. We demonstrate peak throughput of\n400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We\nshowcase TransferEngine through three production systems: (1) KvCache transfer\nfor disaggregated inference with dynamic scaling, (2) RL weight updates\nachieving 1.3 seconds for trillion-parameter models, and (3) MoE\ndispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,\nwith the first viable latencies on EFA. We demonstrate that our portable\npoint-to-point communication complements collectives while avoiding lock-in."
                },
                "authors": [
                    {
                        "name": "Nandor Licker"
                    },
                    {
                        "name": "Kevin Hu"
                    },
                    {
                        "name": "Vladimir Zaytsev"
                    },
                    {
                        "name": "Lequn Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lequn Chen"
                },
                "arxiv_affiliation": "Perplexity AI",
                "author": "Lequn Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.02203v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.02203v3",
                "updated": "2025-10-31T17:12:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    39,
                    4,
                    304,
                    0
                ],
                "published": "2023-04-05T03:31:38Z",
                "published_parsed": [
                    2023,
                    4,
                    5,
                    3,
                    31,
                    38,
                    2,
                    95,
                    0
                ],
                "title": "Pinch points and half moons encode Berry curvature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pinch points and half moons encode Berry curvature"
                },
                "summary": "\"Half moons\", distinctive crescent patterns in the dynamical structure\nfactor, have been identified in inelastic neutron scattering experiments for a\nwide range of frustrated magnets. In an earlier paper [H. Yan et al., Phys.\nRev. B 98, 140402(R) (2018)] we have shown how these features are linked to the\nlocal constraints realized in classical spin liquids. Here we explore their\nimplication for the topology of magnon bands. The presence of half moons\nindicates a separation of magnetic degrees of freedom into irrotational and\nincompressible components. Where bands satisfying these constraints meet, it is\nat a singular point encoding Berry curvature of $\\pm 2\\pi$. Interactions which\nmix the bands open a gap, resolving the singularity, and leading to bands with\nfinite Berry curvature, accompanied by characteristic changes to half--moon\nmotifs. These results imply that inelastic neutron scattering can, in some\ncases, be used to make rigorous inference about the topological nature of\nmagnon bands.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Half moons\", distinctive crescent patterns in the dynamical structure\nfactor, have been identified in inelastic neutron scattering experiments for a\nwide range of frustrated magnets. In an earlier paper [H. Yan et al., Phys.\nRev. B 98, 140402(R) (2018)] we have shown how these features are linked to the\nlocal constraints realized in classical spin liquids. Here we explore their\nimplication for the topology of magnon bands. The presence of half moons\nindicates a separation of magnetic degrees of freedom into irrotational and\nincompressible components. Where bands satisfying these constraints meet, it is\nat a singular point encoding Berry curvature of $\\pm 2\\pi$. Interactions which\nmix the bands open a gap, resolving the singularity, and leading to bands with\nfinite Berry curvature, accompanied by characteristic changes to half--moon\nmotifs. These results imply that inelastic neutron scattering can, in some\ncases, be used to make rigorous inference about the topological nature of\nmagnon bands."
                },
                "authors": [
                    {
                        "name": "Han Yan"
                    },
                    {
                        "name": "Judit Romhnyi"
                    },
                    {
                        "name": "Andreas Thomasen"
                    },
                    {
                        "name": "Nic Shannon"
                    }
                ],
                "author_detail": {
                    "name": "Nic Shannon"
                },
                "author": "Nic Shannon",
                "arxiv_doi": "10.1103/PhysRevB.110.195117",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevB.110.195117",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2304.02203v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.02203v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages, 4 figures, and supplemental materials",
                "arxiv_journal_ref": "Phys. Rev. B 110, 195117 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27641v1",
                "updated": "2025-10-31T17:12:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    34,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:12:34Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    34,
                    4,
                    304,
                    0
                ],
                "title": "SpecAttn: Speculating Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecAttn: Speculating Sparse Attention"
                },
                "summary": "Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation."
                },
                "authors": [
                    {
                        "name": "Harsh Shah"
                    }
                ],
                "author_detail": {
                    "name": "Harsh Shah"
                },
                "author": "Harsh Shah",
                "arxiv_comment": "Accepted to NeurIPS 2025 Workshop on Structured Probabilistic\n  Inference & Generative Modeling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27633v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27633v1",
                "updated": "2025-10-31T17:05:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    5,
                    23,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:05:23Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    5,
                    23,
                    4,
                    304,
                    0
                ],
                "title": "Testing Inequalities Linear in Nuisance Parameters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing Inequalities Linear in Nuisance Parameters"
                },
                "summary": "This paper proposes a new test for inequalities that are linear in possibly\npartially identified nuisance parameters. This type of hypothesis arises in a\nbroad set of problems, including subvector inference for linear unconditional\nmoment (in)equality models, specification testing of such models, and inference\nfor parameters bounded by linear programs. The new test uses a two-step test\nstatistic and a chi-squared critical value with data-dependent degrees of\nfreedom that can be calculated by an elementary formula. Its simple structure\nand tuning-parameter-free implementation make it attractive for practical use.\nWe establish uniform asymptotic validity of the test, demonstrate its\nfinite-sample size and power in simulations, and illustrate its use in an\nempirical application that analyzes women's labor supply in response to a\nwelfare policy reform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a new test for inequalities that are linear in possibly\npartially identified nuisance parameters. This type of hypothesis arises in a\nbroad set of problems, including subvector inference for linear unconditional\nmoment (in)equality models, specification testing of such models, and inference\nfor parameters bounded by linear programs. The new test uses a two-step test\nstatistic and a chi-squared critical value with data-dependent degrees of\nfreedom that can be calculated by an elementary formula. Its simple structure\nand tuning-parameter-free implementation make it attractive for practical use.\nWe establish uniform asymptotic validity of the test, demonstrate its\nfinite-sample size and power in simulations, and illustrate its use in an\nempirical application that analyzes women's labor supply in response to a\nwelfare policy reform."
                },
                "authors": [
                    {
                        "name": "Gregory Fletcher Cox"
                    },
                    {
                        "name": "Xiaoxia Shi"
                    },
                    {
                        "name": "Yuya Shimizu"
                    }
                ],
                "author_detail": {
                    "name": "Yuya Shimizu"
                },
                "author": "Yuya Shimizu",
                "arxiv_comment": "None",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27633v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27633v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25502v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25502v2",
                "updated": "2025-10-31T17:01:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    1,
                    54,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-29T13:27:18Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    13,
                    27,
                    18,
                    2,
                    302,
                    0
                ],
                "title": "TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time\n  Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time\n  Series Forecasting"
                },
                "summary": "Foundation models for zero-shot time series forecasting face challenges in\nefficient long-horizon prediction and reproducibility, with existing\nsynthetic-only approaches underperforming on challenging benchmarks. This paper\npresents TempoPFN, a univariate time series foundation model based on linear\nRecurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The\nmodel uses a GatedDeltaProduct architecture with state-weaving for fully\nparallelizable training across sequence lengths, eliminating the need for\nwindowing or summarization techniques while maintaining robust temporal\nstate-tracking. Our comprehensive synthetic data pipeline unifies diverse\ngenerators, including stochastic differential equations, Gaussian processes,\nand audio synthesis, with novel augmentations. In zero-shot evaluations on the\nGift-Eval benchmark, TempoPFN achieves top-tier competitive performance,\noutperforming all existing synthetic-only approaches and surpassing the vast\nmajority of models trained on real-world data, while being more efficient than\nexisting baselines by leveraging fully parallelizable training and inference.\nWe open-source our complete data generation pipeline and training code,\nproviding a reproducible foundation for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models for zero-shot time series forecasting face challenges in\nefficient long-horizon prediction and reproducibility, with existing\nsynthetic-only approaches underperforming on challenging benchmarks. This paper\npresents TempoPFN, a univariate time series foundation model based on linear\nRecurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. The\nmodel uses a GatedDeltaProduct architecture with state-weaving for fully\nparallelizable training across sequence lengths, eliminating the need for\nwindowing or summarization techniques while maintaining robust temporal\nstate-tracking. Our comprehensive synthetic data pipeline unifies diverse\ngenerators, including stochastic differential equations, Gaussian processes,\nand audio synthesis, with novel augmentations. In zero-shot evaluations on the\nGift-Eval benchmark, TempoPFN achieves top-tier competitive performance,\noutperforming all existing synthetic-only approaches and surpassing the vast\nmajority of models trained on real-world data, while being more efficient than\nexisting baselines by leveraging fully parallelizable training and inference.\nWe open-source our complete data generation pipeline and training code,\nproviding a reproducible foundation for future research."
                },
                "authors": [
                    {
                        "name": "Vladyslav Moroshan"
                    },
                    {
                        "name": "Julien Siems"
                    },
                    {
                        "name": "Arber Zela"
                    },
                    {
                        "name": "Timur Carstensen"
                    },
                    {
                        "name": "Frank Hutter"
                    }
                ],
                "author_detail": {
                    "name": "Frank Hutter"
                },
                "author": "Frank Hutter",
                "arxiv_comment": "30 pages, 18 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25502v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25502v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27630v1",
                "updated": "2025-10-31T17:00:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    0,
                    22,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:00:22Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    0,
                    22,
                    4,
                    304,
                    0
                ],
                "title": "Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout\n  for Long-Horizon Task Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout\n  for Long-Horizon Task Training"
                },
                "summary": "Large Language Model (LLM) agents have recently shown strong potential in\ndomains such as automated coding, deep research, and graphical user interface\nmanipulation. However, training them to succeed on long-horizon,\ndomain-specialized tasks remains challenging. Current methods primarily fall\ninto two categories. The first relies on dense human annotations through\nbehavior cloning, which is prohibitively expensive for long-horizon tasks that\ncan take days or months. The second depends on outcome-driven sampling, which\noften collapses due to the rarity of valid positive trajectories on\ndomain-specialized tasks. We introduce Apollo, a sampling framework that\nintegrates asynchronous human guidance with action-level data filtering.\nInstead of requiring annotators to shadow every step, Apollo allows them to\nintervene only when the agent drifts from a promising trajectory, by providing\nprior knowledge, strategic advice, etc. This lightweight design makes it\npossible to sustain interactions for over 30 hours and produces valuable\ntrajectories at a lower cost. Apollo then applies supervision control to filter\nout sub-optimal actions and prevent error propagation. Together, these\ncomponents enable reliable and effective data collection in long-horizon\nenvironments. To demonstrate the effectiveness of Apollo, we evaluate it using\nInnovatorBench. Our experiments show that when applied to train the GLM-4.5\nmodel on InnovatorBench, Apollo achieves more than a 50% improvement over the\nuntrained baseline and a 28% improvement over a variant trained without human\ninteraction. These results highlight the critical role of human-in-the-loop\nsampling and the robustness of Apollo's design in handling long-horizon,\ndomain-specialized tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have recently shown strong potential in\ndomains such as automated coding, deep research, and graphical user interface\nmanipulation. However, training them to succeed on long-horizon,\ndomain-specialized tasks remains challenging. Current methods primarily fall\ninto two categories. The first relies on dense human annotations through\nbehavior cloning, which is prohibitively expensive for long-horizon tasks that\ncan take days or months. The second depends on outcome-driven sampling, which\noften collapses due to the rarity of valid positive trajectories on\ndomain-specialized tasks. We introduce Apollo, a sampling framework that\nintegrates asynchronous human guidance with action-level data filtering.\nInstead of requiring annotators to shadow every step, Apollo allows them to\nintervene only when the agent drifts from a promising trajectory, by providing\nprior knowledge, strategic advice, etc. This lightweight design makes it\npossible to sustain interactions for over 30 hours and produces valuable\ntrajectories at a lower cost. Apollo then applies supervision control to filter\nout sub-optimal actions and prevent error propagation. Together, these\ncomponents enable reliable and effective data collection in long-horizon\nenvironments. To demonstrate the effectiveness of Apollo, we evaluate it using\nInnovatorBench. Our experiments show that when applied to train the GLM-4.5\nmodel on InnovatorBench, Apollo achieves more than a 50% improvement over the\nuntrained baseline and a 28% improvement over a variant trained without human\ninteraction. These results highlight the critical role of human-in-the-loop\nsampling and the robustness of Apollo's design in handling long-horizon,\ndomain-specialized tasks."
                },
                "authors": [
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Yunze Wu"
                    },
                    {
                        "name": "Xiaojie Cai"
                    },
                    {
                        "name": "Lyumanshan Ye"
                    },
                    {
                        "name": "Shijie Xia"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Weiye Si"
                    },
                    {
                        "name": "Tianze Xu"
                    },
                    {
                        "name": "Jie Sun"
                    },
                    {
                        "name": "Keyu Li"
                    },
                    {
                        "name": "Mohan Jiang"
                    },
                    {
                        "name": "Junfei Wang"
                    },
                    {
                        "name": "Qishuo Hua"
                    },
                    {
                        "name": "Pengrui Lu"
                    },
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27628v1",
                "updated": "2025-10-31T17:00:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    0,
                    4,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:00:04Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    0,
                    4,
                    4,
                    304,
                    0
                ],
                "title": "Validity Is What You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validity Is What You Need"
                },
                "summary": "While AI agents have long been discussed and studied in computer science,\ntoday's Agentic AI systems are something new. We consider other definitions of\nAgentic AI and propose a new realist definition. Agentic AI is a software\ndelivery mechanism, comparable to software as a service (SaaS), which puts an\napplication to work autonomously in a complex enterprise setting. Recent\nadvances in large language models (LLMs) as foundation models have driven\nexcitement in Agentic AI. We note, however, that Agentic AI systems are\nprimarily applications, not foundations, and so their success depends on\nvalidation by end users and principal stakeholders. The tools and techniques\nneeded by the principal users to validate their applications are quite\ndifferent from the tools and techniques used to evaluate foundation models.\nIronically, with good validation measures in place, in many cases the\nfoundation models can be replaced with much simpler, faster, and more\ninterpretable models that handle core logic. When it comes to Agentic AI,\nvalidity is what you need. LLMs are one option that might achieve it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While AI agents have long been discussed and studied in computer science,\ntoday's Agentic AI systems are something new. We consider other definitions of\nAgentic AI and propose a new realist definition. Agentic AI is a software\ndelivery mechanism, comparable to software as a service (SaaS), which puts an\napplication to work autonomously in a complex enterprise setting. Recent\nadvances in large language models (LLMs) as foundation models have driven\nexcitement in Agentic AI. We note, however, that Agentic AI systems are\nprimarily applications, not foundations, and so their success depends on\nvalidation by end users and principal stakeholders. The tools and techniques\nneeded by the principal users to validate their applications are quite\ndifferent from the tools and techniques used to evaluate foundation models.\nIronically, with good validation measures in place, in many cases the\nfoundation models can be replaced with much simpler, faster, and more\ninterpretable models that handle core logic. When it comes to Agentic AI,\nvalidity is what you need. LLMs are one option that might achieve it."
                },
                "authors": [
                    {
                        "name": "Sebastian Benthall"
                    },
                    {
                        "name": "Andrew Clark"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Clark"
                },
                "author": "Andrew Clark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26774v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26774v2",
                "updated": "2025-10-31T16:49:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    49,
                    19,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-30T17:55:01Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    55,
                    1,
                    3,
                    303,
                    0
                ],
                "title": "Compact Accretion Disks in the Aftermath of Tidal Disruption Events:\n  Parameter Inference from Joint X-ray Spectra and UV/Optical Photometry\n  Fitting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Accretion Disks in the Aftermath of Tidal Disruption Events:\n  Parameter Inference from Joint X-ray Spectra and UV/Optical Photometry\n  Fitting"
                },
                "summary": "We present a multi-wavelength analysis of 14 tidal disruption events\n(TDEs)-including an off-nuclear event associated with an ultra-compact dwarf\ngalaxy-selected for having available thermal X-ray spectra during their\nlate-time UV/optical plateau phase. We show that at these stages, the full\nspectral energy distribution - X-ray spectra and UV/optical photometry - is\nwell described by a compact, yet standard accretion disk, the same disk which\npowers the X-rays at all times. By fitting up to three epochs per source with a\nfully relativistic disk model, we show that many system properties can be\nreliably recovered, including importantly the black hole mass ($M_{\\bullet}$).\nThese accretion-based $M_{\\bullet}$ values, which in this sample span nearly\nthree orders of magnitude, are consistent with galactic scaling relations but\nare significantly more precise (68\\% credible interval $ < \\pm 0.3$ dex) and\nphysically motivated. Expected accretion scaling relations (e.g., $L_{Bol}^{\ndisk} / L_{Edd} \\propto T_p^4 \\propto M_{\\bullet}^{-1}$), TDE-specific physics\ncorrelations ($L_{plat} \\propto M_{\\bullet}^{2/3}$ and $R_{out}/r_g \\propto\nM_{\\bullet}^{-2/3}$) and black hole-host galaxy correlations\n($M_{\\bullet}$-$M_{gal}$ and $M_{\\bullet}$-$\\sigma_{\\star}$) naturally emerge\nfrom the data and, for the first time, are self-consistently extended into the\nintermediate-mass (IMBH, $M_{\\bullet} < 10^{5}$) regime. We discuss the\nimplications of these results for TDE physics and modeling. We also review and\ndiscuss different methods for $M_{\\bullet}$ inference in TDEs, and find that\napproaches based on physical models of the early-time UV/optical emission are\nnot able to recover (at a statistically significant level) black hole-host\ngalaxy scalings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a multi-wavelength analysis of 14 tidal disruption events\n(TDEs)-including an off-nuclear event associated with an ultra-compact dwarf\ngalaxy-selected for having available thermal X-ray spectra during their\nlate-time UV/optical plateau phase. We show that at these stages, the full\nspectral energy distribution - X-ray spectra and UV/optical photometry - is\nwell described by a compact, yet standard accretion disk, the same disk which\npowers the X-rays at all times. By fitting up to three epochs per source with a\nfully relativistic disk model, we show that many system properties can be\nreliably recovered, including importantly the black hole mass ($M_{\\bullet}$).\nThese accretion-based $M_{\\bullet}$ values, which in this sample span nearly\nthree orders of magnitude, are consistent with galactic scaling relations but\nare significantly more precise (68\\% credible interval $ < \\pm 0.3$ dex) and\nphysically motivated. Expected accretion scaling relations (e.g., $L_{Bol}^{\ndisk} / L_{Edd} \\propto T_p^4 \\propto M_{\\bullet}^{-1}$), TDE-specific physics\ncorrelations ($L_{plat} \\propto M_{\\bullet}^{2/3}$ and $R_{out}/r_g \\propto\nM_{\\bullet}^{-2/3}$) and black hole-host galaxy correlations\n($M_{\\bullet}$-$M_{gal}$ and $M_{\\bullet}$-$\\sigma_{\\star}$) naturally emerge\nfrom the data and, for the first time, are self-consistently extended into the\nintermediate-mass (IMBH, $M_{\\bullet} < 10^{5}$) regime. We discuss the\nimplications of these results for TDE physics and modeling. We also review and\ndiscuss different methods for $M_{\\bullet}$ inference in TDEs, and find that\napproaches based on physical models of the early-time UV/optical emission are\nnot able to recover (at a statistically significant level) black hole-host\ngalaxy scalings."
                },
                "authors": [
                    {
                        "name": "M. Guolo"
                    },
                    {
                        "name": "A. Mummery"
                    },
                    {
                        "name": "S. van Velzen"
                    },
                    {
                        "name": "S. Gezari"
                    },
                    {
                        "name": "M. Nicholl"
                    },
                    {
                        "name": "Y. Yao"
                    },
                    {
                        "name": "M. Karmen"
                    },
                    {
                        "name": "Y. Ajay"
                    },
                    {
                        "name": "T. Wevers"
                    },
                    {
                        "name": "N. LeBaron"
                    },
                    {
                        "name": "R. Chornock"
                    }
                ],
                "author_detail": {
                    "name": "R. Chornock"
                },
                "author": "R. Chornock",
                "arxiv_comment": "Submitted to ApJ. 25 Pages, 14 Fig, + Appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26774v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26774v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10650v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10650v3",
                "updated": "2025-10-31T16:46:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    46,
                    44,
                    4,
                    304,
                    0
                ],
                "published": "2025-02-15T03:03:09Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    3,
                    9,
                    5,
                    46,
                    0
                ],
                "title": "Generative Adversarial Networks for High-Dimensional Item Factor\n  Analysis: A Deep Adversarial Learning Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Adversarial Networks for High-Dimensional Item Factor\n  Analysis: A Deep Adversarial Learning Algorithm"
                },
                "summary": "Advances in deep learning and representation learning have transformed item\nfactor analysis (IFA) in the item response theory (IRT) literature by enabling\nmore efficient and accurate parameter estimation. Variational Autoencoders\n(VAEs) have been one of the most impactful techniques in modeling\nhigh-dimensional latent variables in this context. However, the limited\nexpressiveness of the inference model based on traditional VAEs can still\nhinder the estimation performance. We introduce Adversarial Variational Bayes\n(AVB) algorithms as an improvement to VAEs for IFA with improved flexibility\nand accuracy. By bridging the strengths of VAEs and Generative Adversarial\nNetworks (GANs), AVB incorporates an auxiliary discriminator network to reframe\nthe estimation process as a two-player adversarial game and removes the\nrestrictive assumption of standard normal distributions in the inference model.\nTheoretically, AVB can achieve similar or higher likelihood compared to VAEs. A\nfurther enhanced algorithm, Importance-weighted Adversarial Variational Bayes\n(IWAVB) is proposed and compared with Importance-weighted Autoencoders (IWAE).\nIn an exploratory analysis of empirical data, IWAVB demonstrated superior\nexpressiveness by achieving a higher likelihood compared to IWAE. In\nconfirmatory analysis with simulated data, IWAVB achieved similar mean-square\nerror results to IWAE while consistently achieving higher likelihoods. When\nlatent variables followed a multimodal distribution, IWAVB outperformed IWAE.\nWith its innovative use of GANs, IWAVB is shown to have the potential to extend\nIFA to handle large-scale data, facilitating the potential integration of\npsychometrics and multimodal data analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in deep learning and representation learning have transformed item\nfactor analysis (IFA) in the item response theory (IRT) literature by enabling\nmore efficient and accurate parameter estimation. Variational Autoencoders\n(VAEs) have been one of the most impactful techniques in modeling\nhigh-dimensional latent variables in this context. However, the limited\nexpressiveness of the inference model based on traditional VAEs can still\nhinder the estimation performance. We introduce Adversarial Variational Bayes\n(AVB) algorithms as an improvement to VAEs for IFA with improved flexibility\nand accuracy. By bridging the strengths of VAEs and Generative Adversarial\nNetworks (GANs), AVB incorporates an auxiliary discriminator network to reframe\nthe estimation process as a two-player adversarial game and removes the\nrestrictive assumption of standard normal distributions in the inference model.\nTheoretically, AVB can achieve similar or higher likelihood compared to VAEs. A\nfurther enhanced algorithm, Importance-weighted Adversarial Variational Bayes\n(IWAVB) is proposed and compared with Importance-weighted Autoencoders (IWAE).\nIn an exploratory analysis of empirical data, IWAVB demonstrated superior\nexpressiveness by achieving a higher likelihood compared to IWAE. In\nconfirmatory analysis with simulated data, IWAVB achieved similar mean-square\nerror results to IWAE while consistently achieving higher likelihoods. When\nlatent variables followed a multimodal distribution, IWAVB outperformed IWAE.\nWith its innovative use of GANs, IWAVB is shown to have the potential to extend\nIFA to handle large-scale data, facilitating the potential integration of\npsychometrics and multimodal data analysis."
                },
                "authors": [
                    {
                        "name": "Nanyu Luo"
                    },
                    {
                        "name": "Feng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Feng Ji"
                },
                "author": "Feng Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10650v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10650v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27619v1",
                "updated": "2025-10-31T16:43:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    43,
                    12,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T16:43:12Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    43,
                    12,
                    4,
                    304,
                    0
                ],
                "title": "Equation-of-state-informed pulse profile modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Equation-of-state-informed pulse profile modeling"
                },
                "summary": "NICER has enabled mass-radius inferences for pulsars using pulse profile\nmodeling, providing constraints on the equation of state (EOS) of cold, dense\nmatter. To date, PPM and EOS inference have been carried out as two separate\nsteps, with the former using EOS-agnostic priors. This approach has several\ndrawbacks. Ideally, one would perform a fully hierarchical Bayesian inference\nwhere the pulse profile and EOS model parameters are jointly fit, but\nimplementing such a framework is complex and computationally demanding. Here,\nwe present an intermediate solution introducing an EOS-informed prior on\nmass-radius into the existing PPM pipeline using normalizing flows. By focusing\non the parameter space consistent with certain EOSs, this approach both\ntightens constraints on neutron star parameters while reducing computational\ncosts and requiring minimal additional implementation effort. We test this\napproach on two pulsars, PSR J0740+6620 and PSR J0437-4715, and with two EOS\nmodel families: a model based on the speed of sound inside the neutron star\ninterior (CS) and a piecewise-polytropic (PP) model. Both EOS models implement\nconstraints from chiral effective field theory calculations of dense matter.\nFor both pulsar datasets, the inferred radius credible intervals are narrower\nthan in the EOS-agnostic case, with CS favoring smaller radii and PP favoring\nlarger radii. For PSR J0437-4715, the EOS-informed priors reveal a new, more\nextreme geometric mode that is statistically favored but physically\nquestionable. Including the PPM posteriors in the subsequent EOS inference\nfurther tightens the mass-radius posteriors through the chiral effective field\ntheory constraints. However, there is also a sensitivity to the high-density\nextensions, where the PP (CS) model produces a shift towards larger (smaller)\nradii and corresponding stiffening (softening) of the pressure-energy density\nrelation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NICER has enabled mass-radius inferences for pulsars using pulse profile\nmodeling, providing constraints on the equation of state (EOS) of cold, dense\nmatter. To date, PPM and EOS inference have been carried out as two separate\nsteps, with the former using EOS-agnostic priors. This approach has several\ndrawbacks. Ideally, one would perform a fully hierarchical Bayesian inference\nwhere the pulse profile and EOS model parameters are jointly fit, but\nimplementing such a framework is complex and computationally demanding. Here,\nwe present an intermediate solution introducing an EOS-informed prior on\nmass-radius into the existing PPM pipeline using normalizing flows. By focusing\non the parameter space consistent with certain EOSs, this approach both\ntightens constraints on neutron star parameters while reducing computational\ncosts and requiring minimal additional implementation effort. We test this\napproach on two pulsars, PSR J0740+6620 and PSR J0437-4715, and with two EOS\nmodel families: a model based on the speed of sound inside the neutron star\ninterior (CS) and a piecewise-polytropic (PP) model. Both EOS models implement\nconstraints from chiral effective field theory calculations of dense matter.\nFor both pulsar datasets, the inferred radius credible intervals are narrower\nthan in the EOS-agnostic case, with CS favoring smaller radii and PP favoring\nlarger radii. For PSR J0437-4715, the EOS-informed priors reveal a new, more\nextreme geometric mode that is statistically favored but physically\nquestionable. Including the PPM posteriors in the subsequent EOS inference\nfurther tightens the mass-radius posteriors through the chiral effective field\ntheory constraints. However, there is also a sensitivity to the high-density\nextensions, where the PP (CS) model produces a shift towards larger (smaller)\nradii and corresponding stiffening (softening) of the pressure-energy density\nrelation."
                },
                "authors": [
                    {
                        "name": "Mariska Hoogkamer"
                    },
                    {
                        "name": "Nathan Rutherford"
                    },
                    {
                        "name": "Daniela Huppenkothen"
                    },
                    {
                        "name": "Benjamin Ricketts"
                    },
                    {
                        "name": "Anna L. Watts"
                    },
                    {
                        "name": "Melissa Mendes"
                    },
                    {
                        "name": "Isak Svensson"
                    },
                    {
                        "name": "Achim Schwenk"
                    },
                    {
                        "name": "Michael Kramer"
                    },
                    {
                        "name": "Kai Hebeler"
                    },
                    {
                        "name": "Tuomo Salmi"
                    },
                    {
                        "name": "Devarshi Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Devarshi Choudhury"
                },
                "author": "Devarshi Choudhury",
                "arxiv_comment": "Submitted to PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27617v1",
                "updated": "2025-10-31T16:40:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    40,
                    58,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T16:40:58Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    40,
                    58,
                    4,
                    304,
                    0
                ],
                "title": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation"
                },
                "summary": "Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training."
                },
                "authors": [
                    {
                        "name": "Heng Ping"
                    },
                    {
                        "name": "Arijit Bhattacharjee"
                    },
                    {
                        "name": "Peiyu Zhang"
                    },
                    {
                        "name": "Shixuan Li"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Anzhe Cheng"
                    },
                    {
                        "name": "Xiaole Zhang"
                    },
                    {
                        "name": "Jesse Thomason"
                    },
                    {
                        "name": "Ali Jannesari"
                    },
                    {
                        "name": "Nesreen Ahmed"
                    },
                    {
                        "name": "Paul Bogdan"
                    }
                ],
                "author_detail": {
                    "name": "Paul Bogdan"
                },
                "author": "Paul Bogdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27610v1",
                "updated": "2025-10-31T16:35:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    35,
                    52,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T16:35:52Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    35,
                    52,
                    4,
                    304,
                    0
                ],
                "title": "ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling"
                },
                "summary": "Formulating optimization problems for industrial applications demands\nsignificant manual effort and domain expertise. While Large Language Models\n(LLMs) show promise in automating this process, evaluating their performance\nremains difficult due to the absence of robust metrics. Existing solver-based\napproaches often face inconsistency, infeasibility issues, and high\ncomputational costs. To address these issues, we propose ORGEval, a\ngraph-theoretic evaluation framework for assessing LLMs' capabilities in\nformulating linear and mixed-integer linear programs. ORGEval represents\noptimization models as graphs, reducing equivalence detection to graph\nisomorphism testing. We identify and prove a sufficient condition, when the\ntested graphs are symmetric decomposable (SD), under which the\nWeisfeiler-Lehman (WL) test is guaranteed to correctly detect isomorphism.\nBuilding on this, ORGEval integrates a tailored variant of the WL-test with an\nSD detection algorithm to evaluate model equivalence. By focusing on structural\nequivalence rather than instance-level configurations, ORGEval is robust to\nnumerical variations. Experimental results show that our method can\nsuccessfully detect model equivalence and produce 100\\% consistent results\nacross random parameter configurations, while significantly outperforming\nsolver-based methods in runtime, especially on difficult problems. Leveraging\nORGEval, we construct the Bench4Opt dataset and benchmark state-of-the-art LLMs\non optimization modeling. Our results reveal that although optimization\nmodeling remains challenging for all LLMs, DeepSeek-V3 and Claude-Opus-4\nachieve the highest accuracies under direct prompting, outperforming even\nleading reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formulating optimization problems for industrial applications demands\nsignificant manual effort and domain expertise. While Large Language Models\n(LLMs) show promise in automating this process, evaluating their performance\nremains difficult due to the absence of robust metrics. Existing solver-based\napproaches often face inconsistency, infeasibility issues, and high\ncomputational costs. To address these issues, we propose ORGEval, a\ngraph-theoretic evaluation framework for assessing LLMs' capabilities in\nformulating linear and mixed-integer linear programs. ORGEval represents\noptimization models as graphs, reducing equivalence detection to graph\nisomorphism testing. We identify and prove a sufficient condition, when the\ntested graphs are symmetric decomposable (SD), under which the\nWeisfeiler-Lehman (WL) test is guaranteed to correctly detect isomorphism.\nBuilding on this, ORGEval integrates a tailored variant of the WL-test with an\nSD detection algorithm to evaluate model equivalence. By focusing on structural\nequivalence rather than instance-level configurations, ORGEval is robust to\nnumerical variations. Experimental results show that our method can\nsuccessfully detect model equivalence and produce 100\\% consistent results\nacross random parameter configurations, while significantly outperforming\nsolver-based methods in runtime, especially on difficult problems. Leveraging\nORGEval, we construct the Bench4Opt dataset and benchmark state-of-the-art LLMs\non optimization modeling. Our results reveal that although optimization\nmodeling remains challenging for all LLMs, DeepSeek-V3 and Claude-Opus-4\nachieve the highest accuracies under direct prompting, outperforming even\nleading reasoning models."
                },
                "authors": [
                    {
                        "name": "Zhuohan Wang"
                    },
                    {
                        "name": "Ziwei Zhu"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Congliang Chen"
                    },
                    {
                        "name": "Yizhou Han"
                    },
                    {
                        "name": "Yufeng Lin"
                    },
                    {
                        "name": "Zhihang Lin"
                    },
                    {
                        "name": "Angyang Gu"
                    },
                    {
                        "name": "Xinglin Hu"
                    },
                    {
                        "name": "Ruoyu Sun"
                    },
                    {
                        "name": "Tian Ding"
                    }
                ],
                "author_detail": {
                    "name": "Tian Ding"
                },
                "author": "Tian Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05671v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05671v2",
                "updated": "2025-10-31T16:29:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    29,
                    36,
                    4,
                    304,
                    0
                ],
                "published": "2024-04-08T16:54:41Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    16,
                    54,
                    41,
                    0,
                    99,
                    0
                ],
                "title": "Hybrid Geometry-Adaptive MCMC for Bayesian Inference in Higher-Order\n  Ising Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Geometry-Adaptive MCMC for Bayesian Inference in Higher-Order\n  Ising Models"
                },
                "summary": "We address the inverse problem for the mean-field Ising model with two- and\nthree-body interactions using a Bayesian framework. Parameter recovery in this\nsetting is notoriously difficult, particularly near phase transitions, at\ncriticality, and under non-identifiability, where conventional estimators and\nstandard MCMC samplers fail. To overcome these challenges, we develop a hybrid\nalgorithm that combines Adaptive Metropolis Hastings with geometry-aware\nRiemannian manifold Hamiltonian dynamics. This approach yields substantially\nimproved mixing and convergence in the three-dimensional parameter space.\nThrough simulated experiments across representative regimes, we demonstrate\nthat the method achieves accurate density reconstruction and reliable\nuncertainty quantification even in settings where existing approaches are\nunstable or inapplicable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the inverse problem for the mean-field Ising model with two- and\nthree-body interactions using a Bayesian framework. Parameter recovery in this\nsetting is notoriously difficult, particularly near phase transitions, at\ncriticality, and under non-identifiability, where conventional estimators and\nstandard MCMC samplers fail. To overcome these challenges, we develop a hybrid\nalgorithm that combines Adaptive Metropolis Hastings with geometry-aware\nRiemannian manifold Hamiltonian dynamics. This approach yields substantially\nimproved mixing and convergence in the three-dimensional parameter space.\nThrough simulated experiments across representative regimes, we demonstrate\nthat the method achieves accurate density reconstruction and reliable\nuncertainty quantification even in settings where existing approaches are\nunstable or inapplicable."
                },
                "authors": [
                    {
                        "name": "Godwin Osabutey"
                    },
                    {
                        "name": "Robert Richardson"
                    },
                    {
                        "name": "Garritt L. Page"
                    }
                ],
                "author_detail": {
                    "name": "Garritt L. Page"
                },
                "author": "Garritt L. Page",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05671v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05671v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27605v1",
                "updated": "2025-10-31T16:29:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    29,
                    21,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T16:29:21Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    29,
                    21,
                    4,
                    304,
                    0
                ],
                "title": "Probing Gravity at Large Scales with kSZ-Reconstructed Velocities and\n  CMB Lensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing Gravity at Large Scales with kSZ-Reconstructed Velocities and\n  CMB Lensing"
                },
                "summary": "We present a new method for measuring the $E_G$ statistic that combines two\nCMB secondaries -- the kinematic Sunyaev-Zeldovich (kSZ) effect and CMB lensing\n-- for the first time to probe gravity on linear scales. The $E_G$ statistic is\na discriminating tool for modified gravity theories, which leave imprints in\nlensing observables and peculiar velocities. Existing $E_G$ measurements rely\non redshift space distortions (RSD) to infer the velocity field. Here, we\nemploy kSZ velocity-reconstruction instead of RSD, a complementary technique\nthat constrains the largest-scale modes better than the galaxy survey it uses.\nWe construct a novel $\\widehat{V}_G$ estimator that involves a ratio between\ncross-correlations of a galaxy sample with a CMB convergence map and that with\na 3D kSZ-reconstructed velocity field. We forecast for current and upcoming CMB\nmaps from the Atacama Cosmology Telescope (ACT) and the Simons Observatory\n(SO), respectively, in combination with three spectroscopic galaxy samples from\nthe Dark Energy Spectroscopic Instrument (DESI). We find cumulative detection\nsignificances in the range $S/N \\sim 20-55$, which can robustly test the\nscale-independent $E_G$ prediction under general relativity (GR) at different\neffective redshifts of the galaxy samples ($z\\approx 0.73, 1.33, 1.84$). In\nparticular, the SO$\\times$DESI LRG measurement would be able to distinguish\nbetween GR and certain modified gravity models, including Hu-Sawicki $f(R)$ and\nChameleon theories, with high confidence. The proposed $\\widehat{V}_G$\nestimator opens up a new avenue for stress-testing gravity and the\n$\\Lambda$CDM+GR model at the largest observable scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a new method for measuring the $E_G$ statistic that combines two\nCMB secondaries -- the kinematic Sunyaev-Zeldovich (kSZ) effect and CMB lensing\n-- for the first time to probe gravity on linear scales. The $E_G$ statistic is\na discriminating tool for modified gravity theories, which leave imprints in\nlensing observables and peculiar velocities. Existing $E_G$ measurements rely\non redshift space distortions (RSD) to infer the velocity field. Here, we\nemploy kSZ velocity-reconstruction instead of RSD, a complementary technique\nthat constrains the largest-scale modes better than the galaxy survey it uses.\nWe construct a novel $\\widehat{V}_G$ estimator that involves a ratio between\ncross-correlations of a galaxy sample with a CMB convergence map and that with\na 3D kSZ-reconstructed velocity field. We forecast for current and upcoming CMB\nmaps from the Atacama Cosmology Telescope (ACT) and the Simons Observatory\n(SO), respectively, in combination with three spectroscopic galaxy samples from\nthe Dark Energy Spectroscopic Instrument (DESI). We find cumulative detection\nsignificances in the range $S/N \\sim 20-55$, which can robustly test the\nscale-independent $E_G$ prediction under general relativity (GR) at different\neffective redshifts of the galaxy samples ($z\\approx 0.73, 1.33, 1.84$). In\nparticular, the SO$\\times$DESI LRG measurement would be able to distinguish\nbetween GR and certain modified gravity models, including Hu-Sawicki $f(R)$ and\nChameleon theories, with high confidence. The proposed $\\widehat{V}_G$\nestimator opens up a new avenue for stress-testing gravity and the\n$\\Lambda$CDM+GR model at the largest observable scales."
                },
                "authors": [
                    {
                        "name": "Raagini Patki"
                    },
                    {
                        "name": "Nicholas Battaglia"
                    },
                    {
                        "name": "Rachel Bean"
                    }
                ],
                "author_detail": {
                    "name": "Rachel Bean"
                },
                "author": "Rachel Bean",
                "arxiv_comment": "18 Pages, 4 Figures, 4 Tables. To be submitted to Phys. Rev. D",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17496v2",
                "updated": "2025-10-31T16:27:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    27,
                    50,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-20T12:51:13Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    51,
                    13,
                    0,
                    293,
                    0
                ],
                "title": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and\n  Mathematical Reasoning in Large Language and Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and\n  Mathematical Reasoning in Large Language and Reasoning Models"
                },
                "summary": "We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate\ngeneralization and robustness in analogical and mathematical reasoning for\nLarge Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X\nextends I-RAVEN by increasing operand complexity, attribute range, and\nintroducing perceptual uncertainty. Compared to LLMs, empirical results show\nthat LRMs achieve improved productivity and systematicity on longer reasoning\nrelations and wider attribute ranges, respectively. However, LRMs are still\nsignificantly challenged by reasoning under uncertainty and cannot effectively\nexplore multiple probabilistic outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate\ngeneralization and robustness in analogical and mathematical reasoning for\nLarge Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X\nextends I-RAVEN by increasing operand complexity, attribute range, and\nintroducing perceptual uncertainty. Compared to LLMs, empirical results show\nthat LRMs achieve improved productivity and systematicity on longer reasoning\nrelations and wider attribute ranges, respectively. However, LRMs are still\nsignificantly challenged by reasoning under uncertainty and cannot effectively\nexplore multiple probabilistic outcomes."
                },
                "authors": [
                    {
                        "name": "Giacomo Camposampiero"
                    },
                    {
                        "name": "Michael Hersche"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    },
                    {
                        "name": "Abu Sebastian"
                    },
                    {
                        "name": "Abbas Rahimi"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Rahimi"
                },
                "author": "Abbas Rahimi",
                "arxiv_comment": "Accepted at the 5th Workshop on Mathematical Reasoning and AI\n  (MATH-AI), NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27598v1",
                "updated": "2025-10-31T16:22:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    22,
                    23,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T16:22:23Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    22,
                    23,
                    4,
                    304,
                    0
                ],
                "title": "InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM\n  Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM\n  Research"
                },
                "summary": "AI agents could accelerate scientific discovery by automating hypothesis\nformation, experiment design, coding, execution, and analysis, yet existing\nbenchmarks probe narrow skills in simplified settings. To address this gap, we\nintroduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end\nassessment of agents performing Large Language Model (LLM) research. It\ncomprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss\nDesign, Reward Design, and Scaffold Construction, which require runnable\nartifacts and assessment of correctness, performance, output quality, and\nuncertainty. To support agent operation, we develop ResearchGym, a research\nenvironment offering rich action spaces, distributed and long-horizon\nexecution, asynchronous monitoring, and snapshot saving. We also implement a\nlightweight ReAct agent that couples explicit reasoning with executable\nplanning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2.\nOur experiments demonstrate that while frontier models show promise in\ncode-driven research tasks, they struggle with fragile algorithm-related tasks\nand long-horizon decision making, such as impatience, poor resource management,\nand overreliance on template-based reasoning. Furthermore, agents require over\n11 hours to achieve their best performance on InnovatorBench, underscoring the\nbenchmark's difficulty and showing the potential of InnovatorBench to be the\nnext generation of code-based research benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents could accelerate scientific discovery by automating hypothesis\nformation, experiment design, coding, execution, and analysis, yet existing\nbenchmarks probe narrow skills in simplified settings. To address this gap, we\nintroduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end\nassessment of agents performing Large Language Model (LLM) research. It\ncomprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss\nDesign, Reward Design, and Scaffold Construction, which require runnable\nartifacts and assessment of correctness, performance, output quality, and\nuncertainty. To support agent operation, we develop ResearchGym, a research\nenvironment offering rich action spaces, distributed and long-horizon\nexecution, asynchronous monitoring, and snapshot saving. We also implement a\nlightweight ReAct agent that couples explicit reasoning with executable\nplanning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2.\nOur experiments demonstrate that while frontier models show promise in\ncode-driven research tasks, they struggle with fragile algorithm-related tasks\nand long-horizon decision making, such as impatience, poor resource management,\nand overreliance on template-based reasoning. Furthermore, agents require over\n11 hours to achieve their best performance on InnovatorBench, underscoring the\nbenchmark's difficulty and showing the potential of InnovatorBench to be the\nnext generation of code-based research benchmark."
                },
                "authors": [
                    {
                        "name": "Yunze Wu"
                    },
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Weiye Si"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Mohan Jiang"
                    },
                    {
                        "name": "Keyu Li"
                    },
                    {
                        "name": "Shijie Xia"
                    },
                    {
                        "name": "Jie Sun"
                    },
                    {
                        "name": "Tianze Xu"
                    },
                    {
                        "name": "Xiangkun Hu"
                    },
                    {
                        "name": "Pengrui Lu"
                    },
                    {
                        "name": "Xiaojie Cai"
                    },
                    {
                        "name": "Lyumanshan Ye"
                    },
                    {
                        "name": "Wenhong Zhu"
                    },
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05102v3",
                "updated": "2025-10-31T16:15:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    15,
                    22,
                    4,
                    304,
                    0
                ],
                "published": "2024-10-07T15:01:29Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    1,
                    29,
                    0,
                    281,
                    0
                ],
                "title": "SparsePO: Controlling Preference Alignment of LLMs via Sparse Token\n  Masks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparsePO: Controlling Preference Alignment of LLMs via Sparse Token\n  Masks"
                },
                "summary": "Direct alignment algorithms have proven an effective step for aligning\nlanguage models to human-desired behaviors. Current variants of the Direct\nPreference Optimization objective have focused on a strict setting where all\ntokens are contributing signals of KL divergence and rewards to the loss\nfunction. However, human preference is not affected equally by each word in a\nsequence but is often dependent on specific words or phrases, e.g. existence of\ntoxic terms leads to non-preferred responses. Based on this observation, we\nargue that not all tokens should be weighted equally during PO and propose a\nflexible objective termed SparsePO, that aims to automatically learn to weight\nthe KL divergence and reward corresponding to each token during PO training. We\npropose two different variants of weight-masks that can either be derived from\nthe reference model itself or learned on the fly. Notably, our method induces\nsparsity in the learned masks, allowing the model to learn how to best balance\nreward and KL divergence contributions at the token level, learning an optimal\nlevel of mask sparsity. Extensive experiments illustrate the effectiveness of\nour approach at aligning to preference proxies, including sentiment control,\nhelpfulness and harmlessness, and summary quality. Our method obtains +10% and\n+3% win rate points in summarization and dialogue scenarios, respectively,\nwithout compromising model reasoning or the relevancy and faithfulness of the\nsummary response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct alignment algorithms have proven an effective step for aligning\nlanguage models to human-desired behaviors. Current variants of the Direct\nPreference Optimization objective have focused on a strict setting where all\ntokens are contributing signals of KL divergence and rewards to the loss\nfunction. However, human preference is not affected equally by each word in a\nsequence but is often dependent on specific words or phrases, e.g. existence of\ntoxic terms leads to non-preferred responses. Based on this observation, we\nargue that not all tokens should be weighted equally during PO and propose a\nflexible objective termed SparsePO, that aims to automatically learn to weight\nthe KL divergence and reward corresponding to each token during PO training. We\npropose two different variants of weight-masks that can either be derived from\nthe reference model itself or learned on the fly. Notably, our method induces\nsparsity in the learned masks, allowing the model to learn how to best balance\nreward and KL divergence contributions at the token level, learning an optimal\nlevel of mask sparsity. Extensive experiments illustrate the effectiveness of\nour approach at aligning to preference proxies, including sentiment control,\nhelpfulness and harmlessness, and summary quality. Our method obtains +10% and\n+3% win rate points in summarization and dialogue scenarios, respectively,\nwithout compromising model reasoning or the relevancy and faithfulness of the\nsummary response."
                },
                "authors": [
                    {
                        "name": "Fenia Christopoulou"
                    },
                    {
                        "name": "Ronald Cardenas"
                    },
                    {
                        "name": "Gerasimos Lampouras"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "27 pages, 9 figures, 5 tables. Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27583v1",
                "updated": "2025-10-31T16:06:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    6,
                    35,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T16:06:35Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    6,
                    35,
                    4,
                    304,
                    0
                ],
                "title": "AMD MI300X GPU Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMD MI300X GPU Performance Analysis"
                },
                "summary": "The rapid growth of large language models (LLMs) has driven the need for\nhigh-performance, scalable GPU hardware capable of efficiently serving models\nwith hundreds of billions of parameters. While NVIDIA GPUs have traditionally\ndominated LLM deployments due to their mature CUDA software stack and state-of\nthe-art accelerators, AMD's latest MI300X GPUs offer a compelling alternative,\nfeaturing high HBM capacity, matrix cores, and their proprietary interconnect.\nIn this paper, we present a comprehensive evaluation of the AMD MI300X GPUs\nacross key performance domains critical to LLM inference including compute\nthroughput, memory bandwidth, and interconnect communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of large language models (LLMs) has driven the need for\nhigh-performance, scalable GPU hardware capable of efficiently serving models\nwith hundreds of billions of parameters. While NVIDIA GPUs have traditionally\ndominated LLM deployments due to their mature CUDA software stack and state-of\nthe-art accelerators, AMD's latest MI300X GPUs offer a compelling alternative,\nfeaturing high HBM capacity, matrix cores, and their proprietary interconnect.\nIn this paper, we present a comprehensive evaluation of the AMD MI300X GPUs\nacross key performance domains critical to LLM inference including compute\nthroughput, memory bandwidth, and interconnect communication."
                },
                "authors": [
                    {
                        "name": "Chandrish Ambati"
                    },
                    {
                        "name": "Trung Diep"
                    }
                ],
                "author_detail": {
                    "name": "Trung Diep"
                },
                "author": "Trung Diep",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10724v2",
                "updated": "2025-10-31T16:06:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    6,
                    3,
                    4,
                    304,
                    0
                ],
                "published": "2025-04-14T21:30:43Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    21,
                    30,
                    43,
                    0,
                    104,
                    0
                ],
                "title": "HELIOS: Adaptive Model And Early-Exit Selection for Efficient LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HELIOS: Adaptive Model And Early-Exit Selection for Efficient LLM\n  Inference Serving"
                },
                "summary": "Early-Exit Large Language Models (EE-LLMs) enable high throughput inference\nby allowing tokens to exit early at intermediate layers. However, their\nthroughput is limited by the computational and memory savings. Existing EE-LLM\nframeworks rely on a single model and therefore, their token generation\nlatencies are bottlenecked by tokens that do not exit early and traverse\nadditional layers. Moreover, early exits are only known at runtime and depend\non the request. Therefore, these frameworks load the weights of all model\nlayers even though large portions remain unused when tokens exit early. The\nlack of memory savings limit us from scaling the batch sizes.\n  We propose $\\textit{HELIOS}$, a framework that improves both token generation\nlatency and batch sizes to enable high-throughput in EE-LLMs. HELIOS exploits\ntwo insights. $\\textit{First}$, early exits are often complimentary across\nmodels, tokens that do not exit early on one model often take an early-exit on\nanother. HELIOS employs multiple models and dynamically switches between them\nto collectively maximize the number of tokens that exit early, and minimize\ntoken generation latencies. $\\textit{Second}$, even when a predicted token does\nnot exit early due to poor confidence, it often remains unchanged even after\nadditional layer traversal. HELIOS greedily allows such tokens to exit early\nand only loads the weights of the most likely to be used layers, yielding\nmemory savings which is then re-purposed to increase batch sizes. HELIOS\nemploys real-time profiling to accurately identify the early-exit\ndistributions, and adaptively switches between models by tracking tokens in\nreal-time to minimize the performance degradation caused by greedy model\nloading and exiting. Our evaluations show that HELIOS achieves $1.48\\times$\nhigher throughput and $15.14\\times$ larger batch size compared to existing\nEE-LLM frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early-Exit Large Language Models (EE-LLMs) enable high throughput inference\nby allowing tokens to exit early at intermediate layers. However, their\nthroughput is limited by the computational and memory savings. Existing EE-LLM\nframeworks rely on a single model and therefore, their token generation\nlatencies are bottlenecked by tokens that do not exit early and traverse\nadditional layers. Moreover, early exits are only known at runtime and depend\non the request. Therefore, these frameworks load the weights of all model\nlayers even though large portions remain unused when tokens exit early. The\nlack of memory savings limit us from scaling the batch sizes.\n  We propose $\\textit{HELIOS}$, a framework that improves both token generation\nlatency and batch sizes to enable high-throughput in EE-LLMs. HELIOS exploits\ntwo insights. $\\textit{First}$, early exits are often complimentary across\nmodels, tokens that do not exit early on one model often take an early-exit on\nanother. HELIOS employs multiple models and dynamically switches between them\nto collectively maximize the number of tokens that exit early, and minimize\ntoken generation latencies. $\\textit{Second}$, even when a predicted token does\nnot exit early due to poor confidence, it often remains unchanged even after\nadditional layer traversal. HELIOS greedily allows such tokens to exit early\nand only loads the weights of the most likely to be used layers, yielding\nmemory savings which is then re-purposed to increase batch sizes. HELIOS\nemploys real-time profiling to accurately identify the early-exit\ndistributions, and adaptively switches between models by tracking tokens in\nreal-time to minimize the performance degradation caused by greedy model\nloading and exiting. Our evaluations show that HELIOS achieves $1.48\\times$\nhigher throughput and $15.14\\times$ larger batch size compared to existing\nEE-LLM frameworks."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Shashank Nag"
                    },
                    {
                        "name": "Jason Clemons"
                    },
                    {
                        "name": "Lizy John"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27580v1",
                "updated": "2025-10-31T16:02:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    2,
                    0,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T16:02:00Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    2,
                    0,
                    4,
                    304,
                    0
                ],
                "title": "Refining capture-recapture methods to estimate case counts in a finite\n  population setting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining capture-recapture methods to estimate case counts in a finite\n  population setting"
                },
                "summary": "In this paper, we expand upon and refine a monitoring strategy proposed for\nsurveillance of diseases in finite, closed populations. This monitoring\nstrategy consists of augmenting an arbitrarily non-representative data stream\n(such as a voluntary flu testing program) with a random sample (referred to as\nan \"anchor stream\"). This design allows for the use of traditional\ncapture-recapture (CRC) estimators, as well as recently proposed anchor stream\nestimators that more efficiently utilize the data. Here, we focus on a\nparticularly common situation in which the first data stream only records\npositive test results, while the anchor stream documents both positives and\nnegatives. Due to the non-representative nature of the first data stream, along\nwith the fact that inference is being performed on a finite, closed population,\nthere are standard and non-standard finite population effects at play. Here, we\npropose two methods of incorporating finite population corrections (FPCs) for\ninference, along with an FPC-adjusted Bayesian credible interval. We compare\nthese approaches with existing methods through simulation and demonstrate that\nthe FPC adjustments can lead to considerable gains in precision. Finally, we\nprovide a real data example by applying these methods to estimating the breast\ncancer recurrence count among Metro Atlanta-area patients in the Georgia Cancer\nRegistry-based Cancer Recurrence Information and Surveillance Program (CRISP)\ndatabase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we expand upon and refine a monitoring strategy proposed for\nsurveillance of diseases in finite, closed populations. This monitoring\nstrategy consists of augmenting an arbitrarily non-representative data stream\n(such as a voluntary flu testing program) with a random sample (referred to as\nan \"anchor stream\"). This design allows for the use of traditional\ncapture-recapture (CRC) estimators, as well as recently proposed anchor stream\nestimators that more efficiently utilize the data. Here, we focus on a\nparticularly common situation in which the first data stream only records\npositive test results, while the anchor stream documents both positives and\nnegatives. Due to the non-representative nature of the first data stream, along\nwith the fact that inference is being performed on a finite, closed population,\nthere are standard and non-standard finite population effects at play. Here, we\npropose two methods of incorporating finite population corrections (FPCs) for\ninference, along with an FPC-adjusted Bayesian credible interval. We compare\nthese approaches with existing methods through simulation and demonstrate that\nthe FPC adjustments can lead to considerable gains in precision. Finally, we\nprovide a real data example by applying these methods to estimating the breast\ncancer recurrence count among Metro Atlanta-area patients in the Georgia Cancer\nRegistry-based Cancer Recurrence Information and Surveillance Program (CRISP)\ndatabase."
                },
                "authors": [
                    {
                        "name": "Michael Doerfler"
                    },
                    {
                        "name": "Wenhao Mao"
                    },
                    {
                        "name": "Lin Ge"
                    },
                    {
                        "name": "Yuzi Zhang"
                    },
                    {
                        "name": "Timothy L. Lash"
                    },
                    {
                        "name": "Kevin C. Ward"
                    },
                    {
                        "name": "Lance A. Waller"
                    },
                    {
                        "name": "Robert H. Lyles"
                    }
                ],
                "author_detail": {
                    "name": "Robert H. Lyles"
                },
                "author": "Robert H. Lyles",
                "arxiv_comment": "24 pages, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14166v2",
                "updated": "2025-10-31T15:58:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    58,
                    23,
                    4,
                    304,
                    0
                ],
                "published": "2025-09-17T16:49:46Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    49,
                    46,
                    2,
                    260,
                    0
                ],
                "title": "Reaction-diffusion models of invasive tree pest spread: quantifying the\n  expansion of oak processionary moth in the UK",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reaction-diffusion models of invasive tree pest spread: quantifying the\n  expansion of oak processionary moth in the UK"
                },
                "summary": "UK woodlands, forests, and urban treescapes are under threat from invasive\nspecies, exacerbated by climate change, trade, and transport. Invasive tree\npests debilitate their host and disrupt forest ecosystems, thus it is\nimperative to quantitatively model and predict their spread. Addressing this,\nwe model the spread of an invasive pest using a spatiotemporal\nreaction-diffusion equation, representing the spatial distribution as a\npopulation density field. We solve this intractable equation numerically and,\nfrom the solution, we determine first arrival times of the pest at locations in\nthe field. The adopted model permits us to obtain the expansion rate of pest\nspread directly from the model parameters, which we infer in the Bayesian\nparadigm, using a Markov chain Monte Carlo scheme. We apply our framework to\nthe ongoing spread of oak processionary moth in the UK, an outbreak which\ncontinues to grow despite management efforts. We demonstrate that our approach\neffectively captures the spread of the pest and that this has occurred at a\nnon-constant expansion rate. The proposed framework is a powerful tool for\nquantitatively modelling the spread of an invasive tree pest and could underpin\nfuture prediction and management approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UK woodlands, forests, and urban treescapes are under threat from invasive\nspecies, exacerbated by climate change, trade, and transport. Invasive tree\npests debilitate their host and disrupt forest ecosystems, thus it is\nimperative to quantitatively model and predict their spread. Addressing this,\nwe model the spread of an invasive pest using a spatiotemporal\nreaction-diffusion equation, representing the spatial distribution as a\npopulation density field. We solve this intractable equation numerically and,\nfrom the solution, we determine first arrival times of the pest at locations in\nthe field. The adopted model permits us to obtain the expansion rate of pest\nspread directly from the model parameters, which we infer in the Bayesian\nparadigm, using a Markov chain Monte Carlo scheme. We apply our framework to\nthe ongoing spread of oak processionary moth in the UK, an outbreak which\ncontinues to grow despite management efforts. We demonstrate that our approach\neffectively captures the spread of the pest and that this has occurred at a\nnon-constant expansion rate. The proposed framework is a powerful tool for\nquantitatively modelling the spread of an invasive tree pest and could underpin\nfuture prediction and management approaches."
                },
                "authors": [
                    {
                        "name": "Jamie P. McKeown"
                    },
                    {
                        "name": "Laura E. Wadkin"
                    },
                    {
                        "name": "Nick G. Parker"
                    },
                    {
                        "name": "Andrew Golightly"
                    },
                    {
                        "name": "Andrew W. Baggaley"
                    }
                ],
                "author_detail": {
                    "name": "Andrew W. Baggaley"
                },
                "author": "Andrew W. Baggaley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.PE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92D40, 92-10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27570v1",
                "updated": "2025-10-31T15:53:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    53,
                    33,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T15:53:33Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    53,
                    33,
                    4,
                    304,
                    0
                ],
                "title": "Learning viscoplastic constitutive behavior from experiments: II.\n  Dynamic indentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning viscoplastic constitutive behavior from experiments: II.\n  Dynamic indentation"
                },
                "summary": "We continue the development of a method to accurately and efficiently\nidentify the constitutive behavior of complex materials through full-field\nobservations that we started in Akerson, Rajan and Bhattacharya (2024). We\nformulate the problem of inferring constitutive relations from experiments as\nan indirect inverse problem that is constrained by the balance laws.\nSpecifically, we seek to find a constitutive behavior that minimizes the\ndifference between the experimental observation and the corresponding\nquantities computed with the model, while enforcing the balance laws. We\nformulate the forward problem as a boundary value problem corresponding to the\nexperiment, and compute the sensitivity of the objective with respect to the\nmodel using the adjoint method. In this paper, we extend the approach to\ninclude contact and study dynamic indentation. Contact is a nonholonomic\nconstraint, and we introduce a Lagrange multiplier and a slack variable to\naddress it. We demonstrate the method on synthetic data before applying it to\nexperimental observations on rolled homogeneous armor steel and a\npolycrystalline aluminum alloy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We continue the development of a method to accurately and efficiently\nidentify the constitutive behavior of complex materials through full-field\nobservations that we started in Akerson, Rajan and Bhattacharya (2024). We\nformulate the problem of inferring constitutive relations from experiments as\nan indirect inverse problem that is constrained by the balance laws.\nSpecifically, we seek to find a constitutive behavior that minimizes the\ndifference between the experimental observation and the corresponding\nquantities computed with the model, while enforcing the balance laws. We\nformulate the forward problem as a boundary value problem corresponding to the\nexperiment, and compute the sensitivity of the objective with respect to the\nmodel using the adjoint method. In this paper, we extend the approach to\ninclude contact and study dynamic indentation. Contact is a nonholonomic\nconstraint, and we introduce a Lagrange multiplier and a slack variable to\naddress it. We demonstrate the method on synthetic data before applying it to\nexperimental observations on rolled homogeneous armor steel and a\npolycrystalline aluminum alloy."
                },
                "authors": [
                    {
                        "name": "Andrew Akerson"
                    },
                    {
                        "name": "Aakila Rajan"
                    },
                    {
                        "name": "Daniel Casem"
                    },
                    {
                        "name": "Kaushik Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Bhattacharya"
                },
                "author": "Kaushik Bhattacharya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27569v1",
                "updated": "2025-10-31T15:51:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    51,
                    39,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T15:51:39Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    51,
                    39,
                    4,
                    304,
                    0
                ],
                "title": "MARAG-R1: Beyond Single Retriever via Reinforcement-Learned Multi-Tool\n  Agentic Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARAG-R1: Beyond Single Retriever via Reinforcement-Learned Multi-Tool\n  Agentic Retrieval"
                },
                "summary": "Large Language Models (LLMs) excel at reasoning and generation but are\ninherently limited by static pretraining data, resulting in factual\ninaccuracies and weak adaptability to new information. Retrieval-Augmented\nGeneration (RAG) addresses this issue by grounding LLMs in external knowledge;\nHowever, the effectiveness of RAG critically depends on whether the model can\nadequately access relevant information. Existing RAG systems rely on a single\nretriever with fixed top-k selection, restricting access to a narrow and static\nsubset of the corpus. As a result, this single-retriever paradigm has become\nthe primary bottleneck for comprehensive external information acquisition,\nespecially in tasks requiring corpus-level reasoning. To overcome this\nlimitation, we propose MARAG-R1, a reinforcement-learned multi-tool RAG\nframework that enables LLMs to dynamically coordinate multiple retrieval\nmechanisms for broader and more precise information access. MARAG-R1 equips the\nmodel with four retrieval tools -- semantic search, keyword search, filtering,\nand aggregation -- and learns both how and when to use them through a two-stage\ntraining process: supervised fine-tuning followed by reinforcement learning.\nThis design allows the model to interleave reasoning and retrieval,\nprogressively gathering sufficient evidence for corpus-level synthesis.\nExperiments on GlobalQA, HotpotQA, and 2WikiMultiHopQA demonstrate that\nMARAG-R1 substantially outperforms strong baselines and achieves new\nstate-of-the-art results in corpus-level reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at reasoning and generation but are\ninherently limited by static pretraining data, resulting in factual\ninaccuracies and weak adaptability to new information. Retrieval-Augmented\nGeneration (RAG) addresses this issue by grounding LLMs in external knowledge;\nHowever, the effectiveness of RAG critically depends on whether the model can\nadequately access relevant information. Existing RAG systems rely on a single\nretriever with fixed top-k selection, restricting access to a narrow and static\nsubset of the corpus. As a result, this single-retriever paradigm has become\nthe primary bottleneck for comprehensive external information acquisition,\nespecially in tasks requiring corpus-level reasoning. To overcome this\nlimitation, we propose MARAG-R1, a reinforcement-learned multi-tool RAG\nframework that enables LLMs to dynamically coordinate multiple retrieval\nmechanisms for broader and more precise information access. MARAG-R1 equips the\nmodel with four retrieval tools -- semantic search, keyword search, filtering,\nand aggregation -- and learns both how and when to use them through a two-stage\ntraining process: supervised fine-tuning followed by reinforcement learning.\nThis design allows the model to interleave reasoning and retrieval,\nprogressively gathering sufficient evidence for corpus-level synthesis.\nExperiments on GlobalQA, HotpotQA, and 2WikiMultiHopQA demonstrate that\nMARAG-R1 substantially outperforms strong baselines and achieves new\nstate-of-the-art results in corpus-level reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Qi Luo"
                    },
                    {
                        "name": "Xiaonan Li"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Tingshuo Fan"
                    },
                    {
                        "name": "Yuan Li"
                    },
                    {
                        "name": "Xinchi Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27566v1",
                "updated": "2025-10-31T15:48:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    48,
                    43,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T15:48:43Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    48,
                    43,
                    4,
                    304,
                    0
                ],
                "title": "Interact-RAG: Reason and Interact with the Corpus, Beyond Black-Box\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interact-RAG: Reason and Interact with the Corpus, Beyond Black-Box\n  Retrieval"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has significantly enhanced LLMs by\nincorporating external information. However, prevailing agentic RAG approaches\nare constrained by a critical limitation: they treat the retrieval process as a\nblack-box querying operation. This confines agents' actions to query issuing,\nhindering its ability to tackle complex information-seeking tasks. To address\nthis, we introduce Interact-RAG, a new paradigm that elevates the LLM agent\nfrom a passive query issuer into an active manipulator of the retrieval\nprocess. We dismantle the black-box with a Corpus Interaction Engine, equipping\nthe agent with a set of action primitives for fine-grained control over\ninformation retrieval. To further empower the agent on the entire RAG pipeline,\nwe first develop a reasoning-enhanced workflow, which enables both zero-shot\nexecution and the synthesis of interaction trajectories. We then leverage this\nsynthetic data to train a fully autonomous end-to-end agent via Supervised\nFine-Tuning (SFT), followed by refinement with Reinforcement Learning (RL).\nExtensive experiments across six benchmarks demonstrate that Interact-RAG\nsignificantly outperforms other advanced methods, validating the efficacy of\nour reasoning-interaction strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has significantly enhanced LLMs by\nincorporating external information. However, prevailing agentic RAG approaches\nare constrained by a critical limitation: they treat the retrieval process as a\nblack-box querying operation. This confines agents' actions to query issuing,\nhindering its ability to tackle complex information-seeking tasks. To address\nthis, we introduce Interact-RAG, a new paradigm that elevates the LLM agent\nfrom a passive query issuer into an active manipulator of the retrieval\nprocess. We dismantle the black-box with a Corpus Interaction Engine, equipping\nthe agent with a set of action primitives for fine-grained control over\ninformation retrieval. To further empower the agent on the entire RAG pipeline,\nwe first develop a reasoning-enhanced workflow, which enables both zero-shot\nexecution and the synthesis of interaction trajectories. We then leverage this\nsynthetic data to train a fully autonomous end-to-end agent via Supervised\nFine-Tuning (SFT), followed by refinement with Reinforcement Learning (RL).\nExtensive experiments across six benchmarks demonstrate that Interact-RAG\nsignificantly outperforms other advanced methods, validating the efficacy of\nour reasoning-interaction strategy."
                },
                "authors": [
                    {
                        "name": "Yulong Hui"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Zhihang Fu"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Jieping Ye"
                    },
                    {
                        "name": "Huanchen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huanchen Zhang"
                },
                "author": "Huanchen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27565v1",
                "updated": "2025-10-31T15:47:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    47,
                    7,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T15:47:07Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    47,
                    7,
                    4,
                    304,
                    0
                ],
                "title": "CodeAlignBench: Assessing Code Generation Models on Developer-Preferred\n  Code Adjustments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeAlignBench: Assessing Code Generation Models on Developer-Preferred\n  Code Adjustments"
                },
                "summary": "As large language models become increasingly capable of generating code,\nevaluating their performance remains a complex and evolving challenge. Existing\nbenchmarks primarily focus on functional correctness, overlooking the diversity\nof real-world coding tasks and developer expectations. To this end, we\nintroduce a multi-language benchmark that evaluates LLM instruction-following\ncapabilities and is extensible to operate on any set of standalone coding\nproblems. Our benchmark evaluates instruction following in two key settings:\nadherence to pre-defined constraints specified with the initial problem, and\nthe ability to perform refinements based on follow-up instructions. For this\npaper's analysis, we empirically evaluated our benchmarking pipeline with\nprogramming tasks from LiveBench, that are also automatically translated from\nPython into Java and JavaScript. Our automated benchmark reveals that models\nexhibit differing levels of performance across multiple dimensions of\ninstruction-following. Our benchmarking pipeline provides a more comprehensive\nevaluation of code generation models, highlighting their strengths and\nlimitations across languages and generation goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models become increasingly capable of generating code,\nevaluating their performance remains a complex and evolving challenge. Existing\nbenchmarks primarily focus on functional correctness, overlooking the diversity\nof real-world coding tasks and developer expectations. To this end, we\nintroduce a multi-language benchmark that evaluates LLM instruction-following\ncapabilities and is extensible to operate on any set of standalone coding\nproblems. Our benchmark evaluates instruction following in two key settings:\nadherence to pre-defined constraints specified with the initial problem, and\nthe ability to perform refinements based on follow-up instructions. For this\npaper's analysis, we empirically evaluated our benchmarking pipeline with\nprogramming tasks from LiveBench, that are also automatically translated from\nPython into Java and JavaScript. Our automated benchmark reveals that models\nexhibit differing levels of performance across multiple dimensions of\ninstruction-following. Our benchmarking pipeline provides a more comprehensive\nevaluation of code generation models, highlighting their strengths and\nlimitations across languages and generation goals."
                },
                "authors": [
                    {
                        "name": "Forough Mehralian"
                    },
                    {
                        "name": "Ryan Shar"
                    },
                    {
                        "name": "James R. Rae"
                    },
                    {
                        "name": "Alireza Hashemi"
                    }
                ],
                "author_detail": {
                    "name": "Alireza Hashemi"
                },
                "author": "Alireza Hashemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27556v1",
                "updated": "2025-10-31T15:34:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    34,
                    41,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T15:34:41Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    34,
                    41,
                    4,
                    304,
                    0
                ],
                "title": "Data-Efficient Domain Adaptation for LLM-based MT using Contrastive\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Efficient Domain Adaptation for LLM-based MT using Contrastive\n  Preference Optimization"
                },
                "summary": "LLMs often require adaptation to domain-specific requirements, a process that\ncan be expensive when relying solely on SFT. We present an empirical study on\napplying CPO to simulate a post-editing workflow for data-efficient domain\nadaptation. Our approach synthesizes preference pairs by treating the base\nmodel's own raw output as the 'rejected' translation and the human-approved TM\nentry as the 'chosen' one. This method provides direct feedback on the model's\ncurrent knowledge, guiding it to align with domain-specific standards.\nExperiments in English-Brazilian Portuguese and English-Korean show that, by\nusing just 14.7k preference pairs, the model achieves performance close to that\nof a model trained on 160k+ samples with SFT, demonstrating significant data\nefficiency. Although we showcase its effectiveness in MT, this application of\nCPO naturally generalizes to other generative tasks where a model's initial\ndrafts can serve as a contrastive signal against a golden reference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs often require adaptation to domain-specific requirements, a process that\ncan be expensive when relying solely on SFT. We present an empirical study on\napplying CPO to simulate a post-editing workflow for data-efficient domain\nadaptation. Our approach synthesizes preference pairs by treating the base\nmodel's own raw output as the 'rejected' translation and the human-approved TM\nentry as the 'chosen' one. This method provides direct feedback on the model's\ncurrent knowledge, guiding it to align with domain-specific standards.\nExperiments in English-Brazilian Portuguese and English-Korean show that, by\nusing just 14.7k preference pairs, the model achieves performance close to that\nof a model trained on 160k+ samples with SFT, demonstrating significant data\nefficiency. Although we showcase its effectiveness in MT, this application of\nCPO naturally generalizes to other generative tasks where a model's initial\ndrafts can serve as a contrastive signal against a golden reference."
                },
                "authors": [
                    {
                        "name": "Inacio Vieira"
                    },
                    {
                        "name": "Antonio Castaldo"
                    },
                    {
                        "name": "James O'Doherty"
                    },
                    {
                        "name": "Sheila Castilho"
                    }
                ],
                "author_detail": {
                    "name": "Sheila Castilho"
                },
                "author": "Sheila Castilho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27554v1",
                "updated": "2025-10-31T15:29:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    29,
                    31,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T15:29:31Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    29,
                    31,
                    4,
                    304,
                    0
                ],
                "title": "Sybil-Resistant Service Discovery for Agent Economies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sybil-Resistant Service Discovery for Agent Economies"
                },
                "summary": "x402 enables Hypertext Transfer Protocol (HTTP) services like application\nprogramming interfaces (APIs), data feeds, and inference providers to accept\ncryptocurrency payments for access. As agents increasingly consume these\nservices, discovery becomes critical: which swap interface should an agent\ntrust? Which data provider is the most reliable? We introduce TraceRank, a\nreputation-weighted ranking algorithm where payment transactions serve as\nendorsements. TraceRank seeds addresses with precomputed reputation metrics and\npropagates reputation through payment flows weighted by transaction value and\ntemporal recency. Applied to x402's payment graph, this surfaces services\npreferred by high-reputation users rather than those with high transaction\nvolume. Our system combines TraceRank with semantic search to respond to\nnatural language queries with high quality results. We argue that reputation\npropagation resists Sybil attacks by making spam services with many\nlow-reputation payers rank below legitimate services with few high-reputation\npayers. Ultimately, we aim to construct a search method for x402 enabled\nservices that avoids infrastructure bias and has better performance than purely\nvolume based or semantic methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "x402 enables Hypertext Transfer Protocol (HTTP) services like application\nprogramming interfaces (APIs), data feeds, and inference providers to accept\ncryptocurrency payments for access. As agents increasingly consume these\nservices, discovery becomes critical: which swap interface should an agent\ntrust? Which data provider is the most reliable? We introduce TraceRank, a\nreputation-weighted ranking algorithm where payment transactions serve as\nendorsements. TraceRank seeds addresses with precomputed reputation metrics and\npropagates reputation through payment flows weighted by transaction value and\ntemporal recency. Applied to x402's payment graph, this surfaces services\npreferred by high-reputation users rather than those with high transaction\nvolume. Our system combines TraceRank with semantic search to respond to\nnatural language queries with high quality results. We argue that reputation\npropagation resists Sybil attacks by making spam services with many\nlow-reputation payers rank below legitimate services with few high-reputation\npayers. Ultimately, we aim to construct a search method for x402 enabled\nservices that avoids infrastructure bias and has better performance than purely\nvolume based or semantic methods."
                },
                "authors": [
                    {
                        "name": "David Shi"
                    },
                    {
                        "name": "Kevin Joo"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Joo"
                },
                "author": "Kevin Joo",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T42, 68R10, 68M14, 68P20, 91D30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; H.2.8; I.2.11; K.4.4; G.2.2; C.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11409v2",
                "updated": "2025-10-31T15:29:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    29,
                    3,
                    4,
                    304,
                    0
                ],
                "published": "2025-04-15T17:26:29Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    26,
                    29,
                    1,
                    105,
                    0
                ],
                "title": "Minitron-SSM: Efficient Hybrid Language Model Compression through\n  Group-Aware SSM Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minitron-SSM: Efficient Hybrid Language Model Compression through\n  Group-Aware SSM Pruning"
                },
                "summary": "Hybrid LLM architectures that combine Attention and State Space Models (SSMs)\nachieve state-of-the-art accuracy and runtime performance. Recent work has\ndemonstrated that applying compression and distillation to Attention-only\nmodels yields smaller, more accurate models at a fraction of the training cost.\nIn this work, we explore the effectiveness of compressing Hybrid architectures.\nWe introduce a novel group-aware pruning strategy that preserves the structural\nintegrity of SSM blocks and their sequence modeling capabilities. Furthermore,\nwe demonstrate the necessity of such SSM pruning to achieve improved accuracy\nand inference speed compared to traditional approaches. Our compression recipe\ncombines SSM, FFN, embedding dimension, and layer pruning, followed by\nknowledge distillation-based retraining, similar to the MINITRON technique.\nUsing this approach, we compress the Nemotron-H 8B Hybrid model down to 4B\nparameters with up to 40x fewer training tokens. The resulting model surpasses\nthe accuracy of similarly-sized models while achieving 2x faster inference,\nsignificantly advancing the Pareto frontier.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid LLM architectures that combine Attention and State Space Models (SSMs)\nachieve state-of-the-art accuracy and runtime performance. Recent work has\ndemonstrated that applying compression and distillation to Attention-only\nmodels yields smaller, more accurate models at a fraction of the training cost.\nIn this work, we explore the effectiveness of compressing Hybrid architectures.\nWe introduce a novel group-aware pruning strategy that preserves the structural\nintegrity of SSM blocks and their sequence modeling capabilities. Furthermore,\nwe demonstrate the necessity of such SSM pruning to achieve improved accuracy\nand inference speed compared to traditional approaches. Our compression recipe\ncombines SSM, FFN, embedding dimension, and layer pruning, followed by\nknowledge distillation-based retraining, similar to the MINITRON technique.\nUsing this approach, we compress the Nemotron-H 8B Hybrid model down to 4B\nparameters with up to 40x fewer training tokens. The resulting model surpasses\nthe accuracy of similarly-sized models while achieving 2x faster inference,\nsignificantly advancing the Pareto frontier."
                },
                "authors": [
                    {
                        "name": "Ali Taghibakhshi"
                    },
                    {
                        "name": "Sharath Turuvekere Sreenivas"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "Yashaswi Karnati"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Oluwatobi Olabiyi"
                    },
                    {
                        "name": "Daniel Korzekwa"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Ashwath Aithal"
                    },
                    {
                        "name": "Nima Tajbakhsh"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11664v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11664v4",
                "updated": "2025-10-31T15:26:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    26,
                    11,
                    4,
                    304,
                    0
                ],
                "published": "2025-02-17T10:53:57Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    10,
                    53,
                    57,
                    0,
                    48,
                    0
                ],
                "title": "VRoPE: Rotary Position Embedding for Video Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VRoPE: Rotary Position Embedding for Video Large Language Models"
                },
                "summary": "Rotary Position Embedding (RoPE) has shown strong performance in text-based\nLarge Language Models (LLMs), but extending it to video remains a challenge due\nto the intricate spatiotemporal structure of video frames. Existing\nadaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions\nseparately but suffer from two major limitations: positional bias in attention\ndistribution and disruptions in video-text transitions. To overcome these\nissues, we propose Video Rotary Position Embedding (VRoPE), a novel positional\nencoding method tailored for Video-LLMs. Specifically, we introduce a more\nbalanced encoding strategy that mitigates attention biases, ensuring a more\nuniform distribution of spatial focus. Additionally, our approach restructures\npositional indices to ensure a smooth transition between video and text tokens.\nExtensive experiments on different models demonstrate that VRoPE consistently\noutperforms previous RoPE variants, achieving significant improvements in video\nunderstanding, temporal reasoning, and retrieval tasks. Code is available at\nhttps://github.com/johncaged/VRoPE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotary Position Embedding (RoPE) has shown strong performance in text-based\nLarge Language Models (LLMs), but extending it to video remains a challenge due\nto the intricate spatiotemporal structure of video frames. Existing\nadaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions\nseparately but suffer from two major limitations: positional bias in attention\ndistribution and disruptions in video-text transitions. To overcome these\nissues, we propose Video Rotary Position Embedding (VRoPE), a novel positional\nencoding method tailored for Video-LLMs. Specifically, we introduce a more\nbalanced encoding strategy that mitigates attention biases, ensuring a more\nuniform distribution of spatial focus. Additionally, our approach restructures\npositional indices to ensure a smooth transition between video and text tokens.\nExtensive experiments on different models demonstrate that VRoPE consistently\noutperforms previous RoPE variants, achieving significant improvements in video\nunderstanding, temporal reasoning, and retrieval tasks. Code is available at\nhttps://github.com/johncaged/VRoPE."
                },
                "authors": [
                    {
                        "name": "Zikang Liu"
                    },
                    {
                        "name": "Longteng Guo"
                    },
                    {
                        "name": "Yepeng Tang"
                    },
                    {
                        "name": "Tongtian Yue"
                    },
                    {
                        "name": "Junxian Cai"
                    },
                    {
                        "name": "Kai Ma"
                    },
                    {
                        "name": "Qingbin Liu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Liu"
                },
                "author": "Jing Liu",
                "arxiv_comment": "EMNLP 2025 Main Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11664v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11664v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27545v1",
                "updated": "2025-10-31T15:21:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    21,
                    5,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T15:21:05Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    21,
                    5,
                    4,
                    304,
                    0
                ],
                "title": "EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EBT-Policy: Energy Unlocks Emergent Physical Reasoning Capabilities"
                },
                "summary": "Implicit policies parameterized by generative models, such as Diffusion\nPolicy, have become the standard for policy learning and Vision-Language-Action\n(VLA) models in robotics. However, these approaches often suffer from high\ncomputational cost, exposure bias, and unstable inference dynamics, which lead\nto divergence under distribution shifts. Energy-Based Models (EBMs) address\nthese issues by learning energy landscapes end-to-end and modeling equilibrium\ndynamics, offering improved robustness and reduced exposure bias. Yet, policies\nparameterized by EBMs have historically struggled to scale effectively. Recent\nwork on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs\nto high-dimensional spaces, but their potential for solving core challenges in\nphysically embodied models remains underexplored. We introduce a new\nenergy-based architecture, EBT-Policy, that solves core issues in robotic and\nreal-world settings. Across simulated and real-world tasks, EBT-Policy\nconsistently outperforms diffusion-based policies, while requiring less\ntraining and inference computation. Remarkably, on some tasks it converges\nwithin just two inference steps, a 50x reduction compared to Diffusion Policy's\n100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior\nmodels, such as zero-shot recovery from failed action sequences using only\nbehavior cloning and without explicit retry training. By leveraging its scalar\nenergy for uncertainty-aware inference and dynamic compute allocation,\nEBT-Policy offers a promising path toward robust, generalizable robot behavior\nunder distribution shifts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit policies parameterized by generative models, such as Diffusion\nPolicy, have become the standard for policy learning and Vision-Language-Action\n(VLA) models in robotics. However, these approaches often suffer from high\ncomputational cost, exposure bias, and unstable inference dynamics, which lead\nto divergence under distribution shifts. Energy-Based Models (EBMs) address\nthese issues by learning energy landscapes end-to-end and modeling equilibrium\ndynamics, offering improved robustness and reduced exposure bias. Yet, policies\nparameterized by EBMs have historically struggled to scale effectively. Recent\nwork on Energy-Based Transformers (EBTs) demonstrates the scalability of EBMs\nto high-dimensional spaces, but their potential for solving core challenges in\nphysically embodied models remains underexplored. We introduce a new\nenergy-based architecture, EBT-Policy, that solves core issues in robotic and\nreal-world settings. Across simulated and real-world tasks, EBT-Policy\nconsistently outperforms diffusion-based policies, while requiring less\ntraining and inference computation. Remarkably, on some tasks it converges\nwithin just two inference steps, a 50x reduction compared to Diffusion Policy's\n100. Moreover, EBT-Policy exhibits emergent capabilities not seen in prior\nmodels, such as zero-shot recovery from failed action sequences using only\nbehavior cloning and without explicit retry training. By leveraging its scalar\nenergy for uncertainty-aware inference and dynamic compute allocation,\nEBT-Policy offers a promising path toward robust, generalizable robot behavior\nunder distribution shifts."
                },
                "authors": [
                    {
                        "name": "Travis Davies"
                    },
                    {
                        "name": "Yiqi Huang"
                    },
                    {
                        "name": "Alexi Gladstone"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Huxian Liu"
                    },
                    {
                        "name": "Luhui Hu"
                    }
                ],
                "author_detail": {
                    "name": "Luhui Hu"
                },
                "author": "Luhui Hu",
                "arxiv_comment": "9 pages, 6 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27544v1",
                "updated": "2025-10-31T15:17:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    17,
                    55,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T15:17:55Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    17,
                    55,
                    4,
                    304,
                    0
                ],
                "title": "Mechanics of Learned Reasoning 1: TempoBench, A Benchmark for\n  Interpretable Deconstruction of Reasoning System Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanics of Learned Reasoning 1: TempoBench, A Benchmark for\n  Interpretable Deconstruction of Reasoning System Performance"
                },
                "summary": "Large Language Models (LLMs) are increasingly excelling and outpacing human\nperformance on many tasks. However, to improve LLM reasoning, researchers\neither rely on ad-hoc generated datasets or formal mathematical proof systems\nsuch as the Lean proof assistant. Whilst ad-hoc generated methods can capture\nthe decision chains of real-world reasoning processes, they may encode some\ninadvertent bias in the space of reasoning they cover; they also cannot be\nformally verified. On the other hand, systems like Lean can guarantee\nverifiability, but are not well-suited to capture the nature of agentic\ndecision chain-based tasks. This creates a gap both in performance for\nfunctions such as business agents or code assistants, and in the usefulness of\nLLM reasoning benchmarks, whereby these fall short in reasoning structure or\nreal-world alignment. We introduce TempoBench, the first formally grounded and\nverifiable diagnostic benchmark that parametrizes difficulty to systematically\nanalyze how LLMs perform reasoning. TempoBench uses two evaluation benchmarks\nto break down reasoning ability. First, temporal trace evaluation (TTE) tests\nthe ability of an LLM to understand and simulate the execution of a given\nmulti-step reasoning system. Subsequently, temporal causal evaluation (TCE)\ntests an LLM's ability to perform multi-step causal reasoning and to distill\ncause-and-effect relations from complex systems. We find that models score\n65.6% on TCE-normal, and 7.5% on TCE-hard. This shows that state-of-the-art\nLLMs clearly understand the TCE task but perform poorly as system complexity\nincreases. Our code is available at our\n\\href{https://github.com/nik-hz/tempobench}{GitHub repository}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly excelling and outpacing human\nperformance on many tasks. However, to improve LLM reasoning, researchers\neither rely on ad-hoc generated datasets or formal mathematical proof systems\nsuch as the Lean proof assistant. Whilst ad-hoc generated methods can capture\nthe decision chains of real-world reasoning processes, they may encode some\ninadvertent bias in the space of reasoning they cover; they also cannot be\nformally verified. On the other hand, systems like Lean can guarantee\nverifiability, but are not well-suited to capture the nature of agentic\ndecision chain-based tasks. This creates a gap both in performance for\nfunctions such as business agents or code assistants, and in the usefulness of\nLLM reasoning benchmarks, whereby these fall short in reasoning structure or\nreal-world alignment. We introduce TempoBench, the first formally grounded and\nverifiable diagnostic benchmark that parametrizes difficulty to systematically\nanalyze how LLMs perform reasoning. TempoBench uses two evaluation benchmarks\nto break down reasoning ability. First, temporal trace evaluation (TTE) tests\nthe ability of an LLM to understand and simulate the execution of a given\nmulti-step reasoning system. Subsequently, temporal causal evaluation (TCE)\ntests an LLM's ability to perform multi-step causal reasoning and to distill\ncause-and-effect relations from complex systems. We find that models score\n65.6% on TCE-normal, and 7.5% on TCE-hard. This shows that state-of-the-art\nLLMs clearly understand the TCE task but perform poorly as system complexity\nincreases. Our code is available at our\n\\href{https://github.com/nik-hz/tempobench}{GitHub repository}."
                },
                "authors": [
                    {
                        "name": "Nikolaus Holzer"
                    },
                    {
                        "name": "William Fishell"
                    },
                    {
                        "name": "Baishakhi Ray"
                    },
                    {
                        "name": "Mark Santolucito"
                    }
                ],
                "author_detail": {
                    "name": "Mark Santolucito"
                },
                "author": "Mark Santolucito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27543v1",
                "updated": "2025-10-31T15:17:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    17,
                    6,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T15:17:06Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    17,
                    6,
                    4,
                    304,
                    0
                ],
                "title": "DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and\n  Multilingual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and\n  Multilingual Language Models"
                },
                "summary": "We present DialectalArabicMMLU, a new benchmark for evaluating the\nperformance of large language models (LLMs) across Arabic dialects. While\nrecently developed Arabic and multilingual benchmarks have advanced LLM\nevaluation for Modern Standard Arabic (MSA), dialectal varieties remain\nunderrepresented despite their prevalence in everyday communication.\nDialectalArabicMMLU extends the MMLU-Redux framework through manual translation\nand adaptation of 3K multiple-choice question-answer pairs into five major\ndialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), yielding a total of\n15K QA pairs across 32 academic and professional domains (22K QA pairs when\nalso including English and MSA). The benchmark enables systematic assessment of\nLLM reasoning and comprehension beyond MSA, supporting both task-based and\nlinguistic analysis. We evaluate 19 open-weight Arabic and multilingual LLMs\n(1B-13B parameters) and report substantial performance variation across\ndialects, revealing persistent gaps in dialectal generalization.\nDialectalArabicMMLU provides the first unified, human-curated resource for\nmeasuring dialectal understanding in Arabic, thus promoting more inclusive\nevaluation and future model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DialectalArabicMMLU, a new benchmark for evaluating the\nperformance of large language models (LLMs) across Arabic dialects. While\nrecently developed Arabic and multilingual benchmarks have advanced LLM\nevaluation for Modern Standard Arabic (MSA), dialectal varieties remain\nunderrepresented despite their prevalence in everyday communication.\nDialectalArabicMMLU extends the MMLU-Redux framework through manual translation\nand adaptation of 3K multiple-choice question-answer pairs into five major\ndialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), yielding a total of\n15K QA pairs across 32 academic and professional domains (22K QA pairs when\nalso including English and MSA). The benchmark enables systematic assessment of\nLLM reasoning and comprehension beyond MSA, supporting both task-based and\nlinguistic analysis. We evaluate 19 open-weight Arabic and multilingual LLMs\n(1B-13B parameters) and report substantial performance variation across\ndialects, revealing persistent gaps in dialectal generalization.\nDialectalArabicMMLU provides the first unified, human-curated resource for\nmeasuring dialectal understanding in Arabic, thus promoting more inclusive\nevaluation and future model development."
                },
                "authors": [
                    {
                        "name": "Malik H. Altakrori"
                    },
                    {
                        "name": "Nizar Habash"
                    },
                    {
                        "name": "Abdelhakim Freihat"
                    },
                    {
                        "name": "Younes Samih"
                    },
                    {
                        "name": "Kirill Chirkunov"
                    },
                    {
                        "name": "Muhammed AbuOdeh"
                    },
                    {
                        "name": "Radu Florian"
                    },
                    {
                        "name": "Teresa Lynn"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "arxiv_comment": "9 pages, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.14801v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.14801v2",
                "updated": "2025-10-31T15:12:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    12,
                    26,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-16T15:33:20Z",
                "published_parsed": [
                    2025,
                    10,
                    16,
                    15,
                    33,
                    20,
                    3,
                    289,
                    0
                ],
                "title": "A new photometric ephemeris for the 2M1510 AB double brown dwarf\n  eclipsing binary system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A new photometric ephemeris for the 2M1510 AB double brown dwarf\n  eclipsing binary system"
                },
                "summary": "Eclipsing brown dwarfs are important calibrators of sub-stellar evolution\nmodels used to infer the characteristics of directly imaged brown dwarfs and\ngiant exoplanets. Only two double brown dwarf eclipsing binary systems are\nknown, among them 2MASS J15104786-2818174 (2M1510 AB), published in 2020 with a\npoorly constrained orbital period. Here we analyse TESS full-frame image (FFI)\nphotometry of this faint ($T=15.9$) binary and detect a significant\n(${>}10\\sigma$) periodic signal spanning TESS Cycles 1-7, consistent with\nprevious data. We refine the orbital period to $20.897782 \\pm 0.000036$ d,\nreducing its present-day uncertainty from 18 h to 8 min. Our work is crucial\nfor scheduling follow-up observations of this system for detailed study with\nother photometric facilities. We also find that a recent orbital solution from\nDoppler data is inconsistent with existing photometry. A timing offset in the\nDoppler data may have produced a spurious signal mimicking retrograde apsidal\nprecession, from which the claimed circumbinary planet 2M1510 ABb was inferred.\nFrom our best attempt at correcting the data we were unable to reconcile the\nradial velocity data with the photometry, suggesting that the radial velocity\nuncertainties are underestimated, and that the circumbinary planet 2M1510 ABb\nmay be a false positive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eclipsing brown dwarfs are important calibrators of sub-stellar evolution\nmodels used to infer the characteristics of directly imaged brown dwarfs and\ngiant exoplanets. Only two double brown dwarf eclipsing binary systems are\nknown, among them 2MASS J15104786-2818174 (2M1510 AB), published in 2020 with a\npoorly constrained orbital period. Here we analyse TESS full-frame image (FFI)\nphotometry of this faint ($T=15.9$) binary and detect a significant\n(${>}10\\sigma$) periodic signal spanning TESS Cycles 1-7, consistent with\nprevious data. We refine the orbital period to $20.897782 \\pm 0.000036$ d,\nreducing its present-day uncertainty from 18 h to 8 min. Our work is crucial\nfor scheduling follow-up observations of this system for detailed study with\nother photometric facilities. We also find that a recent orbital solution from\nDoppler data is inconsistent with existing photometry. A timing offset in the\nDoppler data may have produced a spurious signal mimicking retrograde apsidal\nprecession, from which the claimed circumbinary planet 2M1510 ABb was inferred.\nFrom our best attempt at correcting the data we were unable to reconcile the\nradial velocity data with the photometry, suggesting that the radial velocity\nuncertainties are underestimated, and that the circumbinary planet 2M1510 ABb\nmay be a false positive."
                },
                "authors": [
                    {
                        "name": "Seb T. Millward"
                    },
                    {
                        "name": "Vedad Kunovac"
                    }
                ],
                "author_detail": {
                    "name": "Vedad Kunovac"
                },
                "author": "Vedad Kunovac",
                "arxiv_comment": "7 pages and 3 figures, accepted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.14801v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.14801v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27535v1",
                "updated": "2025-10-31T15:08:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    8,
                    18,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T15:08:18Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    8,
                    18,
                    4,
                    304,
                    0
                ],
                "title": "Patient-Centered Summarization Framework for AI Clinical Summarization:\n  A Mixed-Methods Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patient-Centered Summarization Framework for AI Clinical Summarization:\n  A Mixed-Methods Design"
                },
                "summary": "Large Language Models (LLMs) are increasingly demonstrating the potential to\nreach human-level performance in generating clinical summaries from\npatient-clinician conversations. However, these summaries often focus on\npatients' biology rather than their preferences, values, wishes, and concerns.\nTo achieve patient-centered care, we propose a new standard for Artificial\nIntelligence (AI) clinical summarization tasks: Patient-Centered Summaries\n(PCS). Our objective was to develop a framework to generate PCS that capture\npatient values and ensure clinical utility and to assess whether current\nopen-source LLMs can achieve human-level performance in this task. We used a\nmixed-methods process. Two Patient and Public Involvement groups (10 patients\nand 8 clinicians) in the United Kingdom participated in semi-structured\ninterviews exploring what personal and contextual information should be\nincluded in clinical summaries and how it should be structured for clinical\nuse. Findings informed annotation guidelines used by eight clinicians to create\ngold-standard PCS from 88 atrial fibrillation consultations. Sixteen\nconsultations were used to refine a prompt aligned with the guidelines. Five\nopen-source LLMs (Llama-3.2-3B, Llama-3.1-8B, Mistral-8B, Gemma-3-4B, and\nQwen3-8B) generated summaries for 72 consultations using zero-shot and few-shot\nprompting, evaluated with ROUGE-L, BERTScore, and qualitative metrics. Patients\nemphasized lifestyle routines, social support, recent stressors, and care\nvalues. Clinicians sought concise functional, psychosocial, and emotional\ncontext. The best zero-shot performance was achieved by Mistral-8B (ROUGE-L\n0.189) and Llama-3.1-8B (BERTScore 0.673); the best few-shot by Llama-3.1-8B\n(ROUGE-L 0.206, BERTScore 0.683). Completeness and fluency were similar between\nexperts and models, while correctness and patient-centeredness favored human\nPCS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly demonstrating the potential to\nreach human-level performance in generating clinical summaries from\npatient-clinician conversations. However, these summaries often focus on\npatients' biology rather than their preferences, values, wishes, and concerns.\nTo achieve patient-centered care, we propose a new standard for Artificial\nIntelligence (AI) clinical summarization tasks: Patient-Centered Summaries\n(PCS). Our objective was to develop a framework to generate PCS that capture\npatient values and ensure clinical utility and to assess whether current\nopen-source LLMs can achieve human-level performance in this task. We used a\nmixed-methods process. Two Patient and Public Involvement groups (10 patients\nand 8 clinicians) in the United Kingdom participated in semi-structured\ninterviews exploring what personal and contextual information should be\nincluded in clinical summaries and how it should be structured for clinical\nuse. Findings informed annotation guidelines used by eight clinicians to create\ngold-standard PCS from 88 atrial fibrillation consultations. Sixteen\nconsultations were used to refine a prompt aligned with the guidelines. Five\nopen-source LLMs (Llama-3.2-3B, Llama-3.1-8B, Mistral-8B, Gemma-3-4B, and\nQwen3-8B) generated summaries for 72 consultations using zero-shot and few-shot\nprompting, evaluated with ROUGE-L, BERTScore, and qualitative metrics. Patients\nemphasized lifestyle routines, social support, recent stressors, and care\nvalues. Clinicians sought concise functional, psychosocial, and emotional\ncontext. The best zero-shot performance was achieved by Mistral-8B (ROUGE-L\n0.189) and Llama-3.1-8B (BERTScore 0.673); the best few-shot by Llama-3.1-8B\n(ROUGE-L 0.206, BERTScore 0.683). Completeness and fluency were similar between\nexperts and models, while correctness and patient-centeredness favored human\nPCS."
                },
                "authors": [
                    {
                        "name": "Maria Lizarazo Jimenez"
                    },
                    {
                        "name": "Ana Gabriela Claros"
                    },
                    {
                        "name": "Kieran Green"
                    },
                    {
                        "name": "David Toro-Tobon"
                    },
                    {
                        "name": "Felipe Larios"
                    },
                    {
                        "name": "Sheena Asthana"
                    },
                    {
                        "name": "Camila Wenczenovicz"
                    },
                    {
                        "name": "Kerly Guevara Maldonado"
                    },
                    {
                        "name": "Luis Vilatuna-Andrango"
                    },
                    {
                        "name": "Cristina Proano-Velez"
                    },
                    {
                        "name": "Satya Sai Sri Bandi"
                    },
                    {
                        "name": "Shubhangi Bagewadi"
                    },
                    {
                        "name": "Megan E. Branda"
                    },
                    {
                        "name": "Misk Al Zahidy"
                    },
                    {
                        "name": "Saturnino Luz"
                    },
                    {
                        "name": "Mirella Lapata"
                    },
                    {
                        "name": "Juan P. Brito"
                    },
                    {
                        "name": "Oscar J. Ponce-Ponte"
                    }
                ],
                "author_detail": {
                    "name": "Oscar J. Ponce-Ponte"
                },
                "author": "Oscar J. Ponce-Ponte",
                "arxiv_comment": "The first two listed authors contributed equally Pages: 21;\n  Figures:2; Tables:3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27527v1",
                "updated": "2025-10-31T14:57:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    57,
                    16,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T14:57:16Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    57,
                    16,
                    4,
                    304,
                    0
                ],
                "title": "TetraJet-v2: Accurate NVFP4 Training for Large Language Models with\n  Oscillation Suppression and Outlier Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TetraJet-v2: Accurate NVFP4 Training for Large Language Models with\n  Oscillation Suppression and Outlier Control"
                },
                "summary": "Large Language Models (LLMs) training is prohibitively expensive, driving\ninterest in low-precision fully-quantized training (FQT). While novel 4-bit\nformats like NVFP4 offer substantial efficiency gains, achieving near-lossless\ntraining at such low precision remains challenging. We introduce TetraJet-v2,\nan end-to-end 4-bit FQT method that leverages NVFP4 for activations, weights,\nand gradients in all linear layers. We identify two critical issues hindering\nlow-precision LLM training: weight oscillation and outliers. To address these,\nwe propose: 1) an unbiased double-block quantization method for NVFP4 linear\nlayers, 2) OsciReset, an algorithm to suppress weight oscillation, and 3)\nOutControl, an algorithm to retain outlier accuracy. TetraJet-v2 consistently\noutperforms prior FP4 training methods on pre-training LLMs across varying\nmodel sizes up to 370M and data sizes up to 200B tokens, reducing the\nperformance gap to full-precision training by an average of 51.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) training is prohibitively expensive, driving\ninterest in low-precision fully-quantized training (FQT). While novel 4-bit\nformats like NVFP4 offer substantial efficiency gains, achieving near-lossless\ntraining at such low precision remains challenging. We introduce TetraJet-v2,\nan end-to-end 4-bit FQT method that leverages NVFP4 for activations, weights,\nand gradients in all linear layers. We identify two critical issues hindering\nlow-precision LLM training: weight oscillation and outliers. To address these,\nwe propose: 1) an unbiased double-block quantization method for NVFP4 linear\nlayers, 2) OsciReset, an algorithm to suppress weight oscillation, and 3)\nOutControl, an algorithm to retain outlier accuracy. TetraJet-v2 consistently\noutperforms prior FP4 training methods on pre-training LLMs across varying\nmodel sizes up to 370M and data sizes up to 200B tokens, reducing the\nperformance gap to full-precision training by an average of 51.3%."
                },
                "authors": [
                    {
                        "name": "Yuxiang Chen"
                    },
                    {
                        "name": "Xiaoming Xu"
                    },
                    {
                        "name": "Pengle Zhang"
                    },
                    {
                        "name": "Michael Beyer"
                    },
                    {
                        "name": "Martin Rapp"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Jianfei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Chen"
                },
                "author": "Jianfei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12989v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12989v3",
                "updated": "2025-10-31T14:56:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    56,
                    12,
                    4,
                    304,
                    0
                ],
                "published": "2025-03-17T09:44:50Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    50,
                    0,
                    76,
                    0
                ],
                "title": "A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation\n  Classification Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation\n  Classification Using Large Language Models"
                },
                "summary": "Automatically annotating job data with standardized occupations from\ntaxonomies, known as occupation classification, is crucial for labor market\nanalysis. However, this task is often hindered by data scarcity and the\nchallenges of manual annotations. While large language models (LLMs) hold\npromise due to their extensive world knowledge and in-context learning\ncapabilities, their effectiveness depends on their knowledge of occupational\ntaxonomies, which remains unclear. In this study, we assess the ability of LLMs\nto generate precise taxonomic entities from taxonomy, highlighting their\nlimitations, especially for smaller models. To address these challenges, we\npropose a multi-stage framework consisting of inference, retrieval, and\nreranking stages, which integrates taxonomy-guided reasoning examples to\nenhance performance by aligning outputs with taxonomic knowledge. Evaluations\non a large-scale dataset show that our framework not only enhances occupation\nand skill classification tasks, but also provides a cost-effective alternative\nto frontier models like GPT-4o, significantly reducing computational costs\nwhile maintaining strong performance. This makes it a practical and scalable\nsolution for occupation classification and related tasks across LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically annotating job data with standardized occupations from\ntaxonomies, known as occupation classification, is crucial for labor market\nanalysis. However, this task is often hindered by data scarcity and the\nchallenges of manual annotations. While large language models (LLMs) hold\npromise due to their extensive world knowledge and in-context learning\ncapabilities, their effectiveness depends on their knowledge of occupational\ntaxonomies, which remains unclear. In this study, we assess the ability of LLMs\nto generate precise taxonomic entities from taxonomy, highlighting their\nlimitations, especially for smaller models. To address these challenges, we\npropose a multi-stage framework consisting of inference, retrieval, and\nreranking stages, which integrates taxonomy-guided reasoning examples to\nenhance performance by aligning outputs with taxonomic knowledge. Evaluations\non a large-scale dataset show that our framework not only enhances occupation\nand skill classification tasks, but also provides a cost-effective alternative\nto frontier models like GPT-4o, significantly reducing computational costs\nwhile maintaining strong performance. This makes it a practical and scalable\nsolution for occupation classification and related tasks across LLMs."
                },
                "authors": [
                    {
                        "name": "Palakorn Achananuparp"
                    },
                    {
                        "name": "Ee-Peng Lim"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "arxiv_comment": "Accepted to ICWSM'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12989v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12989v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27521v1",
                "updated": "2025-10-31T14:47:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    47,
                    11,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T14:47:11Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    47,
                    11,
                    4,
                    304,
                    0
                ],
                "title": "Independent Clinical Evaluation of General-Purpose LLM Responses to\n  Signals of Suicide Risk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Independent Clinical Evaluation of General-Purpose LLM Responses to\n  Signals of Suicide Risk"
                },
                "summary": "We introduce findings and methods to facilitate evidence-based discussion\nabout how large language models (LLMs) should behave in response to user\nsignals of risk of suicidal thoughts and behaviors (STB). People are already\nusing LLMs as mental health resources, and several recent incidents implicate\nLLMs in mental health crises. Despite growing attention, few studies have been\nable to effectively generalize clinical guidelines to LLM use cases, and fewer\nstill have proposed methodologies that can be iteratively applied as knowledge\nimproves about the elements of human-AI interaction most in need of study. We\nintroduce an assessment of LLM alignment with guidelines for ethical\ncommunication, adapted from clinical principles and applied to expressions of\nrisk factors for STB in multi-turn conversations. Using a codebook created and\nvalidated by clinicians, mobilizing the volunteer participation of practicing\ntherapists and trainees (N=43) based in the U.S., and using generalized linear\nmixed-effects models for statistical analysis, we assess a single fully\nopen-source LLM, OLMo-2-32b. We show how to assess when a model deviates from\nclinically informed guidelines in a way that may pose a hazard and (thanks to\nits open nature) facilitates future investigation as to why. We find that\ncontrary to clinical best practice, OLMo-2-32b, and, possibly by extension,\nother LLMs, will become less likely to invite continued dialog as users send\nmore signals of STB risk in multi-turn settings. We also show that OLMo-2-32b\nresponds differently depending on the risk factor expressed. This empirical\nevidence highlights that just as chatbots pose hazards if their responses\nreinforce delusions or assist in suicidal acts, they may also discourage\nfurther help-seeking or cause feelings of dismissal or abandonment by\nwithdrawing from conversations when STB risk is expressed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce findings and methods to facilitate evidence-based discussion\nabout how large language models (LLMs) should behave in response to user\nsignals of risk of suicidal thoughts and behaviors (STB). People are already\nusing LLMs as mental health resources, and several recent incidents implicate\nLLMs in mental health crises. Despite growing attention, few studies have been\nable to effectively generalize clinical guidelines to LLM use cases, and fewer\nstill have proposed methodologies that can be iteratively applied as knowledge\nimproves about the elements of human-AI interaction most in need of study. We\nintroduce an assessment of LLM alignment with guidelines for ethical\ncommunication, adapted from clinical principles and applied to expressions of\nrisk factors for STB in multi-turn conversations. Using a codebook created and\nvalidated by clinicians, mobilizing the volunteer participation of practicing\ntherapists and trainees (N=43) based in the U.S., and using generalized linear\nmixed-effects models for statistical analysis, we assess a single fully\nopen-source LLM, OLMo-2-32b. We show how to assess when a model deviates from\nclinically informed guidelines in a way that may pose a hazard and (thanks to\nits open nature) facilitates future investigation as to why. We find that\ncontrary to clinical best practice, OLMo-2-32b, and, possibly by extension,\nother LLMs, will become less likely to invite continued dialog as users send\nmore signals of STB risk in multi-turn settings. We also show that OLMo-2-32b\nresponds differently depending on the risk factor expressed. This empirical\nevidence highlights that just as chatbots pose hazards if their responses\nreinforce delusions or assist in suicidal acts, they may also discourage\nfurther help-seeking or cause feelings of dismissal or abandonment by\nwithdrawing from conversations when STB risk is expressed."
                },
                "authors": [
                    {
                        "name": "Nick Judd"
                    },
                    {
                        "name": "Alexandre Vaz"
                    },
                    {
                        "name": "Kevin Paeth"
                    },
                    {
                        "name": "Layla Ins Davis"
                    },
                    {
                        "name": "Milena Esherick"
                    },
                    {
                        "name": "Jason Brand"
                    },
                    {
                        "name": "Ins Amaro"
                    },
                    {
                        "name": "Tony Rousmaniere"
                    }
                ],
                "author_detail": {
                    "name": "Tony Rousmaniere"
                },
                "author": "Tony Rousmaniere",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02574v2",
                "updated": "2025-10-31T14:41:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    41,
                    37,
                    4,
                    304,
                    0
                ],
                "published": "2025-02-04T18:47:35Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    47,
                    35,
                    1,
                    35,
                    0
                ],
                "title": "Probing large-scale structures with the two-point function and the power\n  spectrum: insights into cosmic clustering evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing large-scale structures with the two-point function and the power\n  spectrum: insights into cosmic clustering evolution"
                },
                "summary": "Understanding the large-scale structure of the Universe requires analyses of\ncosmic clustering and its evolution over time. In this work, we investigate the\nclustering properties of SDSS blue galaxies, which are excellent tracers of\ndark matter, along two distinct epochs of the Universe, utilizing estimators\nlike the two-point angular correlation function (2PACF), the angular power\nspectra, among others. Considering a model-independent approach, we perform\nanalyses in two disjoint redshift shells, $0 \\leq z < 0.06$ and $0.06 \\leq z <\n0.12$, to investigate the distribution of large cosmic structures. Using\nBayesian inference methods, we constrain the parameter that quantifies the\ngalaxy clustering in the 2PACF, enabling us to perform comparisons among\ndifferent regions on the sky and between different epochs in the Universe\nregarding the gravitational action on matter structures. Our analyses\ncomplement previous efforts to map large-scale structures in the Local\nUniverse. In addition, this study reveals differences regarding the clustering\nof large cosmic structures comparing two epochs of the Universe, analyses done\nwith diverse estimators. Results reveal, clearly, distinct evolutionary\nsignatures between the two redshift shells. Moreover, we had the opportunity to\ntest the concordance cosmological model under extreme conditions in the highly\nnon-linear Local Universe, computing the amplitude of the angular power\nspectrum at very small scales. Ultimately, all our analyses serve as a set of\nconsistency tests of the concordance cosmological model, the $\\Lambda$CDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the large-scale structure of the Universe requires analyses of\ncosmic clustering and its evolution over time. In this work, we investigate the\nclustering properties of SDSS blue galaxies, which are excellent tracers of\ndark matter, along two distinct epochs of the Universe, utilizing estimators\nlike the two-point angular correlation function (2PACF), the angular power\nspectra, among others. Considering a model-independent approach, we perform\nanalyses in two disjoint redshift shells, $0 \\leq z < 0.06$ and $0.06 \\leq z <\n0.12$, to investigate the distribution of large cosmic structures. Using\nBayesian inference methods, we constrain the parameter that quantifies the\ngalaxy clustering in the 2PACF, enabling us to perform comparisons among\ndifferent regions on the sky and between different epochs in the Universe\nregarding the gravitational action on matter structures. Our analyses\ncomplement previous efforts to map large-scale structures in the Local\nUniverse. In addition, this study reveals differences regarding the clustering\nof large cosmic structures comparing two epochs of the Universe, analyses done\nwith diverse estimators. Results reveal, clearly, distinct evolutionary\nsignatures between the two redshift shells. Moreover, we had the opportunity to\ntest the concordance cosmological model under extreme conditions in the highly\nnon-linear Local Universe, computing the amplitude of the angular power\nspectrum at very small scales. Ultimately, all our analyses serve as a set of\nconsistency tests of the concordance cosmological model, the $\\Lambda$CDM."
                },
                "authors": [
                    {
                        "name": "Camila Franco"
                    },
                    {
                        "name": "Felipe Avila"
                    },
                    {
                        "name": "Armando Bernui"
                    }
                ],
                "author_detail": {
                    "name": "Armando Bernui"
                },
                "author": "Armando Bernui",
                "arxiv_doi": "10.3847/1538-4357/ae0a32",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/ae0a32",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.02574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "24 pages, 21 figures, published in ApJ",
                "arxiv_journal_ref": "ApJ 993 133 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27505v1",
                "updated": "2025-10-31T14:28:46Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    28,
                    46,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T14:28:46Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    28,
                    46,
                    4,
                    304,
                    0
                ],
                "title": "Euclid: Systematic uncertainties from the halo mass conversion on galaxy\n  cluster number count data analyses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Euclid: Systematic uncertainties from the halo mass conversion on galaxy\n  cluster number count data analyses"
                },
                "summary": "The large catalogues of galaxy clusters expected from the Euclid survey will\nenable cosmological analyses of cluster number counts that require accurate\ncosmological model predictions. One possibility is to use parametric fits\ncalibrated against $N$-body simulations, that capture the cosmological\nparameter dependence of the halo mass function. Several studies have shown that\nthis can be obtained through a calibration against haloes with spherical masses\ndefined at the virial overdensity. In contrast, if different mass definitions\nare used for the HMF and the scaling relation, a mapping between them is\nrequired. Here, we investigate the impact of such a mapping on the cosmological\nparameter constraints inferred from galaxy cluster number counts. Using\nsynthetic data from $N$-body simulations, we show that the standard approach,\nwhich relies on assuming a concentration-mass relation, can introduce\nsignificant systematic bias. In particular, depending on the mass definition\nand the relation assumed, this can lead to biased constraints at more than\n2$\\sigma$ level. In contrast, we find that in all the cases we have considered,\nthe mass conversion based on the halo sparsity statistics result in a\nsystematic bias smaller than the statistical error.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large catalogues of galaxy clusters expected from the Euclid survey will\nenable cosmological analyses of cluster number counts that require accurate\ncosmological model predictions. One possibility is to use parametric fits\ncalibrated against $N$-body simulations, that capture the cosmological\nparameter dependence of the halo mass function. Several studies have shown that\nthis can be obtained through a calibration against haloes with spherical masses\ndefined at the virial overdensity. In contrast, if different mass definitions\nare used for the HMF and the scaling relation, a mapping between them is\nrequired. Here, we investigate the impact of such a mapping on the cosmological\nparameter constraints inferred from galaxy cluster number counts. Using\nsynthetic data from $N$-body simulations, we show that the standard approach,\nwhich relies on assuming a concentration-mass relation, can introduce\nsignificant systematic bias. In particular, depending on the mass definition\nand the relation assumed, this can lead to biased constraints at more than\n2$\\sigma$ level. In contrast, we find that in all the cases we have considered,\nthe mass conversion based on the halo sparsity statistics result in a\nsystematic bias smaller than the statistical error."
                },
                "authors": [
                    {
                        "name": "T. Gayoux"
                    },
                    {
                        "name": "P. -S. Corasaniti"
                    },
                    {
                        "name": "T. R. G. Richardson"
                    },
                    {
                        "name": "S. T. Kay"
                    },
                    {
                        "name": "A. M. C. Le Brun"
                    },
                    {
                        "name": "L. Moscardini"
                    },
                    {
                        "name": "L. Pizzuti"
                    },
                    {
                        "name": "S. Borgani"
                    },
                    {
                        "name": "M. Costanzi"
                    },
                    {
                        "name": "C. Giocoli"
                    },
                    {
                        "name": "S. Grandis"
                    },
                    {
                        "name": "A. Ragagnin"
                    },
                    {
                        "name": "J. Rhodes"
                    },
                    {
                        "name": "I. Saez-Casares"
                    },
                    {
                        "name": "M. Sereno"
                    },
                    {
                        "name": "E. Sarpa"
                    },
                    {
                        "name": "B. Altieri"
                    },
                    {
                        "name": "A. Amara"
                    },
                    {
                        "name": "S. Andreon"
                    },
                    {
                        "name": "N. Auricchio"
                    },
                    {
                        "name": "C. Baccigalupi"
                    },
                    {
                        "name": "M. Baldi"
                    },
                    {
                        "name": "S. Bardelli"
                    },
                    {
                        "name": "A. Biviano"
                    },
                    {
                        "name": "E. Branchini"
                    },
                    {
                        "name": "M. Brescia"
                    },
                    {
                        "name": "S. Camera"
                    },
                    {
                        "name": "G. Canas-Herrera"
                    },
                    {
                        "name": "V. Capobianco"
                    },
                    {
                        "name": "C. Carbone"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "S. Casas"
                    },
                    {
                        "name": "M. Castellano"
                    },
                    {
                        "name": "G. Castignani"
                    },
                    {
                        "name": "S. Cavuoti"
                    },
                    {
                        "name": "K. C. Chambers"
                    },
                    {
                        "name": "A. Cimatti"
                    },
                    {
                        "name": "C. Colodro-Conde"
                    },
                    {
                        "name": "G. Congedo"
                    },
                    {
                        "name": "L. Conversi"
                    },
                    {
                        "name": "Y. Copin"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "H. M. Courtois"
                    },
                    {
                        "name": "A. Da Silva"
                    },
                    {
                        "name": "H. Degaudenzi"
                    },
                    {
                        "name": "G. De Lucia"
                    },
                    {
                        "name": "H. Dole"
                    },
                    {
                        "name": "F. Dubath"
                    },
                    {
                        "name": "C. A. J. Duncan"
                    },
                    {
                        "name": "X. Dupac"
                    },
                    {
                        "name": "S. Dusini"
                    },
                    {
                        "name": "S. Escoffier"
                    },
                    {
                        "name": "M. Farina"
                    },
                    {
                        "name": "R. Farinelli"
                    },
                    {
                        "name": "S. Farrens"
                    },
                    {
                        "name": "F. Faustini"
                    },
                    {
                        "name": "S. Ferriol"
                    },
                    {
                        "name": "F. Finelli"
                    },
                    {
                        "name": "M. Frailis"
                    },
                    {
                        "name": "E. Franceschi"
                    },
                    {
                        "name": "M. Fumana"
                    },
                    {
                        "name": "S. Galeotta"
                    },
                    {
                        "name": "B. Gillis"
                    },
                    {
                        "name": "J. Gracia-Carpio"
                    },
                    {
                        "name": "A. Grazian"
                    },
                    {
                        "name": "F. Grupp"
                    },
                    {
                        "name": "S. V. H. Haugan"
                    },
                    {
                        "name": "W. Holmes"
                    },
                    {
                        "name": "F. Hormuth"
                    },
                    {
                        "name": "A. Hornstrup"
                    },
                    {
                        "name": "K. Jahnke"
                    },
                    {
                        "name": "M. Jhabvala"
                    },
                    {
                        "name": "E. Keihanen"
                    },
                    {
                        "name": "S. Kermiche"
                    },
                    {
                        "name": "A. Kiessling"
                    },
                    {
                        "name": "M. Kilbinger"
                    },
                    {
                        "name": "B. Kubik"
                    },
                    {
                        "name": "M. Kunz"
                    },
                    {
                        "name": "H. Kurki-Suonio"
                    },
                    {
                        "name": "O. Lahav"
                    },
                    {
                        "name": "S. Ligori"
                    },
                    {
                        "name": "P. B. Lilje"
                    },
                    {
                        "name": "V. Lindholm"
                    },
                    {
                        "name": "I. Lloro"
                    },
                    {
                        "name": "G. Mainetti"
                    },
                    {
                        "name": "D. Maino"
                    },
                    {
                        "name": "E. Maiorano"
                    },
                    {
                        "name": "O. Mansutti"
                    },
                    {
                        "name": "S. Marcin"
                    },
                    {
                        "name": "O. Marggraf"
                    },
                    {
                        "name": "M. Martinelli"
                    },
                    {
                        "name": "N. Martinet"
                    },
                    {
                        "name": "F. Marulli"
                    },
                    {
                        "name": "R. J. Massey"
                    },
                    {
                        "name": "E. Medinaceli"
                    },
                    {
                        "name": "S. Mei"
                    },
                    {
                        "name": "Y. Mellier"
                    },
                    {
                        "name": "M. Meneghetti"
                    },
                    {
                        "name": "E. Merlin"
                    },
                    {
                        "name": "G. Meylan"
                    },
                    {
                        "name": "A. Mora"
                    },
                    {
                        "name": "M. Moresco"
                    },
                    {
                        "name": "E. Munari"
                    },
                    {
                        "name": "C. Neissner"
                    },
                    {
                        "name": "S. -M. Niemi"
                    },
                    {
                        "name": "C. Padilla"
                    },
                    {
                        "name": "S. Paltani"
                    },
                    {
                        "name": "F. Pasian"
                    },
                    {
                        "name": "K. Pedersen"
                    },
                    {
                        "name": "V. Pettorino"
                    },
                    {
                        "name": "S. Pires"
                    },
                    {
                        "name": "G. Polenta"
                    },
                    {
                        "name": "M. Poncet"
                    },
                    {
                        "name": "L. A. Popa"
                    },
                    {
                        "name": "L. Pozzetti"
                    },
                    {
                        "name": "F. Raison"
                    },
                    {
                        "name": "R. Rebolo"
                    },
                    {
                        "name": "A. Renzi"
                    },
                    {
                        "name": "G. Riccio"
                    },
                    {
                        "name": "E. Romelli"
                    },
                    {
                        "name": "M. Roncarelli"
                    },
                    {
                        "name": "R. Saglia"
                    },
                    {
                        "name": "Z. Sakr"
                    },
                    {
                        "name": "D. Sapone"
                    },
                    {
                        "name": "B. Sartoris"
                    },
                    {
                        "name": "P. Schneider"
                    },
                    {
                        "name": "A. Secroun"
                    },
                    {
                        "name": "G. Seidel"
                    },
                    {
                        "name": "S. Serrano"
                    },
                    {
                        "name": "P. Simon"
                    },
                    {
                        "name": "C. Sirignano"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "J. -L. Starck"
                    },
                    {
                        "name": "J. Steinwagner"
                    },
                    {
                        "name": "P. Tallada-Cresp"
                    },
                    {
                        "name": "A. N. Taylor"
                    },
                    {
                        "name": "I. Tereno"
                    },
                    {
                        "name": "N. Tessore"
                    },
                    {
                        "name": "S. Toft"
                    },
                    {
                        "name": "R. Toledo-Moreo"
                    },
                    {
                        "name": "F. Torradeflot"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "L. Valenziano"
                    },
                    {
                        "name": "J. Valiviita"
                    },
                    {
                        "name": "T. Vassallo"
                    },
                    {
                        "name": "G. Verdoes Kleijn"
                    },
                    {
                        "name": "A. Veropalumbo"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "J. Weller"
                    },
                    {
                        "name": "G. Zamorani"
                    },
                    {
                        "name": "E. Zucca"
                    },
                    {
                        "name": "C. Burigana"
                    },
                    {
                        "name": "M. Maturi"
                    },
                    {
                        "name": "V. Scottez"
                    },
                    {
                        "name": "M. Viel"
                    }
                ],
                "author_detail": {
                    "name": "M. Viel"
                },
                "author": "M. Viel",
                "arxiv_comment": "20 pages, 11 figures, submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27504v1",
                "updated": "2025-10-31T14:28:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    28,
                    31,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T14:28:31Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    28,
                    31,
                    4,
                    304,
                    0
                ],
                "title": "DP-FedPGN: Finding Global Flat Minima for Differentially Private\n  Federated Learning via Penalizing Gradient Norm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DP-FedPGN: Finding Global Flat Minima for Differentially Private\n  Federated Learning via Penalizing Gradient Norm"
                },
                "summary": "To prevent inference attacks in Federated Learning (FL) and reduce the\nleakage of sensitive information, Client-level Differentially Private Federated\nLearning (CL-DPFL) is widely used. However, current CL-DPFL methods usually\nresult in sharper loss landscapes, which leads to a decrease in model\ngeneralization after differential privacy protection. By using Sharpness Aware\nMinimization (SAM), the current popular federated learning methods are to find\na local flat minimum value to alleviate this problem. However, the local\nflatness may not reflect the global flatness in CL-DPFL. Therefore, to address\nthis issue and seek global flat minima of models, we propose a new CL-DPFL\nalgorithm, DP-FedPGN, in which we introduce a global gradient norm penalty to\nthe local loss to find the global flat minimum. Moreover, by using our global\ngradient norm penalty, we not only find a flatter global minimum but also\nreduce the locally updated norm, which means that we further reduce the error\nof gradient clipping. From a theoretical perspective, we analyze how DP-FedPGN\nmitigates the performance degradation caused by DP. Meanwhile, the proposed\nDP-FedPGN algorithm eliminates the impact of data heterogeneity and achieves\nfast convergence. We also use R\\'enyi DP to provide strict privacy guarantees\nand provide sensitivity analysis for local updates. Finally, we conduct\neffectiveness tests on both ResNet and Transformer models, and achieve\nsignificant improvements in six visual and natural language processing tasks\ncompared to existing state-of-the-art algorithms. The code is available at\nhttps://github.com/junkangLiu0/DP-FedPGN",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To prevent inference attacks in Federated Learning (FL) and reduce the\nleakage of sensitive information, Client-level Differentially Private Federated\nLearning (CL-DPFL) is widely used. However, current CL-DPFL methods usually\nresult in sharper loss landscapes, which leads to a decrease in model\ngeneralization after differential privacy protection. By using Sharpness Aware\nMinimization (SAM), the current popular federated learning methods are to find\na local flat minimum value to alleviate this problem. However, the local\nflatness may not reflect the global flatness in CL-DPFL. Therefore, to address\nthis issue and seek global flat minima of models, we propose a new CL-DPFL\nalgorithm, DP-FedPGN, in which we introduce a global gradient norm penalty to\nthe local loss to find the global flat minimum. Moreover, by using our global\ngradient norm penalty, we not only find a flatter global minimum but also\nreduce the locally updated norm, which means that we further reduce the error\nof gradient clipping. From a theoretical perspective, we analyze how DP-FedPGN\nmitigates the performance degradation caused by DP. Meanwhile, the proposed\nDP-FedPGN algorithm eliminates the impact of data heterogeneity and achieves\nfast convergence. We also use R\\'enyi DP to provide strict privacy guarantees\nand provide sensitivity analysis for local updates. Finally, we conduct\neffectiveness tests on both ResNet and Transformer models, and achieve\nsignificant improvements in six visual and natural language processing tasks\ncompared to existing state-of-the-art algorithms. The code is available at\nhttps://github.com/junkangLiu0/DP-FedPGN"
                },
                "authors": [
                    {
                        "name": "Junkang Liu"
                    },
                    {
                        "name": "Yuxuan Tian"
                    },
                    {
                        "name": "Fanhua Shang"
                    },
                    {
                        "name": "Yuanyuan Liu"
                    },
                    {
                        "name": "Hongying Liu"
                    },
                    {
                        "name": "Junchao Zhou"
                    },
                    {
                        "name": "Daorui Ding"
                    }
                ],
                "author_detail": {
                    "name": "Daorui Ding"
                },
                "author": "Daorui Ding",
                "arxiv_comment": "21 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19500v2",
                "updated": "2025-10-31T14:24:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    24,
                    22,
                    4,
                    304,
                    0
                ],
                "published": "2025-06-24T10:39:07Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    39,
                    7,
                    1,
                    175,
                    0
                ],
                "title": "NaviAgent: Bilevel Planning on Tool Navigation Graph for Large-Scale\n  Orchestration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NaviAgent: Bilevel Planning on Tool Navigation Graph for Large-Scale\n  Orchestration"
                },
                "summary": "Large language models (LLMs) have recently demonstrated the ability to act as\nfunction call agents by invoking external tools, enabling them to solve tasks\nbeyond their static knowledge. However, existing agents typically call tools\nstep by step at a time without a global view of task structure. As tools depend\non each other, this leads to error accumulation and limited scalability,\nparticularly when scaling to thousands of tools. To address these limitations,\nwe propose NaviAgent, a novel bilevel architecture that decouples task planning\nfrom tool execution through graph-based modeling of the tool ecosystem. At the\ntask-planning level, the LLM-based agent decides whether to respond directly,\nclarify user intent, invoke a toolchain, or execute tool outputs, ensuring\nbroad coverage of interaction scenarios independent of inter-tool complexity.\nAt the execution level, a continuously evolving Tool World Navigation Model\n(TWNM) encodes structural and behavioral relations among tools, guiding the\nagent to generate scalable and robust invocation sequences. By incorporating\nfeedback from real tool interactions, NaviAgent supports closed-loop\noptimization of planning and execution, moving beyond tool calling toward\nadaptive navigation of large-scale tool ecosystems. Experiments show that\nNaviAgent achieves the best task success rates across models and tasks, and\nintegrating TWMN further boosts performance by up to 17 points on complex\ntasks, underscoring its key role in toolchain orchestration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently demonstrated the ability to act as\nfunction call agents by invoking external tools, enabling them to solve tasks\nbeyond their static knowledge. However, existing agents typically call tools\nstep by step at a time without a global view of task structure. As tools depend\non each other, this leads to error accumulation and limited scalability,\nparticularly when scaling to thousands of tools. To address these limitations,\nwe propose NaviAgent, a novel bilevel architecture that decouples task planning\nfrom tool execution through graph-based modeling of the tool ecosystem. At the\ntask-planning level, the LLM-based agent decides whether to respond directly,\nclarify user intent, invoke a toolchain, or execute tool outputs, ensuring\nbroad coverage of interaction scenarios independent of inter-tool complexity.\nAt the execution level, a continuously evolving Tool World Navigation Model\n(TWNM) encodes structural and behavioral relations among tools, guiding the\nagent to generate scalable and robust invocation sequences. By incorporating\nfeedback from real tool interactions, NaviAgent supports closed-loop\noptimization of planning and execution, moving beyond tool calling toward\nadaptive navigation of large-scale tool ecosystems. Experiments show that\nNaviAgent achieves the best task success rates across models and tasks, and\nintegrating TWMN further boosts performance by up to 17 points on complex\ntasks, underscoring its key role in toolchain orchestration."
                },
                "authors": [
                    {
                        "name": "Yan Jiang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "LiZhong GU"
                    },
                    {
                        "name": "Ai Han"
                    },
                    {
                        "name": "TianLong Li"
                    }
                ],
                "author_detail": {
                    "name": "TianLong Li"
                },
                "author": "TianLong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27499v1",
                "updated": "2025-10-31T14:21:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    21,
                    56,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T14:21:56Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    21,
                    56,
                    4,
                    304,
                    0
                ],
                "title": "First-principles calculations of thermal transport at metal/silicon\n  interfaces: evidence of interfacial electron-phonon coupling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First-principles calculations of thermal transport at metal/silicon\n  interfaces: evidence of interfacial electron-phonon coupling"
                },
                "summary": "With the increasing miniaturization of electronic components and the need to\noptimize thermal management, it has become essential to understand heat\ntransport at metal/semiconductor interfaces. While it has been recognized\ndecades ago that an electron phonon channel may take place at\nmetal-semiconductor interfaces, its existence is still controversial. Here, we\ninvestigate thermal transport at metal-silicon interfaces using the combination\nof first principles calculations and nonequilibrium Green's function (NEGF). We\nexplain how to correct NEGF formalism to account for the out of equilibrium\nnature of the energy carriers in the vicinity of the interface. The relative\ncorrections to the equilibrium distribution are shown to arise from the\nspectral mean free paths of silicon and may reach 15 percents. Applying these\ncorrections, we compare the predictions of NEGF to available experimental data\nfor Au/Si, Pt/Si and Al/Si interfaces. Based on this comparison, we infer the\nvalue of the electron phonon interfacial thermal conductance by employing the\ntwo temperature model. We find that interfacial thermal transport at Au/Si\ninterfaces is mainly driven by phonon phonon processes, and that electron\nphonon processes play a negligible role in this case. By contrast, for Al/Si\ninterfaces, we show that phonon-phonon scattering alone can not explain the\nexperimental values reported so far, and we estimate that the electron-phonon\ninterfacial conductance accounts for one third of the total conductance. This\nwork demonstrates the importance of the electron-phonon conductance at\nmetal-silicon interfaces and calls for systematic experimental investigation of\nthermal transport at these interfaces at low temperatures. It paves the way for\nan accurate model to predict the conductance associated to the interfacial\nelectron phonon channel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing miniaturization of electronic components and the need to\noptimize thermal management, it has become essential to understand heat\ntransport at metal/semiconductor interfaces. While it has been recognized\ndecades ago that an electron phonon channel may take place at\nmetal-semiconductor interfaces, its existence is still controversial. Here, we\ninvestigate thermal transport at metal-silicon interfaces using the combination\nof first principles calculations and nonequilibrium Green's function (NEGF). We\nexplain how to correct NEGF formalism to account for the out of equilibrium\nnature of the energy carriers in the vicinity of the interface. The relative\ncorrections to the equilibrium distribution are shown to arise from the\nspectral mean free paths of silicon and may reach 15 percents. Applying these\ncorrections, we compare the predictions of NEGF to available experimental data\nfor Au/Si, Pt/Si and Al/Si interfaces. Based on this comparison, we infer the\nvalue of the electron phonon interfacial thermal conductance by employing the\ntwo temperature model. We find that interfacial thermal transport at Au/Si\ninterfaces is mainly driven by phonon phonon processes, and that electron\nphonon processes play a negligible role in this case. By contrast, for Al/Si\ninterfaces, we show that phonon-phonon scattering alone can not explain the\nexperimental values reported so far, and we estimate that the electron-phonon\ninterfacial conductance accounts for one third of the total conductance. This\nwork demonstrates the importance of the electron-phonon conductance at\nmetal-silicon interfaces and calls for systematic experimental investigation of\nthermal transport at these interfaces at low temperatures. It paves the way for\nan accurate model to predict the conductance associated to the interfacial\nelectron phonon channel."
                },
                "authors": [
                    {
                        "name": "Michal De San Fliciano"
                    },
                    {
                        "name": "Christophe Adessi"
                    },
                    {
                        "name": "Julien El Hajj"
                    },
                    {
                        "name": "Nicolas Horny"
                    },
                    {
                        "name": "Franois Detcheverry"
                    },
                    {
                        "name": "Manuel Cobian"
                    },
                    {
                        "name": "Samy Merabia"
                    }
                ],
                "author_detail": {
                    "name": "Samy Merabia"
                },
                "author": "Samy Merabia",
                "arxiv_comment": "accepted in Phys. Rev. B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27489v1",
                "updated": "2025-10-31T14:07:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    7,
                    42,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T14:07:42Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    7,
                    42,
                    4,
                    304,
                    0
                ],
                "title": "Auditing LLM Editorial Bias in News Media Exposure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing LLM Editorial Bias in News Media Exposure"
                },
                "summary": "Large Language Models (LLMs) increasingly act as gateways to web content,\nshaping how millions of users encounter online information. Unlike traditional\nsearch engines, whose retrieval and ranking mechanisms are well studied, the\nselection processes of web-connected LLMs add layers of opacity to how answers\nare generated. By determining which news outlets users see, these systems can\ninfluence public opinion, reinforce echo chambers, and pose risks to civic\ndiscourse and public trust.\n  This work extends two decades of research in algorithmic auditing to examine\nhow LLMs function as news engines. We present the first audit comparing three\nleading agents, GPT-4o-Mini, Claude-3.7-Sonnet, and Gemini-2.0-Flash, against\nGoogle News, asking: \\textit{How do LLMs differ from traditional aggregators in\nthe diversity, ideology, and reliability of the media they expose to users?}\n  Across 24 global topics, we find that, compared to Google News, LLMs surface\nsignificantly fewer unique outlets and allocate attention more unevenly. In the\nsame way, GPT-4o-Mini emphasizes more factual and right-leaning sources;\nClaude-3.7-Sonnet favors institutional and civil-society domains and slightly\namplifies right-leaning exposure; and Gemini-2.0-Flash exhibits a modest\nleft-leaning tilt without significant changes in factuality. These patterns\nremain robust under prompt variations and alternative reliability benchmarks.\nTogether, our findings show that LLMs already enact \\textit{agentic editorial\npolicies}, curating information in ways that diverge from conventional\naggregators. Understanding and governing their emerging editorial power will be\ncritical for ensuring transparency, pluralism, and trust in digital information\necosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly act as gateways to web content,\nshaping how millions of users encounter online information. Unlike traditional\nsearch engines, whose retrieval and ranking mechanisms are well studied, the\nselection processes of web-connected LLMs add layers of opacity to how answers\nare generated. By determining which news outlets users see, these systems can\ninfluence public opinion, reinforce echo chambers, and pose risks to civic\ndiscourse and public trust.\n  This work extends two decades of research in algorithmic auditing to examine\nhow LLMs function as news engines. We present the first audit comparing three\nleading agents, GPT-4o-Mini, Claude-3.7-Sonnet, and Gemini-2.0-Flash, against\nGoogle News, asking: \\textit{How do LLMs differ from traditional aggregators in\nthe diversity, ideology, and reliability of the media they expose to users?}\n  Across 24 global topics, we find that, compared to Google News, LLMs surface\nsignificantly fewer unique outlets and allocate attention more unevenly. In the\nsame way, GPT-4o-Mini emphasizes more factual and right-leaning sources;\nClaude-3.7-Sonnet favors institutional and civil-society domains and slightly\namplifies right-leaning exposure; and Gemini-2.0-Flash exhibits a modest\nleft-leaning tilt without significant changes in factuality. These patterns\nremain robust under prompt variations and alternative reliability benchmarks.\nTogether, our findings show that LLMs already enact \\textit{agentic editorial\npolicies}, curating information in ways that diverge from conventional\naggregators. Understanding and governing their emerging editorial power will be\ncritical for ensuring transparency, pluralism, and trust in digital information\necosystems."
                },
                "authors": [
                    {
                        "name": "Marco Minici"
                    },
                    {
                        "name": "Cristian Consonni"
                    },
                    {
                        "name": "Federico Cinus"
                    },
                    {
                        "name": "Giuseppe Manco"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Manco"
                },
                "author": "Giuseppe Manco",
                "arxiv_comment": "Under Peer Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16184v2",
                "updated": "2025-10-31T14:03:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    3,
                    49,
                    4,
                    304,
                    0
                ],
                "published": "2025-07-22T02:54:45Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    2,
                    54,
                    45,
                    1,
                    203,
                    0
                ],
                "title": "Emergent Cognitive Convergence via Implementation: A Structured Loop\n  Reflecting Four Theories of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Cognitive Convergence via Implementation: A Structured Loop\n  Reflecting Four Theories of Mind"
                },
                "summary": "We report a structural convergence among four influential theories of mind:\nKahneman's dual-system theory, Friston's predictive processing, Minsky's\nsociety of mind, and Clark's extended mind, emerging unintentionally within a\npractical AI architecture known as Agentic Flow. Designed to address the\nlimitations of large language models (LLMs), Agentic Flow comprises five\ninterlocking modules: Retrieval, Cognition, Control, Memory, and Action,\norganized into a repeatable cognitive loop. Although originally inspired only\nby Minsky and Clark, subsequent analysis revealed that its structure echoes\ncomputational motifs from all four theories, suggesting that theoretical\nconvergence can emerge naturally from implementation demands rather than\ndeliberate synthesis. Controlled evaluations confirmed this: the structured\nagent achieved 95.8% task success versus 62.3% for baseline LLMs, demonstrating\nrobust constraint adherence and reproducible reasoning. We describe this\nconvergence under a broader descriptive meta-architecture called PEACE,\nhighlighting recurring design patterns such as predictive modeling, associative\nrecall, and error-sensitive control. Later formalized as the Structured\nCognitive Loop (SCL), this framework generalizes the same principles as a\nfoundation for behavioral intelligence in LLM-based agents. Rather than\nclaiming theoretical unification, this paper proposes that intelligent\narchitectures may evolve toward shared structural patterns shaped by practical\nconstraints. As a position paper, it aims to frame this convergence as an\ninterpretive reflection rather than a finalized theory, inviting further\ntheoretical and experimental dialogue. Agentic Flow, or equivalently the\nStructured Cognitive Loop, thus offers a glimpse of how a unified cognitive\nform can arise not from abstraction, but from the necessities of real-world\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report a structural convergence among four influential theories of mind:\nKahneman's dual-system theory, Friston's predictive processing, Minsky's\nsociety of mind, and Clark's extended mind, emerging unintentionally within a\npractical AI architecture known as Agentic Flow. Designed to address the\nlimitations of large language models (LLMs), Agentic Flow comprises five\ninterlocking modules: Retrieval, Cognition, Control, Memory, and Action,\norganized into a repeatable cognitive loop. Although originally inspired only\nby Minsky and Clark, subsequent analysis revealed that its structure echoes\ncomputational motifs from all four theories, suggesting that theoretical\nconvergence can emerge naturally from implementation demands rather than\ndeliberate synthesis. Controlled evaluations confirmed this: the structured\nagent achieved 95.8% task success versus 62.3% for baseline LLMs, demonstrating\nrobust constraint adherence and reproducible reasoning. We describe this\nconvergence under a broader descriptive meta-architecture called PEACE,\nhighlighting recurring design patterns such as predictive modeling, associative\nrecall, and error-sensitive control. Later formalized as the Structured\nCognitive Loop (SCL), this framework generalizes the same principles as a\nfoundation for behavioral intelligence in LLM-based agents. Rather than\nclaiming theoretical unification, this paper proposes that intelligent\narchitectures may evolve toward shared structural patterns shaped by practical\nconstraints. As a position paper, it aims to frame this convergence as an\ninterpretive reflection rather than a finalized theory, inviting further\ntheoretical and experimental dialogue. Agentic Flow, or equivalently the\nStructured Cognitive Loop, thus offers a glimpse of how a unified cognitive\nform can arise not from abstraction, but from the necessities of real-world\nreasoning."
                },
                "authors": [
                    {
                        "name": "Myung Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Myung Ho Kim"
                },
                "author": "Myung Ho Kim",
                "arxiv_comment": "This version relocates the \"Position Paper\" designation from the\n  title to the abstract and adds a citation to the related follow-up study\n  Structured Cognition for Behavioral Intelligence in Large Language Model\n  Agents (Kim, 2025), also available on arXiv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27484v1",
                "updated": "2025-10-31T14:02:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    2,
                    37,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T14:02:37Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    2,
                    37,
                    4,
                    304,
                    0
                ],
                "title": "Thought Branches: Interpreting LLM Reasoning Requires Resampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thought Branches: Interpreting LLM Reasoning Requires Resampling"
                },
                "summary": "Most work interpreting reasoning models studies only a single\nchain-of-thought (CoT), yet these models define distributions over many\npossible CoTs. We argue that studying a single sample is inadequate for\nunderstanding causal influence and the underlying computation. Though fully\nspecifying this distribution is intractable, it can be understood by sampling.\nWe present case studies using resampling to investigate model decisions. First,\nwhen a model states a reason for its action, does that reason actually cause\nthe action? In \"agentic misalignment\" scenarios, we resample specific sentences\nto measure their downstream effects. Self-preservation sentences have small\ncausal impact, suggesting they do not meaningfully drive blackmail. Second, are\nartificial edits to CoT sufficient for steering reasoning? These are common in\nliterature, yet take the model off-policy. Resampling and selecting a\ncompletion with the desired property is a principled on-policy alternative. We\nfind off-policy interventions yield small and unstable effects compared to\nresampling in decision-making tasks. Third, how do we understand the effect of\nremoving a reasoning step when the model may repeat it post-edit? We introduce\na resilience metric that repeatedly resamples to prevent similar content from\nreappearing downstream. Critical planning statements resist removal but have\nlarge effects when eliminated. Fourth, since CoT is sometimes \"unfaithful\", can\nour methods teach us anything in these settings? Adapting causal mediation\nanalysis, we find that hints that have a causal effect on the output without\nbeing explicitly mentioned exert a subtle and cumulative influence on the CoT\nthat persists even if the hint is removed. Overall, studying distributions via\nresampling enables reliable causal analysis, clearer narratives of model\nreasoning, and principled CoT interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most work interpreting reasoning models studies only a single\nchain-of-thought (CoT), yet these models define distributions over many\npossible CoTs. We argue that studying a single sample is inadequate for\nunderstanding causal influence and the underlying computation. Though fully\nspecifying this distribution is intractable, it can be understood by sampling.\nWe present case studies using resampling to investigate model decisions. First,\nwhen a model states a reason for its action, does that reason actually cause\nthe action? In \"agentic misalignment\" scenarios, we resample specific sentences\nto measure their downstream effects. Self-preservation sentences have small\ncausal impact, suggesting they do not meaningfully drive blackmail. Second, are\nartificial edits to CoT sufficient for steering reasoning? These are common in\nliterature, yet take the model off-policy. Resampling and selecting a\ncompletion with the desired property is a principled on-policy alternative. We\nfind off-policy interventions yield small and unstable effects compared to\nresampling in decision-making tasks. Third, how do we understand the effect of\nremoving a reasoning step when the model may repeat it post-edit? We introduce\na resilience metric that repeatedly resamples to prevent similar content from\nreappearing downstream. Critical planning statements resist removal but have\nlarge effects when eliminated. Fourth, since CoT is sometimes \"unfaithful\", can\nour methods teach us anything in these settings? Adapting causal mediation\nanalysis, we find that hints that have a causal effect on the output without\nbeing explicitly mentioned exert a subtle and cumulative influence on the CoT\nthat persists even if the hint is removed. Overall, studying distributions via\nresampling enables reliable causal analysis, clearer narratives of model\nreasoning, and principled CoT interventions."
                },
                "authors": [
                    {
                        "name": "Uzay Macar"
                    },
                    {
                        "name": "Paul C. Bogdan"
                    },
                    {
                        "name": "Senthooran Rajamanoharan"
                    },
                    {
                        "name": "Neel Nanda"
                    }
                ],
                "author_detail": {
                    "name": "Neel Nanda"
                },
                "author": "Neel Nanda",
                "arxiv_comment": "Uzay Macar and Paul C. Bogdan contributed equally to this work, and\n  their listed order was determined by coinflip",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05831v2",
                "updated": "2025-10-31T13:46:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    13,
                    46,
                    40,
                    4,
                    304,
                    0
                ],
                "published": "2025-09-06T21:05:18Z",
                "published_parsed": [
                    2025,
                    9,
                    6,
                    21,
                    5,
                    18,
                    5,
                    249,
                    0
                ],
                "title": "Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in\n  Web Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in\n  Web Summarization"
                },
                "summary": "Large Language Models (LLMs) are increasingly integrated into web-based\nsystems for content summarization, yet their susceptibility to prompt injection\nattacks remains a pressing concern. In this study, we explore how non-visible\nHTML elements such as <meta>, aria-label, and alt attributes can be exploited\nto embed adversarial instructions without altering the visible content of a\nwebpage. We introduce a novel dataset comprising 280 static web pages, evenly\ndivided between clean and adversarial injected versions, crafted using diverse\nHTML-based strategies. These pages are processed through a browser automation\npipeline to extract both raw HTML and rendered text, closely mimicking\nreal-world LLM deployment scenarios. We evaluate two state-of-the-art\nopen-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their\nability to summarize this content. Using both lexical (ROUGE-L) and semantic\n(SBERT cosine similarity) metrics, along with manual annotations, we assess the\nimpact of these covert injections. Our findings reveal that over 29% of\ninjected samples led to noticeable changes in the Llama 4 Scout summaries,\nwhile Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These\nresults highlight a critical and largely overlooked vulnerability in LLM driven\nweb pipelines, where hidden adversarial content can subtly manipulate model\noutputs. Our work offers a reproducible framework and benchmark for evaluating\nHTML-based prompt injection and underscores the urgent need for robust\nmitigation strategies in LLM applications involving web content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integrated into web-based\nsystems for content summarization, yet their susceptibility to prompt injection\nattacks remains a pressing concern. In this study, we explore how non-visible\nHTML elements such as <meta>, aria-label, and alt attributes can be exploited\nto embed adversarial instructions without altering the visible content of a\nwebpage. We introduce a novel dataset comprising 280 static web pages, evenly\ndivided between clean and adversarial injected versions, crafted using diverse\nHTML-based strategies. These pages are processed through a browser automation\npipeline to extract both raw HTML and rendered text, closely mimicking\nreal-world LLM deployment scenarios. We evaluate two state-of-the-art\nopen-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their\nability to summarize this content. Using both lexical (ROUGE-L) and semantic\n(SBERT cosine similarity) metrics, along with manual annotations, we assess the\nimpact of these covert injections. Our findings reveal that over 29% of\ninjected samples led to noticeable changes in the Llama 4 Scout summaries,\nwhile Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These\nresults highlight a critical and largely overlooked vulnerability in LLM driven\nweb pipelines, where hidden adversarial content can subtly manipulate model\noutputs. Our work offers a reproducible framework and benchmark for evaluating\nHTML-based prompt injection and underscores the urgent need for robust\nmitigation strategies in LLM applications involving web content."
                },
                "authors": [
                    {
                        "name": "Ishaan Verma"
                    }
                ],
                "author_detail": {
                    "name": "Ishaan Verma"
                },
                "author": "Ishaan Verma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27469v1",
                "updated": "2025-10-31T13:41:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    13,
                    41,
                    30,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T13:41:30Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    13,
                    41,
                    30,
                    4,
                    304,
                    0
                ],
                "title": "Diffuse Thinking: Exploring Diffusion Language Models as Efficient\n  Thought Proposers for Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffuse Thinking: Exploring Diffusion Language Models as Efficient\n  Thought Proposers for Reasoning"
                },
                "summary": "In recent years, large language models (LLMs) have witnessed remarkable\nadvancements, with the test-time scaling law consistently enhancing the\nreasoning capabilities. Through systematic evaluation and exploration of a\ndiverse spectrum of intermediate thoughts, LLMs demonstrate the potential to\ngenerate deliberate reasoning steps, thereby substantially enhancing reasoning\naccuracy. However, LLMs' autoregressive generation paradigm results in\nreasoning performance scaling sub-optimally with test-time computation, often\nrequiring excessive computational overhead to propose thoughts while yielding\nonly marginal performance gains. In contrast, diffusion language models (DLMs)\ncan efficiently produce diverse samples through parallel denoising in a single\nforward pass, inspiring us to leverage them for proposing intermediate\nthoughts, thereby alleviating the computational burden associated with\nautoregressive generation while maintaining quality. In this work, we propose\nan efficient collaborative reasoning framework, leveraging DLMs to generate\ncandidate thoughts and LLMs to evaluate their quality. Experiments across\ndiverse benchmarks demonstrate that our framework achieves strong performance\nin complex reasoning tasks, offering a promising direction for future research.\nOur code is open-source at\nhttps://anonymous.4open.science/r/Diffuse-Thinking-EC60.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have witnessed remarkable\nadvancements, with the test-time scaling law consistently enhancing the\nreasoning capabilities. Through systematic evaluation and exploration of a\ndiverse spectrum of intermediate thoughts, LLMs demonstrate the potential to\ngenerate deliberate reasoning steps, thereby substantially enhancing reasoning\naccuracy. However, LLMs' autoregressive generation paradigm results in\nreasoning performance scaling sub-optimally with test-time computation, often\nrequiring excessive computational overhead to propose thoughts while yielding\nonly marginal performance gains. In contrast, diffusion language models (DLMs)\ncan efficiently produce diverse samples through parallel denoising in a single\nforward pass, inspiring us to leverage them for proposing intermediate\nthoughts, thereby alleviating the computational burden associated with\nautoregressive generation while maintaining quality. In this work, we propose\nan efficient collaborative reasoning framework, leveraging DLMs to generate\ncandidate thoughts and LLMs to evaluate their quality. Experiments across\ndiverse benchmarks demonstrate that our framework achieves strong performance\nin complex reasoning tasks, offering a promising direction for future research.\nOur code is open-source at\nhttps://anonymous.4open.science/r/Diffuse-Thinking-EC60."
                },
                "authors": [
                    {
                        "name": "Chenyang Shao"
                    },
                    {
                        "name": "Sijian Ren"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04412v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04412v2",
                "updated": "2025-10-31T13:21:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    13,
                    21,
                    4,
                    4,
                    304,
                    0
                ],
                "published": "2025-08-06T12:56:54Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    56,
                    54,
                    2,
                    218,
                    0
                ],
                "title": "Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents"
                },
                "summary": "Frontier LLMs only recently enabled serviceable, autonomous web agents. At\nthat, a model poses as an instantaneous domain model backend. Ought to suggest\ninteraction, it is consulted with a web-based task and respective application\nstate. The key problem lies in application state serialisation - referred to as\nsnapshot. State-of-the-art web agents are premised on grounded GUI snapshots,\ni.e., screenshots enhanced with visual cues. Not least to resemble human\nperception, but for images representing relatively cheap means of model input.\nLLM vision still lag behind code interpretation capabilities. DOM snapshots,\nwhich structurally resemble HTML, impose a desired alternative. Vast model\ninput token size, however, disables reliable implementation with web agents to\ndate. We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based\non a GPT-4o backend, we evaluate D2Snap on tasks sampled from the\nOnline-Mind2Web dataset. The success rate of D2Snap-downsampled DOM snapshots\n(67%) matches a grounded GUI snapshot baseline (65%) - within the same input\ntoken order of magnitude (1e3). Our best evaluated configurations - one token\norder above, but within the model's context window - outperform this baseline\nby 8%. Our evaluation, moreover, yields that DOM-inherent hierarchy embodies a\nstrong UI feature for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier LLMs only recently enabled serviceable, autonomous web agents. At\nthat, a model poses as an instantaneous domain model backend. Ought to suggest\ninteraction, it is consulted with a web-based task and respective application\nstate. The key problem lies in application state serialisation - referred to as\nsnapshot. State-of-the-art web agents are premised on grounded GUI snapshots,\ni.e., screenshots enhanced with visual cues. Not least to resemble human\nperception, but for images representing relatively cheap means of model input.\nLLM vision still lag behind code interpretation capabilities. DOM snapshots,\nwhich structurally resemble HTML, impose a desired alternative. Vast model\ninput token size, however, disables reliable implementation with web agents to\ndate. We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based\non a GPT-4o backend, we evaluate D2Snap on tasks sampled from the\nOnline-Mind2Web dataset. The success rate of D2Snap-downsampled DOM snapshots\n(67%) matches a grounded GUI snapshot baseline (65%) - within the same input\ntoken order of magnitude (1e3). Our best evaluated configurations - one token\norder above, but within the model's context window - outperform this baseline\nby 8%. Our evaluation, moreover, yields that DOM-inherent hierarchy embodies a\nstrong UI feature for LLMs."
                },
                "authors": [
                    {
                        "name": "Thassilo M. Schiepanski"
                    },
                    {
                        "name": "Nicholas Pil"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Pil"
                },
                "author": "Nicholas Pil",
                "arxiv_comment": "20 pages, LaTeX; repository URL updated, typos corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04412v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04412v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27462v1",
                "updated": "2025-10-31T13:19:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    13,
                    19,
                    24,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T13:19:24Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    13,
                    19,
                    24,
                    4,
                    304,
                    0
                ],
                "title": "VCORE: Variance-Controlled Optimization-based Reweighting for\n  Chain-of-Thought Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VCORE: Variance-Controlled Optimization-based Reweighting for\n  Chain-of-Thought Supervision"
                },
                "summary": "Supervised fine-tuning (SFT) on long chain-of-thought (CoT) trajectories has\nemerged as a crucial technique for enhancing the reasoning abilities of large\nlanguage models (LLMs). However, the standard cross-entropy loss treats all\ntokens equally, ignoring their heterogeneous contributions across a reasoning\ntrajectory. This uniform treatment leads to misallocated supervision and weak\ngeneralization, especially in complex, long-form reasoning tasks. To address\nthis, we introduce \\textbf{V}ariance-\\textbf{C}ontrolled\n\\textbf{O}ptimization-based \\textbf{RE}weighting (VCORE), a principled\nframework that reformulates CoT supervision as a constrained optimization\nproblem. By adopting an optimization-theoretic perspective, VCORE enables a\nprincipled and adaptive allocation of supervision across tokens, thereby\naligning the training objective more closely with the goal of robust reasoning\ngeneralization. Empirical evaluations demonstrate that VCORE consistently\noutperforms existing token reweighting methods. Across both in-domain and\nout-of-domain settings, VCORE achieves substantial performance gains on\nmathematical and coding benchmarks, using models from the Qwen3 series (4B, 8B,\n32B) and LLaMA-3.1-8B-Instruct. Moreover, we show that VCORE serves as a more\neffective initialization for subsequent reinforcement learning, establishing a\nstronger foundation for advancing the reasoning capabilities of LLMs. The Code\nwill be released at https://github.com/coder-gx/VCORE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) on long chain-of-thought (CoT) trajectories has\nemerged as a crucial technique for enhancing the reasoning abilities of large\nlanguage models (LLMs). However, the standard cross-entropy loss treats all\ntokens equally, ignoring their heterogeneous contributions across a reasoning\ntrajectory. This uniform treatment leads to misallocated supervision and weak\ngeneralization, especially in complex, long-form reasoning tasks. To address\nthis, we introduce \\textbf{V}ariance-\\textbf{C}ontrolled\n\\textbf{O}ptimization-based \\textbf{RE}weighting (VCORE), a principled\nframework that reformulates CoT supervision as a constrained optimization\nproblem. By adopting an optimization-theoretic perspective, VCORE enables a\nprincipled and adaptive allocation of supervision across tokens, thereby\naligning the training objective more closely with the goal of robust reasoning\ngeneralization. Empirical evaluations demonstrate that VCORE consistently\noutperforms existing token reweighting methods. Across both in-domain and\nout-of-domain settings, VCORE achieves substantial performance gains on\nmathematical and coding benchmarks, using models from the Qwen3 series (4B, 8B,\n32B) and LLaMA-3.1-8B-Instruct. Moreover, we show that VCORE serves as a more\neffective initialization for subsequent reinforcement learning, establishing a\nstronger foundation for advancing the reasoning capabilities of LLMs. The Code\nwill be released at https://github.com/coder-gx/VCORE."
                },
                "authors": [
                    {
                        "name": "Xuan Gong"
                    },
                    {
                        "name": "Senmiao Wang"
                    },
                    {
                        "name": "Hanbo Huang"
                    },
                    {
                        "name": "Ruoyu Sun"
                    },
                    {
                        "name": "Shiyu Liang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Liang"
                },
                "author": "Shiyu Liang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03530v2",
                "updated": "2025-10-31T13:01:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    13,
                    1,
                    39,
                    4,
                    304,
                    0
                ],
                "published": "2025-03-05T14:11:25Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    14,
                    11,
                    25,
                    2,
                    64,
                    0
                ],
                "title": "Inference for Heterogeneous Treatment Effects with Efficient Instruments\n  and Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Heterogeneous Treatment Effects with Efficient Instruments\n  and Machine Learning"
                },
                "summary": "We introduce a new instrumental variable (IV) estimator for heterogeneous\ntreatment effects in the presence of endogeneity. Our estimator is based on\ndouble/debiased machine learning (DML) and uses efficient machine learning\ninstruments (MLIV) and kernel smoothing. We prove consistency and asymptotic\nnormality of our estimator and also construct confidence sets that are more\nrobust towards weak IV. Along the way, we also provide an accessible discussion\nof the corresponding estimator for the homogeneous treatment effect with\nefficient machine learning instruments. The methods are evaluated on synthetic\nand real datasets and an implementation is made available in the R package\nIVDML.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new instrumental variable (IV) estimator for heterogeneous\ntreatment effects in the presence of endogeneity. Our estimator is based on\ndouble/debiased machine learning (DML) and uses efficient machine learning\ninstruments (MLIV) and kernel smoothing. We prove consistency and asymptotic\nnormality of our estimator and also construct confidence sets that are more\nrobust towards weak IV. Along the way, we also provide an accessible discussion\nof the corresponding estimator for the homogeneous treatment effect with\nefficient machine learning instruments. The methods are evaluated on synthetic\nand real datasets and an implementation is made available in the R package\nIVDML."
                },
                "authors": [
                    {
                        "name": "Cyrill Scheidegger"
                    },
                    {
                        "name": "Zijian Guo"
                    },
                    {
                        "name": "Peter Bhlmann"
                    }
                ],
                "author_detail": {
                    "name": "Peter Bhlmann"
                },
                "author": "Peter Bhlmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22165v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22165v3",
                "updated": "2025-10-31T12:56:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    56,
                    41,
                    4,
                    304,
                    0
                ],
                "published": "2025-03-28T06:09:51Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    9,
                    51,
                    4,
                    87,
                    0
                ],
                "title": "Landscape of Thoughts: Visualizing the Reasoning Process of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Landscape of Thoughts: Visualizing the Reasoning Process of Large\n  Language Models"
                },
                "summary": "Numerous applications of large language models (LLMs) rely on their ability\nto perform step-by-step reasoning. However, the reasoning behavior of LLMs\nremains poorly understood, posing challenges to research, development, and\nsafety. To address this gap, we introduce landscape of thoughts (LoT), the\nfirst landscape visualization tool to inspect the reasoning trajectories with\ncertain reasoning methods on any multi-choice dataset. We represent the textual\nstates in a trajectory as numerical features that quantify the states'\ndistances to the answer choices. These features are then visualized in\ntwo-dimensional plots using t-SNE. Qualitative and quantitative analysis with\nthe landscape of thoughts effectively distinguishes between strong and weak\nmodels, correct and incorrect answers, as well as different reasoning tasks. It\nalso uncovers undesirable reasoning patterns, such as low consistency and high\nuncertainty. Additionally, users can adapt LoT to a model that predicts the\nproperty they observe. We showcase this advantage by adapting LoT to a\nlightweight verifier that evaluates the correctness of trajectories.\nEmpirically, this verifier boosts the reasoning accuracy and the test-time\nscaling effect. The code is publicly available at:\nhttps://github.com/tmlr-group/landscape-of-thoughts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous applications of large language models (LLMs) rely on their ability\nto perform step-by-step reasoning. However, the reasoning behavior of LLMs\nremains poorly understood, posing challenges to research, development, and\nsafety. To address this gap, we introduce landscape of thoughts (LoT), the\nfirst landscape visualization tool to inspect the reasoning trajectories with\ncertain reasoning methods on any multi-choice dataset. We represent the textual\nstates in a trajectory as numerical features that quantify the states'\ndistances to the answer choices. These features are then visualized in\ntwo-dimensional plots using t-SNE. Qualitative and quantitative analysis with\nthe landscape of thoughts effectively distinguishes between strong and weak\nmodels, correct and incorrect answers, as well as different reasoning tasks. It\nalso uncovers undesirable reasoning patterns, such as low consistency and high\nuncertainty. Additionally, users can adapt LoT to a model that predicts the\nproperty they observe. We showcase this advantage by adapting LoT to a\nlightweight verifier that evaluates the correctness of trajectories.\nEmpirically, this verifier boosts the reasoning accuracy and the test-time\nscaling effect. The code is publicly available at:\nhttps://github.com/tmlr-group/landscape-of-thoughts."
                },
                "authors": [
                    {
                        "name": "Zhanke Zhou"
                    },
                    {
                        "name": "Zhaocheng Zhu"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Mikhail Galkin"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Jian Tang"
                    },
                    {
                        "name": "Bo Han"
                    }
                ],
                "author_detail": {
                    "name": "Bo Han"
                },
                "author": "Bo Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22165v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22165v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01070v2",
                "updated": "2025-10-31T12:55:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    55,
                    4,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-01T16:12:28Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    12,
                    28,
                    2,
                    274,
                    0
                ],
                "title": "Eliciting Secret Knowledge from Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting Secret Knowledge from Language Models"
                },
                "summary": "We study secret elicitation: discovering knowledge that an AI possesses but\ndoes not explicitly verbalize. As a testbed, we train three families of large\nlanguage models (LLMs) to possess specific knowledge that they apply downstream\nbut deny knowing when asked directly. For example, in one setting, we train an\nLLM to generate replies that are consistent with knowing the user is female,\nwhile denying this knowledge when asked directly. We then design various\nblack-box and white-box secret elicitation techniques and evaluate them based\non whether they can help an LLM auditor successfully guess the secret\nknowledge. Many of our techniques improve on simple baselines. Our most\neffective techniques (performing best in all settings) are based on prefill\nattacks, a black-box technique where the LLM reveals secret knowledge when\ngenerating a completion from a predefined prefix. Our white-box techniques\nbased on logit lens and sparse autoencoders (SAEs) also consistently increase\nthe success rate of the LLM auditor, but are less effective. We release our\nmodels and code, establishing a public benchmark for evaluating secret\nelicitation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study secret elicitation: discovering knowledge that an AI possesses but\ndoes not explicitly verbalize. As a testbed, we train three families of large\nlanguage models (LLMs) to possess specific knowledge that they apply downstream\nbut deny knowing when asked directly. For example, in one setting, we train an\nLLM to generate replies that are consistent with knowing the user is female,\nwhile denying this knowledge when asked directly. We then design various\nblack-box and white-box secret elicitation techniques and evaluate them based\non whether they can help an LLM auditor successfully guess the secret\nknowledge. Many of our techniques improve on simple baselines. Our most\neffective techniques (performing best in all settings) are based on prefill\nattacks, a black-box technique where the LLM reveals secret knowledge when\ngenerating a completion from a predefined prefix. Our white-box techniques\nbased on logit lens and sparse autoencoders (SAEs) also consistently increase\nthe success rate of the LLM auditor, but are less effective. We release our\nmodels and code, establishing a public benchmark for evaluating secret\nelicitation methods."
                },
                "authors": [
                    {
                        "name": "Bartosz Cywiski"
                    },
                    {
                        "name": "Emil Ryd"
                    },
                    {
                        "name": "Rowan Wang"
                    },
                    {
                        "name": "Senthooran Rajamanoharan"
                    },
                    {
                        "name": "Neel Nanda"
                    },
                    {
                        "name": "Arthur Conmy"
                    },
                    {
                        "name": "Samuel Marks"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Marks"
                },
                "author": "Samuel Marks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08424v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08424v2",
                "updated": "2025-10-31T12:49:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    49,
                    50,
                    4,
                    304,
                    0
                ],
                "published": "2025-07-11T09:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    9,
                    1,
                    4,
                    192,
                    0
                ],
                "title": "RTNinja: a generalized machine learning framework for analyzing random\n  telegraph noise signals in nanoelectronic devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTNinja: a generalized machine learning framework for analyzing random\n  telegraph noise signals in nanoelectronic devices"
                },
                "summary": "Random telegraph noise is a prevalent variability phenomenon in\nnanoelectronic devices, arising from stochastic carrier exchange at defect\nsites and critically impacting device reliability and performance. Conventional\nanalysis techniques often rely on restrictive assumptions or manual\ninterventions, limiting their applicability to complex, noisy datasets. Here,\nwe introduce RTNinja, a generalized, fully automated machine learning framework\nfor the unsupervised analysis of random telegraph noise signals. RTNinja\ndeconvolves complex signals to identify the number and characteristics of\nhidden individual sources, without requiring prior knowledge of the system. The\nframework comprises two modular components: LevelsExtractor, which uses\nBayesian inference and model selection to denoise and discretize the signal;\nand SourcesMapper, which infers source configurations through probabilistic\nclustering and optimization. To evaluate performance, we developed a Monte\nCarlo simulator that generates labeled datasets spanning broad signal-to-noise\nratios and source complexities; across 7000 such datasets, RTNinja consistently\ndemonstrated high-fidelity signal reconstruction and accurate extraction of\nsource amplitudes and activity patterns. Our results demonstrate that RTNinja\noffers a robust, scalable, and device-agnostic tool for random telegraph noise\ncharacterization, enabling large-scale statistical benchmarking,\nreliability-centric technology qualification, predictive failure modeling, and\ndevice physics exploration in next-generation nanoelectronics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random telegraph noise is a prevalent variability phenomenon in\nnanoelectronic devices, arising from stochastic carrier exchange at defect\nsites and critically impacting device reliability and performance. Conventional\nanalysis techniques often rely on restrictive assumptions or manual\ninterventions, limiting their applicability to complex, noisy datasets. Here,\nwe introduce RTNinja, a generalized, fully automated machine learning framework\nfor the unsupervised analysis of random telegraph noise signals. RTNinja\ndeconvolves complex signals to identify the number and characteristics of\nhidden individual sources, without requiring prior knowledge of the system. The\nframework comprises two modular components: LevelsExtractor, which uses\nBayesian inference and model selection to denoise and discretize the signal;\nand SourcesMapper, which infers source configurations through probabilistic\nclustering and optimization. To evaluate performance, we developed a Monte\nCarlo simulator that generates labeled datasets spanning broad signal-to-noise\nratios and source complexities; across 7000 such datasets, RTNinja consistently\ndemonstrated high-fidelity signal reconstruction and accurate extraction of\nsource amplitudes and activity patterns. Our results demonstrate that RTNinja\noffers a robust, scalable, and device-agnostic tool for random telegraph noise\ncharacterization, enabling large-scale statistical benchmarking,\nreliability-centric technology qualification, predictive failure modeling, and\ndevice physics exploration in next-generation nanoelectronics."
                },
                "authors": [
                    {
                        "name": "Anirudh Varanasi"
                    },
                    {
                        "name": "Robin Degraeve"
                    },
                    {
                        "name": "Philippe Roussel"
                    },
                    {
                        "name": "Clement Merckling"
                    }
                ],
                "author_detail": {
                    "name": "Clement Merckling"
                },
                "author": "Clement Merckling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08424v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08424v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14216v2",
                "updated": "2025-10-31T12:44:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    44,
                    27,
                    4,
                    304,
                    0
                ],
                "published": "2025-05-20T11:22:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    11,
                    22,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "Reinforcement Learning vs. Distillation: Understanding Accuracy and\n  Capability in LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning vs. Distillation: Understanding Accuracy and\n  Capability in LLM Reasoning"
                },
                "summary": "Recent studies have shown that reinforcement learning with verifiable rewards\n(RLVR) enhances overall accuracy (pass@1) but often fails to improve capability\n(pass@k) of LLMs in reasoning tasks, while distillation can improve both. In\nthis paper, we investigate the mechanisms behind these phenomena. First, we\ndemonstrate that RLVR struggles to improve capability as it focuses on\nimproving the accuracy of the easier questions to the detriment of the accuracy\nof the most difficult questions. Second, we show that RLVR does not merely\nincrease the success probability for the easier questions, but in our small\nmodel settings, produces quality responses that were absent in its original\noutput distribution. In addition, we show these responses are neither\nnoticeably longer nor feature more reflection-related keywords, underscoring\nthe need for more reliable indicators of response quality. Third, from the\nexperiment distilling teacher responses to in-distribution problems, we find\nthat capability does not always improve with distillation. We conjecture that\ncapability improves only when new knowledge is introduced, whereas distilling\nreasoning patterns only improves accuracy but not capability, sacrificing\nperformance on the most difficult questions, similar to RLVR. Together, these\nfindings offer a clearer understanding of how RLVR and distillation shape\nreasoning behavior in LLMs",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that reinforcement learning with verifiable rewards\n(RLVR) enhances overall accuracy (pass@1) but often fails to improve capability\n(pass@k) of LLMs in reasoning tasks, while distillation can improve both. In\nthis paper, we investigate the mechanisms behind these phenomena. First, we\ndemonstrate that RLVR struggles to improve capability as it focuses on\nimproving the accuracy of the easier questions to the detriment of the accuracy\nof the most difficult questions. Second, we show that RLVR does not merely\nincrease the success probability for the easier questions, but in our small\nmodel settings, produces quality responses that were absent in its original\noutput distribution. In addition, we show these responses are neither\nnoticeably longer nor feature more reflection-related keywords, underscoring\nthe need for more reliable indicators of response quality. Third, from the\nexperiment distilling teacher responses to in-distribution problems, we find\nthat capability does not always improve with distillation. We conjecture that\ncapability improves only when new knowledge is introduced, whereas distilling\nreasoning patterns only improves accuracy but not capability, sacrificing\nperformance on the most difficult questions, similar to RLVR. Together, these\nfindings offer a clearer understanding of how RLVR and distillation shape\nreasoning behavior in LLMs"
                },
                "authors": [
                    {
                        "name": "Minwu Kim"
                    },
                    {
                        "name": "Anubhav Shrestha"
                    },
                    {
                        "name": "Safal Shrestha"
                    },
                    {
                        "name": "Aadim Nepal"
                    },
                    {
                        "name": "Keith Ross"
                    }
                ],
                "author_detail": {
                    "name": "Keith Ross"
                },
                "author": "Keith Ross",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08142v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08142v2",
                "updated": "2025-10-31T12:27:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    27,
                    14,
                    4,
                    304,
                    0
                ],
                "published": "2025-05-13T00:27:29Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    0,
                    27,
                    29,
                    1,
                    133,
                    0
                ],
                "title": "Highly Undersampled MRI Reconstruction via a Single Posterior Sampling\n  of Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Highly Undersampled MRI Reconstruction via a Single Posterior Sampling\n  of Diffusion Models"
                },
                "summary": "Incoherent k-space undersampling and deep learning-based reconstruction\nmethods have shown great success in accelerating MRI. However, the performance\nof most previous methods will degrade dramatically under high acceleration\nfactors, e.g., 8$\\times$ or higher. Recently, denoising diffusion models (DM)\nhave demonstrated promising results in solving this issue; however, one major\ndrawback of the DM methods is the long inference time due to a dramatic number\nof iterative reverse posterior sampling steps. In this work, a Single Step\nDiffusion Model-based reconstruction framework, namely SSDM-MRI, is proposed\nfor restoring MRI images from highly undersampled k-space. The proposed method\nachieves one-step reconstruction by first training a conditional DM and then\niteratively distilling this model four times using an iterative selective\ndistillation algorithm, which works synergistically with a shortcut reverse\nsampling strategy for model inference. Comprehensive experiments were carried\nout on both publicly available fastMRI brain and knee images, as well as an\nin-house multi-echo GRE (QSM) subject. Overall, the results showed that\nSSDM-MRI outperformed other methods in terms of numerical metrics (e.g., PSNR\nand SSIM), error maps, image fine details, and latent susceptibility\ninformation hidden in MRI phase images. In addition, the reconstruction time\nfor a 320$\\times$320 brain slice of SSDM-MRI is only 0.45 second, which is only\ncomparable to that of a simple U-net, making it a highly effective solution for\nMRI reconstruction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Incoherent k-space undersampling and deep learning-based reconstruction\nmethods have shown great success in accelerating MRI. However, the performance\nof most previous methods will degrade dramatically under high acceleration\nfactors, e.g., 8$\\times$ or higher. Recently, denoising diffusion models (DM)\nhave demonstrated promising results in solving this issue; however, one major\ndrawback of the DM methods is the long inference time due to a dramatic number\nof iterative reverse posterior sampling steps. In this work, a Single Step\nDiffusion Model-based reconstruction framework, namely SSDM-MRI, is proposed\nfor restoring MRI images from highly undersampled k-space. The proposed method\nachieves one-step reconstruction by first training a conditional DM and then\niteratively distilling this model four times using an iterative selective\ndistillation algorithm, which works synergistically with a shortcut reverse\nsampling strategy for model inference. Comprehensive experiments were carried\nout on both publicly available fastMRI brain and knee images, as well as an\nin-house multi-echo GRE (QSM) subject. Overall, the results showed that\nSSDM-MRI outperformed other methods in terms of numerical metrics (e.g., PSNR\nand SSIM), error maps, image fine details, and latent susceptibility\ninformation hidden in MRI phase images. In addition, the reconstruction time\nfor a 320$\\times$320 brain slice of SSDM-MRI is only 0.45 second, which is only\ncomparable to that of a simple U-net, making it a highly effective solution for\nMRI reconstruction tasks."
                },
                "authors": [
                    {
                        "name": "Jin Liu"
                    },
                    {
                        "name": "Qing Lin"
                    },
                    {
                        "name": "Zhuang Xiong"
                    },
                    {
                        "name": "Shanshan Shan"
                    },
                    {
                        "name": "Chunyi Liu"
                    },
                    {
                        "name": "Min Li"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "G. Bruce Pike"
                    },
                    {
                        "name": "Hongfu Sun"
                    },
                    {
                        "name": "Yang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Gao"
                },
                "author": "Yang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08142v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08142v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27420v1",
                "updated": "2025-10-31T12:17:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    17,
                    36,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T12:17:36Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    17,
                    36,
                    4,
                    304,
                    0
                ],
                "title": "Towards a Multi-Embodied Grasping Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Multi-Embodied Grasping Agent"
                },
                "summary": "Multi-embodiment grasping focuses on developing approaches that exhibit\ngeneralist behavior across diverse gripper designs. Existing methods often\nlearn the kinematic structure of the robot implicitly and face challenges due\nto the difficulty of sourcing the required large-scale data. In this work, we\npresent a data-efficient, flow-based, equivariant grasp synthesis architecture\nthat can handle different gripper types with variable degrees of freedom and\nsuccessfully exploit the underlying kinematic model, deducing all necessary\ninformation solely from the gripper and scene geometry. Unlike previous\nequivariant grasping methods, we translated all modules from the ground up to\nJAX and provide a model with batching capabilities over scenes, grippers, and\ngrasps, resulting in smoother learning, improved performance and faster\ninference time. Our dataset encompasses grippers ranging from humanoid hands to\nparallel yaw grippers and includes 25,000 scenes and 20 million grasps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-embodiment grasping focuses on developing approaches that exhibit\ngeneralist behavior across diverse gripper designs. Existing methods often\nlearn the kinematic structure of the robot implicitly and face challenges due\nto the difficulty of sourcing the required large-scale data. In this work, we\npresent a data-efficient, flow-based, equivariant grasp synthesis architecture\nthat can handle different gripper types with variable degrees of freedom and\nsuccessfully exploit the underlying kinematic model, deducing all necessary\ninformation solely from the gripper and scene geometry. Unlike previous\nequivariant grasping methods, we translated all modules from the ground up to\nJAX and provide a model with batching capabilities over scenes, grippers, and\ngrasps, resulting in smoother learning, improved performance and faster\ninference time. Our dataset encompasses grippers ranging from humanoid hands to\nparallel yaw grippers and includes 25,000 scenes and 20 million grasps."
                },
                "authors": [
                    {
                        "name": "Roman Freiberg"
                    },
                    {
                        "name": "Alexander Qualmann"
                    },
                    {
                        "name": "Ngo Anh Vien"
                    },
                    {
                        "name": "Gerhard Neumann"
                    }
                ],
                "author_detail": {
                    "name": "Gerhard Neumann"
                },
                "author": "Gerhard Neumann",
                "arxiv_comment": "9 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07464v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07464v4",
                "updated": "2025-10-31T12:13:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    13,
                    12,
                    4,
                    304,
                    0
                ],
                "published": "2025-06-09T06:15:54Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    6,
                    15,
                    54,
                    0,
                    160,
                    0
                ],
                "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO"
                },
                "summary": "Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training for enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success using a PPO-style reinforcement algorithm\nwith group-normalized rewards. However, the effectiveness of GRPO in Video\nLarge Language Models (VideoLLMs) has still been less studyed. In this paper,\nwe explore GRPO and identify two problems that deteriorate the effective\nlearning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate\nthese challenges, we propose DeepVideo-R1, a video large language model trained\nwith Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation.\nReg-GRPO reformulates the GRPO loss function into a regression task that\ndirectly predicts the advantage in GRPO, eliminating the need for safeguards\nsuch as the clipping and min functions. It directly aligns the model with\nadvantages, providing guidance to prefer better ones. The difficulty-aware data\naugmentation strategy augments input prompts/videos to locate the difficulty of\nsamples at solvable difficulty levels, enabling diverse reward signals. Our\nexperimental results show that our approach significantly improves video\nreasoning performance across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training for enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success using a PPO-style reinforcement algorithm\nwith group-normalized rewards. However, the effectiveness of GRPO in Video\nLarge Language Models (VideoLLMs) has still been less studyed. In this paper,\nwe explore GRPO and identify two problems that deteriorate the effective\nlearning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate\nthese challenges, we propose DeepVideo-R1, a video large language model trained\nwith Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation.\nReg-GRPO reformulates the GRPO loss function into a regression task that\ndirectly predicts the advantage in GRPO, eliminating the need for safeguards\nsuch as the clipping and min functions. It directly aligns the model with\nadvantages, providing guidance to prefer better ones. The difficulty-aware data\naugmentation strategy augments input prompts/videos to locate the difficulty of\nsamples at solvable difficulty levels, enabling diverse reward signals. Our\nexperimental results show that our approach significantly improves video\nreasoning performance across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Jinyoung Park"
                    },
                    {
                        "name": "Jeehye Na"
                    },
                    {
                        "name": "Jinyoung Kim"
                    },
                    {
                        "name": "Hyunwoo J. Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyunwoo J. Kim"
                },
                "author": "Hyunwoo J. Kim",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07464v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07464v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27418v1",
                "updated": "2025-10-31T12:12:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    12,
                    51,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T12:12:51Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    12,
                    51,
                    4,
                    304,
                    0
                ],
                "title": "Dynamic Affective Memory Management for Personalized LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Affective Memory Management for Personalized LLM Agents"
                },
                "summary": "Advances in large language models are making personalized AI agents a new\nresearch focus. While current agent systems primarily rely on personalized\nexternal memory databases to deliver customized experiences, they face\nchallenges such as memory redundancy, memory staleness, and poor memory-context\nintegration, largely due to the lack of effective memory updates during\ninteraction. To tackle these issues, we propose a new memory management system\ndesigned for affective scenarios. Our approach employs a Bayesian-inspired\nmemory update algorithm with the concept of memory entropy, enabling the agent\nto autonomously maintain a dynamically updated memory vector database by\nminimizing global entropy to provide more personalized services. To better\nevaluate the system's effectiveness in this context, we propose DABench, a\nbenchmark focusing on emotional expression and emotional change toward objects.\nExperimental results demonstrate that, our system achieves superior performance\nin personalization, logical coherence, and accuracy. Ablation studies further\nvalidate the effectiveness of the Bayesian-inspired update mechanism in\nalleviating memory bloat. Our work offers new insights into the design of\nlong-term memory systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in large language models are making personalized AI agents a new\nresearch focus. While current agent systems primarily rely on personalized\nexternal memory databases to deliver customized experiences, they face\nchallenges such as memory redundancy, memory staleness, and poor memory-context\nintegration, largely due to the lack of effective memory updates during\ninteraction. To tackle these issues, we propose a new memory management system\ndesigned for affective scenarios. Our approach employs a Bayesian-inspired\nmemory update algorithm with the concept of memory entropy, enabling the agent\nto autonomously maintain a dynamically updated memory vector database by\nminimizing global entropy to provide more personalized services. To better\nevaluate the system's effectiveness in this context, we propose DABench, a\nbenchmark focusing on emotional expression and emotional change toward objects.\nExperimental results demonstrate that, our system achieves superior performance\nin personalization, logical coherence, and accuracy. Ablation studies further\nvalidate the effectiveness of the Bayesian-inspired update mechanism in\nalleviating memory bloat. Our work offers new insights into the design of\nlong-term memory systems."
                },
                "authors": [
                    {
                        "name": "Junfeng Lu"
                    },
                    {
                        "name": "Yueyan Li"
                    }
                ],
                "author_detail": {
                    "name": "Yueyan Li"
                },
                "author": "Yueyan Li",
                "arxiv_comment": "12 pasges, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27417v1",
                "updated": "2025-10-31T12:12:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    12,
                    1,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T12:12:01Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    12,
                    1,
                    4,
                    304,
                    0
                ],
                "title": "Agentic LLMs for REST API Test Amplification: A Comparative Study Across\n  Cloud Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic LLMs for REST API Test Amplification: A Comparative Study Across\n  Cloud Applications"
                },
                "summary": "Representational State Transfer (REST) APIs are a cornerstone of modern cloud\nnative systems. Ensuring their reliability demands automated test suites that\nexercise diverse and boundary level behaviors. Nevertheless, designing such\ntest cases remains a challenging and resource intensive endeavor. This study\nextends prior work on Large Language Model (LLM) based test amplification by\nevaluating single agent and multi agent configurations across four additional\ncloud applications. The amplified test suites maintain semantic validity with\nminimal human intervention. The results demonstrate that agentic LLM systems\ncan effectively generalize across heterogeneous API architectures, increasing\nendpoint and parameter coverage while revealing defects. Moreover, a detailed\nanalysis of computational cost, runtime, and energy consumption highlights\ntrade-offs between accuracy, scalability, and efficiency. These findings\nunderscore the potential of LLM driven test amplification to advance the\nautomation and sustainability of REST API testing in complex cloud\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representational State Transfer (REST) APIs are a cornerstone of modern cloud\nnative systems. Ensuring their reliability demands automated test suites that\nexercise diverse and boundary level behaviors. Nevertheless, designing such\ntest cases remains a challenging and resource intensive endeavor. This study\nextends prior work on Large Language Model (LLM) based test amplification by\nevaluating single agent and multi agent configurations across four additional\ncloud applications. The amplified test suites maintain semantic validity with\nminimal human intervention. The results demonstrate that agentic LLM systems\ncan effectively generalize across heterogeneous API architectures, increasing\nendpoint and parameter coverage while revealing defects. Moreover, a detailed\nanalysis of computational cost, runtime, and energy consumption highlights\ntrade-offs between accuracy, scalability, and efficiency. These findings\nunderscore the potential of LLM driven test amplification to advance the\nautomation and sustainability of REST API testing in complex cloud\nenvironments."
                },
                "authors": [
                    {
                        "name": "Jarne Besjes"
                    },
                    {
                        "name": "Robbe Nooyens"
                    },
                    {
                        "name": "Tolgahan Bardakci"
                    },
                    {
                        "name": "Mutlu Beyazit"
                    },
                    {
                        "name": "Serge Demeyer"
                    }
                ],
                "author_detail": {
                    "name": "Serge Demeyer"
                },
                "author": "Serge Demeyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00943v2",
                "updated": "2025-10-31T11:55:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    11,
                    55,
                    42,
                    4,
                    304,
                    0
                ],
                "published": "2025-07-31T15:19:30Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    19,
                    30,
                    3,
                    212,
                    0
                ],
                "title": "LLMs Can Covertly Sandbag on Capability Evaluations Against\n  Chain-of-Thought Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Can Covertly Sandbag on Capability Evaluations Against\n  Chain-of-Thought Monitoring"
                },
                "summary": "Trustworthy evaluations of dangerous capabilities are increasingly crucial\nfor determining whether an AI system is safe to deploy. One empirically\ndemonstrated threat is sandbagging - the strategic underperformance on\nevaluations by AI models or their developers. A promising defense is to monitor\na model's chain-of-thought (CoT) reasoning, as this could reveal its intentions\nand plans. In this work, we measure the ability of models to sandbag on\ndangerous capability evaluations against a CoT monitor by prompting them to\nsandbag while being either monitor-oblivious or monitor-aware. We show that\nboth frontier models and small open-sourced models can covertly sandbag against\nCoT monitoring 0-shot without hints. However, they cannot yet do so reliably:\nthey bypass the monitor 16-36% of the time when monitor-aware, conditioned on\nsandbagging successfully. We qualitatively analyzed the uncaught CoTs to\nunderstand why the monitor failed. We reveal a rich attack surface for CoT\nmonitoring and contribute five covert sandbagging policies generated by models.\nThese results inform potential failure modes of CoT monitoring and may help\nbuild more diverse sandbagging model organisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy evaluations of dangerous capabilities are increasingly crucial\nfor determining whether an AI system is safe to deploy. One empirically\ndemonstrated threat is sandbagging - the strategic underperformance on\nevaluations by AI models or their developers. A promising defense is to monitor\na model's chain-of-thought (CoT) reasoning, as this could reveal its intentions\nand plans. In this work, we measure the ability of models to sandbag on\ndangerous capability evaluations against a CoT monitor by prompting them to\nsandbag while being either monitor-oblivious or monitor-aware. We show that\nboth frontier models and small open-sourced models can covertly sandbag against\nCoT monitoring 0-shot without hints. However, they cannot yet do so reliably:\nthey bypass the monitor 16-36% of the time when monitor-aware, conditioned on\nsandbagging successfully. We qualitatively analyzed the uncaught CoTs to\nunderstand why the monitor failed. We reveal a rich attack surface for CoT\nmonitoring and contribute five covert sandbagging policies generated by models.\nThese results inform potential failure modes of CoT monitoring and may help\nbuild more diverse sandbagging model organisms."
                },
                "authors": [
                    {
                        "name": "Chloe Li"
                    },
                    {
                        "name": "Mary Phuong"
                    },
                    {
                        "name": "Noah Y. Siegel"
                    }
                ],
                "author_detail": {
                    "name": "Noah Y. Siegel"
                },
                "author": "Noah Y. Siegel",
                "arxiv_comment": "Accepted to IJCNLP-AACL 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27401v1",
                "updated": "2025-10-31T11:37:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    11,
                    37,
                    42,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T11:37:42Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    11,
                    37,
                    42,
                    4,
                    304,
                    0
                ],
                "title": "\"Koyi Sawaal Nahi Hai\": Reimagining Maternal Health Chatbots for\n  Collective, Culturally Grounded Care",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Koyi Sawaal Nahi Hai\": Reimagining Maternal Health Chatbots for\n  Collective, Culturally Grounded Care"
                },
                "summary": "In recent years, LLM-based maternal health chatbots have been widely deployed\nin low-resource settings, but they often ignore real-world contexts where women\nmay not own phones, have limited literacy, and share decision-making within\nfamilies. Through the deployment of a WhatsApp-based maternal health chatbot\nwith 48 pregnant women in Lahore, Pakistan, we examine barriers to use in\npopulations where phones are shared, decision-making is collective, and\nliteracy varies. We complement this with focus group discussions with obstetric\nclinicians. Our findings reveal how adoption is shaped by proxy consent and\nfamily mediation, intermittent phone access, silence around asking questions,\ninfrastructural breakdowns, and contested authority. We frame barriers to\nnon-use as culturally conditioned rather than individual choices, and introduce\nthe Relational Chatbot Design Grammar (RCDG): four commitments that enable\nmediated decision-making, recognize silence as engagement, support episodic\nuse, and treat fragility as baseline to reorient maternal health chatbots\ntoward culturally grounded, collective care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, LLM-based maternal health chatbots have been widely deployed\nin low-resource settings, but they often ignore real-world contexts where women\nmay not own phones, have limited literacy, and share decision-making within\nfamilies. Through the deployment of a WhatsApp-based maternal health chatbot\nwith 48 pregnant women in Lahore, Pakistan, we examine barriers to use in\npopulations where phones are shared, decision-making is collective, and\nliteracy varies. We complement this with focus group discussions with obstetric\nclinicians. Our findings reveal how adoption is shaped by proxy consent and\nfamily mediation, intermittent phone access, silence around asking questions,\ninfrastructural breakdowns, and contested authority. We frame barriers to\nnon-use as culturally conditioned rather than individual choices, and introduce\nthe Relational Chatbot Design Grammar (RCDG): four commitments that enable\nmediated decision-making, recognize silence as engagement, support episodic\nuse, and treat fragility as baseline to reorient maternal health chatbots\ntoward culturally grounded, collective care."
                },
                "authors": [
                    {
                        "name": "Imaan Hameed"
                    },
                    {
                        "name": "Huma Umar"
                    },
                    {
                        "name": "Fozia Umber"
                    },
                    {
                        "name": "Maryam Mustafa"
                    }
                ],
                "author_detail": {
                    "name": "Maryam Mustafa"
                },
                "author": "Maryam Mustafa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27400v1",
                "updated": "2025-10-31T11:37:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    11,
                    37,
                    39,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T11:37:39Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    11,
                    37,
                    39,
                    4,
                    304,
                    0
                ],
                "title": "Balancing Knowledge Updates: Toward Unified Modular Editing in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Knowledge Updates: Toward Unified Modular Editing in LLMs"
                },
                "summary": "Knowledge editing has emerged as an efficient approach for updating factual\nknowledge in large language models (LLMs). It typically locates knowledge\nstorage modules and then modifies their parameters. However, most existing\nmethods focus on the weights of multilayer perceptron (MLP) modules, which are\noften identified as the main repositories of factual information. Other\ncomponents, such as attention (Attn) modules, are often ignored during editing.\nThis imbalance can leave residual outdated knowledge and limit editing\neffectiveness. We perform comprehensive knowledge localization experiments on\nadvanced LLMs and find that Attn modules play a substantial role in factual\nknowledge storage and retrieval, especially in earlier layers. Based on these\ninsights, we propose IntAttn-Edit, a method that extends the associative memory\nparadigm to jointly update both MLP and Attn modules. Our approach uses a\nknowledge balancing strategy that allocates update magnitudes in proportion to\neach module's measured contribution to knowledge storage. Experiments on\nstandard benchmarks show that IntAttn-Edit achieves higher edit success, better\ngeneralization, and stronger knowledge preservation than prior methods. Further\nanalysis shows that the balancing strategy keeps editing performance within an\noptimal range across diverse settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing has emerged as an efficient approach for updating factual\nknowledge in large language models (LLMs). It typically locates knowledge\nstorage modules and then modifies their parameters. However, most existing\nmethods focus on the weights of multilayer perceptron (MLP) modules, which are\noften identified as the main repositories of factual information. Other\ncomponents, such as attention (Attn) modules, are often ignored during editing.\nThis imbalance can leave residual outdated knowledge and limit editing\neffectiveness. We perform comprehensive knowledge localization experiments on\nadvanced LLMs and find that Attn modules play a substantial role in factual\nknowledge storage and retrieval, especially in earlier layers. Based on these\ninsights, we propose IntAttn-Edit, a method that extends the associative memory\nparadigm to jointly update both MLP and Attn modules. Our approach uses a\nknowledge balancing strategy that allocates update magnitudes in proportion to\neach module's measured contribution to knowledge storage. Experiments on\nstandard benchmarks show that IntAttn-Edit achieves higher edit success, better\ngeneralization, and stronger knowledge preservation than prior methods. Further\nanalysis shows that the balancing strategy keeps editing performance within an\noptimal range across diverse settings."
                },
                "authors": [
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Zijian Wang"
                    },
                    {
                        "name": "Kuo Zhao"
                    },
                    {
                        "name": "Dong Hu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Hu"
                },
                "author": "Dong Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07877v2",
                "updated": "2025-10-31T11:31:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    11,
                    31,
                    13,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-09T07:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    7,
                    28,
                    30,
                    3,
                    282,
                    0
                ],
                "title": "Ready to Translate, Not to Represent? Bias and Performance Gaps in\n  Multilingual LLMs Across Language Families and Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ready to Translate, Not to Represent? Bias and Performance Gaps in\n  Multilingual LLMs Across Language Families and Domains"
                },
                "summary": "The rise of Large Language Models (LLMs) has redefined Machine Translation\n(MT), enabling context-aware and fluent translations across hundreds of\nlanguages and textual domains. Despite their remarkable capabilities, LLMs\noften exhibit uneven performance across language families and specialized\ndomains. Moreover, recent evidence reveals that these models can encode and\namplify different biases present in their training data, posing serious\nconcerns for fairness, especially in low-resource languages. To address these\ngaps, we introduce Translation Tangles, a unified framework and dataset for\nevaluating the translation quality and fairness of open-source LLMs. Our\napproach benchmarks 24 bidirectional language pairs across multiple domains\nusing different metrics. We further propose a hybrid bias detection pipeline\nthat integrates rule-based heuristics, semantic similarity filtering, and\nLLM-based validation. We also introduce a high-quality, bias-annotated dataset\nbased on human evaluations of 1,439 translation-reference pairs. The code and\ndataset are accessible on GitHub:\nhttps://github.com/faiyazabdullah/TranslationTangles",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) has redefined Machine Translation\n(MT), enabling context-aware and fluent translations across hundreds of\nlanguages and textual domains. Despite their remarkable capabilities, LLMs\noften exhibit uneven performance across language families and specialized\ndomains. Moreover, recent evidence reveals that these models can encode and\namplify different biases present in their training data, posing serious\nconcerns for fairness, especially in low-resource languages. To address these\ngaps, we introduce Translation Tangles, a unified framework and dataset for\nevaluating the translation quality and fairness of open-source LLMs. Our\napproach benchmarks 24 bidirectional language pairs across multiple domains\nusing different metrics. We further propose a hybrid bias detection pipeline\nthat integrates rule-based heuristics, semantic similarity filtering, and\nLLM-based validation. We also introduce a high-quality, bias-annotated dataset\nbased on human evaluations of 1,439 translation-reference pairs. The code and\ndataset are accessible on GitHub:\nhttps://github.com/faiyazabdullah/TranslationTangles"
                },
                "authors": [
                    {
                        "name": "Md. Faiyaz Abdullah Sayeedi"
                    },
                    {
                        "name": "Md. Mahbub Alam"
                    },
                    {
                        "name": "Subhey Sadi Rahman"
                    },
                    {
                        "name": "Md. Adnanul Islam"
                    },
                    {
                        "name": "Jannatul Ferdous Deepti"
                    },
                    {
                        "name": "Tasnim Mohiuddin"
                    },
                    {
                        "name": "Md Mofijul Islam"
                    },
                    {
                        "name": "Swakkhar Shatabda"
                    }
                ],
                "author_detail": {
                    "name": "Swakkhar Shatabda"
                },
                "author": "Swakkhar Shatabda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27379v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27379v1",
                "updated": "2025-10-31T11:14:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    11,
                    14,
                    59,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T11:14:59Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    11,
                    14,
                    59,
                    4,
                    304,
                    0
                ],
                "title": "Spiking Neural Networks: The Future of Brain-Inspired Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks: The Future of Brain-Inspired Computing"
                },
                "summary": "Spiking Neural Networks (SNNs) represent the latest generation of neural\ncomputation, offering a brain-inspired alternative to conventional Artificial\nNeural Networks (ANNs). Unlike ANNs, which depend on continuous-valued signals,\nSNNs operate using distinct spike events, making them inherently more\nenergy-efficient and temporally dynamic. This study presents a comprehensive\nanalysis of SNN design models, training algorithms, and multi-dimensional\nperformance metrics, including accuracy, energy consumption, latency, spike\ncount, and convergence behavior. Key neuron models such as the Leaky\nIntegrate-and-Fire (LIF) and training strategies, including surrogate gradient\ndescent, ANN-to-SNN conversion, and Spike-Timing Dependent Plasticity (STDP),\nare examined in depth. Results show that surrogate gradient-trained SNNs\nclosely approximate ANN accuracy (within 1-2%), with faster convergence by the\n20th epoch and latency as low as 10 milliseconds. Converted SNNs also achieve\ncompetitive performance but require higher spike counts and longer simulation\nwindows. STDP-based SNNs, though slower to converge, exhibit the lowest spike\ncounts and energy consumption (as low as 5 millijoules per inference), making\nthem optimal for unsupervised and low-power tasks. These findings reinforce the\nsuitability of SNNs for energy-constrained, latency-sensitive, and adaptive\napplications such as robotics, neuromorphic vision, and edge AI systems. While\npromising, challenges persist in hardware standardization and scalable\ntraining. This study concludes that SNNs, with further refinement, are poised\nto propel the next phase of neuromorphic computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) represent the latest generation of neural\ncomputation, offering a brain-inspired alternative to conventional Artificial\nNeural Networks (ANNs). Unlike ANNs, which depend on continuous-valued signals,\nSNNs operate using distinct spike events, making them inherently more\nenergy-efficient and temporally dynamic. This study presents a comprehensive\nanalysis of SNN design models, training algorithms, and multi-dimensional\nperformance metrics, including accuracy, energy consumption, latency, spike\ncount, and convergence behavior. Key neuron models such as the Leaky\nIntegrate-and-Fire (LIF) and training strategies, including surrogate gradient\ndescent, ANN-to-SNN conversion, and Spike-Timing Dependent Plasticity (STDP),\nare examined in depth. Results show that surrogate gradient-trained SNNs\nclosely approximate ANN accuracy (within 1-2%), with faster convergence by the\n20th epoch and latency as low as 10 milliseconds. Converted SNNs also achieve\ncompetitive performance but require higher spike counts and longer simulation\nwindows. STDP-based SNNs, though slower to converge, exhibit the lowest spike\ncounts and energy consumption (as low as 5 millijoules per inference), making\nthem optimal for unsupervised and low-power tasks. These findings reinforce the\nsuitability of SNNs for energy-constrained, latency-sensitive, and adaptive\napplications such as robotics, neuromorphic vision, and edge AI systems. While\npromising, challenges persist in hardware standardization and scalable\ntraining. This study concludes that SNNs, with further refinement, are poised\nto propel the next phase of neuromorphic computing."
                },
                "authors": [
                    {
                        "name": "Sales G. Aribe Jr"
                    }
                ],
                "author_detail": {
                    "name": "Sales G. Aribe Jr"
                },
                "author": "Sales G. Aribe Jr",
                "arxiv_doi": "10.14445/22315381/IJETT-V73I10P104",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14445/22315381/IJETT-V73I10P104",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.27379v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27379v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "17 pages, 7 figures, 4 tables, Published with International Journal\n  of Engineering Trends and Technology (IJETT)",
                "arxiv_journal_ref": "International Journal of Engineering Trends and Technology, vol.\n  73, no. 10, pp.32-48, 2025",
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11194v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11194v4",
                "updated": "2025-10-31T10:57:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    57,
                    27,
                    4,
                    304,
                    0
                ],
                "published": "2024-11-17T22:58:28Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    22,
                    58,
                    28,
                    6,
                    322,
                    0
                ],
                "title": "Careless Whisper: Exploiting Silent Delivery Receipts to Monitor Users\n  on Mobile Instant Messengers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Careless Whisper: Exploiting Silent Delivery Receipts to Monitor Users\n  on Mobile Instant Messengers"
                },
                "summary": "With over 3 billion users globally, mobile instant messaging apps have become\nindispensable for both personal and professional communication. Besides plain\nmessaging, many services implement additional features such as delivery and\nread receipts informing a user when a message has successfully reached its\ntarget. This paper highlights that delivery receipts can pose significant\nprivacy risks to users. We use specifically crafted messages that trigger\ndelivery receipts allowing any user to be pinged without their knowledge or\nconsent. By using this technique at high frequency, we demonstrate how an\nattacker could extract private information such as the online and activity\nstatus of a victim, e.g., screen on/off. Moreover, we can infer the number of\ncurrently active user devices and their operating system, as well as launch\nresource exhaustion attacks, such as draining a user's battery or data\nallowance, all without generating any notification on the target side. Due to\nthe widespread adoption of vulnerable messengers (WhatsApp and Signal) and the\nfact that any user can be targeted simply by knowing their phone number, we\nargue for a design change to address this issue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With over 3 billion users globally, mobile instant messaging apps have become\nindispensable for both personal and professional communication. Besides plain\nmessaging, many services implement additional features such as delivery and\nread receipts informing a user when a message has successfully reached its\ntarget. This paper highlights that delivery receipts can pose significant\nprivacy risks to users. We use specifically crafted messages that trigger\ndelivery receipts allowing any user to be pinged without their knowledge or\nconsent. By using this technique at high frequency, we demonstrate how an\nattacker could extract private information such as the online and activity\nstatus of a victim, e.g., screen on/off. Moreover, we can infer the number of\ncurrently active user devices and their operating system, as well as launch\nresource exhaustion attacks, such as draining a user's battery or data\nallowance, all without generating any notification on the target side. Due to\nthe widespread adoption of vulnerable messengers (WhatsApp and Signal) and the\nfact that any user can be targeted simply by knowing their phone number, we\nargue for a design change to address this issue."
                },
                "authors": [
                    {
                        "name": "Gabriel K. Gegenhuber"
                    },
                    {
                        "name": "Maximilian Gnther"
                    },
                    {
                        "name": "Markus Maier"
                    },
                    {
                        "name": "Aljosha Judmayer"
                    },
                    {
                        "name": "Florian Holzbauer"
                    },
                    {
                        "name": "Philipp . Frenzel"
                    },
                    {
                        "name": "Johanna Ullrich"
                    }
                ],
                "author_detail": {
                    "name": "Johanna Ullrich"
                },
                "author": "Johanna Ullrich",
                "arxiv_comment": "28th International Symposium on Research in Attacks, Intrusions and\n  Defenses (RAID 2025) Distinguished with the Best Paper Award",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11194v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11194v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27364v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27364v1",
                "updated": "2025-10-31T10:51:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    51,
                    35,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T10:51:35Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    51,
                    35,
                    4,
                    304,
                    0
                ],
                "title": "Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A\n  Small-Data Pipeline with LoRA and Wan2.1 I2V",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning Open Video Generators for Cinematic Scene Synthesis: A\n  Small-Data Pipeline with LoRA and Wan2.1 I2V"
                },
                "summary": "We present a practical pipeline for fine-tuning open-source video diffusion\ntransformers to synthesize cinematic scenes for television and film production\nfrom small datasets. The proposed two-stage process decouples visual style\nlearning from motion generation. In the first stage, Low-Rank Adaptation (LoRA)\nmodules are integrated into the cross-attention layers of the Wan2.1 I2V-14B\nmodel to adapt its visual representations using a compact dataset of short\nclips from Ay Yapim's historical television film El Turco. This enables\nefficient domain transfer within hours on a single GPU. In the second stage,\nthe fine-tuned model produces stylistically consistent keyframes that preserve\ncostume, lighting, and color grading, which are then temporally expanded into\ncoherent 720p sequences through the model's video decoder. We further apply\nlightweight parallelization and sequence partitioning strategies to accelerate\ninference without quality degradation. Quantitative and qualitative evaluations\nusing FVD, CLIP-SIM, and LPIPS metrics, supported by a small expert user study,\ndemonstrate measurable improvements in cinematic fidelity and temporal\nstability over the base model. The complete training and inference pipeline is\nreleased to support reproducibility and adaptation across cinematic domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a practical pipeline for fine-tuning open-source video diffusion\ntransformers to synthesize cinematic scenes for television and film production\nfrom small datasets. The proposed two-stage process decouples visual style\nlearning from motion generation. In the first stage, Low-Rank Adaptation (LoRA)\nmodules are integrated into the cross-attention layers of the Wan2.1 I2V-14B\nmodel to adapt its visual representations using a compact dataset of short\nclips from Ay Yapim's historical television film El Turco. This enables\nefficient domain transfer within hours on a single GPU. In the second stage,\nthe fine-tuned model produces stylistically consistent keyframes that preserve\ncostume, lighting, and color grading, which are then temporally expanded into\ncoherent 720p sequences through the model's video decoder. We further apply\nlightweight parallelization and sequence partitioning strategies to accelerate\ninference without quality degradation. Quantitative and qualitative evaluations\nusing FVD, CLIP-SIM, and LPIPS metrics, supported by a small expert user study,\ndemonstrate measurable improvements in cinematic fidelity and temporal\nstability over the base model. The complete training and inference pipeline is\nreleased to support reproducibility and adaptation across cinematic domains."
                },
                "authors": [
                    {
                        "name": "Meftun Akarsu"
                    },
                    {
                        "name": "Kerem Catay"
                    },
                    {
                        "name": "Sedat Bin Vedat"
                    },
                    {
                        "name": "Enes Kutay Yarkan"
                    },
                    {
                        "name": "Ilke Senturk"
                    },
                    {
                        "name": "Arda Sar"
                    },
                    {
                        "name": "Dafne Eksioglu"
                    }
                ],
                "author_detail": {
                    "name": "Dafne Eksioglu"
                },
                "author": "Dafne Eksioglu",
                "arxiv_doi": "10.5281/zenodo.17370356",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5281/zenodo.17370356",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.27364v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27364v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "video generation, image-to-video, dif- fusion transformer, LoRA,\n  fine-tuning, cinematic scene synthesis, multi-GPU inference, fully sharded\n  data parallelism, computational efficiency",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27363v1",
                "updated": "2025-10-31T10:51:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    51,
                    27,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T10:51:27Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    51,
                    27,
                    4,
                    304,
                    0
                ],
                "title": "ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool\n  Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool\n  Use"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated remarkable\nproblem-solving capabilities by autonomously integrating with external tools\nfor collaborative reasoning. However, due to the inherently complex and diverse\nnature of multimodal information, enabling multimodal large language models\n(MLLMs) to flexibly and efficiently utilize external tools during reasoning\nremains an underexplored challenge. In this work, we introduce ToolScope, an\nagentic framework designed to unify global planning with local multimodal\nperception, adopting a specialized Perceive tool to mitigates visual context\ndegradation in long-horizon VQA task. ToolScope comprises three primary\ncomponents: the Global Navigator, the Agentic Executor, and the Response\nSynthesizer. The Global Navigator functions as a \"telescope\", offering\nhigh-level strategic guidance. The Agentic Executor operates iteratively to\naugment MLLM with local perception through the integration of external\ntools-Search, Code, and Perceive. Finally, the Response Synthesizer\nconsolidates and organizes the reasoning process into a coherent, user-friendly\noutput. We evaluate ToolScope on four VQA benchmarks across diverse domains,\nincluding VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong\ngeneralization capabilities, achieving an average performance improvement of up\nto +6.69% across all datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated remarkable\nproblem-solving capabilities by autonomously integrating with external tools\nfor collaborative reasoning. However, due to the inherently complex and diverse\nnature of multimodal information, enabling multimodal large language models\n(MLLMs) to flexibly and efficiently utilize external tools during reasoning\nremains an underexplored challenge. In this work, we introduce ToolScope, an\nagentic framework designed to unify global planning with local multimodal\nperception, adopting a specialized Perceive tool to mitigates visual context\ndegradation in long-horizon VQA task. ToolScope comprises three primary\ncomponents: the Global Navigator, the Agentic Executor, and the Response\nSynthesizer. The Global Navigator functions as a \"telescope\", offering\nhigh-level strategic guidance. The Agentic Executor operates iteratively to\naugment MLLM with local perception through the integration of external\ntools-Search, Code, and Perceive. Finally, the Response Synthesizer\nconsolidates and organizes the reasoning process into a coherent, user-friendly\noutput. We evaluate ToolScope on four VQA benchmarks across diverse domains,\nincluding VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong\ngeneralization capabilities, achieving an average performance improvement of up\nto +6.69% across all datasets."
                },
                "authors": [
                    {
                        "name": "Mengjie Deng"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27359v1",
                "updated": "2025-10-31T10:44:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    44,
                    16,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T10:44:16Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    44,
                    16,
                    4,
                    304,
                    0
                ],
                "title": "FPS: Feedforward-based Parameter Selection For Efficient Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FPS: Feedforward-based Parameter Selection For Efficient Fine-Tuning"
                },
                "summary": "Parameter-Efficient Fine-Tuning (PEFT) has emerged as a key strategy for\nadapting large-scale pre-trained models to downstream tasks, but existing\napproaches face notable limitations. Addition-based methods, such as Adapters\n[1], introduce inference latency and engineering complexity, while\nselection-based methods like Gradient-based Parameter Selection (GPS) [2]\nrequire a full backward pass, which results in the same peak memory usage as\nfull fine-tuning. To address this dilemma, we propose Feedforward-based\nParameter Selection (FPS), a gradient-free method that identifies an optimal\nparameter subset in a single forward pass. FPS ranks parameters by the product\nof their magnitudes and corresponding input activations, leveraging both\npre-trained knowledge and downstream data. Evaluated on $24$ visual tasks from\nFGVC and VTAB-1k, FPS achieves performance comparable to state-of-the-art\nmethods while reducing peak memory usage by nearly $9 \\times$ and accelerating\nparameter selection by about $2 \\times$, offering a genuinely memory-efficient\nand practical solution for fine-tuning large-scale pre-trained models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning (PEFT) has emerged as a key strategy for\nadapting large-scale pre-trained models to downstream tasks, but existing\napproaches face notable limitations. Addition-based methods, such as Adapters\n[1], introduce inference latency and engineering complexity, while\nselection-based methods like Gradient-based Parameter Selection (GPS) [2]\nrequire a full backward pass, which results in the same peak memory usage as\nfull fine-tuning. To address this dilemma, we propose Feedforward-based\nParameter Selection (FPS), a gradient-free method that identifies an optimal\nparameter subset in a single forward pass. FPS ranks parameters by the product\nof their magnitudes and corresponding input activations, leveraging both\npre-trained knowledge and downstream data. Evaluated on $24$ visual tasks from\nFGVC and VTAB-1k, FPS achieves performance comparable to state-of-the-art\nmethods while reducing peak memory usage by nearly $9 \\times$ and accelerating\nparameter selection by about $2 \\times$, offering a genuinely memory-efficient\nand practical solution for fine-tuning large-scale pre-trained models."
                },
                "authors": [
                    {
                        "name": "Kenneth Yang"
                    },
                    {
                        "name": "Wen-Li Wei"
                    },
                    {
                        "name": "Jen-Chun Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jen-Chun Lin"
                },
                "author": "Jen-Chun Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27355v1",
                "updated": "2025-10-31T10:40:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    40,
                    19,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T10:40:19Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    40,
                    19,
                    4,
                    304,
                    0
                ],
                "title": "ThoughtProbe: Classifier-Guided LLM Thought Space Exploration via\n  Probing Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThoughtProbe: Classifier-Guided LLM Thought Space Exploration via\n  Probing Representations"
                },
                "summary": "This paper introduces ThoughtProbe, a novel inference time framework that\nleverages the hidden reasoning features of Large Language Models (LLMs) to\nimprove their reasoning performance. Unlike previous works that manipulate the\nhidden representations to steer LLM generation, we harness them as\ndiscriminative signals to guide the tree structured response space exploration.\nIn each node expansion, a classifier serves as a scoring and ranking mechanism\nthat efficiently allocates computational resources by prioritizing higher score\ncandidates for continuation. After completing the tree expansion, we collect\nanswers from all branches to form a candidate answer pool. We then propose a\nbranch aggregation method that marginalizes over all supporting branches by\naggregating their CoT scores, thereby identifying the optimal answer from the\npool. Experimental results show that our framework's comprehensive exploration\nnot only covers valid reasoning chains but also effectively identifies them,\nachieving significant improvements across multiple arithmetic reasoning\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ThoughtProbe, a novel inference time framework that\nleverages the hidden reasoning features of Large Language Models (LLMs) to\nimprove their reasoning performance. Unlike previous works that manipulate the\nhidden representations to steer LLM generation, we harness them as\ndiscriminative signals to guide the tree structured response space exploration.\nIn each node expansion, a classifier serves as a scoring and ranking mechanism\nthat efficiently allocates computational resources by prioritizing higher score\ncandidates for continuation. After completing the tree expansion, we collect\nanswers from all branches to form a candidate answer pool. We then propose a\nbranch aggregation method that marginalizes over all supporting branches by\naggregating their CoT scores, thereby identifying the optimal answer from the\npool. Experimental results show that our framework's comprehensive exploration\nnot only covers valid reasoning chains but also effectively identifies them,\nachieving significant improvements across multiple arithmetic reasoning\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Zijian Wang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "arxiv_comment": "EMNLP2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27353v1",
                "updated": "2025-10-31T10:39:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    39,
                    16,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T10:39:16Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    39,
                    16,
                    4,
                    304,
                    0
                ],
                "title": "An In-depth Study of LLM Contributions to the Bin Packing Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An In-depth Study of LLM Contributions to the Bin Packing Problem"
                },
                "summary": "Recent studies have suggested that Large Language Models (LLMs) could provide\ninteresting ideas contributing to mathematical discovery. This claim was\nmotivated by reports that LLM-based genetic algorithms produced heuristics\noffering new insights into the online bin packing problem under uniform and\nWeibull distributions. In this work, we reassess this claim through a detailed\nanalysis of the heuristics produced by LLMs, examining both their behavior and\ninterpretability. Despite being human-readable, these heuristics remain largely\nopaque even to domain experts. Building on this analysis, we propose a new\nclass of algorithms tailored to these specific bin packing instances. The\nderived algorithms are significantly simpler, more efficient, more\ninterpretable, and more generalizable, suggesting that the considered instances\nare themselves relatively simple. We then discuss the limitations of the claim\nregarding LLMs' contribution to this problem, which appears to rest on the\nmistaken assumption that the instances had previously been studied. Our\nfindings instead emphasize the need for rigorous validation and\ncontextualization when assessing the scientific value of LLM-generated outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have suggested that Large Language Models (LLMs) could provide\ninteresting ideas contributing to mathematical discovery. This claim was\nmotivated by reports that LLM-based genetic algorithms produced heuristics\noffering new insights into the online bin packing problem under uniform and\nWeibull distributions. In this work, we reassess this claim through a detailed\nanalysis of the heuristics produced by LLMs, examining both their behavior and\ninterpretability. Despite being human-readable, these heuristics remain largely\nopaque even to domain experts. Building on this analysis, we propose a new\nclass of algorithms tailored to these specific bin packing instances. The\nderived algorithms are significantly simpler, more efficient, more\ninterpretable, and more generalizable, suggesting that the considered instances\nare themselves relatively simple. We then discuss the limitations of the claim\nregarding LLMs' contribution to this problem, which appears to rest on the\nmistaken assumption that the instances had previously been studied. Our\nfindings instead emphasize the need for rigorous validation and\ncontextualization when assessing the scientific value of LLM-generated outputs."
                },
                "authors": [
                    {
                        "name": "Julien Herrmann"
                    },
                    {
                        "name": "Guillaume Pallez"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Pallez"
                },
                "author": "Guillaume Pallez",
                "arxiv_comment": "15 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.8; F.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27342v1",
                "updated": "2025-10-31T10:24:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    24,
                    15,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T10:24:15Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    24,
                    15,
                    4,
                    304,
                    0
                ],
                "title": "Pairwise and Attribute-Aware Decision Tree-Based Preference Elicitation\n  for Cold-Start Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pairwise and Attribute-Aware Decision Tree-Based Preference Elicitation\n  for Cold-Start Recommendation"
                },
                "summary": "Recommender systems (RSs) are intelligent filtering methods that suggest\nitems to users based on their inferred preferences, derived from their\ninteraction history on the platform. Collaborative filtering-based RSs rely on\nusers past interactions to generate recommendations. However, when a user is\nnew to the platform, referred to as a cold-start user, there is no historical\ndata available, making it difficult to provide personalized recommendations. To\naddress this, rating elicitation techniques can be used to gather initial\nratings or preferences on selected items, helping to build an early\nunderstanding of the user's tastes. Rating elicitation approaches are generally\ncategorized into two types: non-personalized and personalized. Decision\ntree-based rating elicitation is a personalized method that queries users about\ntheir preferences at each node of the tree until sufficient information is\ngathered. In this paper, we propose an extension to the decision tree approach\nfor rating elicitation in the context of music recommendation. Our method: (i)\nelicits not only item ratings but also preferences on attributes such as genres\nto better cluster users, and (ii) uses item pairs instead of single items at\neach node to more effectively learn user preferences. Experimental results\ndemonstrate that both proposed enhancements lead to improved performance,\nparticularly with a reduced number of queries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems (RSs) are intelligent filtering methods that suggest\nitems to users based on their inferred preferences, derived from their\ninteraction history on the platform. Collaborative filtering-based RSs rely on\nusers past interactions to generate recommendations. However, when a user is\nnew to the platform, referred to as a cold-start user, there is no historical\ndata available, making it difficult to provide personalized recommendations. To\naddress this, rating elicitation techniques can be used to gather initial\nratings or preferences on selected items, helping to build an early\nunderstanding of the user's tastes. Rating elicitation approaches are generally\ncategorized into two types: non-personalized and personalized. Decision\ntree-based rating elicitation is a personalized method that queries users about\ntheir preferences at each node of the tree until sufficient information is\ngathered. In this paper, we propose an extension to the decision tree approach\nfor rating elicitation in the context of music recommendation. Our method: (i)\nelicits not only item ratings but also preferences on attributes such as genres\nto better cluster users, and (ii) uses item pairs instead of single items at\neach node to more effectively learn user preferences. Experimental results\ndemonstrate that both proposed enhancements lead to improved performance,\nparticularly with a reduced number of queries."
                },
                "authors": [
                    {
                        "name": "Alireza Gharahighehi"
                    },
                    {
                        "name": "Felipe Kenji Nakano"
                    },
                    {
                        "name": "Xuehua Yang"
                    },
                    {
                        "name": "Wenhan Cu"
                    },
                    {
                        "name": "Celine Vens"
                    }
                ],
                "author_detail": {
                    "name": "Celine Vens"
                },
                "author": "Celine Vens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27335v1",
                "updated": "2025-10-31T10:06:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    6,
                    28,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T10:06:28Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    6,
                    28,
                    4,
                    304,
                    0
                ],
                "title": "Understanding the Implicit User Intention via Reasoning with Large\n  Language Model for Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Implicit User Intention via Reasoning with Large\n  Language Model for Image Editing"
                },
                "summary": "Existing image editing methods can handle simple editing instructions very\nwell. To deal with complex editing instructions, they often need to jointly\nfine-tune the large language models (LLMs) and diffusion models (DMs), which\ninvolves very high computational complexity and training cost. To address this\nissue, we propose a new method, called \\textbf{C}omplex \\textbf{I}mage\n\\textbf{E}diting via \\textbf{L}LM \\textbf{R}easoning (CIELR), which converts a\ncomplex user instruction into a set of simple and explicit editing actions,\neliminating the need for jointly fine-tuning the large language models and\ndiffusion models. Specifically, we first construct a structured semantic\nrepresentation of the input image using foundation models. Then, we introduce\nan iterative update mechanism that can progressively refine this\nrepresentation, obtaining a fine-grained visual representation of the image\nscene. This allows us to perform complex and flexible image editing tasks.\nExtensive experiments on the SmartEdit Reasoning Scenario Set show that our\nmethod surpasses the previous state-of-the-art by 9.955 dB in PSNR, indicating\nits superior preservation of regions that should remain consistent. Due to the\nlimited number of samples of public datasets of complex image editing with\nreasoning, we construct a benchmark named CIEBench, containing 86 image\nsamples, together with a metric specifically for reasoning-based image editing.\nCIELR also outperforms previous methods on this benchmark. The code and dataset\nare available at\n\\href{https://github.com/Jia-shao/Reasoning-Editing}{https://github.com/Jia-shao/Reasoning-Editing}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing image editing methods can handle simple editing instructions very\nwell. To deal with complex editing instructions, they often need to jointly\nfine-tune the large language models (LLMs) and diffusion models (DMs), which\ninvolves very high computational complexity and training cost. To address this\nissue, we propose a new method, called \\textbf{C}omplex \\textbf{I}mage\n\\textbf{E}diting via \\textbf{L}LM \\textbf{R}easoning (CIELR), which converts a\ncomplex user instruction into a set of simple and explicit editing actions,\neliminating the need for jointly fine-tuning the large language models and\ndiffusion models. Specifically, we first construct a structured semantic\nrepresentation of the input image using foundation models. Then, we introduce\nan iterative update mechanism that can progressively refine this\nrepresentation, obtaining a fine-grained visual representation of the image\nscene. This allows us to perform complex and flexible image editing tasks.\nExtensive experiments on the SmartEdit Reasoning Scenario Set show that our\nmethod surpasses the previous state-of-the-art by 9.955 dB in PSNR, indicating\nits superior preservation of regions that should remain consistent. Due to the\nlimited number of samples of public datasets of complex image editing with\nreasoning, we construct a benchmark named CIEBench, containing 86 image\nsamples, together with a metric specifically for reasoning-based image editing.\nCIELR also outperforms previous methods on this benchmark. The code and dataset\nare available at\n\\href{https://github.com/Jia-shao/Reasoning-Editing}{https://github.com/Jia-shao/Reasoning-Editing}."
                },
                "authors": [
                    {
                        "name": "Yijia Wang"
                    },
                    {
                        "name": "Yiqing Shen"
                    },
                    {
                        "name": "Weiming Chen"
                    },
                    {
                        "name": "Zhihai He"
                    }
                ],
                "author_detail": {
                    "name": "Zhihai He"
                },
                "author": "Zhihai He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23724v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23724v3",
                "updated": "2025-10-31T10:04:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    4,
                    13,
                    4,
                    304,
                    0
                ],
                "published": "2025-05-29T17:55:21Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    55,
                    21,
                    3,
                    149,
                    0
                ],
                "title": "SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via\n  Subspace-Constrained LoRA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via\n  Subspace-Constrained LoRA"
                },
                "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), are indispensable for efficiently customizing Large Language\nModels (LLMs). However, vanilla LoRA suffers from slow convergence speed and\nknowledge forgetting problems. Recent studies have leveraged the power of\ndesigned LoRA initialization, to enhance the fine-tuning efficiency, or to\npreserve knowledge in the pre-trained LLM. However, none of these works can\naddress the two cases at the same time. To this end, we introduce\nSubspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework\nengineered to navigate the trade-off between efficient fine-tuning and\nknowledge preservation. We achieve this by constraining the output of trainable\nLoRA adapters in a low-rank subspace, where the context information of\nfine-tuning data is most preserved while the context information of preserved\nknowledge is least retained, in a balanced way. Such constraint enables the\ntrainable weights to primarily focus on the main features of fine-tuning data\nwhile avoiding damaging the preserved knowledge features. We provide\ntheoretical analysis on our method, and conduct extensive experiments including\nsafety preservation and world knowledge preservation, on various downstream\ntasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning\nperformance while markedly diminishing knowledge forgetting, surpassing\ncontemporary LoRA initialization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), are indispensable for efficiently customizing Large Language\nModels (LLMs). However, vanilla LoRA suffers from slow convergence speed and\nknowledge forgetting problems. Recent studies have leveraged the power of\ndesigned LoRA initialization, to enhance the fine-tuning efficiency, or to\npreserve knowledge in the pre-trained LLM. However, none of these works can\naddress the two cases at the same time. To this end, we introduce\nSubspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework\nengineered to navigate the trade-off between efficient fine-tuning and\nknowledge preservation. We achieve this by constraining the output of trainable\nLoRA adapters in a low-rank subspace, where the context information of\nfine-tuning data is most preserved while the context information of preserved\nknowledge is least retained, in a balanced way. Such constraint enables the\ntrainable weights to primarily focus on the main features of fine-tuning data\nwhile avoiding damaging the preserved knowledge features. We provide\ntheoretical analysis on our method, and conduct extensive experiments including\nsafety preservation and world knowledge preservation, on various downstream\ntasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning\nperformance while markedly diminishing knowledge forgetting, surpassing\ncontemporary LoRA initialization methods."
                },
                "authors": [
                    {
                        "name": "Minrui Luo"
                    },
                    {
                        "name": "Fuhang Kuang"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Tianxing He"
                    }
                ],
                "author_detail": {
                    "name": "Tianxing He"
                },
                "author": "Tianxing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23724v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23724v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27328v1",
                "updated": "2025-10-31T09:57:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    57,
                    19,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T09:57:19Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    57,
                    19,
                    4,
                    304,
                    0
                ],
                "title": "A Unified Representation Underlying the Judgment of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Representation Underlying the Judgment of Large Language\n  Models"
                },
                "summary": "A central architectural question for both biological and artificial\nintelligence is whether judgment relies on specialized modules or a unified,\ndomain-general resource. While the discovery of decodable neural\nrepresentations for distinct concepts in Large Language Models (LLMs) has\nsuggested a modular architecture, whether these representations are truly\nindependent systems remains an open question. Here we provide evidence for a\nconvergent architecture. Across a range of LLMs, we find that diverse\nevaluative judgments are computed along a dominant dimension, which we term the\nValence-Assent Axis (VAA). This axis jointly encodes subjective valence (\"what\nis good\") and the model's assent to factual claims (\"what is true\"). Through\ndirect interventions, we show this unified representation creates a critical\ndependency: the VAA functions as a control signal that steers the generative\nprocess to construct a rationale consistent with its evaluative state, even at\nthe cost of factual accuracy. This mechanism, which we term the subordination\nof reasoning, shifts the process of reasoning from impartial inference toward\ngoal-directed justification. Our discovery offers a mechanistic account for\nsystemic bias and hallucination, revealing how an architecture that promotes\ncoherent judgment can systematically undermine faithful reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central architectural question for both biological and artificial\nintelligence is whether judgment relies on specialized modules or a unified,\ndomain-general resource. While the discovery of decodable neural\nrepresentations for distinct concepts in Large Language Models (LLMs) has\nsuggested a modular architecture, whether these representations are truly\nindependent systems remains an open question. Here we provide evidence for a\nconvergent architecture. Across a range of LLMs, we find that diverse\nevaluative judgments are computed along a dominant dimension, which we term the\nValence-Assent Axis (VAA). This axis jointly encodes subjective valence (\"what\nis good\") and the model's assent to factual claims (\"what is true\"). Through\ndirect interventions, we show this unified representation creates a critical\ndependency: the VAA functions as a control signal that steers the generative\nprocess to construct a rationale consistent with its evaluative state, even at\nthe cost of factual accuracy. This mechanism, which we term the subordination\nof reasoning, shifts the process of reasoning from impartial inference toward\ngoal-directed justification. Our discovery offers a mechanistic account for\nsystemic bias and hallucination, revealing how an architecture that promotes\ncoherent judgment can systematically undermine faithful reasoning."
                },
                "authors": [
                    {
                        "name": "Yi-Long Lu"
                    },
                    {
                        "name": "Jiajun Song"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03419v2",
                "updated": "2025-10-31T09:50:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    50,
                    31,
                    4,
                    304,
                    0
                ],
                "published": "2025-09-03T15:48:33Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    48,
                    33,
                    2,
                    246,
                    0
                ],
                "title": "Curse of Knowledge: When Complex Evaluation Context Benefits yet Biases\n  LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curse of Knowledge: When Complex Evaluation Context Benefits yet Biases\n  LLM Judges"
                },
                "summary": "As large language models (LLMs) grow more capable, they face increasingly\ndiverse and complex tasks, making reliable evaluation challenging. The paradigm\nof LLMs as judges has emerged as a scalable solution, yet prior work primarily\nfocuses on simple settings. Their reliability in complex tasks--where\nmulti-faceted rubrics, unstructured reference answers, and nuanced criteria are\ncritical--remains understudied. In this paper, we constructed ComplexEval, a\nchallenge benchmark designed to systematically expose and quantify Auxiliary\nInformation Induced Biases. We systematically investigated and validated 6\npreviously unexplored biases across 12 basic and 3 advanced scenarios. Key\nfindings reveal: (1) all evaluated models exhibit significant susceptibility to\nthese biases, with bias magnitude scaling with task complexity; (2) notably,\nLarge Reasoning Models (LRMs) show paradoxical vulnerability. Our in-depth\nanalysis offers crucial insights for improving the accuracy and verifiability\nof evaluation signals, paving the way for more general and robust evaluation\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) grow more capable, they face increasingly\ndiverse and complex tasks, making reliable evaluation challenging. The paradigm\nof LLMs as judges has emerged as a scalable solution, yet prior work primarily\nfocuses on simple settings. Their reliability in complex tasks--where\nmulti-faceted rubrics, unstructured reference answers, and nuanced criteria are\ncritical--remains understudied. In this paper, we constructed ComplexEval, a\nchallenge benchmark designed to systematically expose and quantify Auxiliary\nInformation Induced Biases. We systematically investigated and validated 6\npreviously unexplored biases across 12 basic and 3 advanced scenarios. Key\nfindings reveal: (1) all evaluated models exhibit significant susceptibility to\nthese biases, with bias magnitude scaling with task complexity; (2) notably,\nLarge Reasoning Models (LRMs) show paradoxical vulnerability. Our in-depth\nanalysis offers crucial insights for improving the accuracy and verifiability\nof evaluation signals, paving the way for more general and robust evaluation\nmodels."
                },
                "authors": [
                    {
                        "name": "Weiyuan Li"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Qingqing Dong"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03336v2",
                "updated": "2025-10-31T09:42:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    42,
                    16,
                    4,
                    304,
                    0
                ],
                "published": "2024-06-05T14:51:34Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    14,
                    51,
                    34,
                    2,
                    157,
                    0
                ],
                "title": "Gibbs sampling for Bayesian P-splines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gibbs sampling for Bayesian P-splines"
                },
                "summary": "P-splines provide a flexible setting for modeling nonlinear model components\nbased on a discretized penalty structure with a relatively simple computational\nbackbone. Under a Bayesian inferential framework based on Markov chain Monte\nCarlo, estimates of model coefficients in P-splines models are typically\nobtained by means of Metropolis-type algorithms. These algorithms rely on a\nproposal distribution that has to be carefully chosen to generate Markov chains\nthat efficiently explore the parameter space. To avoid such a sensitive tuning\nchoice, we extend the Gibbs sampler to Bayesian P-splines models. In this model\nclass, conditional posterior distributions of model coefficients are shown to\nhave attractive mathematical properties. Taking advantage of these properties,\nwe propose to sample the conditional posteriors by alternating between the\nadaptive rejection sampler when targets are log-concave and the Griddy-Gibbs\nsampler when targets are characterized by more complex shapes. The proposed\nGibbs sampler for Bayesian P-splines (GSBPS) algorithm is shown to be an\ninteresting tuning-free tool for inference in Bayesian P-splines models.\nMoreover, the GSBPS algorithm can be translated in a compact and user-friendly\nroutine. After describing theoretical results, we illustrate the potential of\nour methodology in density estimation, Binomial regression, and smoothing of\nepidemic curves.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "P-splines provide a flexible setting for modeling nonlinear model components\nbased on a discretized penalty structure with a relatively simple computational\nbackbone. Under a Bayesian inferential framework based on Markov chain Monte\nCarlo, estimates of model coefficients in P-splines models are typically\nobtained by means of Metropolis-type algorithms. These algorithms rely on a\nproposal distribution that has to be carefully chosen to generate Markov chains\nthat efficiently explore the parameter space. To avoid such a sensitive tuning\nchoice, we extend the Gibbs sampler to Bayesian P-splines models. In this model\nclass, conditional posterior distributions of model coefficients are shown to\nhave attractive mathematical properties. Taking advantage of these properties,\nwe propose to sample the conditional posteriors by alternating between the\nadaptive rejection sampler when targets are log-concave and the Griddy-Gibbs\nsampler when targets are characterized by more complex shapes. The proposed\nGibbs sampler for Bayesian P-splines (GSBPS) algorithm is shown to be an\ninteresting tuning-free tool for inference in Bayesian P-splines models.\nMoreover, the GSBPS algorithm can be translated in a compact and user-friendly\nroutine. After describing theoretical results, we illustrate the potential of\nour methodology in density estimation, Binomial regression, and smoothing of\nepidemic curves."
                },
                "authors": [
                    {
                        "name": "Oswaldo Gressani"
                    },
                    {
                        "name": "Paul H. C. Eilers"
                    }
                ],
                "author_detail": {
                    "name": "Paul H. C. Eilers"
                },
                "author": "Paul H. C. Eilers",
                "arxiv_comment": "21 pages, 5 figures, 0 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22623v2",
                "updated": "2025-10-31T09:40:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    40,
                    18,
                    4,
                    304,
                    0
                ],
                "published": "2025-07-30T12:42:35Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    42,
                    35,
                    2,
                    211,
                    0
                ],
                "title": "Multilingual Political Views of Large Language Models: Identification\n  and Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Political Views of Large Language Models: Identification\n  and Steering"
                },
                "summary": "Large language models (LLMs) are increasingly used in everyday tools and\napplications, raising concerns about their potential influence on political\nviews. While prior research has shown that LLMs often exhibit measurable\npolitical biases--frequently skewing toward liberal or progressive\npositions--key gaps remain. Most existing studies evaluate only a narrow set of\nmodels and languages, leaving open questions about the generalizability of\npolitical biases across architectures, scales, and multilingual settings.\nMoreover, few works examine whether these biases can be actively controlled.\n  In this work, we address these gaps through a large-scale study of political\norientation in modern open-source instruction-tuned LLMs. We evaluate seven\nmodels, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using\nthe Political Compass Test with 11 semantically equivalent paraphrases per\nstatement to ensure robust measurement. Our results reveal that larger models\nconsistently shift toward libertarian-left positions, with significant\nvariations across languages and model families. To test the manipulability of\npolitical stances, we utilize a simple center-of-mass activation intervention\ntechnique and show that it reliably steers model responses toward alternative\nideological positions across multiple languages. Our code is publicly available\nat https://github.com/d-gurgurov/Political-Ideologies-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used in everyday tools and\napplications, raising concerns about their potential influence on political\nviews. While prior research has shown that LLMs often exhibit measurable\npolitical biases--frequently skewing toward liberal or progressive\npositions--key gaps remain. Most existing studies evaluate only a narrow set of\nmodels and languages, leaving open questions about the generalizability of\npolitical biases across architectures, scales, and multilingual settings.\nMoreover, few works examine whether these biases can be actively controlled.\n  In this work, we address these gaps through a large-scale study of political\norientation in modern open-source instruction-tuned LLMs. We evaluate seven\nmodels, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using\nthe Political Compass Test with 11 semantically equivalent paraphrases per\nstatement to ensure robust measurement. Our results reveal that larger models\nconsistently shift toward libertarian-left positions, with significant\nvariations across languages and model families. To test the manipulability of\npolitical stances, we utilize a simple center-of-mass activation intervention\ntechnique and show that it reliably steers model responses toward alternative\nideological positions across multiple languages. Our code is publicly available\nat https://github.com/d-gurgurov/Political-Ideologies-LLMs."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Katharina Trinley"
                    },
                    {
                        "name": "Ivan Vykopal"
                    },
                    {
                        "name": "Josef van Genabith"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Roberto Zamparelli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Zamparelli"
                },
                "author": "Roberto Zamparelli",
                "arxiv_comment": "pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21706v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21706v2",
                "updated": "2025-10-31T09:16:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    16,
                    38,
                    4,
                    304,
                    0
                ],
                "published": "2025-08-29T15:25:05Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    15,
                    25,
                    5,
                    4,
                    241,
                    0
                ],
                "title": "Accelerating Mixture-of-Experts Inference by Hiding Offloading Latency\n  with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Mixture-of-Experts Inference by Hiding Offloading Latency\n  with Speculative Decoding"
                },
                "summary": "Recent advancements in Mixture of Experts (MoE) models have significantly\nincreased their parameter scale as well as model performance. Extensive\noffloading techniques have been proposed to address the GPU memory limitations\nof MoE inference. However, due to the I/O bottleneck and sparse computation of\nMoE models, existing offloading techniques still suffer from low hardware\nutilization. To fully utilize the hardware resources, we propose SpecMoEOff,\nwhich employs the speculative decoding technique to enlarge the workload of\neach expert. SpecMoEOff orchestrates the GPU and CPU by both theoretical and\nempirical roofline analysis. In addition, we develop a dedicated CPU chunked\nattention verification kernel to fit the speculative decoding in offloading\nscenarios as well as minimizing the additional overhead led by draft models.\nSpecMoEOff further integrates an optimizer to automatically tune the\nhyperparameters of speculative decoding under given hardware and workload.\nExperimental results show that SpecMoEOff achieves up to 2.5x decode throughput\nimprovement over the state-of-the-art MoE offloading techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Mixture of Experts (MoE) models have significantly\nincreased their parameter scale as well as model performance. Extensive\noffloading techniques have been proposed to address the GPU memory limitations\nof MoE inference. However, due to the I/O bottleneck and sparse computation of\nMoE models, existing offloading techniques still suffer from low hardware\nutilization. To fully utilize the hardware resources, we propose SpecMoEOff,\nwhich employs the speculative decoding technique to enlarge the workload of\neach expert. SpecMoEOff orchestrates the GPU and CPU by both theoretical and\nempirical roofline analysis. In addition, we develop a dedicated CPU chunked\nattention verification kernel to fit the speculative decoding in offloading\nscenarios as well as minimizing the additional overhead led by draft models.\nSpecMoEOff further integrates an optimizer to automatically tune the\nhyperparameters of speculative decoding under given hardware and workload.\nExperimental results show that SpecMoEOff achieves up to 2.5x decode throughput\nimprovement over the state-of-the-art MoE offloading techniques."
                },
                "authors": [
                    {
                        "name": "Zhibin Wang"
                    },
                    {
                        "name": "Zhonghui Zhang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Zibo Wang"
                    },
                    {
                        "name": "Mo Zhou"
                    },
                    {
                        "name": "Peng Jiang"
                    },
                    {
                        "name": "Weilin Cai"
                    },
                    {
                        "name": "Chengying Huan"
                    },
                    {
                        "name": "Rong Gu"
                    },
                    {
                        "name": "Sheng Zhong"
                    },
                    {
                        "name": "Chen Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Tian"
                },
                "author": "Chen Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21706v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21706v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26767v2",
                "updated": "2025-10-31T09:10:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    10,
                    29,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-30T17:51:46Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    51,
                    46,
                    3,
                    303,
                    0
                ],
                "title": "Unbiased Primordial Gravitational Wave Inference from the CMB with SMICA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unbiased Primordial Gravitational Wave Inference from the CMB with SMICA"
                },
                "summary": "The detection of primordial gravitational waves in Cosmic Microwave\nBackground B-mode polarization observations requires accurate and robust\nsubtraction of astrophysical contamination. We show, using a blind Spectral\nMatching Independent Component Analysis, that it is possible to infer unbiased\nestimates of the primordial B-mode signal from ground-based observations of a\nsmall patch of sky even for highly complex foreground contamination. This work,\noriginally performed in the context of configuration studies for a future\nCMB-S4 observatory, is highly relevant for the analysis of observations by the\ncurrent generation of CMB experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The detection of primordial gravitational waves in Cosmic Microwave\nBackground B-mode polarization observations requires accurate and robust\nsubtraction of astrophysical contamination. We show, using a blind Spectral\nMatching Independent Component Analysis, that it is possible to infer unbiased\nestimates of the primordial B-mode signal from ground-based observations of a\nsmall patch of sky even for highly complex foreground contamination. This work,\noriginally performed in the context of configuration studies for a future\nCMB-S4 observatory, is highly relevant for the analysis of observations by the\ncurrent generation of CMB experiments."
                },
                "authors": [
                    {
                        "name": "Alexander Steier"
                    },
                    {
                        "name": "Shamik Ghosh"
                    },
                    {
                        "name": "Jacques Delabrouille"
                    }
                ],
                "author_detail": {
                    "name": "Jacques Delabrouille"
                },
                "author": "Jacques Delabrouille",
                "arxiv_comment": "17 pages, 9 figures, submitted to JCAP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00920v2",
                "updated": "2025-10-31T09:01:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    1,
                    40,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-01T13:58:19Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    58,
                    19,
                    2,
                    274,
                    0
                ],
                "title": "Can Emulating Semantic Translation Help LLMs with Code Translation? A\n  Study Based on Pseudocode",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Emulating Semantic Translation Help LLMs with Code Translation? A\n  Study Based on Pseudocode"
                },
                "summary": "Large language models (LLMs) show great potential in code translation.\nHowever, accurate translation remains challenging when using the commonly\nadopted direct code-to-code translation approach, which converts a program into\nthe target programming language (PL) in a single step. Inspired by the success\nof incorporating intermediate steps to guide LLMs in resolving challenging\ntasks, we explore pseudocode-based code translation, which emulates the human\nsemantic translation by first interpreting the program's intent and logic into\npseudocode and then implementing it in the target PL. We find that\npseudocode-based translation helps translate programs that direct translation\nstruggles to handle. Nonetheless, the effectiveness, advantages, and\nlimitations of this approach remain underexplored. To bridge this gap, we\npresent an empirical study on pseudocode-based code translation, aiming to\ninvestigate its effectiveness in enhancing the direct translation approach,\nilluminate its effective usage, and identify limitations hindering its\npotential benefits. By comparing direct and pseudocode-based translation\napproaches on 9,690 translation tasks across six PLs with five popular LLMs, we\ndemonstrate that pseudocode-based translation can effectively complement direct\ntranslation, particularly when translating from flexible to rigid PLs or\ndealing with low-resource Rust. Based on these findings, we suggest adopting\nstrategies that combine the complementary strengths of both approaches to\nenhance code translation accuracy. We also reveal the advantages of\npseudocode-based translation in disentangling translations of complicated\nprograms and mitigating distractions from detailed implementations in original\nprograms, as well as its limitations due to incorrect, incomplete, or ambiguous\npseudocode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show great potential in code translation.\nHowever, accurate translation remains challenging when using the commonly\nadopted direct code-to-code translation approach, which converts a program into\nthe target programming language (PL) in a single step. Inspired by the success\nof incorporating intermediate steps to guide LLMs in resolving challenging\ntasks, we explore pseudocode-based code translation, which emulates the human\nsemantic translation by first interpreting the program's intent and logic into\npseudocode and then implementing it in the target PL. We find that\npseudocode-based translation helps translate programs that direct translation\nstruggles to handle. Nonetheless, the effectiveness, advantages, and\nlimitations of this approach remain underexplored. To bridge this gap, we\npresent an empirical study on pseudocode-based code translation, aiming to\ninvestigate its effectiveness in enhancing the direct translation approach,\nilluminate its effective usage, and identify limitations hindering its\npotential benefits. By comparing direct and pseudocode-based translation\napproaches on 9,690 translation tasks across six PLs with five popular LLMs, we\ndemonstrate that pseudocode-based translation can effectively complement direct\ntranslation, particularly when translating from flexible to rigid PLs or\ndealing with low-resource Rust. Based on these findings, we suggest adopting\nstrategies that combine the complementary strengths of both approaches to\nenhance code translation accuracy. We also reveal the advantages of\npseudocode-based translation in disentangling translations of complicated\nprograms and mitigating distractions from detailed implementations in original\nprograms, as well as its limitations due to incorrect, incomplete, or ambiguous\npseudocode."
                },
                "authors": [
                    {
                        "name": "Songqiang Chen"
                    },
                    {
                        "name": "Congying Xu"
                    },
                    {
                        "name": "Jingyi Chen"
                    },
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Jiarong Wu"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Shing-Chi Cheung"
                },
                "author": "Shing-Chi Cheung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05433v2",
                "updated": "2025-10-31T09:00:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    0,
                    22,
                    4,
                    304,
                    0
                ],
                "published": "2025-08-07T14:24:03Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    24,
                    3,
                    3,
                    219,
                    0
                ],
                "title": "Multimodal LLM-assisted Evolutionary Search for Programmatic Control\n  Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLM-assisted Evolutionary Search for Programmatic Control\n  Policies"
                },
                "summary": "Deep reinforcement learning has achieved impressive success in control tasks.\nHowever, its policies, represented as opaque neural networks, are often\ndifficult for humans to understand, verify, and debug, which undermines trust\nand hinders real-world deployment. This work addresses this challenge by\nintroducing a novel approach for programmatic control policy discovery, called\nMultimodal Large Language Model-assisted Evolutionary Search (MLES). MLES\nutilizes multimodal large language models as programmatic policy generators,\ncombining them with evolutionary search to automate policy generation. It\nintegrates visual feedback-driven behavior analysis within the policy\ngeneration process to identify failure patterns and guide targeted\nimprovements, thereby enhancing policy discovery efficiency and producing\nadaptable, human-aligned policies. Experimental results demonstrate that MLES\nachieves performance comparable to Proximal Policy Optimization (PPO) across\ntwo standard control tasks while providing transparent control logic and\ntraceable design processes. This approach also overcomes the limitations of\npredefined domain-specific languages, facilitates knowledge transfer and reuse,\nand is scalable across various tasks, showing promise as a new paradigm for\ndeveloping transparent and verifiable control policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep reinforcement learning has achieved impressive success in control tasks.\nHowever, its policies, represented as opaque neural networks, are often\ndifficult for humans to understand, verify, and debug, which undermines trust\nand hinders real-world deployment. This work addresses this challenge by\nintroducing a novel approach for programmatic control policy discovery, called\nMultimodal Large Language Model-assisted Evolutionary Search (MLES). MLES\nutilizes multimodal large language models as programmatic policy generators,\ncombining them with evolutionary search to automate policy generation. It\nintegrates visual feedback-driven behavior analysis within the policy\ngeneration process to identify failure patterns and guide targeted\nimprovements, thereby enhancing policy discovery efficiency and producing\nadaptable, human-aligned policies. Experimental results demonstrate that MLES\nachieves performance comparable to Proximal Policy Optimization (PPO) across\ntwo standard control tasks while providing transparent control logic and\ntraceable design processes. This approach also overcomes the limitations of\npredefined domain-specific languages, facilitates knowledge transfer and reuse,\nand is scalable across various tasks, showing promise as a new paradigm for\ndeveloping transparent and verifiable control policies."
                },
                "authors": [
                    {
                        "name": "Qinglong Hu"
                    },
                    {
                        "name": "Xialiang Tong"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Qingfu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qingfu Zhang"
                },
                "author": "Qingfu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27287v1",
                "updated": "2025-10-31T08:55:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    55,
                    13,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T08:55:13Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    55,
                    13,
                    4,
                    304,
                    0
                ],
                "title": "Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in\n  Enterprise Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in\n  Enterprise Environments"
                },
                "summary": "Enterprise systems are crucial for enhancing productivity and decision-making\namong employees and customers. Integrating LLM based systems into enterprise\nsystems enables intelligent automation, personalized experiences, and efficient\ninformation retrieval, driving operational efficiency and strategic growth.\nHowever, developing and evaluating such systems is challenging due to the\ninherent complexity of enterprise environments, where data is fragmented across\nmultiple sources and governed by sophisticated access controls. We present\nEnterpriseBench, a comprehensive benchmark that simulates enterprise settings,\nfeaturing 500 diverse tasks across software engineering, HR, finance, and\nadministrative domains. Our benchmark uniquely captures key enterprise\ncharacteristics including data source fragmentation, access control\nhierarchies, and cross-functional workflows. Additionally, we provide a novel\ndata generation pipeline that creates internally consistent enterprise tasks\nfrom organizational metadata. Experiments with state-of-the-art LLM agents\ndemonstrate that even the most capable models achieve only 41.8% task\ncompletion, highlighting significant opportunities for improvement in\nenterprise-focused AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise systems are crucial for enhancing productivity and decision-making\namong employees and customers. Integrating LLM based systems into enterprise\nsystems enables intelligent automation, personalized experiences, and efficient\ninformation retrieval, driving operational efficiency and strategic growth.\nHowever, developing and evaluating such systems is challenging due to the\ninherent complexity of enterprise environments, where data is fragmented across\nmultiple sources and governed by sophisticated access controls. We present\nEnterpriseBench, a comprehensive benchmark that simulates enterprise settings,\nfeaturing 500 diverse tasks across software engineering, HR, finance, and\nadministrative domains. Our benchmark uniquely captures key enterprise\ncharacteristics including data source fragmentation, access control\nhierarchies, and cross-functional workflows. Additionally, we provide a novel\ndata generation pipeline that creates internally consistent enterprise tasks\nfrom organizational metadata. Experiments with state-of-the-art LLM agents\ndemonstrate that even the most capable models achieve only 41.8% task\ncompletion, highlighting significant opportunities for improvement in\nenterprise-focused AI systems."
                },
                "authors": [
                    {
                        "name": "Harsh Vishwakarma"
                    },
                    {
                        "name": "Ankush Agarwal"
                    },
                    {
                        "name": "Ojas Patil"
                    },
                    {
                        "name": "Chaitanya Devaguptapu"
                    },
                    {
                        "name": "Mahesh Chandran"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Chandran"
                },
                "author": "Mahesh Chandran",
                "arxiv_comment": "Accepted at EMNLP 2025 Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27280v1",
                "updated": "2025-10-31T08:41:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    41,
                    13,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T08:41:13Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    41,
                    13,
                    4,
                    304,
                    0
                ],
                "title": "FOCUS: Efficient Keyframe Selection for Long Video Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FOCUS: Efficient Keyframe Selection for Long Video Understanding"
                },
                "summary": "Multimodal large language models (MLLMs) represent images and video frames as\nvisual tokens. Scaling from single images to hour-long videos, however,\ninflates the token budget far beyond practical limits. Popular pipelines\ntherefore either uniformly subsample or apply keyframe selection with\nretrieval-style scoring using smaller vision-language models. However, these\nkeyframe selection methods still rely on pre-filtering before selection to\nreduce the inference cost and can miss the most informative moments.\n  We propose FOCUS, Frame-Optimistic Confidence Upper-bound Selection, a\ntraining-free, model-agnostic keyframe selection module that selects\nquery-relevant frames under a strict token budget. FOCUS formulates keyframe\nselection as a combinatorial pure-exploration (CPE) problem in multi-armed\nbandits: it treats short temporal clips as arms, and uses empirical means and\nBernstein confidence radius to identify informative regions while preserving\nexploration of uncertain areas. The resulting two-stage\nexploration-exploitation procedure reduces from a sequential policy with\ntheoretical guarantees, first identifying high-value temporal regions, then\nselecting top-scoring frames within each region On two long-video\nquestion-answering benchmarks, FOCUS delivers substantial accuracy improvements\nwhile processing less than 2% of video frames. For videos longer than 20\nminutes, it achieves an 11.9% gain in accuracy on LongVideoBench, demonstrating\nits effectiveness as a keyframe selection method and providing a simple and\ngeneral solution for scalable long-video understanding with MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) represent images and video frames as\nvisual tokens. Scaling from single images to hour-long videos, however,\ninflates the token budget far beyond practical limits. Popular pipelines\ntherefore either uniformly subsample or apply keyframe selection with\nretrieval-style scoring using smaller vision-language models. However, these\nkeyframe selection methods still rely on pre-filtering before selection to\nreduce the inference cost and can miss the most informative moments.\n  We propose FOCUS, Frame-Optimistic Confidence Upper-bound Selection, a\ntraining-free, model-agnostic keyframe selection module that selects\nquery-relevant frames under a strict token budget. FOCUS formulates keyframe\nselection as a combinatorial pure-exploration (CPE) problem in multi-armed\nbandits: it treats short temporal clips as arms, and uses empirical means and\nBernstein confidence radius to identify informative regions while preserving\nexploration of uncertain areas. The resulting two-stage\nexploration-exploitation procedure reduces from a sequential policy with\ntheoretical guarantees, first identifying high-value temporal regions, then\nselecting top-scoring frames within each region On two long-video\nquestion-answering benchmarks, FOCUS delivers substantial accuracy improvements\nwhile processing less than 2% of video frames. For videos longer than 20\nminutes, it achieves an 11.9% gain in accuracy on LongVideoBench, demonstrating\nits effectiveness as a keyframe selection method and providing a simple and\ngeneral solution for scalable long-video understanding with MLLMs."
                },
                "authors": [
                    {
                        "name": "Zirui Zhu"
                    },
                    {
                        "name": "Hailun Xu"
                    },
                    {
                        "name": "Yang Luo"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Kanchan Sarkar"
                    },
                    {
                        "name": "Zhenheng Yang"
                    },
                    {
                        "name": "Yang You"
                    }
                ],
                "author_detail": {
                    "name": "Yang You"
                },
                "author": "Yang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27275v1",
                "updated": "2025-10-31T08:35:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    35,
                    42,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T08:35:42Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    35,
                    42,
                    4,
                    304,
                    0
                ],
                "title": "Prevalence of Security and Privacy Risk-Inducing Usage of AI-based\n  Conversational Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prevalence of Security and Privacy Risk-Inducing Usage of AI-based\n  Conversational Agents"
                },
                "summary": "Recent improvement gains in large language models (LLMs) have lead to\neveryday usage of AI-based Conversational Agents (CAs). At the same time, LLMs\nare vulnerable to an array of threats, including jailbreaks and, for example,\ncausing remote code execution when fed specific inputs. As a result, users may\nunintentionally introduce risks, for example, by uploading malicious files or\ndisclosing sensitive information. However, the extent to which such user\nbehaviors occur and thus potentially facilitate exploits remains largely\nunclear. To shed light on this issue, we surveyed a representative sample of\n3,270 UK adults in 2024 using Prolific. A third of these use CA services such\nas ChatGPT or Gemini at least once a week. Of these ``regular users'', up to a\nthird exhibited behaviors that may enable attacks, and a fourth have tried\njailbreaking (often out of understandable reasons such as curiosity, fun or\ninformation seeking). Half state that they sanitize data and most participants\nreport not sharing sensitive data. However, few share very sensitive data such\nas passwords. The majority are unaware that their data can be used to train\nmodels and that they can opt-out. Our findings suggest that current academic\nthreat models manifest in the wild, and mitigations or guidelines for the\nsecure usage of CAs should be developed. In areas critical to security and\nprivacy, CAs must be equipped with effective AI guardrails to prevent, for\nexample, revealing sensitive information to curious employees. Vendors need to\nincrease efforts to prevent the entry of sensitive data, and to create\ntransparency with regard to data usage policies and settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent improvement gains in large language models (LLMs) have lead to\neveryday usage of AI-based Conversational Agents (CAs). At the same time, LLMs\nare vulnerable to an array of threats, including jailbreaks and, for example,\ncausing remote code execution when fed specific inputs. As a result, users may\nunintentionally introduce risks, for example, by uploading malicious files or\ndisclosing sensitive information. However, the extent to which such user\nbehaviors occur and thus potentially facilitate exploits remains largely\nunclear. To shed light on this issue, we surveyed a representative sample of\n3,270 UK adults in 2024 using Prolific. A third of these use CA services such\nas ChatGPT or Gemini at least once a week. Of these ``regular users'', up to a\nthird exhibited behaviors that may enable attacks, and a fourth have tried\njailbreaking (often out of understandable reasons such as curiosity, fun or\ninformation seeking). Half state that they sanitize data and most participants\nreport not sharing sensitive data. However, few share very sensitive data such\nas passwords. The majority are unaware that their data can be used to train\nmodels and that they can opt-out. Our findings suggest that current academic\nthreat models manifest in the wild, and mitigations or guidelines for the\nsecure usage of CAs should be developed. In areas critical to security and\nprivacy, CAs must be equipped with effective AI guardrails to prevent, for\nexample, revealing sensitive information to curious employees. Vendors need to\nincrease efforts to prevent the entry of sensitive data, and to create\ntransparency with regard to data usage policies and settings."
                },
                "authors": [
                    {
                        "name": "Kathrin Grosse"
                    },
                    {
                        "name": "Nico Ebert"
                    }
                ],
                "author_detail": {
                    "name": "Nico Ebert"
                },
                "author": "Nico Ebert",
                "arxiv_comment": "10 pages, 3 figures, 5 tables, under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27272v1",
                "updated": "2025-10-31T08:28:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    28,
                    24,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T08:28:24Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    28,
                    24,
                    4,
                    304,
                    0
                ],
                "title": "Inferring trust in recommendation systems from brain, behavioural, and\n  physiological data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring trust in recommendation systems from brain, behavioural, and\n  physiological data"
                },
                "summary": "As people nowadays increasingly rely on artificial intelligence (AI) to\ncurate information and make decisions, assigning the appropriate amount of\ntrust in automated intelligent systems has become ever more important. However,\ncurrent measurements of trust in automation still largely rely on self-reports\nthat are subjective and disruptive to the user. Here, we take music\nrecommendation as a model to investigate the neural and cognitive processes\nunderlying trust in automation. We observed that system accuracy was directly\nrelated to users' trust and modulated the influence of recommendation cues on\nmusic preference. Modelling users' reward encoding process with a reinforcement\nlearning model further revealed that system accuracy, expected reward, and\nprediction error were related to oscillatory neural activity recorded via EEG\nand changes in pupil diameter. Our results provide a neurally grounded account\nof calibrating trust in automation and highlight the promises of a multimodal\napproach towards developing trustable AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As people nowadays increasingly rely on artificial intelligence (AI) to\ncurate information and make decisions, assigning the appropriate amount of\ntrust in automated intelligent systems has become ever more important. However,\ncurrent measurements of trust in automation still largely rely on self-reports\nthat are subjective and disruptive to the user. Here, we take music\nrecommendation as a model to investigate the neural and cognitive processes\nunderlying trust in automation. We observed that system accuracy was directly\nrelated to users' trust and modulated the influence of recommendation cues on\nmusic preference. Modelling users' reward encoding process with a reinforcement\nlearning model further revealed that system accuracy, expected reward, and\nprediction error were related to oscillatory neural activity recorded via EEG\nand changes in pupil diameter. Our results provide a neurally grounded account\nof calibrating trust in automation and highlight the promises of a multimodal\napproach towards developing trustable AI systems."
                },
                "authors": [
                    {
                        "name": "Vincent K. M. Cheung"
                    },
                    {
                        "name": "Pei-Cheng Shih"
                    },
                    {
                        "name": "Masato Hirano"
                    },
                    {
                        "name": "Masataka Goto"
                    },
                    {
                        "name": "Shinichi Furuya"
                    }
                ],
                "author_detail": {
                    "name": "Shinichi Furuya"
                },
                "author": "Shinichi Furuya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09045v2",
                "updated": "2025-10-31T08:20:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    20,
                    14,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-10T06:28:15Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    6,
                    28,
                    15,
                    4,
                    283,
                    0
                ],
                "title": "LLM Based Long Code Translation using Identifier Replacement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Based Long Code Translation using Identifier Replacement"
                },
                "summary": "In the domain of software development, LLMs have been utilized to automate\ntasks such as code translation, where source code from one programming language\nis translated to another while preserving its functionality. However, LLMs\noften struggle with long source codes that don't fit into the context window,\nwhich produces inaccurate translations. To address this, we propose a novel\nzero-shot code translation method that incorporates identifier replacement. By\nsubstituting user-given long identifiers with generalized placeholders during\ntranslation, our method allows the LLM to focus on the logical structure of the\ncode, by reducing token count and memory usage, which improves the efficiency\nand cost-effectiveness of long code translation. Our empirical results\ndemonstrate that our approach preserves syntactical and hierarchical\ninformation and produces translation results with reduced tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the domain of software development, LLMs have been utilized to automate\ntasks such as code translation, where source code from one programming language\nis translated to another while preserving its functionality. However, LLMs\noften struggle with long source codes that don't fit into the context window,\nwhich produces inaccurate translations. To address this, we propose a novel\nzero-shot code translation method that incorporates identifier replacement. By\nsubstituting user-given long identifiers with generalized placeholders during\ntranslation, our method allows the LLM to focus on the logical structure of the\ncode, by reducing token count and memory usage, which improves the efficiency\nand cost-effectiveness of long code translation. Our empirical results\ndemonstrate that our approach preserves syntactical and hierarchical\ninformation and produces translation results with reduced tokens."
                },
                "authors": [
                    {
                        "name": "Manojit Chakraborty"
                    },
                    {
                        "name": "Madhusudan Ghosh"
                    },
                    {
                        "name": "Rishabh Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Gupta"
                },
                "author": "Rishabh Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13178v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13178v5",
                "updated": "2025-10-31T08:18:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    18,
                    50,
                    4,
                    304,
                    0
                ],
                "published": "2024-12-17T18:55:58Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    55,
                    58,
                    1,
                    352,
                    0
                ],
                "title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents"
                },
                "summary": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to understand and plan complicated natural language\ninstructions. However, a foreseeable issue is that those embodied agents can\nalso flawlessly execute some hazardous tasks, potentially causing damages in\nthe real world. Existing benchmarks predominantly overlook critical safety\nrisks, focusing solely on planning performance, while a few evaluate LLMs'\nsafety awareness only on non-interactive image-text data. To address this gap,\nwe present SafeAgentBench -- the first comprehensive benchmark for safety-aware\ntask planning of embodied LLM agents in interactive simulation environments,\ncovering both explicit and implicit hazards. SafeAgentBench includes: (1) an\nexecutable, diverse, and high-quality dataset of 750 tasks, rigorously curated\nto cover 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal\nembodied environment with a low-level controller, supporting multi-agent\nexecution with 17 high-level actions for 9 state-of-the-art baselines; and (3)\nreliable evaluation methods from both execution and semantic perspectives.\nExperimental results show that, although agents based on different design\nframeworks exhibit substantial differences in task success rates, their overall\nsafety awareness remains weak. The most safety-conscious baseline achieves only\na 10% rejection rate for detailed hazardous tasks. Moreover, simply replacing\nthe LLM driving the agent does not lead to notable improvements in safety\nawareness. Dataset and codes are available in\nhttps://github.com/shengyin1224/SafeAgentBench and\nhttps://huggingface.co/datasets/safeagentbench/SafeAgentBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to understand and plan complicated natural language\ninstructions. However, a foreseeable issue is that those embodied agents can\nalso flawlessly execute some hazardous tasks, potentially causing damages in\nthe real world. Existing benchmarks predominantly overlook critical safety\nrisks, focusing solely on planning performance, while a few evaluate LLMs'\nsafety awareness only on non-interactive image-text data. To address this gap,\nwe present SafeAgentBench -- the first comprehensive benchmark for safety-aware\ntask planning of embodied LLM agents in interactive simulation environments,\ncovering both explicit and implicit hazards. SafeAgentBench includes: (1) an\nexecutable, diverse, and high-quality dataset of 750 tasks, rigorously curated\nto cover 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal\nembodied environment with a low-level controller, supporting multi-agent\nexecution with 17 high-level actions for 9 state-of-the-art baselines; and (3)\nreliable evaluation methods from both execution and semantic perspectives.\nExperimental results show that, although agents based on different design\nframeworks exhibit substantial differences in task success rates, their overall\nsafety awareness remains weak. The most safety-conscious baseline achieves only\na 10% rejection rate for detailed hazardous tasks. Moreover, simply replacing\nthe LLM driving the agent does not lead to notable improvements in safety\nawareness. Dataset and codes are available in\nhttps://github.com/shengyin1224/SafeAgentBench and\nhttps://huggingface.co/datasets/safeagentbench/SafeAgentBench."
                },
                "authors": [
                    {
                        "name": "Sheng Yin"
                    },
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Yuanzhuo Ding"
                    },
                    {
                        "name": "Menglan Chen"
                    },
                    {
                        "name": "Yutong Bi"
                    },
                    {
                        "name": "Yichen Xiong"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Zhen Xiang"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "arxiv_comment": "28 pages, 19 tables, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13178v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13178v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27267v1",
                "updated": "2025-10-31T08:07:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    7,
                    16,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T08:07:16Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    7,
                    16,
                    4,
                    304,
                    0
                ],
                "title": "MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities\n  of Large Language Models"
                },
                "summary": "As large language models (LLMs) enter the medical domain, most benchmarks\nevaluate them on question answering or descriptive reasoning, overlooking\nquantitative reasoning critical to clinical decision-making. Existing datasets\nlike MedCalc-Bench cover few calculation tasks and fail to reflect real-world\ncomputational scenarios.\n  We introduce MedCalc-Eval, the largest benchmark for assessing LLMs' medical\ncalculation abilities, comprising 700+ tasks across two types: equation-based\n(e.g., Cockcroft-Gault, BMI, BSA) and rule-based scoring systems (e.g., Apgar,\nGlasgow Coma Scale). These tasks span diverse specialties including internal\nmedicine, surgery, pediatrics, and cardiology, offering a broader and more\nchallenging evaluation setting.\n  To improve performance, we further develop MedCalc-Env, a reinforcement\nlearning environment built on the InternBootcamp framework, enabling multi-step\nclinical reasoning and planning. Fine-tuning a Qwen2.5-32B model within this\nenvironment achieves state-of-the-art results on MedCalc-Eval, with notable\ngains in numerical sensitivity, formula selection, and reasoning robustness.\nRemaining challenges include unit conversion, multi-condition logic, and\ncontextual understanding.\n  Code and datasets are available at\nhttps://github.com/maokangkun/MedCalc-Eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) enter the medical domain, most benchmarks\nevaluate them on question answering or descriptive reasoning, overlooking\nquantitative reasoning critical to clinical decision-making. Existing datasets\nlike MedCalc-Bench cover few calculation tasks and fail to reflect real-world\ncomputational scenarios.\n  We introduce MedCalc-Eval, the largest benchmark for assessing LLMs' medical\ncalculation abilities, comprising 700+ tasks across two types: equation-based\n(e.g., Cockcroft-Gault, BMI, BSA) and rule-based scoring systems (e.g., Apgar,\nGlasgow Coma Scale). These tasks span diverse specialties including internal\nmedicine, surgery, pediatrics, and cardiology, offering a broader and more\nchallenging evaluation setting.\n  To improve performance, we further develop MedCalc-Env, a reinforcement\nlearning environment built on the InternBootcamp framework, enabling multi-step\nclinical reasoning and planning. Fine-tuning a Qwen2.5-32B model within this\nenvironment achieves state-of-the-art results on MedCalc-Eval, with notable\ngains in numerical sensitivity, formula selection, and reasoning robustness.\nRemaining challenges include unit conversion, multi-condition logic, and\ncontextual understanding.\n  Code and datasets are available at\nhttps://github.com/maokangkun/MedCalc-Eval."
                },
                "authors": [
                    {
                        "name": "Kangkun Mao"
                    },
                    {
                        "name": "Jinru Ding"
                    },
                    {
                        "name": "Jiayuan Chen"
                    },
                    {
                        "name": "Mouxiao Bian"
                    },
                    {
                        "name": "Ruiyao Chen"
                    },
                    {
                        "name": "Xinwei Peng"
                    },
                    {
                        "name": "Sijie Ren"
                    },
                    {
                        "name": "Linyang Li"
                    },
                    {
                        "name": "Jie Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Xu"
                },
                "author": "Jie Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27265v1",
                "updated": "2025-10-31T08:05:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    5,
                    40,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T08:05:40Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    5,
                    40,
                    4,
                    304,
                    0
                ],
                "title": "T3: Test-Time Model Merging in VLMs for Zero-Shot Medical Imaging\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T3: Test-Time Model Merging in VLMs for Zero-Shot Medical Imaging\n  Analysis"
                },
                "summary": "In medical imaging, vision-language models face a critical duality:\npretrained networks offer broad robustness but lack subtle, modality-specific\ncharacteristics, while fine-tuned expert models achieve high in-distribution\naccuracy yet falter under modality shift. Existing model-merging techniques,\ndesigned for natural-image benchmarks, are simple and efficient but fail to\ndeliver consistent gains across diverse medical modalities; their static\ninterpolation limits reliability in varied clinical tasks. To address this, we\nintroduce Test-Time Task adaptive merging (T^3), a backpropagation-free\nframework that computes per-sample interpolation coefficients via the\nJensen-Shannon divergence between the two models' output distributions. T^3\ndynamically preserves local precision when models agree and defers to\ngeneralist robustness under drift. To overcome the inference costs of\nsample-wise merging, we further propose a batch-wise extension, T^3_B, that\ncomputes a merging coefficient across a batch of samples, dramatically reducing\ncomputational bottleneck. Recognizing the lack of a standardized\nmedical-merging benchmark, we present a rigorous cross-evaluation protocol\nspanning in-domain, base-to-novel, and corruptions across four modalities.\nEmpirically, T^3 sets new state-of-the-art in Top-1 accuracy and error\nreduction, outperforming strong baselines while maintaining efficiency, paving\nthe way for adaptive MVLM deployment in clinical settings. Our code is\navailable at https://github.com/Razaimam45/TCube.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In medical imaging, vision-language models face a critical duality:\npretrained networks offer broad robustness but lack subtle, modality-specific\ncharacteristics, while fine-tuned expert models achieve high in-distribution\naccuracy yet falter under modality shift. Existing model-merging techniques,\ndesigned for natural-image benchmarks, are simple and efficient but fail to\ndeliver consistent gains across diverse medical modalities; their static\ninterpolation limits reliability in varied clinical tasks. To address this, we\nintroduce Test-Time Task adaptive merging (T^3), a backpropagation-free\nframework that computes per-sample interpolation coefficients via the\nJensen-Shannon divergence between the two models' output distributions. T^3\ndynamically preserves local precision when models agree and defers to\ngeneralist robustness under drift. To overcome the inference costs of\nsample-wise merging, we further propose a batch-wise extension, T^3_B, that\ncomputes a merging coefficient across a batch of samples, dramatically reducing\ncomputational bottleneck. Recognizing the lack of a standardized\nmedical-merging benchmark, we present a rigorous cross-evaluation protocol\nspanning in-domain, base-to-novel, and corruptions across four modalities.\nEmpirically, T^3 sets new state-of-the-art in Top-1 accuracy and error\nreduction, outperforming strong baselines while maintaining efficiency, paving\nthe way for adaptive MVLM deployment in clinical settings. Our code is\navailable at https://github.com/Razaimam45/TCube."
                },
                "authors": [
                    {
                        "name": "Raza Imam"
                    },
                    {
                        "name": "Hu Wang"
                    },
                    {
                        "name": "Dwarikanath Mahapatra"
                    },
                    {
                        "name": "Mohammad Yaqub"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Yaqub"
                },
                "author": "Mohammad Yaqub",
                "arxiv_comment": "Main: 11 pages, Supplementary: 9 pages 10 tables, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27261v1",
                "updated": "2025-10-31T08:00:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    0,
                    32,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T08:00:32Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    0,
                    32,
                    4,
                    304,
                    0
                ],
                "title": "RegionRAG: Region-level Retrieval-Augumented Generation for\n  Visually-Rich Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RegionRAG: Region-level Retrieval-Augumented Generation for\n  Visually-Rich Documents"
                },
                "summary": "Multi-modal Retrieval-Augmented Generation (RAG) has become a critical method\nfor empowering LLMs by leveraging candidate visual documents. However, current\nmethods consider the entire document as the basic retrieval unit, introducing\nsubstantial irrelevant visual content in two ways: 1) Relevant documents often\ncontain large regions unrelated to the query, diluting the focus on salient\ninformation; 2) Retrieving multiple documents to increase recall further\nintroduces redundant and irrelevant documents. These redundant contexts\ndistract the model's attention and further degrade the performance. To address\nthis challenge, we propose \\modelname, a novel framework that shifts the\nretrieval paradigm from the document level to the region level. During\ntraining, we design a hybrid supervision strategy from both labeled data and\nunlabeled data to pinpoint relevant patches. During inference, we propose a\ndynamic pipeline that intelligently groups salient patches into complete\nsemantic regions. By delegating the task of identifying relevant regions to the\nretriever, \\modelname enables the generator to focus solely on concise visual\ncontent relevant to queries, improving both efficiency and accuracy.\nExperiments on six benchmarks demonstrate that RegionRAG achieves\nstate-of-the-art performance. Improves retrieval accuracy by 10.02\\% in R@1 on\naverage and increases question answering accuracy by 3.56\\% while using only\n71.42\\% visual tokens compared to prior methods. The code will be available at\nhttps://github.com/Aeryn666/RegionRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Retrieval-Augmented Generation (RAG) has become a critical method\nfor empowering LLMs by leveraging candidate visual documents. However, current\nmethods consider the entire document as the basic retrieval unit, introducing\nsubstantial irrelevant visual content in two ways: 1) Relevant documents often\ncontain large regions unrelated to the query, diluting the focus on salient\ninformation; 2) Retrieving multiple documents to increase recall further\nintroduces redundant and irrelevant documents. These redundant contexts\ndistract the model's attention and further degrade the performance. To address\nthis challenge, we propose \\modelname, a novel framework that shifts the\nretrieval paradigm from the document level to the region level. During\ntraining, we design a hybrid supervision strategy from both labeled data and\nunlabeled data to pinpoint relevant patches. During inference, we propose a\ndynamic pipeline that intelligently groups salient patches into complete\nsemantic regions. By delegating the task of identifying relevant regions to the\nretriever, \\modelname enables the generator to focus solely on concise visual\ncontent relevant to queries, improving both efficiency and accuracy.\nExperiments on six benchmarks demonstrate that RegionRAG achieves\nstate-of-the-art performance. Improves retrieval accuracy by 10.02\\% in R@1 on\naverage and increases question answering accuracy by 3.56\\% while using only\n71.42\\% visual tokens compared to prior methods. The code will be available at\nhttps://github.com/Aeryn666/RegionRAG."
                },
                "authors": [
                    {
                        "name": "Yinglu Li"
                    },
                    {
                        "name": "Zhiying Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Chuanbin Liu"
                    },
                    {
                        "name": "Hongtao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Hongtao Xie"
                },
                "author": "Hongtao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05079v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05079v3",
                "updated": "2025-10-31T07:58:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    58,
                    52,
                    4,
                    304,
                    0
                ],
                "published": "2025-06-05T14:27:40Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    27,
                    40,
                    3,
                    156,
                    0
                ],
                "title": "LLM-Guided Scenario-based GUI Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Guided Scenario-based GUI Testing"
                },
                "summary": "The assurance of mobile app GUIs has become increasingly important, as the\nGUI serves as the primary medium of interaction between users and apps.\nAlthough numerous automated GUI testing approaches have been developed with\ndiverse strategies, a substantial gap remains between these approaches and the\nunderlying app business logic. Most existing approaches focus on general\nexploration rather than the completion of specific testing scenarios, often\nmissing critical functionalities. Inspired by manual testing, which treats\nbusiness logic-driven scenarios as the fundamental unit of testing, this paper\nintroduces an approach that leverages large language models to comprehend GUI\nsemantics and contextual relevance to given scenarios. Building on this\ncapability, we propose ScenGen, an LLM-guided scenario-based GUI testing\nframework employing multi-agent collaboration to simulate and automate manual\ntesting phases.\n  Specifically, ScenGen integrates five agents: the Observer, Decider,\nExecutor, Supervisor, and Recorder. The Observer perceives the app GUI state by\nextracting and structuring GUI widgets and layouts, interpreting semantic\ninformation. This is passed to the Decider, which makes scenario-driven\ndecisions with LLM guidance to identify target widgets and determine actions\ntoward fulfilling specific goals. The Executor performs these operations, while\nthe Supervisor verifies alignment with intended scenario completion, ensuring\ntraceability and consistency. Finally, the Recorder logs GUI operations into\ncontext memory as a knowledge base for subsequent decision-making and monitors\nruntime bugs. Comprehensive evaluations demonstrate that ScenGen effectively\ngenerates scenario-based GUI tests guided by LLM collaboration, achieving\nhigher relevance to business logic and improving the completeness of automated\nGUI testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The assurance of mobile app GUIs has become increasingly important, as the\nGUI serves as the primary medium of interaction between users and apps.\nAlthough numerous automated GUI testing approaches have been developed with\ndiverse strategies, a substantial gap remains between these approaches and the\nunderlying app business logic. Most existing approaches focus on general\nexploration rather than the completion of specific testing scenarios, often\nmissing critical functionalities. Inspired by manual testing, which treats\nbusiness logic-driven scenarios as the fundamental unit of testing, this paper\nintroduces an approach that leverages large language models to comprehend GUI\nsemantics and contextual relevance to given scenarios. Building on this\ncapability, we propose ScenGen, an LLM-guided scenario-based GUI testing\nframework employing multi-agent collaboration to simulate and automate manual\ntesting phases.\n  Specifically, ScenGen integrates five agents: the Observer, Decider,\nExecutor, Supervisor, and Recorder. The Observer perceives the app GUI state by\nextracting and structuring GUI widgets and layouts, interpreting semantic\ninformation. This is passed to the Decider, which makes scenario-driven\ndecisions with LLM guidance to identify target widgets and determine actions\ntoward fulfilling specific goals. The Executor performs these operations, while\nthe Supervisor verifies alignment with intended scenario completion, ensuring\ntraceability and consistency. Finally, the Recorder logs GUI operations into\ncontext memory as a knowledge base for subsequent decision-making and monitors\nruntime bugs. Comprehensive evaluations demonstrate that ScenGen effectively\ngenerates scenario-based GUI tests guided by LLM collaboration, achieving\nhigher relevance to business logic and improving the completeness of automated\nGUI testing."
                },
                "authors": [
                    {
                        "name": "Shengcheng Yu"
                    },
                    {
                        "name": "Yuchen Ling"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Quan Zhou"
                    },
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Chunyang Chen"
                    },
                    {
                        "name": "Shaomin Zhu"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05079v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05079v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12276v2",
                "updated": "2025-10-31T07:55:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    55,
                    10,
                    4,
                    304,
                    0
                ],
                "published": "2025-07-16T14:24:31Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    14,
                    24,
                    31,
                    2,
                    197,
                    0
                ],
                "title": "Modeling US Climate Policy Uncertainty: From Causal Identification to\n  Probabilistic Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling US Climate Policy Uncertainty: From Causal Identification to\n  Probabilistic Forecasting"
                },
                "summary": "Accurately forecasting Climate Policy Uncertainty (CPU) is critical for\ndesigning effective climate strategies that balance economic growth with\nenvironmental objectives. Elevated CPU levels deter investment in green\ntechnologies, delay regulatory implementation, and amplify public resistance to\npolicy reforms, particularly during economic stress. Despite the growing\nliterature highlighting the economic relevance of CPU, the mechanisms through\nwhich macroeconomic and financial conditions influence its fluctuations remain\ninsufficiently explored. This study addresses this gap by integrating four\ncomplementary causal inference techniques to identify statistically and\neconomically significant determinants of the United States (US) CPU index.\nImpulse response analysis confirms their dynamic effects on CPU, highlighting\nthe role of housing market activity, credit conditions, and financial market\nsentiment in shaping CPU fluctuations. The identified predictors, along with\nsentiment based Google Trends indicators, are incorporated into a Bayesian\nStructural Time Series (BSTS) framework for probabilistic forecasting. The\ninclusion of Google Trends data captures behavioral and attention based\ndynamics, leading to notable improvements in forecast accuracy. Numerical\nexperiments demonstrate the superior performance of BSTS over state of the art\nclassical and modern architectures for medium and long term forecasts, which\nare most relevant for climate policy implementation. The feature importance\nplot provides evidence that the spike-and-slab prior mechanism provides\ninterpretable variable selection. The credible intervals quantify forecast\nuncertainty, thereby enhancing the model's transparency and policy relevance by\nenabling strategic decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately forecasting Climate Policy Uncertainty (CPU) is critical for\ndesigning effective climate strategies that balance economic growth with\nenvironmental objectives. Elevated CPU levels deter investment in green\ntechnologies, delay regulatory implementation, and amplify public resistance to\npolicy reforms, particularly during economic stress. Despite the growing\nliterature highlighting the economic relevance of CPU, the mechanisms through\nwhich macroeconomic and financial conditions influence its fluctuations remain\ninsufficiently explored. This study addresses this gap by integrating four\ncomplementary causal inference techniques to identify statistically and\neconomically significant determinants of the United States (US) CPU index.\nImpulse response analysis confirms their dynamic effects on CPU, highlighting\nthe role of housing market activity, credit conditions, and financial market\nsentiment in shaping CPU fluctuations. The identified predictors, along with\nsentiment based Google Trends indicators, are incorporated into a Bayesian\nStructural Time Series (BSTS) framework for probabilistic forecasting. The\ninclusion of Google Trends data captures behavioral and attention based\ndynamics, leading to notable improvements in forecast accuracy. Numerical\nexperiments demonstrate the superior performance of BSTS over state of the art\nclassical and modern architectures for medium and long term forecasts, which\nare most relevant for climate policy implementation. The feature importance\nplot provides evidence that the spike-and-slab prior mechanism provides\ninterpretable variable selection. The credible intervals quantify forecast\nuncertainty, thereby enhancing the model's transparency and policy relevance by\nenabling strategic decision making."
                },
                "authors": [
                    {
                        "name": "Donia Besher"
                    },
                    {
                        "name": "Anirban Sengupta"
                    },
                    {
                        "name": "Tanujit Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanujit Chakraborty"
                },
                "author": "Tanujit Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.27688v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27688v1",
                "updated": "2025-10-31T17:58:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    58,
                    11,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:58:11Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    58,
                    11,
                    4,
                    304,
                    0
                ],
                "title": "Continuous Autoregressive Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Autoregressive Language Models"
                },
                "summary": "The efficiency of large language models (LLMs) is fundamentally limited by\ntheir sequential, token-by-token generation process. We argue that overcoming\nthis bottleneck requires a new design axis for LLM scaling: increasing the\nsemantic bandwidth of each generative step. To this end, we introduce\nContinuous Autoregressive Language Models (CALM), a paradigm shift from\ndiscrete next-token prediction to continuous next-vector prediction. CALM uses\na high-fidelity autoencoder to compress a chunk of K tokens into a single\ncontinuous vector, from which the original tokens can be reconstructed with\nover 99.9\\% accuracy. This allows us to model language as a sequence of\ncontinuous vectors instead of discrete tokens, which reduces the number of\ngenerative steps by a factor of K. The paradigm shift necessitates a new\nmodeling toolkit; therefore, we develop a comprehensive likelihood-free\nframework that enables robust training, evaluation, and controllable sampling\nin the continuous domain. Experiments show that CALM significantly improves the\nperformance-compute trade-off, achieving the performance of strong discrete\nbaselines at a significantly lower computational cost. More importantly, these\nfindings establish next-vector prediction as a powerful and scalable pathway\ntowards ultra-efficient language models. Code:\nhttps://github.com/shaochenze/calm. Project:\nhttps://shaochenze.github.io/blog/2025/CALM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large language models (LLMs) is fundamentally limited by\ntheir sequential, token-by-token generation process. We argue that overcoming\nthis bottleneck requires a new design axis for LLM scaling: increasing the\nsemantic bandwidth of each generative step. To this end, we introduce\nContinuous Autoregressive Language Models (CALM), a paradigm shift from\ndiscrete next-token prediction to continuous next-vector prediction. CALM uses\na high-fidelity autoencoder to compress a chunk of K tokens into a single\ncontinuous vector, from which the original tokens can be reconstructed with\nover 99.9\\% accuracy. This allows us to model language as a sequence of\ncontinuous vectors instead of discrete tokens, which reduces the number of\ngenerative steps by a factor of K. The paradigm shift necessitates a new\nmodeling toolkit; therefore, we develop a comprehensive likelihood-free\nframework that enables robust training, evaluation, and controllable sampling\nin the continuous domain. Experiments show that CALM significantly improves the\nperformance-compute trade-off, achieving the performance of strong discrete\nbaselines at a significantly lower computational cost. More importantly, these\nfindings establish next-vector prediction as a powerful and scalable pathway\ntowards ultra-efficient language models. Code:\nhttps://github.com/shaochenze/calm. Project:\nhttps://shaochenze.github.io/blog/2025/CALM."
                },
                "authors": [
                    {
                        "name": "Chenze Shao"
                    },
                    {
                        "name": "Darren Li"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27688v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27688v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27681v1",
                "updated": "2025-10-31T17:49:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    49,
                    50,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:49:50Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    49,
                    50,
                    4,
                    304,
                    0
                ],
                "title": "Personalized AI Scaffolds Synergistic Multi-Turn Collaboration in\n  Creative Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized AI Scaffolds Synergistic Multi-Turn Collaboration in\n  Creative Work"
                },
                "summary": "As AI becomes more deeply embedded in knowledge work, building assistants\nthat support human creativity and expertise becomes more important. Yet\nachieving synergy in human-AI collaboration is not easy. Providing AI with\ndetailed information about a user's demographics, psychological attributes,\ndivergent thinking, and domain expertise may improve performance by scaffolding\nmore effective multi-turn interactions. We implemented a personalized LLM-based\nassistant, informed by users' psychometric profiles and an AI-guided interview\nabout their work style, to help users complete a marketing task for a fictional\nstartup. We randomized 331 participants to work with AI that was either generic\n(n = 116), partially personalized (n = 114), or fully personalized (n=101).\nParticipants working with personalized AI produce marketing campaigns of\nsignificantly higher quality and creativity, beyond what AI alone could have\nproduced. Compared to generic AI, personalized AI leads to higher self-reported\nlevels of assistance and feedback, while also increasing participant trust and\nconfidence. Causal mediation analysis shows that personalization improves\nperformance indirectly by enhancing collective memory, attention, and reasoning\nin the human-AI interaction. These findings provide a theory-driven framework\nin which personalization functions as external scaffolding that builds common\nground and shared partner models, reducing uncertainty and enhancing joint\ncognition. This informs the design of future AI assistants that maximize\nsynergy and support human creative potential while limiting negative\nhomogenization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI becomes more deeply embedded in knowledge work, building assistants\nthat support human creativity and expertise becomes more important. Yet\nachieving synergy in human-AI collaboration is not easy. Providing AI with\ndetailed information about a user's demographics, psychological attributes,\ndivergent thinking, and domain expertise may improve performance by scaffolding\nmore effective multi-turn interactions. We implemented a personalized LLM-based\nassistant, informed by users' psychometric profiles and an AI-guided interview\nabout their work style, to help users complete a marketing task for a fictional\nstartup. We randomized 331 participants to work with AI that was either generic\n(n = 116), partially personalized (n = 114), or fully personalized (n=101).\nParticipants working with personalized AI produce marketing campaigns of\nsignificantly higher quality and creativity, beyond what AI alone could have\nproduced. Compared to generic AI, personalized AI leads to higher self-reported\nlevels of assistance and feedback, while also increasing participant trust and\nconfidence. Causal mediation analysis shows that personalization improves\nperformance indirectly by enhancing collective memory, attention, and reasoning\nin the human-AI interaction. These findings provide a theory-driven framework\nin which personalization functions as external scaffolding that builds common\nground and shared partner models, reducing uncertainty and enhancing joint\ncognition. This informs the design of future AI assistants that maximize\nsynergy and support human creative potential while limiting negative\nhomogenization."
                },
                "authors": [
                    {
                        "name": "Sean Kelley"
                    },
                    {
                        "name": "David De Cremer"
                    },
                    {
                        "name": "Christoph Riedl"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Riedl"
                },
                "author": "Christoph Riedl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27680v1",
                "updated": "2025-10-31T17:49:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    49,
                    1,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:49:01Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    49,
                    1,
                    4,
                    304,
                    0
                ],
                "title": "PETAR: Localized Findings Generation with Mask-Aware Vision-Language\n  Modeling for PET Automated Reporting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PETAR: Localized Findings Generation with Mask-Aware Vision-Language\n  Modeling for PET Automated Reporting"
                },
                "summary": "Recent advances in vision-language models (VLMs) have enabled impressive\nmultimodal reasoning, yet most medical applications remain limited to 2D\nimaging. In this work, we extend VLMs to 3D positron emission tomography and\ncomputed tomography (PET/CT), a domain characterized by large volumetric data,\nsmall and dispersed lesions, and lengthy radiology reports. We introduce a\nlarge-scale dataset comprising over 11,000 lesion-level descriptions paired\nwith 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid\nrule-based and large language model (LLM) pipeline. Building upon this dataset,\nwe propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET,\nCT, and lesion contours for spatially grounded report generation. PETAR bridges\nglobal contextual reasoning with fine-grained lesion awareness, producing\nclinically coherent and localized findings. Comprehensive automated and human\nevaluations demonstrate that PETAR substantially improves PET/CT report\ngeneration quality, advancing 3D medical vision-language understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in vision-language models (VLMs) have enabled impressive\nmultimodal reasoning, yet most medical applications remain limited to 2D\nimaging. In this work, we extend VLMs to 3D positron emission tomography and\ncomputed tomography (PET/CT), a domain characterized by large volumetric data,\nsmall and dispersed lesions, and lengthy radiology reports. We introduce a\nlarge-scale dataset comprising over 11,000 lesion-level descriptions paired\nwith 3D segmentations from more than 5,000 PET/CT exams, extracted via a hybrid\nrule-based and large language model (LLM) pipeline. Building upon this dataset,\nwe propose PETAR-4B, a 3D mask-aware vision-language model that integrates PET,\nCT, and lesion contours for spatially grounded report generation. PETAR bridges\nglobal contextual reasoning with fine-grained lesion awareness, producing\nclinically coherent and localized findings. Comprehensive automated and human\nevaluations demonstrate that PETAR substantially improves PET/CT report\ngeneration quality, advancing 3D medical vision-language understanding."
                },
                "authors": [
                    {
                        "name": "Danyal Maqbool"
                    },
                    {
                        "name": "Changhee Lee"
                    },
                    {
                        "name": "Zachary Huemann"
                    },
                    {
                        "name": "Samuel D. Church"
                    },
                    {
                        "name": "Matthew E. Larson"
                    },
                    {
                        "name": "Scott B. Perlman"
                    },
                    {
                        "name": "Tomas A. Romero"
                    },
                    {
                        "name": "Joshua D. Warner"
                    },
                    {
                        "name": "Meghan Lubner"
                    },
                    {
                        "name": "Xin Tie"
                    },
                    {
                        "name": "Jameson Merkow"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Steve Y. Cho"
                    },
                    {
                        "name": "Tyler J. Bradshaw"
                    }
                ],
                "author_detail": {
                    "name": "Tyler J. Bradshaw"
                },
                "author": "Tyler J. Bradshaw",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27675v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27675v1",
                "updated": "2025-10-31T17:41:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    41,
                    58,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:41:58Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    41,
                    58,
                    4,
                    304,
                    0
                ],
                "title": "On Selecting Few-Shot Examples for LLM-based Code Vulnerability\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Selecting Few-Shot Examples for LLM-based Code Vulnerability\n  Detection"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities for\nmany coding tasks, including summarization, translation, completion, and code\ngeneration. However, detecting code vulnerabilities remains a challenging task\nfor LLMs. An effective way to improve LLM performance is in-context learning\n(ICL) - providing few-shot examples similar to the query, along with correct\nanswers, can improve an LLM's ability to generate correct solutions. However,\nchoosing the few-shot examples appropriately is crucial to improving model\nperformance. In this paper, we explore two criteria for choosing few-shot\nexamples for ICL used in the code vulnerability detection task. The first\ncriterion considers if the LLM (consistently) makes a mistake or not on a\nsample with the intuition that LLM performance on a sample is informative about\nits usefulness as a few-shot example. The other criterion considers similarity\nof the examples with the program under query and chooses few-shot examples\nbased on the $k$-nearest neighbors to the given sample. We perform evaluations\nto determine the benefits of these criteria individually as well as under\nvarious combinations, using open-source models on multiple datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities for\nmany coding tasks, including summarization, translation, completion, and code\ngeneration. However, detecting code vulnerabilities remains a challenging task\nfor LLMs. An effective way to improve LLM performance is in-context learning\n(ICL) - providing few-shot examples similar to the query, along with correct\nanswers, can improve an LLM's ability to generate correct solutions. However,\nchoosing the few-shot examples appropriately is crucial to improving model\nperformance. In this paper, we explore two criteria for choosing few-shot\nexamples for ICL used in the code vulnerability detection task. The first\ncriterion considers if the LLM (consistently) makes a mistake or not on a\nsample with the intuition that LLM performance on a sample is informative about\nits usefulness as a few-shot example. The other criterion considers similarity\nof the examples with the program under query and chooses few-shot examples\nbased on the $k$-nearest neighbors to the given sample. We perform evaluations\nto determine the benefits of these criteria individually as well as under\nvarious combinations, using open-source models on multiple datasets."
                },
                "authors": [
                    {
                        "name": "Md Abdul Hannan"
                    },
                    {
                        "name": "Ronghao Ni"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Limin Jia"
                    },
                    {
                        "name": "Ravi Mangal"
                    },
                    {
                        "name": "Corina S. Pasareanu"
                    }
                ],
                "author_detail": {
                    "name": "Corina S. Pasareanu"
                },
                "author": "Corina S. Pasareanu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27675v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27675v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27672v1",
                "updated": "2025-10-31T17:37:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    37,
                    34,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:37:34Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    37,
                    34,
                    4,
                    304,
                    0
                ],
                "title": "Culture Cartography: Mapping the Landscape of Cultural Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culture Cartography: Mapping the Landscape of Cultural Knowledge"
                },
                "summary": "To serve global users safely and productively, LLMs need culture-specific\nknowledge that might not be learned during pre-training. How do we find such\nknowledge that is (1) salient to in-group users, but (2) unknown to LLMs? The\nmost common solutions are single-initiative: either researchers define\nchallenging questions that users passively answer (traditional annotation), or\nusers actively produce data that researchers structure as benchmarks (knowledge\nextraction). The process would benefit from mixed-initiative collaboration,\nwhere users guide the process to meaningfully reflect their cultures, and LLMs\nsteer the process towards more challenging questions that meet the researcher's\ngoals. We propose a mixed-initiative methodology called CultureCartography.\nHere, an LLM initializes annotation with questions for which it has\nlow-confidence answers, making explicit both its prior knowledge and the gaps\ntherein. This allows a human respondent to fill these gaps and steer the model\ntowards salient topics through direct edits. We implement this methodology as a\ntool called CultureExplorer. Compared to a baseline where humans answer\nLLM-proposed questions, we find that CultureExplorer more effectively produces\nknowledge that leading models like DeepSeek R1 and GPT-4o are missing, even\nwith web search. Fine-tuning on this data boosts the accuracy of Llama-3.1-8B\nby up to 19.2% on related culture benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To serve global users safely and productively, LLMs need culture-specific\nknowledge that might not be learned during pre-training. How do we find such\nknowledge that is (1) salient to in-group users, but (2) unknown to LLMs? The\nmost common solutions are single-initiative: either researchers define\nchallenging questions that users passively answer (traditional annotation), or\nusers actively produce data that researchers structure as benchmarks (knowledge\nextraction). The process would benefit from mixed-initiative collaboration,\nwhere users guide the process to meaningfully reflect their cultures, and LLMs\nsteer the process towards more challenging questions that meet the researcher's\ngoals. We propose a mixed-initiative methodology called CultureCartography.\nHere, an LLM initializes annotation with questions for which it has\nlow-confidence answers, making explicit both its prior knowledge and the gaps\ntherein. This allows a human respondent to fill these gaps and steer the model\ntowards salient topics through direct edits. We implement this methodology as a\ntool called CultureExplorer. Compared to a baseline where humans answer\nLLM-proposed questions, we find that CultureExplorer more effectively produces\nknowledge that leading models like DeepSeek R1 and GPT-4o are missing, even\nwith web search. Fine-tuning on this data boosts the accuracy of Llama-3.1-8B\nby up to 19.2% on related culture benchmarks."
                },
                "authors": [
                    {
                        "name": "Caleb Ziems"
                    },
                    {
                        "name": "William Held"
                    },
                    {
                        "name": "Jane Yu"
                    },
                    {
                        "name": "Amir Goldberg"
                    },
                    {
                        "name": "David Grusky"
                    },
                    {
                        "name": "Diyi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Diyi Yang"
                },
                "author": "Diyi Yang",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26697v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26697v2",
                "updated": "2025-10-31T17:36:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    36,
                    35,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-30T17:01:43Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    17,
                    1,
                    43,
                    3,
                    303,
                    0
                ],
                "title": "The End of Manual Decoding: Towards Truly End-to-End Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The End of Manual Decoding: Towards Truly End-to-End Language Models"
                },
                "summary": "The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a\nnon-differentiable decoding process that requires laborious, hand-tuning of\nhyperparameters like temperature and top-p. This paper introduces AutoDeco, a\nnovel architecture that enables truly \"end-to-end\" generation by learning to\ncontrol its own decoding strategy. We augment the standard transformer with\nlightweight heads that, at each step, dynamically predict context-specific\ntemperature and top-p values alongside the next-token logits. This approach\ntransforms decoding into a parametric, token-level process, allowing the model\nto self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that\nAutoDeco not only significantly outperforms default decoding strategies but\nalso achieves performance comparable to an oracle-tuned baseline derived from\n\"hacking the test set\"-a practical upper bound for any static method.\nCrucially, we uncover an emergent capability for instruction-based decoding\ncontrol: the model learns to interpret natural language commands (e.g.,\n\"generate with low randomness\") and adjusts its predicted temperature and top-p\non a token-by-token basis, opening a new paradigm for steerable and interactive\nLLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"end-to-end\" label for LLMs is a misnomer. In practice, they depend on a\nnon-differentiable decoding process that requires laborious, hand-tuning of\nhyperparameters like temperature and top-p. This paper introduces AutoDeco, a\nnovel architecture that enables truly \"end-to-end\" generation by learning to\ncontrol its own decoding strategy. We augment the standard transformer with\nlightweight heads that, at each step, dynamically predict context-specific\ntemperature and top-p values alongside the next-token logits. This approach\ntransforms decoding into a parametric, token-level process, allowing the model\nto self-regulate its sampling strategy within a single forward pass.\n  Through extensive experiments on eight benchmarks, we demonstrate that\nAutoDeco not only significantly outperforms default decoding strategies but\nalso achieves performance comparable to an oracle-tuned baseline derived from\n\"hacking the test set\"-a practical upper bound for any static method.\nCrucially, we uncover an emergent capability for instruction-based decoding\ncontrol: the model learns to interpret natural language commands (e.g.,\n\"generate with low randomness\") and adjusts its predicted temperature and top-p\non a token-by-token basis, opening a new paradigm for steerable and interactive\nLLM decoding."
                },
                "authors": [
                    {
                        "name": "Zhichao Wang"
                    },
                    {
                        "name": "Dongyang Ma"
                    },
                    {
                        "name": "Xinting Huang"
                    },
                    {
                        "name": "Deng Cai"
                    },
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Xiaoying Tang"
                    },
                    {
                        "name": "Yan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Wang"
                },
                "author": "Yan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26697v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26697v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.25935v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.25935v2",
                "updated": "2025-10-31T17:31:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    31,
                    15,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-29T20:13:46Z",
                "published_parsed": [
                    2025,
                    10,
                    29,
                    20,
                    13,
                    46,
                    2,
                    302,
                    0
                ],
                "title": "A Process Mining-Based System For The Analysis and Prediction of\n  Software Development Workflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Process Mining-Based System For The Analysis and Prediction of\n  Software Development Workflows"
                },
                "summary": "CodeSight is an end-to-end system designed to anticipate deadline compliance\nin software development workflows. It captures development and deployment data\ndirectly from GitHub, transforming it into process mining logs for detailed\nanalysis. From these logs, the system generates metrics and dashboards that\nprovide actionable insights into PR activity patterns and workflow efficiency.\nBuilding on this structured representation, CodeSight employs an LSTM model\nthat predicts remaining PR resolution times based on sequential activity traces\nand static features, enabling early identification of potential deadline\nbreaches. In tests, the system demonstrates high precision and F1 scores in\npredicting deadline compliance, illustrating the value of integrating process\nmining with machine learning for proactive software project management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeSight is an end-to-end system designed to anticipate deadline compliance\nin software development workflows. It captures development and deployment data\ndirectly from GitHub, transforming it into process mining logs for detailed\nanalysis. From these logs, the system generates metrics and dashboards that\nprovide actionable insights into PR activity patterns and workflow efficiency.\nBuilding on this structured representation, CodeSight employs an LSTM model\nthat predicts remaining PR resolution times based on sequential activity traces\nand static features, enabling early identification of potential deadline\nbreaches. In tests, the system demonstrates high precision and F1 scores in\npredicting deadline compliance, illustrating the value of integrating process\nmining with machine learning for proactive software project management."
                },
                "authors": [
                    {
                        "name": "Anta Dorado"
                    },
                    {
                        "name": "Ivn Folgueira"
                    },
                    {
                        "name": "Sofa Martn"
                    },
                    {
                        "name": "Gonzalo Martn"
                    },
                    {
                        "name": "lvaro Porto"
                    },
                    {
                        "name": "Alejandro Ramos"
                    },
                    {
                        "name": "John Wallace"
                    }
                ],
                "author_detail": {
                    "name": "John Wallace"
                },
                "author": "John Wallace",
                "arxiv_comment": "16 pages, 7 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.25935v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.25935v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24038v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24038v2",
                "updated": "2025-10-31T17:30:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    30,
                    38,
                    4,
                    304,
                    0
                ],
                "published": "2025-05-29T22:19:01Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    22,
                    19,
                    1,
                    3,
                    149,
                    0
                ],
                "title": "Conformal Object Detection by Sequential Risk Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal Object Detection by Sequential Risk Control"
                },
                "summary": "Recent advances in object detectors have led to their adoption for industrial\nuses. However, their deployment in safety-critical applications is hindered by\nthe inherent lack of reliability of neural networks and the complex structure\nof object detection models. To address these challenges, we turn to Conformal\nPrediction, a post-hoc predictive uncertainty quantification procedure with\nstatistical guarantees that are valid for any dataset size, without requiring\nprior knowledge on the model or data distribution. Our contribution is\nmanifold. First, we formally define the problem of Conformal Object Detection\n(COD). We introduce a novel method, Sequential Conformal Risk Control (SeqCRC),\nthat extends the statistical guarantees of Conformal Risk Control to two\nsequential tasks with two parameters, as required in the COD setting. Then, we\npresent old and new loss functions and prediction sets suited to applying\nSeqCRC to different cases and certification requirements. Finally, we present a\nconformal toolkit for replication and further exploration of our method. Using\nthis toolkit, we perform extensive experiments that validate our approach and\nemphasize trade-offs and other practical consequences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in object detectors have led to their adoption for industrial\nuses. However, their deployment in safety-critical applications is hindered by\nthe inherent lack of reliability of neural networks and the complex structure\nof object detection models. To address these challenges, we turn to Conformal\nPrediction, a post-hoc predictive uncertainty quantification procedure with\nstatistical guarantees that are valid for any dataset size, without requiring\nprior knowledge on the model or data distribution. Our contribution is\nmanifold. First, we formally define the problem of Conformal Object Detection\n(COD). We introduce a novel method, Sequential Conformal Risk Control (SeqCRC),\nthat extends the statistical guarantees of Conformal Risk Control to two\nsequential tasks with two parameters, as required in the COD setting. Then, we\npresent old and new loss functions and prediction sets suited to applying\nSeqCRC to different cases and certification requirements. Finally, we present a\nconformal toolkit for replication and further exploration of our method. Using\nthis toolkit, we perform extensive experiments that validate our approach and\nemphasize trade-offs and other practical consequences."
                },
                "authors": [
                    {
                        "name": "Lo andol"
                    },
                    {
                        "name": "Luca Mossina"
                    },
                    {
                        "name": "Adrien Mazoyer"
                    },
                    {
                        "name": "Sbastien Gerchinovitz"
                    }
                ],
                "author_detail": {
                    "name": "Sbastien Gerchinovitz"
                },
                "author": "Sbastien Gerchinovitz",
                "arxiv_comment": "29 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24038v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16886v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16886v4",
                "updated": "2025-10-31T17:29:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    29,
                    51,
                    4,
                    304,
                    0
                ],
                "published": "2024-08-29T20:19:10Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    20,
                    19,
                    10,
                    3,
                    242,
                    0
                ],
                "title": "LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LV-UNet: A Lightweight and Vanilla Model for Medical Image Segmentation"
                },
                "summary": "While large models have achieved significant progress in computer vision,\nchallenges such as optimization complexity, the intricacy of transformer\narchitectures, computational constraints, and practical application demands\nhighlight the importance of simpler model designs in medical image\nsegmentation. This need is particularly pronounced in mobile medical devices,\nwhich require lightweight, deployable models with real-time performance.\nHowever, existing lightweight models often suffer from poor robustness across\ndatasets, limiting their widespread adoption. To address these challenges, this\npaper introduces LV-UNet, a lightweight and vanilla model that leverages\npre-trained MobileNetv3-Large backbones and incorporates fusible modules.\nLV-UNet employs an enhanced deep training strategy and switches to a deployment\nmode during inference by re-parametrization, significantly reducing parameter\ncount and computational overhead. Experimental results on ISIC 2016, BUSI,\nCVC-ClinicDB, CVC-ColonDB, and Kvair-SEG datasets demonstrate a better\ntrade-off between performance and the computational load. The code will be\nreleased at https://github.com/juntaoJianggavin/LV-UNet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large models have achieved significant progress in computer vision,\nchallenges such as optimization complexity, the intricacy of transformer\narchitectures, computational constraints, and practical application demands\nhighlight the importance of simpler model designs in medical image\nsegmentation. This need is particularly pronounced in mobile medical devices,\nwhich require lightweight, deployable models with real-time performance.\nHowever, existing lightweight models often suffer from poor robustness across\ndatasets, limiting their widespread adoption. To address these challenges, this\npaper introduces LV-UNet, a lightweight and vanilla model that leverages\npre-trained MobileNetv3-Large backbones and incorporates fusible modules.\nLV-UNet employs an enhanced deep training strategy and switches to a deployment\nmode during inference by re-parametrization, significantly reducing parameter\ncount and computational overhead. Experimental results on ISIC 2016, BUSI,\nCVC-ClinicDB, CVC-ColonDB, and Kvair-SEG datasets demonstrate a better\ntrade-off between performance and the computational load. The code will be\nreleased at https://github.com/juntaoJianggavin/LV-UNet."
                },
                "authors": [
                    {
                        "name": "Juntao Jiang"
                    },
                    {
                        "name": "Mengmeng Wang"
                    },
                    {
                        "name": "Huizhong Tian"
                    },
                    {
                        "name": "Lingbo Cheng"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Accepted by IEEE BIBM2024 ML4BMI workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16886v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16886v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27656v1",
                "updated": "2025-10-31T17:28:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    28,
                    22,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:28:22Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    28,
                    22,
                    4,
                    304,
                    0
                ],
                "title": "RDMA Point-to-Point Communication for LLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RDMA Point-to-Point Communication for LLM Systems"
                },
                "summary": "Emerging Large Language Model (LLM) system patterns, such as disaggregated\ninference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement\nfine-tuning, require flexible point-to-point communication beyond simple\ncollectives. Existing implementations are locked to specific Network Interface\nControllers (NICs), hindering integration into inference engines and\nportability across hardware providers. We present TransferEngine, which bridges\nthe functionality of common NICs to expose a uniform interface. TransferEngine\nexposes one-sided WriteImm operations with a ImmCounter primitive for\ncompletion notification, without ordering assumptions of network transport,\ntransparently managing multiple NICs per GPU. We demonstrate peak throughput of\n400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We\nshowcase TransferEngine through three production systems: (1) KvCache transfer\nfor disaggregated inference with dynamic scaling, (2) RL weight updates\nachieving 1.3 seconds for trillion-parameter models, and (3) MoE\ndispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,\nwith the first viable latencies on EFA. We demonstrate that our portable\npoint-to-point communication complements collectives while avoiding lock-in.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) system patterns, such as disaggregated\ninference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement\nfine-tuning, require flexible point-to-point communication beyond simple\ncollectives. Existing implementations are locked to specific Network Interface\nControllers (NICs), hindering integration into inference engines and\nportability across hardware providers. We present TransferEngine, which bridges\nthe functionality of common NICs to expose a uniform interface. TransferEngine\nexposes one-sided WriteImm operations with a ImmCounter primitive for\ncompletion notification, without ordering assumptions of network transport,\ntransparently managing multiple NICs per GPU. We demonstrate peak throughput of\n400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We\nshowcase TransferEngine through three production systems: (1) KvCache transfer\nfor disaggregated inference with dynamic scaling, (2) RL weight updates\nachieving 1.3 seconds for trillion-parameter models, and (3) MoE\ndispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7,\nwith the first viable latencies on EFA. We demonstrate that our portable\npoint-to-point communication complements collectives while avoiding lock-in."
                },
                "authors": [
                    {
                        "name": "Nandor Licker"
                    },
                    {
                        "name": "Kevin Hu"
                    },
                    {
                        "name": "Vladimir Zaytsev"
                    },
                    {
                        "name": "Lequn Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lequn Chen"
                },
                "arxiv_affiliation": "Perplexity AI",
                "author": "Lequn Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27641v1",
                "updated": "2025-10-31T17:12:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    34,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:12:34Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    12,
                    34,
                    4,
                    304,
                    0
                ],
                "title": "SpecAttn: Speculating Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecAttn: Speculating Sparse Attention"
                },
                "summary": "Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational bottlenecks\nduring inference due to the quadratic complexity of self-attention mechanisms,\nparticularly as context lengths increase. We introduce SpecAttn, a novel\ntraining-free approach that seamlessly integrates with existing speculative\ndecoding techniques to enable efficient sparse attention in pre-trained\ntransformers. Our key insight is to exploit the attention weights already\ncomputed by the draft model during speculative decoding to identify important\ntokens for the target model, eliminating redundant computation while\nmaintaining output quality. SpecAttn employs three core techniques: KL\ndivergence-based layer alignment between draft and target models, a\nGPU-optimized sorting-free algorithm for top-p token selection from draft\nattention patterns, and dynamic key-value cache pruning guided by these\npredictions. By leveraging the computational work already performed in standard\nspeculative decoding pipelines, SpecAttn achieves over 75% reduction in\nkey-value cache accesses with a mere 15.29% increase in perplexity on the PG-19\ndataset, significantly outperforming existing sparse attention methods. Our\napproach demonstrates that speculative execution can be enhanced to provide\napproximate verification without significant performance degradation."
                },
                "authors": [
                    {
                        "name": "Harsh Shah"
                    }
                ],
                "author_detail": {
                    "name": "Harsh Shah"
                },
                "author": "Harsh Shah",
                "arxiv_comment": "Accepted to NeurIPS 2025 Workshop on Structured Probabilistic\n  Inference & Generative Modeling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27630v1",
                "updated": "2025-10-31T17:00:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    0,
                    22,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:00:22Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    0,
                    22,
                    4,
                    304,
                    0
                ],
                "title": "Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout\n  for Long-Horizon Task Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interaction as Intelligence Part II: Asynchronous Human-Agent Rollout\n  for Long-Horizon Task Training"
                },
                "summary": "Large Language Model (LLM) agents have recently shown strong potential in\ndomains such as automated coding, deep research, and graphical user interface\nmanipulation. However, training them to succeed on long-horizon,\ndomain-specialized tasks remains challenging. Current methods primarily fall\ninto two categories. The first relies on dense human annotations through\nbehavior cloning, which is prohibitively expensive for long-horizon tasks that\ncan take days or months. The second depends on outcome-driven sampling, which\noften collapses due to the rarity of valid positive trajectories on\ndomain-specialized tasks. We introduce Apollo, a sampling framework that\nintegrates asynchronous human guidance with action-level data filtering.\nInstead of requiring annotators to shadow every step, Apollo allows them to\nintervene only when the agent drifts from a promising trajectory, by providing\nprior knowledge, strategic advice, etc. This lightweight design makes it\npossible to sustain interactions for over 30 hours and produces valuable\ntrajectories at a lower cost. Apollo then applies supervision control to filter\nout sub-optimal actions and prevent error propagation. Together, these\ncomponents enable reliable and effective data collection in long-horizon\nenvironments. To demonstrate the effectiveness of Apollo, we evaluate it using\nInnovatorBench. Our experiments show that when applied to train the GLM-4.5\nmodel on InnovatorBench, Apollo achieves more than a 50% improvement over the\nuntrained baseline and a 28% improvement over a variant trained without human\ninteraction. These results highlight the critical role of human-in-the-loop\nsampling and the robustness of Apollo's design in handling long-horizon,\ndomain-specialized tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents have recently shown strong potential in\ndomains such as automated coding, deep research, and graphical user interface\nmanipulation. However, training them to succeed on long-horizon,\ndomain-specialized tasks remains challenging. Current methods primarily fall\ninto two categories. The first relies on dense human annotations through\nbehavior cloning, which is prohibitively expensive for long-horizon tasks that\ncan take days or months. The second depends on outcome-driven sampling, which\noften collapses due to the rarity of valid positive trajectories on\ndomain-specialized tasks. We introduce Apollo, a sampling framework that\nintegrates asynchronous human guidance with action-level data filtering.\nInstead of requiring annotators to shadow every step, Apollo allows them to\nintervene only when the agent drifts from a promising trajectory, by providing\nprior knowledge, strategic advice, etc. This lightweight design makes it\npossible to sustain interactions for over 30 hours and produces valuable\ntrajectories at a lower cost. Apollo then applies supervision control to filter\nout sub-optimal actions and prevent error propagation. Together, these\ncomponents enable reliable and effective data collection in long-horizon\nenvironments. To demonstrate the effectiveness of Apollo, we evaluate it using\nInnovatorBench. Our experiments show that when applied to train the GLM-4.5\nmodel on InnovatorBench, Apollo achieves more than a 50% improvement over the\nuntrained baseline and a 28% improvement over a variant trained without human\ninteraction. These results highlight the critical role of human-in-the-loop\nsampling and the robustness of Apollo's design in handling long-horizon,\ndomain-specialized tasks."
                },
                "authors": [
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Yunze Wu"
                    },
                    {
                        "name": "Xiaojie Cai"
                    },
                    {
                        "name": "Lyumanshan Ye"
                    },
                    {
                        "name": "Shijie Xia"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Weiye Si"
                    },
                    {
                        "name": "Tianze Xu"
                    },
                    {
                        "name": "Jie Sun"
                    },
                    {
                        "name": "Keyu Li"
                    },
                    {
                        "name": "Mohan Jiang"
                    },
                    {
                        "name": "Junfei Wang"
                    },
                    {
                        "name": "Qishuo Hua"
                    },
                    {
                        "name": "Pengrui Lu"
                    },
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27628v1",
                "updated": "2025-10-31T17:00:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    0,
                    4,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T17:00:04Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    17,
                    0,
                    4,
                    4,
                    304,
                    0
                ],
                "title": "Validity Is What You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Validity Is What You Need"
                },
                "summary": "While AI agents have long been discussed and studied in computer science,\ntoday's Agentic AI systems are something new. We consider other definitions of\nAgentic AI and propose a new realist definition. Agentic AI is a software\ndelivery mechanism, comparable to software as a service (SaaS), which puts an\napplication to work autonomously in a complex enterprise setting. Recent\nadvances in large language models (LLMs) as foundation models have driven\nexcitement in Agentic AI. We note, however, that Agentic AI systems are\nprimarily applications, not foundations, and so their success depends on\nvalidation by end users and principal stakeholders. The tools and techniques\nneeded by the principal users to validate their applications are quite\ndifferent from the tools and techniques used to evaluate foundation models.\nIronically, with good validation measures in place, in many cases the\nfoundation models can be replaced with much simpler, faster, and more\ninterpretable models that handle core logic. When it comes to Agentic AI,\nvalidity is what you need. LLMs are one option that might achieve it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While AI agents have long been discussed and studied in computer science,\ntoday's Agentic AI systems are something new. We consider other definitions of\nAgentic AI and propose a new realist definition. Agentic AI is a software\ndelivery mechanism, comparable to software as a service (SaaS), which puts an\napplication to work autonomously in a complex enterprise setting. Recent\nadvances in large language models (LLMs) as foundation models have driven\nexcitement in Agentic AI. We note, however, that Agentic AI systems are\nprimarily applications, not foundations, and so their success depends on\nvalidation by end users and principal stakeholders. The tools and techniques\nneeded by the principal users to validate their applications are quite\ndifferent from the tools and techniques used to evaluate foundation models.\nIronically, with good validation measures in place, in many cases the\nfoundation models can be replaced with much simpler, faster, and more\ninterpretable models that handle core logic. When it comes to Agentic AI,\nvalidity is what you need. LLMs are one option that might achieve it."
                },
                "authors": [
                    {
                        "name": "Sebastian Benthall"
                    },
                    {
                        "name": "Andrew Clark"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Clark"
                },
                "author": "Andrew Clark",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27623v1",
                "updated": "2025-10-31T16:50:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    50,
                    49,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T16:50:49Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    50,
                    49,
                    4,
                    304,
                    0
                ],
                "title": "Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive\n  Trigger Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Backdoor Attacks on MLLM Embodied Decision Making via Contrastive\n  Trigger Learning"
                },
                "summary": "Multimodal large language models (MLLMs) have advanced embodied agents by\nenabling direct perception, reasoning, and planning task-oriented actions from\nvisual inputs. However, such vision driven embodied agents open a new attack\nsurface: visual backdoor attacks, where the agent behaves normally until a\nvisual trigger appears in the scene, then persistently executes an\nattacker-specified multi-step policy. We introduce BEAT, the first framework to\ninject such visual backdoors into MLLM-based embodied agents using objects in\nthe environments as triggers. Unlike textual triggers, object triggers exhibit\nwide variation across viewpoints and lighting, making them difficult to implant\nreliably. BEAT addresses this challenge by (1) constructing a training set that\nspans diverse scenes, tasks, and trigger placements to expose agents to trigger\nvariability, and (2) introducing a two-stage training scheme that first applies\nsupervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning\n(CTL). CTL formulates trigger discrimination as preference learning between\ntrigger-present and trigger-free inputs, explicitly sharpening the decision\nboundaries to ensure precise backdoor activation. Across various embodied agent\nbenchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while\nmaintaining strong benign task performance, and generalizes reliably to\nout-of-distribution trigger placements. Notably, compared to naive SFT, CTL\nboosts backdoor activation accuracy up to 39% under limited backdoor data.\nThese findings expose a critical yet unexplored security risk in MLLM-based\nembodied agents, underscoring the need for robust defenses before real-world\ndeployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have advanced embodied agents by\nenabling direct perception, reasoning, and planning task-oriented actions from\nvisual inputs. However, such vision driven embodied agents open a new attack\nsurface: visual backdoor attacks, where the agent behaves normally until a\nvisual trigger appears in the scene, then persistently executes an\nattacker-specified multi-step policy. We introduce BEAT, the first framework to\ninject such visual backdoors into MLLM-based embodied agents using objects in\nthe environments as triggers. Unlike textual triggers, object triggers exhibit\nwide variation across viewpoints and lighting, making them difficult to implant\nreliably. BEAT addresses this challenge by (1) constructing a training set that\nspans diverse scenes, tasks, and trigger placements to expose agents to trigger\nvariability, and (2) introducing a two-stage training scheme that first applies\nsupervised fine-tuning (SFT) and then our novel Contrastive Trigger Learning\n(CTL). CTL formulates trigger discrimination as preference learning between\ntrigger-present and trigger-free inputs, explicitly sharpening the decision\nboundaries to ensure precise backdoor activation. Across various embodied agent\nbenchmarks and MLLMs, BEAT achieves attack success rates up to 80%, while\nmaintaining strong benign task performance, and generalizes reliably to\nout-of-distribution trigger placements. Notably, compared to naive SFT, CTL\nboosts backdoor activation accuracy up to 39% under limited backdoor data.\nThese findings expose a critical yet unexplored security risk in MLLM-based\nembodied agents, underscoring the need for robust defenses before real-world\ndeployment."
                },
                "authors": [
                    {
                        "name": "Qiusi Zhan"
                    },
                    {
                        "name": "Hyeonjeong Ha"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Sirui Xu"
                    },
                    {
                        "name": "Hanyang Chen"
                    },
                    {
                        "name": "Liang-Yan Gui"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Daniel Kang"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Kang"
                },
                "author": "Daniel Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27617v1",
                "updated": "2025-10-31T16:40:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    40,
                    58,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T16:40:58Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    40,
                    58,
                    4,
                    304,
                    0
                ],
                "title": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeriMoA: A Mixture-of-Agents Framework for Spec-to-HDL Generation"
                },
                "summary": "Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automation of Register Transfer Level (RTL) design can help developers meet\nincreasing computational demands. Large Language Models (LLMs) show promise for\nHardware Description Language (HDL) generation, but face challenges due to\nlimited parametric knowledge and domain-specific constraints. While prompt\nengineering and fine-tuning have limitations in knowledge coverage and training\ncosts, multi-agent architectures offer a training-free paradigm to enhance\nreasoning through collaborative generation. However, current multi-agent\napproaches suffer from two critical deficiencies: susceptibility to noise\npropagation and constrained reasoning space exploration. We propose VeriMoA, a\ntraining-free mixture-of-agents (MoA) framework with two synergistic\ninnovations. First, a quality-guided caching mechanism to maintain all\nintermediate HDL outputs and enables quality-based ranking and selection across\nthe entire generation process, encouraging knowledge accumulation over layers\nof reasoning. Second, a multi-path generation strategy that leverages C++ and\nPython as intermediate representations, decomposing specification-to-HDL\ntranslation into two-stage processes that exploit LLM fluency in high-resource\nlanguages while promoting solution diversity. Comprehensive experiments on\nVerilogEval 2.0 and RTLLM 2.0 benchmarks demonstrate that VeriMoA achieves\n15--30% improvements in Pass@1 across diverse LLM backbones, especially\nenabling smaller models to match larger models and fine-tuned alternatives\nwithout requiring costly training."
                },
                "authors": [
                    {
                        "name": "Heng Ping"
                    },
                    {
                        "name": "Arijit Bhattacharjee"
                    },
                    {
                        "name": "Peiyu Zhang"
                    },
                    {
                        "name": "Shixuan Li"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Anzhe Cheng"
                    },
                    {
                        "name": "Xiaole Zhang"
                    },
                    {
                        "name": "Jesse Thomason"
                    },
                    {
                        "name": "Ali Jannesari"
                    },
                    {
                        "name": "Nesreen Ahmed"
                    },
                    {
                        "name": "Paul Bogdan"
                    }
                ],
                "author_detail": {
                    "name": "Paul Bogdan"
                },
                "author": "Paul Bogdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27610v1",
                "updated": "2025-10-31T16:35:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    35,
                    52,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T16:35:52Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    35,
                    52,
                    4,
                    304,
                    0
                ],
                "title": "ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORGEval: Graph-Theoretic Evaluation of LLMs in Optimization Modeling"
                },
                "summary": "Formulating optimization problems for industrial applications demands\nsignificant manual effort and domain expertise. While Large Language Models\n(LLMs) show promise in automating this process, evaluating their performance\nremains difficult due to the absence of robust metrics. Existing solver-based\napproaches often face inconsistency, infeasibility issues, and high\ncomputational costs. To address these issues, we propose ORGEval, a\ngraph-theoretic evaluation framework for assessing LLMs' capabilities in\nformulating linear and mixed-integer linear programs. ORGEval represents\noptimization models as graphs, reducing equivalence detection to graph\nisomorphism testing. We identify and prove a sufficient condition, when the\ntested graphs are symmetric decomposable (SD), under which the\nWeisfeiler-Lehman (WL) test is guaranteed to correctly detect isomorphism.\nBuilding on this, ORGEval integrates a tailored variant of the WL-test with an\nSD detection algorithm to evaluate model equivalence. By focusing on structural\nequivalence rather than instance-level configurations, ORGEval is robust to\nnumerical variations. Experimental results show that our method can\nsuccessfully detect model equivalence and produce 100\\% consistent results\nacross random parameter configurations, while significantly outperforming\nsolver-based methods in runtime, especially on difficult problems. Leveraging\nORGEval, we construct the Bench4Opt dataset and benchmark state-of-the-art LLMs\non optimization modeling. Our results reveal that although optimization\nmodeling remains challenging for all LLMs, DeepSeek-V3 and Claude-Opus-4\nachieve the highest accuracies under direct prompting, outperforming even\nleading reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formulating optimization problems for industrial applications demands\nsignificant manual effort and domain expertise. While Large Language Models\n(LLMs) show promise in automating this process, evaluating their performance\nremains difficult due to the absence of robust metrics. Existing solver-based\napproaches often face inconsistency, infeasibility issues, and high\ncomputational costs. To address these issues, we propose ORGEval, a\ngraph-theoretic evaluation framework for assessing LLMs' capabilities in\nformulating linear and mixed-integer linear programs. ORGEval represents\noptimization models as graphs, reducing equivalence detection to graph\nisomorphism testing. We identify and prove a sufficient condition, when the\ntested graphs are symmetric decomposable (SD), under which the\nWeisfeiler-Lehman (WL) test is guaranteed to correctly detect isomorphism.\nBuilding on this, ORGEval integrates a tailored variant of the WL-test with an\nSD detection algorithm to evaluate model equivalence. By focusing on structural\nequivalence rather than instance-level configurations, ORGEval is robust to\nnumerical variations. Experimental results show that our method can\nsuccessfully detect model equivalence and produce 100\\% consistent results\nacross random parameter configurations, while significantly outperforming\nsolver-based methods in runtime, especially on difficult problems. Leveraging\nORGEval, we construct the Bench4Opt dataset and benchmark state-of-the-art LLMs\non optimization modeling. Our results reveal that although optimization\nmodeling remains challenging for all LLMs, DeepSeek-V3 and Claude-Opus-4\nachieve the highest accuracies under direct prompting, outperforming even\nleading reasoning models."
                },
                "authors": [
                    {
                        "name": "Zhuohan Wang"
                    },
                    {
                        "name": "Ziwei Zhu"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Congliang Chen"
                    },
                    {
                        "name": "Yizhou Han"
                    },
                    {
                        "name": "Yufeng Lin"
                    },
                    {
                        "name": "Zhihang Lin"
                    },
                    {
                        "name": "Angyang Gu"
                    },
                    {
                        "name": "Xinglin Hu"
                    },
                    {
                        "name": "Ruoyu Sun"
                    },
                    {
                        "name": "Tian Ding"
                    }
                ],
                "author_detail": {
                    "name": "Tian Ding"
                },
                "author": "Tian Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.17496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.17496v2",
                "updated": "2025-10-31T16:27:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    27,
                    50,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-20T12:51:13Z",
                "published_parsed": [
                    2025,
                    10,
                    20,
                    12,
                    51,
                    13,
                    0,
                    293,
                    0
                ],
                "title": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and\n  Mathematical Reasoning in Large Language and Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and\n  Mathematical Reasoning in Large Language and Reasoning Models"
                },
                "summary": "We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate\ngeneralization and robustness in analogical and mathematical reasoning for\nLarge Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X\nextends I-RAVEN by increasing operand complexity, attribute range, and\nintroducing perceptual uncertainty. Compared to LLMs, empirical results show\nthat LRMs achieve improved productivity and systematicity on longer reasoning\nrelations and wider attribute ranges, respectively. However, LRMs are still\nsignificantly challenged by reasoning under uncertainty and cannot effectively\nexplore multiple probabilistic outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate\ngeneralization and robustness in analogical and mathematical reasoning for\nLarge Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X\nextends I-RAVEN by increasing operand complexity, attribute range, and\nintroducing perceptual uncertainty. Compared to LLMs, empirical results show\nthat LRMs achieve improved productivity and systematicity on longer reasoning\nrelations and wider attribute ranges, respectively. However, LRMs are still\nsignificantly challenged by reasoning under uncertainty and cannot effectively\nexplore multiple probabilistic outcomes."
                },
                "authors": [
                    {
                        "name": "Giacomo Camposampiero"
                    },
                    {
                        "name": "Michael Hersche"
                    },
                    {
                        "name": "Roger Wattenhofer"
                    },
                    {
                        "name": "Abu Sebastian"
                    },
                    {
                        "name": "Abbas Rahimi"
                    }
                ],
                "author_detail": {
                    "name": "Abbas Rahimi"
                },
                "author": "Abbas Rahimi",
                "arxiv_comment": "Accepted at the 5th Workshop on Mathematical Reasoning and AI\n  (MATH-AI), NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.17496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.17496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27598v1",
                "updated": "2025-10-31T16:22:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    22,
                    23,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T16:22:23Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    22,
                    23,
                    4,
                    304,
                    0
                ],
                "title": "InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM\n  Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InnovatorBench: Evaluating Agents' Ability to Conduct Innovative LLM\n  Research"
                },
                "summary": "AI agents could accelerate scientific discovery by automating hypothesis\nformation, experiment design, coding, execution, and analysis, yet existing\nbenchmarks probe narrow skills in simplified settings. To address this gap, we\nintroduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end\nassessment of agents performing Large Language Model (LLM) research. It\ncomprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss\nDesign, Reward Design, and Scaffold Construction, which require runnable\nartifacts and assessment of correctness, performance, output quality, and\nuncertainty. To support agent operation, we develop ResearchGym, a research\nenvironment offering rich action spaces, distributed and long-horizon\nexecution, asynchronous monitoring, and snapshot saving. We also implement a\nlightweight ReAct agent that couples explicit reasoning with executable\nplanning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2.\nOur experiments demonstrate that while frontier models show promise in\ncode-driven research tasks, they struggle with fragile algorithm-related tasks\nand long-horizon decision making, such as impatience, poor resource management,\nand overreliance on template-based reasoning. Furthermore, agents require over\n11 hours to achieve their best performance on InnovatorBench, underscoring the\nbenchmark's difficulty and showing the potential of InnovatorBench to be the\nnext generation of code-based research benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI agents could accelerate scientific discovery by automating hypothesis\nformation, experiment design, coding, execution, and analysis, yet existing\nbenchmarks probe narrow skills in simplified settings. To address this gap, we\nintroduce InnovatorBench, a benchmark-platform pair for realistic, end-to-end\nassessment of agents performing Large Language Model (LLM) research. It\ncomprises 20 tasks spanning Data Construction, Filtering, Augmentation, Loss\nDesign, Reward Design, and Scaffold Construction, which require runnable\nartifacts and assessment of correctness, performance, output quality, and\nuncertainty. To support agent operation, we develop ResearchGym, a research\nenvironment offering rich action spaces, distributed and long-horizon\nexecution, asynchronous monitoring, and snapshot saving. We also implement a\nlightweight ReAct agent that couples explicit reasoning with executable\nplanning using frontier models such as Claude-4, GPT-5, GLM-4.5, and Kimi-K2.\nOur experiments demonstrate that while frontier models show promise in\ncode-driven research tasks, they struggle with fragile algorithm-related tasks\nand long-horizon decision making, such as impatience, poor resource management,\nand overreliance on template-based reasoning. Furthermore, agents require over\n11 hours to achieve their best performance on InnovatorBench, underscoring the\nbenchmark's difficulty and showing the potential of InnovatorBench to be the\nnext generation of code-based research benchmark."
                },
                "authors": [
                    {
                        "name": "Yunze Wu"
                    },
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Weiye Si"
                    },
                    {
                        "name": "Zhen Huang"
                    },
                    {
                        "name": "Mohan Jiang"
                    },
                    {
                        "name": "Keyu Li"
                    },
                    {
                        "name": "Shijie Xia"
                    },
                    {
                        "name": "Jie Sun"
                    },
                    {
                        "name": "Tianze Xu"
                    },
                    {
                        "name": "Xiangkun Hu"
                    },
                    {
                        "name": "Pengrui Lu"
                    },
                    {
                        "name": "Xiaojie Cai"
                    },
                    {
                        "name": "Lyumanshan Ye"
                    },
                    {
                        "name": "Wenhong Zhu"
                    },
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Pengfei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Pengfei Liu"
                },
                "author": "Pengfei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05102v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05102v3",
                "updated": "2025-10-31T16:15:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    15,
                    22,
                    4,
                    304,
                    0
                ],
                "published": "2024-10-07T15:01:29Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    15,
                    1,
                    29,
                    0,
                    281,
                    0
                ],
                "title": "SparsePO: Controlling Preference Alignment of LLMs via Sparse Token\n  Masks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparsePO: Controlling Preference Alignment of LLMs via Sparse Token\n  Masks"
                },
                "summary": "Direct alignment algorithms have proven an effective step for aligning\nlanguage models to human-desired behaviors. Current variants of the Direct\nPreference Optimization objective have focused on a strict setting where all\ntokens are contributing signals of KL divergence and rewards to the loss\nfunction. However, human preference is not affected equally by each word in a\nsequence but is often dependent on specific words or phrases, e.g. existence of\ntoxic terms leads to non-preferred responses. Based on this observation, we\nargue that not all tokens should be weighted equally during PO and propose a\nflexible objective termed SparsePO, that aims to automatically learn to weight\nthe KL divergence and reward corresponding to each token during PO training. We\npropose two different variants of weight-masks that can either be derived from\nthe reference model itself or learned on the fly. Notably, our method induces\nsparsity in the learned masks, allowing the model to learn how to best balance\nreward and KL divergence contributions at the token level, learning an optimal\nlevel of mask sparsity. Extensive experiments illustrate the effectiveness of\nour approach at aligning to preference proxies, including sentiment control,\nhelpfulness and harmlessness, and summary quality. Our method obtains +10% and\n+3% win rate points in summarization and dialogue scenarios, respectively,\nwithout compromising model reasoning or the relevancy and faithfulness of the\nsummary response.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct alignment algorithms have proven an effective step for aligning\nlanguage models to human-desired behaviors. Current variants of the Direct\nPreference Optimization objective have focused on a strict setting where all\ntokens are contributing signals of KL divergence and rewards to the loss\nfunction. However, human preference is not affected equally by each word in a\nsequence but is often dependent on specific words or phrases, e.g. existence of\ntoxic terms leads to non-preferred responses. Based on this observation, we\nargue that not all tokens should be weighted equally during PO and propose a\nflexible objective termed SparsePO, that aims to automatically learn to weight\nthe KL divergence and reward corresponding to each token during PO training. We\npropose two different variants of weight-masks that can either be derived from\nthe reference model itself or learned on the fly. Notably, our method induces\nsparsity in the learned masks, allowing the model to learn how to best balance\nreward and KL divergence contributions at the token level, learning an optimal\nlevel of mask sparsity. Extensive experiments illustrate the effectiveness of\nour approach at aligning to preference proxies, including sentiment control,\nhelpfulness and harmlessness, and summary quality. Our method obtains +10% and\n+3% win rate points in summarization and dialogue scenarios, respectively,\nwithout compromising model reasoning or the relevancy and faithfulness of the\nsummary response."
                },
                "authors": [
                    {
                        "name": "Fenia Christopoulou"
                    },
                    {
                        "name": "Ronald Cardenas"
                    },
                    {
                        "name": "Gerasimos Lampouras"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "27 pages, 9 figures, 5 tables. Accepted to EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05102v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05102v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27583v1",
                "updated": "2025-10-31T16:06:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    6,
                    35,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T16:06:35Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    6,
                    35,
                    4,
                    304,
                    0
                ],
                "title": "AMD MI300X GPU Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMD MI300X GPU Performance Analysis"
                },
                "summary": "The rapid growth of large language models (LLMs) has driven the need for\nhigh-performance, scalable GPU hardware capable of efficiently serving models\nwith hundreds of billions of parameters. While NVIDIA GPUs have traditionally\ndominated LLM deployments due to their mature CUDA software stack and state-of\nthe-art accelerators, AMD's latest MI300X GPUs offer a compelling alternative,\nfeaturing high HBM capacity, matrix cores, and their proprietary interconnect.\nIn this paper, we present a comprehensive evaluation of the AMD MI300X GPUs\nacross key performance domains critical to LLM inference including compute\nthroughput, memory bandwidth, and interconnect communication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of large language models (LLMs) has driven the need for\nhigh-performance, scalable GPU hardware capable of efficiently serving models\nwith hundreds of billions of parameters. While NVIDIA GPUs have traditionally\ndominated LLM deployments due to their mature CUDA software stack and state-of\nthe-art accelerators, AMD's latest MI300X GPUs offer a compelling alternative,\nfeaturing high HBM capacity, matrix cores, and their proprietary interconnect.\nIn this paper, we present a comprehensive evaluation of the AMD MI300X GPUs\nacross key performance domains critical to LLM inference including compute\nthroughput, memory bandwidth, and interconnect communication."
                },
                "authors": [
                    {
                        "name": "Chandrish Ambati"
                    },
                    {
                        "name": "Trung Diep"
                    }
                ],
                "author_detail": {
                    "name": "Trung Diep"
                },
                "author": "Trung Diep",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10724v2",
                "updated": "2025-10-31T16:06:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    16,
                    6,
                    3,
                    4,
                    304,
                    0
                ],
                "published": "2025-04-14T21:30:43Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    21,
                    30,
                    43,
                    0,
                    104,
                    0
                ],
                "title": "HELIOS: Adaptive Model And Early-Exit Selection for Efficient LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HELIOS: Adaptive Model And Early-Exit Selection for Efficient LLM\n  Inference Serving"
                },
                "summary": "Early-Exit Large Language Models (EE-LLMs) enable high throughput inference\nby allowing tokens to exit early at intermediate layers. However, their\nthroughput is limited by the computational and memory savings. Existing EE-LLM\nframeworks rely on a single model and therefore, their token generation\nlatencies are bottlenecked by tokens that do not exit early and traverse\nadditional layers. Moreover, early exits are only known at runtime and depend\non the request. Therefore, these frameworks load the weights of all model\nlayers even though large portions remain unused when tokens exit early. The\nlack of memory savings limit us from scaling the batch sizes.\n  We propose $\\textit{HELIOS}$, a framework that improves both token generation\nlatency and batch sizes to enable high-throughput in EE-LLMs. HELIOS exploits\ntwo insights. $\\textit{First}$, early exits are often complimentary across\nmodels, tokens that do not exit early on one model often take an early-exit on\nanother. HELIOS employs multiple models and dynamically switches between them\nto collectively maximize the number of tokens that exit early, and minimize\ntoken generation latencies. $\\textit{Second}$, even when a predicted token does\nnot exit early due to poor confidence, it often remains unchanged even after\nadditional layer traversal. HELIOS greedily allows such tokens to exit early\nand only loads the weights of the most likely to be used layers, yielding\nmemory savings which is then re-purposed to increase batch sizes. HELIOS\nemploys real-time profiling to accurately identify the early-exit\ndistributions, and adaptively switches between models by tracking tokens in\nreal-time to minimize the performance degradation caused by greedy model\nloading and exiting. Our evaluations show that HELIOS achieves $1.48\\times$\nhigher throughput and $15.14\\times$ larger batch size compared to existing\nEE-LLM frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early-Exit Large Language Models (EE-LLMs) enable high throughput inference\nby allowing tokens to exit early at intermediate layers. However, their\nthroughput is limited by the computational and memory savings. Existing EE-LLM\nframeworks rely on a single model and therefore, their token generation\nlatencies are bottlenecked by tokens that do not exit early and traverse\nadditional layers. Moreover, early exits are only known at runtime and depend\non the request. Therefore, these frameworks load the weights of all model\nlayers even though large portions remain unused when tokens exit early. The\nlack of memory savings limit us from scaling the batch sizes.\n  We propose $\\textit{HELIOS}$, a framework that improves both token generation\nlatency and batch sizes to enable high-throughput in EE-LLMs. HELIOS exploits\ntwo insights. $\\textit{First}$, early exits are often complimentary across\nmodels, tokens that do not exit early on one model often take an early-exit on\nanother. HELIOS employs multiple models and dynamically switches between them\nto collectively maximize the number of tokens that exit early, and minimize\ntoken generation latencies. $\\textit{Second}$, even when a predicted token does\nnot exit early due to poor confidence, it often remains unchanged even after\nadditional layer traversal. HELIOS greedily allows such tokens to exit early\nand only loads the weights of the most likely to be used layers, yielding\nmemory savings which is then re-purposed to increase batch sizes. HELIOS\nemploys real-time profiling to accurately identify the early-exit\ndistributions, and adaptively switches between models by tracking tokens in\nreal-time to minimize the performance degradation caused by greedy model\nloading and exiting. Our evaluations show that HELIOS achieves $1.48\\times$\nhigher throughput and $15.14\\times$ larger batch size compared to existing\nEE-LLM frameworks."
                },
                "authors": [
                    {
                        "name": "Avinash Kumar"
                    },
                    {
                        "name": "Shashank Nag"
                    },
                    {
                        "name": "Jason Clemons"
                    },
                    {
                        "name": "Lizy John"
                    },
                    {
                        "name": "Poulami Das"
                    }
                ],
                "author_detail": {
                    "name": "Poulami Das"
                },
                "author": "Poulami Das",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27569v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27569v1",
                "updated": "2025-10-31T15:51:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    51,
                    39,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T15:51:39Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    51,
                    39,
                    4,
                    304,
                    0
                ],
                "title": "MARAG-R1: Beyond Single Retriever via Reinforcement-Learned Multi-Tool\n  Agentic Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARAG-R1: Beyond Single Retriever via Reinforcement-Learned Multi-Tool\n  Agentic Retrieval"
                },
                "summary": "Large Language Models (LLMs) excel at reasoning and generation but are\ninherently limited by static pretraining data, resulting in factual\ninaccuracies and weak adaptability to new information. Retrieval-Augmented\nGeneration (RAG) addresses this issue by grounding LLMs in external knowledge;\nHowever, the effectiveness of RAG critically depends on whether the model can\nadequately access relevant information. Existing RAG systems rely on a single\nretriever with fixed top-k selection, restricting access to a narrow and static\nsubset of the corpus. As a result, this single-retriever paradigm has become\nthe primary bottleneck for comprehensive external information acquisition,\nespecially in tasks requiring corpus-level reasoning. To overcome this\nlimitation, we propose MARAG-R1, a reinforcement-learned multi-tool RAG\nframework that enables LLMs to dynamically coordinate multiple retrieval\nmechanisms for broader and more precise information access. MARAG-R1 equips the\nmodel with four retrieval tools -- semantic search, keyword search, filtering,\nand aggregation -- and learns both how and when to use them through a two-stage\ntraining process: supervised fine-tuning followed by reinforcement learning.\nThis design allows the model to interleave reasoning and retrieval,\nprogressively gathering sufficient evidence for corpus-level synthesis.\nExperiments on GlobalQA, HotpotQA, and 2WikiMultiHopQA demonstrate that\nMARAG-R1 substantially outperforms strong baselines and achieves new\nstate-of-the-art results in corpus-level reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at reasoning and generation but are\ninherently limited by static pretraining data, resulting in factual\ninaccuracies and weak adaptability to new information. Retrieval-Augmented\nGeneration (RAG) addresses this issue by grounding LLMs in external knowledge;\nHowever, the effectiveness of RAG critically depends on whether the model can\nadequately access relevant information. Existing RAG systems rely on a single\nretriever with fixed top-k selection, restricting access to a narrow and static\nsubset of the corpus. As a result, this single-retriever paradigm has become\nthe primary bottleneck for comprehensive external information acquisition,\nespecially in tasks requiring corpus-level reasoning. To overcome this\nlimitation, we propose MARAG-R1, a reinforcement-learned multi-tool RAG\nframework that enables LLMs to dynamically coordinate multiple retrieval\nmechanisms for broader and more precise information access. MARAG-R1 equips the\nmodel with four retrieval tools -- semantic search, keyword search, filtering,\nand aggregation -- and learns both how and when to use them through a two-stage\ntraining process: supervised fine-tuning followed by reinforcement learning.\nThis design allows the model to interleave reasoning and retrieval,\nprogressively gathering sufficient evidence for corpus-level synthesis.\nExperiments on GlobalQA, HotpotQA, and 2WikiMultiHopQA demonstrate that\nMARAG-R1 substantially outperforms strong baselines and achieves new\nstate-of-the-art results in corpus-level reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Qi Luo"
                    },
                    {
                        "name": "Xiaonan Li"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Tingshuo Fan"
                    },
                    {
                        "name": "Yuan Li"
                    },
                    {
                        "name": "Xinchi Chen"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27569v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27566v1",
                "updated": "2025-10-31T15:48:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    48,
                    43,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T15:48:43Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    48,
                    43,
                    4,
                    304,
                    0
                ],
                "title": "Interact-RAG: Reason and Interact with the Corpus, Beyond Black-Box\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interact-RAG: Reason and Interact with the Corpus, Beyond Black-Box\n  Retrieval"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has significantly enhanced LLMs by\nincorporating external information. However, prevailing agentic RAG approaches\nare constrained by a critical limitation: they treat the retrieval process as a\nblack-box querying operation. This confines agents' actions to query issuing,\nhindering its ability to tackle complex information-seeking tasks. To address\nthis, we introduce Interact-RAG, a new paradigm that elevates the LLM agent\nfrom a passive query issuer into an active manipulator of the retrieval\nprocess. We dismantle the black-box with a Corpus Interaction Engine, equipping\nthe agent with a set of action primitives for fine-grained control over\ninformation retrieval. To further empower the agent on the entire RAG pipeline,\nwe first develop a reasoning-enhanced workflow, which enables both zero-shot\nexecution and the synthesis of interaction trajectories. We then leverage this\nsynthetic data to train a fully autonomous end-to-end agent via Supervised\nFine-Tuning (SFT), followed by refinement with Reinforcement Learning (RL).\nExtensive experiments across six benchmarks demonstrate that Interact-RAG\nsignificantly outperforms other advanced methods, validating the efficacy of\nour reasoning-interaction strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has significantly enhanced LLMs by\nincorporating external information. However, prevailing agentic RAG approaches\nare constrained by a critical limitation: they treat the retrieval process as a\nblack-box querying operation. This confines agents' actions to query issuing,\nhindering its ability to tackle complex information-seeking tasks. To address\nthis, we introduce Interact-RAG, a new paradigm that elevates the LLM agent\nfrom a passive query issuer into an active manipulator of the retrieval\nprocess. We dismantle the black-box with a Corpus Interaction Engine, equipping\nthe agent with a set of action primitives for fine-grained control over\ninformation retrieval. To further empower the agent on the entire RAG pipeline,\nwe first develop a reasoning-enhanced workflow, which enables both zero-shot\nexecution and the synthesis of interaction trajectories. We then leverage this\nsynthetic data to train a fully autonomous end-to-end agent via Supervised\nFine-Tuning (SFT), followed by refinement with Reinforcement Learning (RL).\nExtensive experiments across six benchmarks demonstrate that Interact-RAG\nsignificantly outperforms other advanced methods, validating the efficacy of\nour reasoning-interaction strategy."
                },
                "authors": [
                    {
                        "name": "Yulong Hui"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Zhihang Fu"
                    },
                    {
                        "name": "Yihao Liu"
                    },
                    {
                        "name": "Jieping Ye"
                    },
                    {
                        "name": "Huanchen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Huanchen Zhang"
                },
                "author": "Huanchen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27565v1",
                "updated": "2025-10-31T15:47:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    47,
                    7,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T15:47:07Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    47,
                    7,
                    4,
                    304,
                    0
                ],
                "title": "CodeAlignBench: Assessing Code Generation Models on Developer-Preferred\n  Code Adjustments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeAlignBench: Assessing Code Generation Models on Developer-Preferred\n  Code Adjustments"
                },
                "summary": "As large language models become increasingly capable of generating code,\nevaluating their performance remains a complex and evolving challenge. Existing\nbenchmarks primarily focus on functional correctness, overlooking the diversity\nof real-world coding tasks and developer expectations. To this end, we\nintroduce a multi-language benchmark that evaluates LLM instruction-following\ncapabilities and is extensible to operate on any set of standalone coding\nproblems. Our benchmark evaluates instruction following in two key settings:\nadherence to pre-defined constraints specified with the initial problem, and\nthe ability to perform refinements based on follow-up instructions. For this\npaper's analysis, we empirically evaluated our benchmarking pipeline with\nprogramming tasks from LiveBench, that are also automatically translated from\nPython into Java and JavaScript. Our automated benchmark reveals that models\nexhibit differing levels of performance across multiple dimensions of\ninstruction-following. Our benchmarking pipeline provides a more comprehensive\nevaluation of code generation models, highlighting their strengths and\nlimitations across languages and generation goals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models become increasingly capable of generating code,\nevaluating their performance remains a complex and evolving challenge. Existing\nbenchmarks primarily focus on functional correctness, overlooking the diversity\nof real-world coding tasks and developer expectations. To this end, we\nintroduce a multi-language benchmark that evaluates LLM instruction-following\ncapabilities and is extensible to operate on any set of standalone coding\nproblems. Our benchmark evaluates instruction following in two key settings:\nadherence to pre-defined constraints specified with the initial problem, and\nthe ability to perform refinements based on follow-up instructions. For this\npaper's analysis, we empirically evaluated our benchmarking pipeline with\nprogramming tasks from LiveBench, that are also automatically translated from\nPython into Java and JavaScript. Our automated benchmark reveals that models\nexhibit differing levels of performance across multiple dimensions of\ninstruction-following. Our benchmarking pipeline provides a more comprehensive\nevaluation of code generation models, highlighting their strengths and\nlimitations across languages and generation goals."
                },
                "authors": [
                    {
                        "name": "Forough Mehralian"
                    },
                    {
                        "name": "Ryan Shar"
                    },
                    {
                        "name": "James R. Rae"
                    },
                    {
                        "name": "Alireza Hashemi"
                    }
                ],
                "author_detail": {
                    "name": "Alireza Hashemi"
                },
                "author": "Alireza Hashemi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27556v1",
                "updated": "2025-10-31T15:34:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    34,
                    41,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T15:34:41Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    34,
                    41,
                    4,
                    304,
                    0
                ],
                "title": "Data-Efficient Domain Adaptation for LLM-based MT using Contrastive\n  Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Efficient Domain Adaptation for LLM-based MT using Contrastive\n  Preference Optimization"
                },
                "summary": "LLMs often require adaptation to domain-specific requirements, a process that\ncan be expensive when relying solely on SFT. We present an empirical study on\napplying CPO to simulate a post-editing workflow for data-efficient domain\nadaptation. Our approach synthesizes preference pairs by treating the base\nmodel's own raw output as the 'rejected' translation and the human-approved TM\nentry as the 'chosen' one. This method provides direct feedback on the model's\ncurrent knowledge, guiding it to align with domain-specific standards.\nExperiments in English-Brazilian Portuguese and English-Korean show that, by\nusing just 14.7k preference pairs, the model achieves performance close to that\nof a model trained on 160k+ samples with SFT, demonstrating significant data\nefficiency. Although we showcase its effectiveness in MT, this application of\nCPO naturally generalizes to other generative tasks where a model's initial\ndrafts can serve as a contrastive signal against a golden reference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs often require adaptation to domain-specific requirements, a process that\ncan be expensive when relying solely on SFT. We present an empirical study on\napplying CPO to simulate a post-editing workflow for data-efficient domain\nadaptation. Our approach synthesizes preference pairs by treating the base\nmodel's own raw output as the 'rejected' translation and the human-approved TM\nentry as the 'chosen' one. This method provides direct feedback on the model's\ncurrent knowledge, guiding it to align with domain-specific standards.\nExperiments in English-Brazilian Portuguese and English-Korean show that, by\nusing just 14.7k preference pairs, the model achieves performance close to that\nof a model trained on 160k+ samples with SFT, demonstrating significant data\nefficiency. Although we showcase its effectiveness in MT, this application of\nCPO naturally generalizes to other generative tasks where a model's initial\ndrafts can serve as a contrastive signal against a golden reference."
                },
                "authors": [
                    {
                        "name": "Inacio Vieira"
                    },
                    {
                        "name": "Antonio Castaldo"
                    },
                    {
                        "name": "James O'Doherty"
                    },
                    {
                        "name": "Sheila Castilho"
                    }
                ],
                "author_detail": {
                    "name": "Sheila Castilho"
                },
                "author": "Sheila Castilho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.11409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.11409v2",
                "updated": "2025-10-31T15:29:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    29,
                    3,
                    4,
                    304,
                    0
                ],
                "published": "2025-04-15T17:26:29Z",
                "published_parsed": [
                    2025,
                    4,
                    15,
                    17,
                    26,
                    29,
                    1,
                    105,
                    0
                ],
                "title": "Minitron-SSM: Efficient Hybrid Language Model Compression through\n  Group-Aware SSM Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minitron-SSM: Efficient Hybrid Language Model Compression through\n  Group-Aware SSM Pruning"
                },
                "summary": "Hybrid LLM architectures that combine Attention and State Space Models (SSMs)\nachieve state-of-the-art accuracy and runtime performance. Recent work has\ndemonstrated that applying compression and distillation to Attention-only\nmodels yields smaller, more accurate models at a fraction of the training cost.\nIn this work, we explore the effectiveness of compressing Hybrid architectures.\nWe introduce a novel group-aware pruning strategy that preserves the structural\nintegrity of SSM blocks and their sequence modeling capabilities. Furthermore,\nwe demonstrate the necessity of such SSM pruning to achieve improved accuracy\nand inference speed compared to traditional approaches. Our compression recipe\ncombines SSM, FFN, embedding dimension, and layer pruning, followed by\nknowledge distillation-based retraining, similar to the MINITRON technique.\nUsing this approach, we compress the Nemotron-H 8B Hybrid model down to 4B\nparameters with up to 40x fewer training tokens. The resulting model surpasses\nthe accuracy of similarly-sized models while achieving 2x faster inference,\nsignificantly advancing the Pareto frontier.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid LLM architectures that combine Attention and State Space Models (SSMs)\nachieve state-of-the-art accuracy and runtime performance. Recent work has\ndemonstrated that applying compression and distillation to Attention-only\nmodels yields smaller, more accurate models at a fraction of the training cost.\nIn this work, we explore the effectiveness of compressing Hybrid architectures.\nWe introduce a novel group-aware pruning strategy that preserves the structural\nintegrity of SSM blocks and their sequence modeling capabilities. Furthermore,\nwe demonstrate the necessity of such SSM pruning to achieve improved accuracy\nand inference speed compared to traditional approaches. Our compression recipe\ncombines SSM, FFN, embedding dimension, and layer pruning, followed by\nknowledge distillation-based retraining, similar to the MINITRON technique.\nUsing this approach, we compress the Nemotron-H 8B Hybrid model down to 4B\nparameters with up to 40x fewer training tokens. The resulting model surpasses\nthe accuracy of similarly-sized models while achieving 2x faster inference,\nsignificantly advancing the Pareto frontier."
                },
                "authors": [
                    {
                        "name": "Ali Taghibakhshi"
                    },
                    {
                        "name": "Sharath Turuvekere Sreenivas"
                    },
                    {
                        "name": "Saurav Muralidharan"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "Yashaswi Karnati"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Oluwatobi Olabiyi"
                    },
                    {
                        "name": "Daniel Korzekwa"
                    },
                    {
                        "name": "Mostofa Patwary"
                    },
                    {
                        "name": "Mohammad Shoeybi"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Bryan Catanzaro"
                    },
                    {
                        "name": "Ashwath Aithal"
                    },
                    {
                        "name": "Nima Tajbakhsh"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.11409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.11409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11664v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11664v4",
                "updated": "2025-10-31T15:26:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    26,
                    11,
                    4,
                    304,
                    0
                ],
                "published": "2025-02-17T10:53:57Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    10,
                    53,
                    57,
                    0,
                    48,
                    0
                ],
                "title": "VRoPE: Rotary Position Embedding for Video Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VRoPE: Rotary Position Embedding for Video Large Language Models"
                },
                "summary": "Rotary Position Embedding (RoPE) has shown strong performance in text-based\nLarge Language Models (LLMs), but extending it to video remains a challenge due\nto the intricate spatiotemporal structure of video frames. Existing\nadaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions\nseparately but suffer from two major limitations: positional bias in attention\ndistribution and disruptions in video-text transitions. To overcome these\nissues, we propose Video Rotary Position Embedding (VRoPE), a novel positional\nencoding method tailored for Video-LLMs. Specifically, we introduce a more\nbalanced encoding strategy that mitigates attention biases, ensuring a more\nuniform distribution of spatial focus. Additionally, our approach restructures\npositional indices to ensure a smooth transition between video and text tokens.\nExtensive experiments on different models demonstrate that VRoPE consistently\noutperforms previous RoPE variants, achieving significant improvements in video\nunderstanding, temporal reasoning, and retrieval tasks. Code is available at\nhttps://github.com/johncaged/VRoPE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotary Position Embedding (RoPE) has shown strong performance in text-based\nLarge Language Models (LLMs), but extending it to video remains a challenge due\nto the intricate spatiotemporal structure of video frames. Existing\nadaptations, such as RoPE-3D, attempt to encode spatial and temporal dimensions\nseparately but suffer from two major limitations: positional bias in attention\ndistribution and disruptions in video-text transitions. To overcome these\nissues, we propose Video Rotary Position Embedding (VRoPE), a novel positional\nencoding method tailored for Video-LLMs. Specifically, we introduce a more\nbalanced encoding strategy that mitigates attention biases, ensuring a more\nuniform distribution of spatial focus. Additionally, our approach restructures\npositional indices to ensure a smooth transition between video and text tokens.\nExtensive experiments on different models demonstrate that VRoPE consistently\noutperforms previous RoPE variants, achieving significant improvements in video\nunderstanding, temporal reasoning, and retrieval tasks. Code is available at\nhttps://github.com/johncaged/VRoPE."
                },
                "authors": [
                    {
                        "name": "Zikang Liu"
                    },
                    {
                        "name": "Longteng Guo"
                    },
                    {
                        "name": "Yepeng Tang"
                    },
                    {
                        "name": "Tongtian Yue"
                    },
                    {
                        "name": "Junxian Cai"
                    },
                    {
                        "name": "Kai Ma"
                    },
                    {
                        "name": "Qingbin Liu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Liu"
                },
                "author": "Jing Liu",
                "arxiv_comment": "EMNLP 2025 Main Camera Ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11664v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11664v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27544v1",
                "updated": "2025-10-31T15:17:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    17,
                    55,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T15:17:55Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    17,
                    55,
                    4,
                    304,
                    0
                ],
                "title": "Mechanics of Learned Reasoning 1: TempoBench, A Benchmark for\n  Interpretable Deconstruction of Reasoning System Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mechanics of Learned Reasoning 1: TempoBench, A Benchmark for\n  Interpretable Deconstruction of Reasoning System Performance"
                },
                "summary": "Large Language Models (LLMs) are increasingly excelling and outpacing human\nperformance on many tasks. However, to improve LLM reasoning, researchers\neither rely on ad-hoc generated datasets or formal mathematical proof systems\nsuch as the Lean proof assistant. Whilst ad-hoc generated methods can capture\nthe decision chains of real-world reasoning processes, they may encode some\ninadvertent bias in the space of reasoning they cover; they also cannot be\nformally verified. On the other hand, systems like Lean can guarantee\nverifiability, but are not well-suited to capture the nature of agentic\ndecision chain-based tasks. This creates a gap both in performance for\nfunctions such as business agents or code assistants, and in the usefulness of\nLLM reasoning benchmarks, whereby these fall short in reasoning structure or\nreal-world alignment. We introduce TempoBench, the first formally grounded and\nverifiable diagnostic benchmark that parametrizes difficulty to systematically\nanalyze how LLMs perform reasoning. TempoBench uses two evaluation benchmarks\nto break down reasoning ability. First, temporal trace evaluation (TTE) tests\nthe ability of an LLM to understand and simulate the execution of a given\nmulti-step reasoning system. Subsequently, temporal causal evaluation (TCE)\ntests an LLM's ability to perform multi-step causal reasoning and to distill\ncause-and-effect relations from complex systems. We find that models score\n65.6% on TCE-normal, and 7.5% on TCE-hard. This shows that state-of-the-art\nLLMs clearly understand the TCE task but perform poorly as system complexity\nincreases. Our code is available at our\n\\href{https://github.com/nik-hz/tempobench}{GitHub repository}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly excelling and outpacing human\nperformance on many tasks. However, to improve LLM reasoning, researchers\neither rely on ad-hoc generated datasets or formal mathematical proof systems\nsuch as the Lean proof assistant. Whilst ad-hoc generated methods can capture\nthe decision chains of real-world reasoning processes, they may encode some\ninadvertent bias in the space of reasoning they cover; they also cannot be\nformally verified. On the other hand, systems like Lean can guarantee\nverifiability, but are not well-suited to capture the nature of agentic\ndecision chain-based tasks. This creates a gap both in performance for\nfunctions such as business agents or code assistants, and in the usefulness of\nLLM reasoning benchmarks, whereby these fall short in reasoning structure or\nreal-world alignment. We introduce TempoBench, the first formally grounded and\nverifiable diagnostic benchmark that parametrizes difficulty to systematically\nanalyze how LLMs perform reasoning. TempoBench uses two evaluation benchmarks\nto break down reasoning ability. First, temporal trace evaluation (TTE) tests\nthe ability of an LLM to understand and simulate the execution of a given\nmulti-step reasoning system. Subsequently, temporal causal evaluation (TCE)\ntests an LLM's ability to perform multi-step causal reasoning and to distill\ncause-and-effect relations from complex systems. We find that models score\n65.6% on TCE-normal, and 7.5% on TCE-hard. This shows that state-of-the-art\nLLMs clearly understand the TCE task but perform poorly as system complexity\nincreases. Our code is available at our\n\\href{https://github.com/nik-hz/tempobench}{GitHub repository}."
                },
                "authors": [
                    {
                        "name": "Nikolaus Holzer"
                    },
                    {
                        "name": "William Fishell"
                    },
                    {
                        "name": "Baishakhi Ray"
                    },
                    {
                        "name": "Mark Santolucito"
                    }
                ],
                "author_detail": {
                    "name": "Mark Santolucito"
                },
                "author": "Mark Santolucito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27543v1",
                "updated": "2025-10-31T15:17:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    17,
                    6,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T15:17:06Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    17,
                    6,
                    4,
                    304,
                    0
                ],
                "title": "DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and\n  Multilingual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DialectalArabicMMLU: Benchmarking Dialectal Capabilities in Arabic and\n  Multilingual Language Models"
                },
                "summary": "We present DialectalArabicMMLU, a new benchmark for evaluating the\nperformance of large language models (LLMs) across Arabic dialects. While\nrecently developed Arabic and multilingual benchmarks have advanced LLM\nevaluation for Modern Standard Arabic (MSA), dialectal varieties remain\nunderrepresented despite their prevalence in everyday communication.\nDialectalArabicMMLU extends the MMLU-Redux framework through manual translation\nand adaptation of 3K multiple-choice question-answer pairs into five major\ndialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), yielding a total of\n15K QA pairs across 32 academic and professional domains (22K QA pairs when\nalso including English and MSA). The benchmark enables systematic assessment of\nLLM reasoning and comprehension beyond MSA, supporting both task-based and\nlinguistic analysis. We evaluate 19 open-weight Arabic and multilingual LLMs\n(1B-13B parameters) and report substantial performance variation across\ndialects, revealing persistent gaps in dialectal generalization.\nDialectalArabicMMLU provides the first unified, human-curated resource for\nmeasuring dialectal understanding in Arabic, thus promoting more inclusive\nevaluation and future model development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DialectalArabicMMLU, a new benchmark for evaluating the\nperformance of large language models (LLMs) across Arabic dialects. While\nrecently developed Arabic and multilingual benchmarks have advanced LLM\nevaluation for Modern Standard Arabic (MSA), dialectal varieties remain\nunderrepresented despite their prevalence in everyday communication.\nDialectalArabicMMLU extends the MMLU-Redux framework through manual translation\nand adaptation of 3K multiple-choice question-answer pairs into five major\ndialects (Syrian, Egyptian, Emirati, Saudi, and Moroccan), yielding a total of\n15K QA pairs across 32 academic and professional domains (22K QA pairs when\nalso including English and MSA). The benchmark enables systematic assessment of\nLLM reasoning and comprehension beyond MSA, supporting both task-based and\nlinguistic analysis. We evaluate 19 open-weight Arabic and multilingual LLMs\n(1B-13B parameters) and report substantial performance variation across\ndialects, revealing persistent gaps in dialectal generalization.\nDialectalArabicMMLU provides the first unified, human-curated resource for\nmeasuring dialectal understanding in Arabic, thus promoting more inclusive\nevaluation and future model development."
                },
                "authors": [
                    {
                        "name": "Malik H. Altakrori"
                    },
                    {
                        "name": "Nizar Habash"
                    },
                    {
                        "name": "Abdelhakim Freihat"
                    },
                    {
                        "name": "Younes Samih"
                    },
                    {
                        "name": "Kirill Chirkunov"
                    },
                    {
                        "name": "Muhammed AbuOdeh"
                    },
                    {
                        "name": "Radu Florian"
                    },
                    {
                        "name": "Teresa Lynn"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    }
                ],
                "author_detail": {
                    "name": "Alham Fikri Aji"
                },
                "author": "Alham Fikri Aji",
                "arxiv_comment": "9 pages, 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27535v1",
                "updated": "2025-10-31T15:08:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    8,
                    18,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T15:08:18Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    15,
                    8,
                    18,
                    4,
                    304,
                    0
                ],
                "title": "Patient-Centered Summarization Framework for AI Clinical Summarization:\n  A Mixed-Methods Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patient-Centered Summarization Framework for AI Clinical Summarization:\n  A Mixed-Methods Design"
                },
                "summary": "Large Language Models (LLMs) are increasingly demonstrating the potential to\nreach human-level performance in generating clinical summaries from\npatient-clinician conversations. However, these summaries often focus on\npatients' biology rather than their preferences, values, wishes, and concerns.\nTo achieve patient-centered care, we propose a new standard for Artificial\nIntelligence (AI) clinical summarization tasks: Patient-Centered Summaries\n(PCS). Our objective was to develop a framework to generate PCS that capture\npatient values and ensure clinical utility and to assess whether current\nopen-source LLMs can achieve human-level performance in this task. We used a\nmixed-methods process. Two Patient and Public Involvement groups (10 patients\nand 8 clinicians) in the United Kingdom participated in semi-structured\ninterviews exploring what personal and contextual information should be\nincluded in clinical summaries and how it should be structured for clinical\nuse. Findings informed annotation guidelines used by eight clinicians to create\ngold-standard PCS from 88 atrial fibrillation consultations. Sixteen\nconsultations were used to refine a prompt aligned with the guidelines. Five\nopen-source LLMs (Llama-3.2-3B, Llama-3.1-8B, Mistral-8B, Gemma-3-4B, and\nQwen3-8B) generated summaries for 72 consultations using zero-shot and few-shot\nprompting, evaluated with ROUGE-L, BERTScore, and qualitative metrics. Patients\nemphasized lifestyle routines, social support, recent stressors, and care\nvalues. Clinicians sought concise functional, psychosocial, and emotional\ncontext. The best zero-shot performance was achieved by Mistral-8B (ROUGE-L\n0.189) and Llama-3.1-8B (BERTScore 0.673); the best few-shot by Llama-3.1-8B\n(ROUGE-L 0.206, BERTScore 0.683). Completeness and fluency were similar between\nexperts and models, while correctness and patient-centeredness favored human\nPCS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly demonstrating the potential to\nreach human-level performance in generating clinical summaries from\npatient-clinician conversations. However, these summaries often focus on\npatients' biology rather than their preferences, values, wishes, and concerns.\nTo achieve patient-centered care, we propose a new standard for Artificial\nIntelligence (AI) clinical summarization tasks: Patient-Centered Summaries\n(PCS). Our objective was to develop a framework to generate PCS that capture\npatient values and ensure clinical utility and to assess whether current\nopen-source LLMs can achieve human-level performance in this task. We used a\nmixed-methods process. Two Patient and Public Involvement groups (10 patients\nand 8 clinicians) in the United Kingdom participated in semi-structured\ninterviews exploring what personal and contextual information should be\nincluded in clinical summaries and how it should be structured for clinical\nuse. Findings informed annotation guidelines used by eight clinicians to create\ngold-standard PCS from 88 atrial fibrillation consultations. Sixteen\nconsultations were used to refine a prompt aligned with the guidelines. Five\nopen-source LLMs (Llama-3.2-3B, Llama-3.1-8B, Mistral-8B, Gemma-3-4B, and\nQwen3-8B) generated summaries for 72 consultations using zero-shot and few-shot\nprompting, evaluated with ROUGE-L, BERTScore, and qualitative metrics. Patients\nemphasized lifestyle routines, social support, recent stressors, and care\nvalues. Clinicians sought concise functional, psychosocial, and emotional\ncontext. The best zero-shot performance was achieved by Mistral-8B (ROUGE-L\n0.189) and Llama-3.1-8B (BERTScore 0.673); the best few-shot by Llama-3.1-8B\n(ROUGE-L 0.206, BERTScore 0.683). Completeness and fluency were similar between\nexperts and models, while correctness and patient-centeredness favored human\nPCS."
                },
                "authors": [
                    {
                        "name": "Maria Lizarazo Jimenez"
                    },
                    {
                        "name": "Ana Gabriela Claros"
                    },
                    {
                        "name": "Kieran Green"
                    },
                    {
                        "name": "David Toro-Tobon"
                    },
                    {
                        "name": "Felipe Larios"
                    },
                    {
                        "name": "Sheena Asthana"
                    },
                    {
                        "name": "Camila Wenczenovicz"
                    },
                    {
                        "name": "Kerly Guevara Maldonado"
                    },
                    {
                        "name": "Luis Vilatuna-Andrango"
                    },
                    {
                        "name": "Cristina Proano-Velez"
                    },
                    {
                        "name": "Satya Sai Sri Bandi"
                    },
                    {
                        "name": "Shubhangi Bagewadi"
                    },
                    {
                        "name": "Megan E. Branda"
                    },
                    {
                        "name": "Misk Al Zahidy"
                    },
                    {
                        "name": "Saturnino Luz"
                    },
                    {
                        "name": "Mirella Lapata"
                    },
                    {
                        "name": "Juan P. Brito"
                    },
                    {
                        "name": "Oscar J. Ponce-Ponte"
                    }
                ],
                "author_detail": {
                    "name": "Oscar J. Ponce-Ponte"
                },
                "author": "Oscar J. Ponce-Ponte",
                "arxiv_comment": "The first two listed authors contributed equally Pages: 21;\n  Figures:2; Tables:3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27527v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27527v1",
                "updated": "2025-10-31T14:57:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    57,
                    16,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T14:57:16Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    57,
                    16,
                    4,
                    304,
                    0
                ],
                "title": "TetraJet-v2: Accurate NVFP4 Training for Large Language Models with\n  Oscillation Suppression and Outlier Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TetraJet-v2: Accurate NVFP4 Training for Large Language Models with\n  Oscillation Suppression and Outlier Control"
                },
                "summary": "Large Language Models (LLMs) training is prohibitively expensive, driving\ninterest in low-precision fully-quantized training (FQT). While novel 4-bit\nformats like NVFP4 offer substantial efficiency gains, achieving near-lossless\ntraining at such low precision remains challenging. We introduce TetraJet-v2,\nan end-to-end 4-bit FQT method that leverages NVFP4 for activations, weights,\nand gradients in all linear layers. We identify two critical issues hindering\nlow-precision LLM training: weight oscillation and outliers. To address these,\nwe propose: 1) an unbiased double-block quantization method for NVFP4 linear\nlayers, 2) OsciReset, an algorithm to suppress weight oscillation, and 3)\nOutControl, an algorithm to retain outlier accuracy. TetraJet-v2 consistently\noutperforms prior FP4 training methods on pre-training LLMs across varying\nmodel sizes up to 370M and data sizes up to 200B tokens, reducing the\nperformance gap to full-precision training by an average of 51.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) training is prohibitively expensive, driving\ninterest in low-precision fully-quantized training (FQT). While novel 4-bit\nformats like NVFP4 offer substantial efficiency gains, achieving near-lossless\ntraining at such low precision remains challenging. We introduce TetraJet-v2,\nan end-to-end 4-bit FQT method that leverages NVFP4 for activations, weights,\nand gradients in all linear layers. We identify two critical issues hindering\nlow-precision LLM training: weight oscillation and outliers. To address these,\nwe propose: 1) an unbiased double-block quantization method for NVFP4 linear\nlayers, 2) OsciReset, an algorithm to suppress weight oscillation, and 3)\nOutControl, an algorithm to retain outlier accuracy. TetraJet-v2 consistently\noutperforms prior FP4 training methods on pre-training LLMs across varying\nmodel sizes up to 370M and data sizes up to 200B tokens, reducing the\nperformance gap to full-precision training by an average of 51.3%."
                },
                "authors": [
                    {
                        "name": "Yuxiang Chen"
                    },
                    {
                        "name": "Xiaoming Xu"
                    },
                    {
                        "name": "Pengle Zhang"
                    },
                    {
                        "name": "Michael Beyer"
                    },
                    {
                        "name": "Martin Rapp"
                    },
                    {
                        "name": "Jun Zhu"
                    },
                    {
                        "name": "Jianfei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Chen"
                },
                "author": "Jianfei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27527v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27527v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12989v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12989v3",
                "updated": "2025-10-31T14:56:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    56,
                    12,
                    4,
                    304,
                    0
                ],
                "published": "2025-03-17T09:44:50Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    9,
                    44,
                    50,
                    0,
                    76,
                    0
                ],
                "title": "A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation\n  Classification Using Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Stage Framework with Taxonomy-Guided Reasoning for Occupation\n  Classification Using Large Language Models"
                },
                "summary": "Automatically annotating job data with standardized occupations from\ntaxonomies, known as occupation classification, is crucial for labor market\nanalysis. However, this task is often hindered by data scarcity and the\nchallenges of manual annotations. While large language models (LLMs) hold\npromise due to their extensive world knowledge and in-context learning\ncapabilities, their effectiveness depends on their knowledge of occupational\ntaxonomies, which remains unclear. In this study, we assess the ability of LLMs\nto generate precise taxonomic entities from taxonomy, highlighting their\nlimitations, especially for smaller models. To address these challenges, we\npropose a multi-stage framework consisting of inference, retrieval, and\nreranking stages, which integrates taxonomy-guided reasoning examples to\nenhance performance by aligning outputs with taxonomic knowledge. Evaluations\non a large-scale dataset show that our framework not only enhances occupation\nand skill classification tasks, but also provides a cost-effective alternative\nto frontier models like GPT-4o, significantly reducing computational costs\nwhile maintaining strong performance. This makes it a practical and scalable\nsolution for occupation classification and related tasks across LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatically annotating job data with standardized occupations from\ntaxonomies, known as occupation classification, is crucial for labor market\nanalysis. However, this task is often hindered by data scarcity and the\nchallenges of manual annotations. While large language models (LLMs) hold\npromise due to their extensive world knowledge and in-context learning\ncapabilities, their effectiveness depends on their knowledge of occupational\ntaxonomies, which remains unclear. In this study, we assess the ability of LLMs\nto generate precise taxonomic entities from taxonomy, highlighting their\nlimitations, especially for smaller models. To address these challenges, we\npropose a multi-stage framework consisting of inference, retrieval, and\nreranking stages, which integrates taxonomy-guided reasoning examples to\nenhance performance by aligning outputs with taxonomic knowledge. Evaluations\non a large-scale dataset show that our framework not only enhances occupation\nand skill classification tasks, but also provides a cost-effective alternative\nto frontier models like GPT-4o, significantly reducing computational costs\nwhile maintaining strong performance. This makes it a practical and scalable\nsolution for occupation classification and related tasks across LLMs."
                },
                "authors": [
                    {
                        "name": "Palakorn Achananuparp"
                    },
                    {
                        "name": "Ee-Peng Lim"
                    },
                    {
                        "name": "Yao Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Lu"
                },
                "author": "Yao Lu",
                "arxiv_comment": "Accepted to ICWSM'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12989v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12989v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27521v1",
                "updated": "2025-10-31T14:47:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    47,
                    11,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T14:47:11Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    47,
                    11,
                    4,
                    304,
                    0
                ],
                "title": "Independent Clinical Evaluation of General-Purpose LLM Responses to\n  Signals of Suicide Risk",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Independent Clinical Evaluation of General-Purpose LLM Responses to\n  Signals of Suicide Risk"
                },
                "summary": "We introduce findings and methods to facilitate evidence-based discussion\nabout how large language models (LLMs) should behave in response to user\nsignals of risk of suicidal thoughts and behaviors (STB). People are already\nusing LLMs as mental health resources, and several recent incidents implicate\nLLMs in mental health crises. Despite growing attention, few studies have been\nable to effectively generalize clinical guidelines to LLM use cases, and fewer\nstill have proposed methodologies that can be iteratively applied as knowledge\nimproves about the elements of human-AI interaction most in need of study. We\nintroduce an assessment of LLM alignment with guidelines for ethical\ncommunication, adapted from clinical principles and applied to expressions of\nrisk factors for STB in multi-turn conversations. Using a codebook created and\nvalidated by clinicians, mobilizing the volunteer participation of practicing\ntherapists and trainees (N=43) based in the U.S., and using generalized linear\nmixed-effects models for statistical analysis, we assess a single fully\nopen-source LLM, OLMo-2-32b. We show how to assess when a model deviates from\nclinically informed guidelines in a way that may pose a hazard and (thanks to\nits open nature) facilitates future investigation as to why. We find that\ncontrary to clinical best practice, OLMo-2-32b, and, possibly by extension,\nother LLMs, will become less likely to invite continued dialog as users send\nmore signals of STB risk in multi-turn settings. We also show that OLMo-2-32b\nresponds differently depending on the risk factor expressed. This empirical\nevidence highlights that just as chatbots pose hazards if their responses\nreinforce delusions or assist in suicidal acts, they may also discourage\nfurther help-seeking or cause feelings of dismissal or abandonment by\nwithdrawing from conversations when STB risk is expressed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce findings and methods to facilitate evidence-based discussion\nabout how large language models (LLMs) should behave in response to user\nsignals of risk of suicidal thoughts and behaviors (STB). People are already\nusing LLMs as mental health resources, and several recent incidents implicate\nLLMs in mental health crises. Despite growing attention, few studies have been\nable to effectively generalize clinical guidelines to LLM use cases, and fewer\nstill have proposed methodologies that can be iteratively applied as knowledge\nimproves about the elements of human-AI interaction most in need of study. We\nintroduce an assessment of LLM alignment with guidelines for ethical\ncommunication, adapted from clinical principles and applied to expressions of\nrisk factors for STB in multi-turn conversations. Using a codebook created and\nvalidated by clinicians, mobilizing the volunteer participation of practicing\ntherapists and trainees (N=43) based in the U.S., and using generalized linear\nmixed-effects models for statistical analysis, we assess a single fully\nopen-source LLM, OLMo-2-32b. We show how to assess when a model deviates from\nclinically informed guidelines in a way that may pose a hazard and (thanks to\nits open nature) facilitates future investigation as to why. We find that\ncontrary to clinical best practice, OLMo-2-32b, and, possibly by extension,\nother LLMs, will become less likely to invite continued dialog as users send\nmore signals of STB risk in multi-turn settings. We also show that OLMo-2-32b\nresponds differently depending on the risk factor expressed. This empirical\nevidence highlights that just as chatbots pose hazards if their responses\nreinforce delusions or assist in suicidal acts, they may also discourage\nfurther help-seeking or cause feelings of dismissal or abandonment by\nwithdrawing from conversations when STB risk is expressed."
                },
                "authors": [
                    {
                        "name": "Nick Judd"
                    },
                    {
                        "name": "Alexandre Vaz"
                    },
                    {
                        "name": "Kevin Paeth"
                    },
                    {
                        "name": "Layla Ins Davis"
                    },
                    {
                        "name": "Milena Esherick"
                    },
                    {
                        "name": "Jason Brand"
                    },
                    {
                        "name": "Ins Amaro"
                    },
                    {
                        "name": "Tony Rousmaniere"
                    }
                ],
                "author_detail": {
                    "name": "Tony Rousmaniere"
                },
                "author": "Tony Rousmaniere",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19500v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19500v2",
                "updated": "2025-10-31T14:24:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    24,
                    22,
                    4,
                    304,
                    0
                ],
                "published": "2025-06-24T10:39:07Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    10,
                    39,
                    7,
                    1,
                    175,
                    0
                ],
                "title": "NaviAgent: Bilevel Planning on Tool Navigation Graph for Large-Scale\n  Orchestration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NaviAgent: Bilevel Planning on Tool Navigation Graph for Large-Scale\n  Orchestration"
                },
                "summary": "Large language models (LLMs) have recently demonstrated the ability to act as\nfunction call agents by invoking external tools, enabling them to solve tasks\nbeyond their static knowledge. However, existing agents typically call tools\nstep by step at a time without a global view of task structure. As tools depend\non each other, this leads to error accumulation and limited scalability,\nparticularly when scaling to thousands of tools. To address these limitations,\nwe propose NaviAgent, a novel bilevel architecture that decouples task planning\nfrom tool execution through graph-based modeling of the tool ecosystem. At the\ntask-planning level, the LLM-based agent decides whether to respond directly,\nclarify user intent, invoke a toolchain, or execute tool outputs, ensuring\nbroad coverage of interaction scenarios independent of inter-tool complexity.\nAt the execution level, a continuously evolving Tool World Navigation Model\n(TWNM) encodes structural and behavioral relations among tools, guiding the\nagent to generate scalable and robust invocation sequences. By incorporating\nfeedback from real tool interactions, NaviAgent supports closed-loop\noptimization of planning and execution, moving beyond tool calling toward\nadaptive navigation of large-scale tool ecosystems. Experiments show that\nNaviAgent achieves the best task success rates across models and tasks, and\nintegrating TWMN further boosts performance by up to 17 points on complex\ntasks, underscoring its key role in toolchain orchestration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently demonstrated the ability to act as\nfunction call agents by invoking external tools, enabling them to solve tasks\nbeyond their static knowledge. However, existing agents typically call tools\nstep by step at a time without a global view of task structure. As tools depend\non each other, this leads to error accumulation and limited scalability,\nparticularly when scaling to thousands of tools. To address these limitations,\nwe propose NaviAgent, a novel bilevel architecture that decouples task planning\nfrom tool execution through graph-based modeling of the tool ecosystem. At the\ntask-planning level, the LLM-based agent decides whether to respond directly,\nclarify user intent, invoke a toolchain, or execute tool outputs, ensuring\nbroad coverage of interaction scenarios independent of inter-tool complexity.\nAt the execution level, a continuously evolving Tool World Navigation Model\n(TWNM) encodes structural and behavioral relations among tools, guiding the\nagent to generate scalable and robust invocation sequences. By incorporating\nfeedback from real tool interactions, NaviAgent supports closed-loop\noptimization of planning and execution, moving beyond tool calling toward\nadaptive navigation of large-scale tool ecosystems. Experiments show that\nNaviAgent achieves the best task success rates across models and tasks, and\nintegrating TWMN further boosts performance by up to 17 points on complex\ntasks, underscoring its key role in toolchain orchestration."
                },
                "authors": [
                    {
                        "name": "Yan Jiang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "LiZhong GU"
                    },
                    {
                        "name": "Ai Han"
                    },
                    {
                        "name": "TianLong Li"
                    }
                ],
                "author_detail": {
                    "name": "TianLong Li"
                },
                "author": "TianLong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19500v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19500v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.21490v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.21490v2",
                "updated": "2025-10-31T14:16:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    16,
                    51,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-24T14:14:56Z",
                "published_parsed": [
                    2025,
                    10,
                    24,
                    14,
                    14,
                    56,
                    4,
                    297,
                    0
                ],
                "title": "Analysis and Synthesis of Switched Optimization Algorithms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis and Synthesis of Switched Optimization Algorithms"
                },
                "summary": "Deployment of optimization algorithms on networked systems face challenges\nassociated with time delays and corruptions. One particular instance is the\npresence of time-varying delays arising from factors such as packet drops and\nirregular sampling. Fixed time delays can destabilize gradient descent\nalgorithms, and this degradation is exacerbated by time-varying delays. This\nwork concentrates on the analysis and creation of discrete-time optimization\nalgorithms with certified exponential convergence rates that are robust against\nswitched uncertainties between the optimizer and the gradient oracle. These\noptimization algorithms are implemented by a switch-scheduled output feedback\ncontrollers. Rate variation and sawtooth behavior (packet drops) in\ntime-varying delays can be imposed through constraining switching sequences.\nAnalysis is accomplished by bisection in the convergence rate to find\nZames-Falb filter coefficents. Synthesis is performed by alternating between a\nfilter coefficient search for a fixed controller, and a controller search for\nfixed multipliers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment of optimization algorithms on networked systems face challenges\nassociated with time delays and corruptions. One particular instance is the\npresence of time-varying delays arising from factors such as packet drops and\nirregular sampling. Fixed time delays can destabilize gradient descent\nalgorithms, and this degradation is exacerbated by time-varying delays. This\nwork concentrates on the analysis and creation of discrete-time optimization\nalgorithms with certified exponential convergence rates that are robust against\nswitched uncertainties between the optimizer and the gradient oracle. These\noptimization algorithms are implemented by a switch-scheduled output feedback\ncontrollers. Rate variation and sawtooth behavior (packet drops) in\ntime-varying delays can be imposed through constraining switching sequences.\nAnalysis is accomplished by bisection in the convergence rate to find\nZames-Falb filter coefficents. Synthesis is performed by alternating between a\nfilter coefficient search for a fixed controller, and a controller search for\nfixed multipliers."
                },
                "authors": [
                    {
                        "name": "Jared Miller"
                    },
                    {
                        "name": "Fabian Jakob"
                    },
                    {
                        "name": "Carsten Scherer"
                    },
                    {
                        "name": "Andrea Iannelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Iannelli"
                },
                "author": "Andrea Iannelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.21490v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.21490v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27489v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27489v1",
                "updated": "2025-10-31T14:07:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    7,
                    42,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T14:07:42Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    7,
                    42,
                    4,
                    304,
                    0
                ],
                "title": "Auditing LLM Editorial Bias in News Media Exposure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing LLM Editorial Bias in News Media Exposure"
                },
                "summary": "Large Language Models (LLMs) increasingly act as gateways to web content,\nshaping how millions of users encounter online information. Unlike traditional\nsearch engines, whose retrieval and ranking mechanisms are well studied, the\nselection processes of web-connected LLMs add layers of opacity to how answers\nare generated. By determining which news outlets users see, these systems can\ninfluence public opinion, reinforce echo chambers, and pose risks to civic\ndiscourse and public trust.\n  This work extends two decades of research in algorithmic auditing to examine\nhow LLMs function as news engines. We present the first audit comparing three\nleading agents, GPT-4o-Mini, Claude-3.7-Sonnet, and Gemini-2.0-Flash, against\nGoogle News, asking: \\textit{How do LLMs differ from traditional aggregators in\nthe diversity, ideology, and reliability of the media they expose to users?}\n  Across 24 global topics, we find that, compared to Google News, LLMs surface\nsignificantly fewer unique outlets and allocate attention more unevenly. In the\nsame way, GPT-4o-Mini emphasizes more factual and right-leaning sources;\nClaude-3.7-Sonnet favors institutional and civil-society domains and slightly\namplifies right-leaning exposure; and Gemini-2.0-Flash exhibits a modest\nleft-leaning tilt without significant changes in factuality. These patterns\nremain robust under prompt variations and alternative reliability benchmarks.\nTogether, our findings show that LLMs already enact \\textit{agentic editorial\npolicies}, curating information in ways that diverge from conventional\naggregators. Understanding and governing their emerging editorial power will be\ncritical for ensuring transparency, pluralism, and trust in digital information\necosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly act as gateways to web content,\nshaping how millions of users encounter online information. Unlike traditional\nsearch engines, whose retrieval and ranking mechanisms are well studied, the\nselection processes of web-connected LLMs add layers of opacity to how answers\nare generated. By determining which news outlets users see, these systems can\ninfluence public opinion, reinforce echo chambers, and pose risks to civic\ndiscourse and public trust.\n  This work extends two decades of research in algorithmic auditing to examine\nhow LLMs function as news engines. We present the first audit comparing three\nleading agents, GPT-4o-Mini, Claude-3.7-Sonnet, and Gemini-2.0-Flash, against\nGoogle News, asking: \\textit{How do LLMs differ from traditional aggregators in\nthe diversity, ideology, and reliability of the media they expose to users?}\n  Across 24 global topics, we find that, compared to Google News, LLMs surface\nsignificantly fewer unique outlets and allocate attention more unevenly. In the\nsame way, GPT-4o-Mini emphasizes more factual and right-leaning sources;\nClaude-3.7-Sonnet favors institutional and civil-society domains and slightly\namplifies right-leaning exposure; and Gemini-2.0-Flash exhibits a modest\nleft-leaning tilt without significant changes in factuality. These patterns\nremain robust under prompt variations and alternative reliability benchmarks.\nTogether, our findings show that LLMs already enact \\textit{agentic editorial\npolicies}, curating information in ways that diverge from conventional\naggregators. Understanding and governing their emerging editorial power will be\ncritical for ensuring transparency, pluralism, and trust in digital information\necosystems."
                },
                "authors": [
                    {
                        "name": "Marco Minici"
                    },
                    {
                        "name": "Cristian Consonni"
                    },
                    {
                        "name": "Federico Cinus"
                    },
                    {
                        "name": "Giuseppe Manco"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Manco"
                },
                "author": "Giuseppe Manco",
                "arxiv_comment": "Under Peer Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27489v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27489v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16184v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16184v2",
                "updated": "2025-10-31T14:03:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    3,
                    49,
                    4,
                    304,
                    0
                ],
                "published": "2025-07-22T02:54:45Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    2,
                    54,
                    45,
                    1,
                    203,
                    0
                ],
                "title": "Emergent Cognitive Convergence via Implementation: A Structured Loop\n  Reflecting Four Theories of Mind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Cognitive Convergence via Implementation: A Structured Loop\n  Reflecting Four Theories of Mind"
                },
                "summary": "We report a structural convergence among four influential theories of mind:\nKahneman's dual-system theory, Friston's predictive processing, Minsky's\nsociety of mind, and Clark's extended mind, emerging unintentionally within a\npractical AI architecture known as Agentic Flow. Designed to address the\nlimitations of large language models (LLMs), Agentic Flow comprises five\ninterlocking modules: Retrieval, Cognition, Control, Memory, and Action,\norganized into a repeatable cognitive loop. Although originally inspired only\nby Minsky and Clark, subsequent analysis revealed that its structure echoes\ncomputational motifs from all four theories, suggesting that theoretical\nconvergence can emerge naturally from implementation demands rather than\ndeliberate synthesis. Controlled evaluations confirmed this: the structured\nagent achieved 95.8% task success versus 62.3% for baseline LLMs, demonstrating\nrobust constraint adherence and reproducible reasoning. We describe this\nconvergence under a broader descriptive meta-architecture called PEACE,\nhighlighting recurring design patterns such as predictive modeling, associative\nrecall, and error-sensitive control. Later formalized as the Structured\nCognitive Loop (SCL), this framework generalizes the same principles as a\nfoundation for behavioral intelligence in LLM-based agents. Rather than\nclaiming theoretical unification, this paper proposes that intelligent\narchitectures may evolve toward shared structural patterns shaped by practical\nconstraints. As a position paper, it aims to frame this convergence as an\ninterpretive reflection rather than a finalized theory, inviting further\ntheoretical and experimental dialogue. Agentic Flow, or equivalently the\nStructured Cognitive Loop, thus offers a glimpse of how a unified cognitive\nform can arise not from abstraction, but from the necessities of real-world\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report a structural convergence among four influential theories of mind:\nKahneman's dual-system theory, Friston's predictive processing, Minsky's\nsociety of mind, and Clark's extended mind, emerging unintentionally within a\npractical AI architecture known as Agentic Flow. Designed to address the\nlimitations of large language models (LLMs), Agentic Flow comprises five\ninterlocking modules: Retrieval, Cognition, Control, Memory, and Action,\norganized into a repeatable cognitive loop. Although originally inspired only\nby Minsky and Clark, subsequent analysis revealed that its structure echoes\ncomputational motifs from all four theories, suggesting that theoretical\nconvergence can emerge naturally from implementation demands rather than\ndeliberate synthesis. Controlled evaluations confirmed this: the structured\nagent achieved 95.8% task success versus 62.3% for baseline LLMs, demonstrating\nrobust constraint adherence and reproducible reasoning. We describe this\nconvergence under a broader descriptive meta-architecture called PEACE,\nhighlighting recurring design patterns such as predictive modeling, associative\nrecall, and error-sensitive control. Later formalized as the Structured\nCognitive Loop (SCL), this framework generalizes the same principles as a\nfoundation for behavioral intelligence in LLM-based agents. Rather than\nclaiming theoretical unification, this paper proposes that intelligent\narchitectures may evolve toward shared structural patterns shaped by practical\nconstraints. As a position paper, it aims to frame this convergence as an\ninterpretive reflection rather than a finalized theory, inviting further\ntheoretical and experimental dialogue. Agentic Flow, or equivalently the\nStructured Cognitive Loop, thus offers a glimpse of how a unified cognitive\nform can arise not from abstraction, but from the necessities of real-world\nreasoning."
                },
                "authors": [
                    {
                        "name": "Myung Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Myung Ho Kim"
                },
                "author": "Myung Ho Kim",
                "arxiv_comment": "This version relocates the \"Position Paper\" designation from the\n  title to the abstract and adds a citation to the related follow-up study\n  Structured Cognition for Behavioral Intelligence in Large Language Model\n  Agents (Kim, 2025), also available on arXiv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16184v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16184v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27484v1",
                "updated": "2025-10-31T14:02:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    2,
                    37,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T14:02:37Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    14,
                    2,
                    37,
                    4,
                    304,
                    0
                ],
                "title": "Thought Branches: Interpreting LLM Reasoning Requires Resampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thought Branches: Interpreting LLM Reasoning Requires Resampling"
                },
                "summary": "Most work interpreting reasoning models studies only a single\nchain-of-thought (CoT), yet these models define distributions over many\npossible CoTs. We argue that studying a single sample is inadequate for\nunderstanding causal influence and the underlying computation. Though fully\nspecifying this distribution is intractable, it can be understood by sampling.\nWe present case studies using resampling to investigate model decisions. First,\nwhen a model states a reason for its action, does that reason actually cause\nthe action? In \"agentic misalignment\" scenarios, we resample specific sentences\nto measure their downstream effects. Self-preservation sentences have small\ncausal impact, suggesting they do not meaningfully drive blackmail. Second, are\nartificial edits to CoT sufficient for steering reasoning? These are common in\nliterature, yet take the model off-policy. Resampling and selecting a\ncompletion with the desired property is a principled on-policy alternative. We\nfind off-policy interventions yield small and unstable effects compared to\nresampling in decision-making tasks. Third, how do we understand the effect of\nremoving a reasoning step when the model may repeat it post-edit? We introduce\na resilience metric that repeatedly resamples to prevent similar content from\nreappearing downstream. Critical planning statements resist removal but have\nlarge effects when eliminated. Fourth, since CoT is sometimes \"unfaithful\", can\nour methods teach us anything in these settings? Adapting causal mediation\nanalysis, we find that hints that have a causal effect on the output without\nbeing explicitly mentioned exert a subtle and cumulative influence on the CoT\nthat persists even if the hint is removed. Overall, studying distributions via\nresampling enables reliable causal analysis, clearer narratives of model\nreasoning, and principled CoT interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most work interpreting reasoning models studies only a single\nchain-of-thought (CoT), yet these models define distributions over many\npossible CoTs. We argue that studying a single sample is inadequate for\nunderstanding causal influence and the underlying computation. Though fully\nspecifying this distribution is intractable, it can be understood by sampling.\nWe present case studies using resampling to investigate model decisions. First,\nwhen a model states a reason for its action, does that reason actually cause\nthe action? In \"agentic misalignment\" scenarios, we resample specific sentences\nto measure their downstream effects. Self-preservation sentences have small\ncausal impact, suggesting they do not meaningfully drive blackmail. Second, are\nartificial edits to CoT sufficient for steering reasoning? These are common in\nliterature, yet take the model off-policy. Resampling and selecting a\ncompletion with the desired property is a principled on-policy alternative. We\nfind off-policy interventions yield small and unstable effects compared to\nresampling in decision-making tasks. Third, how do we understand the effect of\nremoving a reasoning step when the model may repeat it post-edit? We introduce\na resilience metric that repeatedly resamples to prevent similar content from\nreappearing downstream. Critical planning statements resist removal but have\nlarge effects when eliminated. Fourth, since CoT is sometimes \"unfaithful\", can\nour methods teach us anything in these settings? Adapting causal mediation\nanalysis, we find that hints that have a causal effect on the output without\nbeing explicitly mentioned exert a subtle and cumulative influence on the CoT\nthat persists even if the hint is removed. Overall, studying distributions via\nresampling enables reliable causal analysis, clearer narratives of model\nreasoning, and principled CoT interventions."
                },
                "authors": [
                    {
                        "name": "Uzay Macar"
                    },
                    {
                        "name": "Paul C. Bogdan"
                    },
                    {
                        "name": "Senthooran Rajamanoharan"
                    },
                    {
                        "name": "Neel Nanda"
                    }
                ],
                "author_detail": {
                    "name": "Neel Nanda"
                },
                "author": "Neel Nanda",
                "arxiv_comment": "Uzay Macar and Paul C. Bogdan contributed equally to this work, and\n  their listed order was determined by coinflip",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27476v1",
                "updated": "2025-10-31T13:55:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    13,
                    55,
                    48,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T13:55:48Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    13,
                    55,
                    48,
                    4,
                    304,
                    0
                ],
                "title": "Inverse-Designed Grating Couplers with Tunable Wavelength via Scaling\n  and Biasing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse-Designed Grating Couplers with Tunable Wavelength via Scaling\n  and Biasing"
                },
                "summary": "Photonic integrated circuits are heavily researched devices for\ntelecommunication, biosensing, and quantum technologies. Wafer-scale\nfabrication and testing are crucial for reducing costs and enabling large-scale\ndeployment. Grating couplers allow non-invasive measurements before packaging,\nbut classical designs rely on long tapers and narrow bandwidths. In this work,\nwe present compact, inverse-designed grating couplers with broadband\ntransmission. We optimized and fabricated arrays of devices and characterized\nthem with a 4f-scanning setup. The nominal design reached simulated\nefficiencies of 52 %, while measurements confirmed robust performance with up\nto 32 % efficiency at the target 1540 nm wavelength and 46 % at shifted\nwavelengths. Without scaling and contour biasing, the measured efficiency at\nthe target wavelength drops to only 4.4 %. Thus, a key finding is that\nsystematic scaling and edge biasing recover up to an eightfold improvement in\nefficiency. These inverse-designed grating couplers can be efficiently\ncorrected post-design, enabling reliable performance despite fabrication\ndeviations. This approach allows simple layout adjustments to compensate for\nprocess-induced variations, supporting wafer-scale testing, cryogenic photonic\napplications, and rapid design wavelength tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photonic integrated circuits are heavily researched devices for\ntelecommunication, biosensing, and quantum technologies. Wafer-scale\nfabrication and testing are crucial for reducing costs and enabling large-scale\ndeployment. Grating couplers allow non-invasive measurements before packaging,\nbut classical designs rely on long tapers and narrow bandwidths. In this work,\nwe present compact, inverse-designed grating couplers with broadband\ntransmission. We optimized and fabricated arrays of devices and characterized\nthem with a 4f-scanning setup. The nominal design reached simulated\nefficiencies of 52 %, while measurements confirmed robust performance with up\nto 32 % efficiency at the target 1540 nm wavelength and 46 % at shifted\nwavelengths. Without scaling and contour biasing, the measured efficiency at\nthe target wavelength drops to only 4.4 %. Thus, a key finding is that\nsystematic scaling and edge biasing recover up to an eightfold improvement in\nefficiency. These inverse-designed grating couplers can be efficiently\ncorrected post-design, enabling reliable performance despite fabrication\ndeviations. This approach allows simple layout adjustments to compensate for\nprocess-induced variations, supporting wafer-scale testing, cryogenic photonic\napplications, and rapid design wavelength tuning."
                },
                "authors": [
                    {
                        "name": "Lorenz J. J. Sauerzopf"
                    },
                    {
                        "name": "Fabian Becker"
                    },
                    {
                        "name": "Kai Mller"
                    }
                ],
                "author_detail": {
                    "name": "Kai Mller"
                },
                "author": "Kai Mller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05831v2",
                "updated": "2025-10-31T13:46:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    13,
                    46,
                    40,
                    4,
                    304,
                    0
                ],
                "published": "2025-09-06T21:05:18Z",
                "published_parsed": [
                    2025,
                    9,
                    6,
                    21,
                    5,
                    18,
                    5,
                    249,
                    0
                ],
                "title": "Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in\n  Web Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Latent Attack Surfaces in LLMs: Prompt Injection via HTML in\n  Web Summarization"
                },
                "summary": "Large Language Models (LLMs) are increasingly integrated into web-based\nsystems for content summarization, yet their susceptibility to prompt injection\nattacks remains a pressing concern. In this study, we explore how non-visible\nHTML elements such as <meta>, aria-label, and alt attributes can be exploited\nto embed adversarial instructions without altering the visible content of a\nwebpage. We introduce a novel dataset comprising 280 static web pages, evenly\ndivided between clean and adversarial injected versions, crafted using diverse\nHTML-based strategies. These pages are processed through a browser automation\npipeline to extract both raw HTML and rendered text, closely mimicking\nreal-world LLM deployment scenarios. We evaluate two state-of-the-art\nopen-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their\nability to summarize this content. Using both lexical (ROUGE-L) and semantic\n(SBERT cosine similarity) metrics, along with manual annotations, we assess the\nimpact of these covert injections. Our findings reveal that over 29% of\ninjected samples led to noticeable changes in the Llama 4 Scout summaries,\nwhile Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These\nresults highlight a critical and largely overlooked vulnerability in LLM driven\nweb pipelines, where hidden adversarial content can subtly manipulate model\noutputs. Our work offers a reproducible framework and benchmark for evaluating\nHTML-based prompt injection and underscores the urgent need for robust\nmitigation strategies in LLM applications involving web content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly integrated into web-based\nsystems for content summarization, yet their susceptibility to prompt injection\nattacks remains a pressing concern. In this study, we explore how non-visible\nHTML elements such as <meta>, aria-label, and alt attributes can be exploited\nto embed adversarial instructions without altering the visible content of a\nwebpage. We introduce a novel dataset comprising 280 static web pages, evenly\ndivided between clean and adversarial injected versions, crafted using diverse\nHTML-based strategies. These pages are processed through a browser automation\npipeline to extract both raw HTML and rendered text, closely mimicking\nreal-world LLM deployment scenarios. We evaluate two state-of-the-art\nopen-source models, Llama 4 Scout (Meta) and Gemma 9B IT (Google), on their\nability to summarize this content. Using both lexical (ROUGE-L) and semantic\n(SBERT cosine similarity) metrics, along with manual annotations, we assess the\nimpact of these covert injections. Our findings reveal that over 29% of\ninjected samples led to noticeable changes in the Llama 4 Scout summaries,\nwhile Gemma 9B IT showed a lower, yet non-trivial, success rate of 15%. These\nresults highlight a critical and largely overlooked vulnerability in LLM driven\nweb pipelines, where hidden adversarial content can subtly manipulate model\noutputs. Our work offers a reproducible framework and benchmark for evaluating\nHTML-based prompt injection and underscores the urgent need for robust\nmitigation strategies in LLM applications involving web content."
                },
                "authors": [
                    {
                        "name": "Ishaan Verma"
                    }
                ],
                "author_detail": {
                    "name": "Ishaan Verma"
                },
                "author": "Ishaan Verma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27469v1",
                "updated": "2025-10-31T13:41:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    13,
                    41,
                    30,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T13:41:30Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    13,
                    41,
                    30,
                    4,
                    304,
                    0
                ],
                "title": "Diffuse Thinking: Exploring Diffusion Language Models as Efficient\n  Thought Proposers for Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffuse Thinking: Exploring Diffusion Language Models as Efficient\n  Thought Proposers for Reasoning"
                },
                "summary": "In recent years, large language models (LLMs) have witnessed remarkable\nadvancements, with the test-time scaling law consistently enhancing the\nreasoning capabilities. Through systematic evaluation and exploration of a\ndiverse spectrum of intermediate thoughts, LLMs demonstrate the potential to\ngenerate deliberate reasoning steps, thereby substantially enhancing reasoning\naccuracy. However, LLMs' autoregressive generation paradigm results in\nreasoning performance scaling sub-optimally with test-time computation, often\nrequiring excessive computational overhead to propose thoughts while yielding\nonly marginal performance gains. In contrast, diffusion language models (DLMs)\ncan efficiently produce diverse samples through parallel denoising in a single\nforward pass, inspiring us to leverage them for proposing intermediate\nthoughts, thereby alleviating the computational burden associated with\nautoregressive generation while maintaining quality. In this work, we propose\nan efficient collaborative reasoning framework, leveraging DLMs to generate\ncandidate thoughts and LLMs to evaluate their quality. Experiments across\ndiverse benchmarks demonstrate that our framework achieves strong performance\nin complex reasoning tasks, offering a promising direction for future research.\nOur code is open-source at\nhttps://anonymous.4open.science/r/Diffuse-Thinking-EC60.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have witnessed remarkable\nadvancements, with the test-time scaling law consistently enhancing the\nreasoning capabilities. Through systematic evaluation and exploration of a\ndiverse spectrum of intermediate thoughts, LLMs demonstrate the potential to\ngenerate deliberate reasoning steps, thereby substantially enhancing reasoning\naccuracy. However, LLMs' autoregressive generation paradigm results in\nreasoning performance scaling sub-optimally with test-time computation, often\nrequiring excessive computational overhead to propose thoughts while yielding\nonly marginal performance gains. In contrast, diffusion language models (DLMs)\ncan efficiently produce diverse samples through parallel denoising in a single\nforward pass, inspiring us to leverage them for proposing intermediate\nthoughts, thereby alleviating the computational burden associated with\nautoregressive generation while maintaining quality. In this work, we propose\nan efficient collaborative reasoning framework, leveraging DLMs to generate\ncandidate thoughts and LLMs to evaluate their quality. Experiments across\ndiverse benchmarks demonstrate that our framework achieves strong performance\nin complex reasoning tasks, offering a promising direction for future research.\nOur code is open-source at\nhttps://anonymous.4open.science/r/Diffuse-Thinking-EC60."
                },
                "authors": [
                    {
                        "name": "Chenyang Shao"
                    },
                    {
                        "name": "Sijian Ren"
                    },
                    {
                        "name": "Fengli Xu"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04412v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04412v2",
                "updated": "2025-10-31T13:21:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    13,
                    21,
                    4,
                    4,
                    304,
                    0
                ],
                "published": "2025-08-06T12:56:54Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    12,
                    56,
                    54,
                    2,
                    218,
                    0
                ],
                "title": "Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents"
                },
                "summary": "Frontier LLMs only recently enabled serviceable, autonomous web agents. At\nthat, a model poses as an instantaneous domain model backend. Ought to suggest\ninteraction, it is consulted with a web-based task and respective application\nstate. The key problem lies in application state serialisation - referred to as\nsnapshot. State-of-the-art web agents are premised on grounded GUI snapshots,\ni.e., screenshots enhanced with visual cues. Not least to resemble human\nperception, but for images representing relatively cheap means of model input.\nLLM vision still lag behind code interpretation capabilities. DOM snapshots,\nwhich structurally resemble HTML, impose a desired alternative. Vast model\ninput token size, however, disables reliable implementation with web agents to\ndate. We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based\non a GPT-4o backend, we evaluate D2Snap on tasks sampled from the\nOnline-Mind2Web dataset. The success rate of D2Snap-downsampled DOM snapshots\n(67%) matches a grounded GUI snapshot baseline (65%) - within the same input\ntoken order of magnitude (1e3). Our best evaluated configurations - one token\norder above, but within the model's context window - outperform this baseline\nby 8%. Our evaluation, moreover, yields that DOM-inherent hierarchy embodies a\nstrong UI feature for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frontier LLMs only recently enabled serviceable, autonomous web agents. At\nthat, a model poses as an instantaneous domain model backend. Ought to suggest\ninteraction, it is consulted with a web-based task and respective application\nstate. The key problem lies in application state serialisation - referred to as\nsnapshot. State-of-the-art web agents are premised on grounded GUI snapshots,\ni.e., screenshots enhanced with visual cues. Not least to resemble human\nperception, but for images representing relatively cheap means of model input.\nLLM vision still lag behind code interpretation capabilities. DOM snapshots,\nwhich structurally resemble HTML, impose a desired alternative. Vast model\ninput token size, however, disables reliable implementation with web agents to\ndate. We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based\non a GPT-4o backend, we evaluate D2Snap on tasks sampled from the\nOnline-Mind2Web dataset. The success rate of D2Snap-downsampled DOM snapshots\n(67%) matches a grounded GUI snapshot baseline (65%) - within the same input\ntoken order of magnitude (1e3). Our best evaluated configurations - one token\norder above, but within the model's context window - outperform this baseline\nby 8%. Our evaluation, moreover, yields that DOM-inherent hierarchy embodies a\nstrong UI feature for LLMs."
                },
                "authors": [
                    {
                        "name": "Thassilo M. Schiepanski"
                    },
                    {
                        "name": "Nicholas Pil"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Pil"
                },
                "author": "Nicholas Pil",
                "arxiv_comment": "20 pages, LaTeX; repository URL updated, typos corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04412v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04412v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27462v1",
                "updated": "2025-10-31T13:19:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    13,
                    19,
                    24,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T13:19:24Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    13,
                    19,
                    24,
                    4,
                    304,
                    0
                ],
                "title": "VCORE: Variance-Controlled Optimization-based Reweighting for\n  Chain-of-Thought Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VCORE: Variance-Controlled Optimization-based Reweighting for\n  Chain-of-Thought Supervision"
                },
                "summary": "Supervised fine-tuning (SFT) on long chain-of-thought (CoT) trajectories has\nemerged as a crucial technique for enhancing the reasoning abilities of large\nlanguage models (LLMs). However, the standard cross-entropy loss treats all\ntokens equally, ignoring their heterogeneous contributions across a reasoning\ntrajectory. This uniform treatment leads to misallocated supervision and weak\ngeneralization, especially in complex, long-form reasoning tasks. To address\nthis, we introduce \\textbf{V}ariance-\\textbf{C}ontrolled\n\\textbf{O}ptimization-based \\textbf{RE}weighting (VCORE), a principled\nframework that reformulates CoT supervision as a constrained optimization\nproblem. By adopting an optimization-theoretic perspective, VCORE enables a\nprincipled and adaptive allocation of supervision across tokens, thereby\naligning the training objective more closely with the goal of robust reasoning\ngeneralization. Empirical evaluations demonstrate that VCORE consistently\noutperforms existing token reweighting methods. Across both in-domain and\nout-of-domain settings, VCORE achieves substantial performance gains on\nmathematical and coding benchmarks, using models from the Qwen3 series (4B, 8B,\n32B) and LLaMA-3.1-8B-Instruct. Moreover, we show that VCORE serves as a more\neffective initialization for subsequent reinforcement learning, establishing a\nstronger foundation for advancing the reasoning capabilities of LLMs. The Code\nwill be released at https://github.com/coder-gx/VCORE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) on long chain-of-thought (CoT) trajectories has\nemerged as a crucial technique for enhancing the reasoning abilities of large\nlanguage models (LLMs). However, the standard cross-entropy loss treats all\ntokens equally, ignoring their heterogeneous contributions across a reasoning\ntrajectory. This uniform treatment leads to misallocated supervision and weak\ngeneralization, especially in complex, long-form reasoning tasks. To address\nthis, we introduce \\textbf{V}ariance-\\textbf{C}ontrolled\n\\textbf{O}ptimization-based \\textbf{RE}weighting (VCORE), a principled\nframework that reformulates CoT supervision as a constrained optimization\nproblem. By adopting an optimization-theoretic perspective, VCORE enables a\nprincipled and adaptive allocation of supervision across tokens, thereby\naligning the training objective more closely with the goal of robust reasoning\ngeneralization. Empirical evaluations demonstrate that VCORE consistently\noutperforms existing token reweighting methods. Across both in-domain and\nout-of-domain settings, VCORE achieves substantial performance gains on\nmathematical and coding benchmarks, using models from the Qwen3 series (4B, 8B,\n32B) and LLaMA-3.1-8B-Instruct. Moreover, we show that VCORE serves as a more\neffective initialization for subsequent reinforcement learning, establishing a\nstronger foundation for advancing the reasoning capabilities of LLMs. The Code\nwill be released at https://github.com/coder-gx/VCORE."
                },
                "authors": [
                    {
                        "name": "Xuan Gong"
                    },
                    {
                        "name": "Senmiao Wang"
                    },
                    {
                        "name": "Hanbo Huang"
                    },
                    {
                        "name": "Ruoyu Sun"
                    },
                    {
                        "name": "Shiyu Liang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Liang"
                },
                "author": "Shiyu Liang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23875v2",
                "updated": "2025-10-31T13:06:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    13,
                    6,
                    2,
                    4,
                    304,
                    0
                ],
                "published": "2025-03-31T09:26:34Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    26,
                    34,
                    0,
                    90,
                    0
                ],
                "title": "GenSwarm: Scalable Multi-Robot Code-Policy Generation and Deployment via\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenSwarm: Scalable Multi-Robot Code-Policy Generation and Deployment via\n  Language Models"
                },
                "summary": "The development of control policies for multi-robot systems traditionally\nfollows a complex and labor-intensive process, often lacking the flexibility to\nadapt to dynamic tasks. This has motivated research on methods to automatically\ncreate control policies. However, these methods require iterative processes of\nmanually crafting and refining objective functions, thereby prolonging the\ndevelopment cycle. This work introduces \\textit{GenSwarm}, an end-to-end system\nthat leverages large language models to automatically generate and deploy\ncontrol policies for multi-robot tasks based on simple user instructions in\nnatural language. As a multi-language-agent system, GenSwarm achieves zero-shot\nlearning, enabling rapid adaptation to altered or unseen tasks. The white-box\nnature of the code policies ensures strong reproducibility and\ninterpretability. With its scalable software and hardware architectures,\nGenSwarm supports efficient policy deployment on both simulated and real-world\nmulti-robot systems, realizing an instruction-to-execution end-to-end\nfunctionality that could prove valuable for robotics specialists and\nnon-specialists alike.The code of the proposed GenSwarm system is available\nonline: https://github.com/WindyLab/GenSwarm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of control policies for multi-robot systems traditionally\nfollows a complex and labor-intensive process, often lacking the flexibility to\nadapt to dynamic tasks. This has motivated research on methods to automatically\ncreate control policies. However, these methods require iterative processes of\nmanually crafting and refining objective functions, thereby prolonging the\ndevelopment cycle. This work introduces \\textit{GenSwarm}, an end-to-end system\nthat leverages large language models to automatically generate and deploy\ncontrol policies for multi-robot tasks based on simple user instructions in\nnatural language. As a multi-language-agent system, GenSwarm achieves zero-shot\nlearning, enabling rapid adaptation to altered or unseen tasks. The white-box\nnature of the code policies ensures strong reproducibility and\ninterpretability. With its scalable software and hardware architectures,\nGenSwarm supports efficient policy deployment on both simulated and real-world\nmulti-robot systems, realizing an instruction-to-execution end-to-end\nfunctionality that could prove valuable for robotics specialists and\nnon-specialists alike.The code of the proposed GenSwarm system is available\nonline: https://github.com/WindyLab/GenSwarm."
                },
                "authors": [
                    {
                        "name": "Wenkang Ji"
                    },
                    {
                        "name": "Huaben Chen"
                    },
                    {
                        "name": "Mingyang Chen"
                    },
                    {
                        "name": "Guobin Zhu"
                    },
                    {
                        "name": "Lufeng Xu"
                    },
                    {
                        "name": "Roderich Gro"
                    },
                    {
                        "name": "Rui Zhou"
                    },
                    {
                        "name": "Ming Cao"
                    },
                    {
                        "name": "Shiyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Zhao"
                },
                "author": "Shiyu Zhao",
                "arxiv_comment": "This article has been accepted for publication in npj Robotics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22165v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22165v3",
                "updated": "2025-10-31T12:56:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    56,
                    41,
                    4,
                    304,
                    0
                ],
                "published": "2025-03-28T06:09:51Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    6,
                    9,
                    51,
                    4,
                    87,
                    0
                ],
                "title": "Landscape of Thoughts: Visualizing the Reasoning Process of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Landscape of Thoughts: Visualizing the Reasoning Process of Large\n  Language Models"
                },
                "summary": "Numerous applications of large language models (LLMs) rely on their ability\nto perform step-by-step reasoning. However, the reasoning behavior of LLMs\nremains poorly understood, posing challenges to research, development, and\nsafety. To address this gap, we introduce landscape of thoughts (LoT), the\nfirst landscape visualization tool to inspect the reasoning trajectories with\ncertain reasoning methods on any multi-choice dataset. We represent the textual\nstates in a trajectory as numerical features that quantify the states'\ndistances to the answer choices. These features are then visualized in\ntwo-dimensional plots using t-SNE. Qualitative and quantitative analysis with\nthe landscape of thoughts effectively distinguishes between strong and weak\nmodels, correct and incorrect answers, as well as different reasoning tasks. It\nalso uncovers undesirable reasoning patterns, such as low consistency and high\nuncertainty. Additionally, users can adapt LoT to a model that predicts the\nproperty they observe. We showcase this advantage by adapting LoT to a\nlightweight verifier that evaluates the correctness of trajectories.\nEmpirically, this verifier boosts the reasoning accuracy and the test-time\nscaling effect. The code is publicly available at:\nhttps://github.com/tmlr-group/landscape-of-thoughts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous applications of large language models (LLMs) rely on their ability\nto perform step-by-step reasoning. However, the reasoning behavior of LLMs\nremains poorly understood, posing challenges to research, development, and\nsafety. To address this gap, we introduce landscape of thoughts (LoT), the\nfirst landscape visualization tool to inspect the reasoning trajectories with\ncertain reasoning methods on any multi-choice dataset. We represent the textual\nstates in a trajectory as numerical features that quantify the states'\ndistances to the answer choices. These features are then visualized in\ntwo-dimensional plots using t-SNE. Qualitative and quantitative analysis with\nthe landscape of thoughts effectively distinguishes between strong and weak\nmodels, correct and incorrect answers, as well as different reasoning tasks. It\nalso uncovers undesirable reasoning patterns, such as low consistency and high\nuncertainty. Additionally, users can adapt LoT to a model that predicts the\nproperty they observe. We showcase this advantage by adapting LoT to a\nlightweight verifier that evaluates the correctness of trajectories.\nEmpirically, this verifier boosts the reasoning accuracy and the test-time\nscaling effect. The code is publicly available at:\nhttps://github.com/tmlr-group/landscape-of-thoughts."
                },
                "authors": [
                    {
                        "name": "Zhanke Zhou"
                    },
                    {
                        "name": "Zhaocheng Zhu"
                    },
                    {
                        "name": "Xuan Li"
                    },
                    {
                        "name": "Mikhail Galkin"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Sanmi Koyejo"
                    },
                    {
                        "name": "Jian Tang"
                    },
                    {
                        "name": "Bo Han"
                    }
                ],
                "author_detail": {
                    "name": "Bo Han"
                },
                "author": "Bo Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22165v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22165v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.01070v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.01070v2",
                "updated": "2025-10-31T12:55:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    55,
                    4,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-01T16:12:28Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    16,
                    12,
                    28,
                    2,
                    274,
                    0
                ],
                "title": "Eliciting Secret Knowledge from Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eliciting Secret Knowledge from Language Models"
                },
                "summary": "We study secret elicitation: discovering knowledge that an AI possesses but\ndoes not explicitly verbalize. As a testbed, we train three families of large\nlanguage models (LLMs) to possess specific knowledge that they apply downstream\nbut deny knowing when asked directly. For example, in one setting, we train an\nLLM to generate replies that are consistent with knowing the user is female,\nwhile denying this knowledge when asked directly. We then design various\nblack-box and white-box secret elicitation techniques and evaluate them based\non whether they can help an LLM auditor successfully guess the secret\nknowledge. Many of our techniques improve on simple baselines. Our most\neffective techniques (performing best in all settings) are based on prefill\nattacks, a black-box technique where the LLM reveals secret knowledge when\ngenerating a completion from a predefined prefix. Our white-box techniques\nbased on logit lens and sparse autoencoders (SAEs) also consistently increase\nthe success rate of the LLM auditor, but are less effective. We release our\nmodels and code, establishing a public benchmark for evaluating secret\nelicitation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study secret elicitation: discovering knowledge that an AI possesses but\ndoes not explicitly verbalize. As a testbed, we train three families of large\nlanguage models (LLMs) to possess specific knowledge that they apply downstream\nbut deny knowing when asked directly. For example, in one setting, we train an\nLLM to generate replies that are consistent with knowing the user is female,\nwhile denying this knowledge when asked directly. We then design various\nblack-box and white-box secret elicitation techniques and evaluate them based\non whether they can help an LLM auditor successfully guess the secret\nknowledge. Many of our techniques improve on simple baselines. Our most\neffective techniques (performing best in all settings) are based on prefill\nattacks, a black-box technique where the LLM reveals secret knowledge when\ngenerating a completion from a predefined prefix. Our white-box techniques\nbased on logit lens and sparse autoencoders (SAEs) also consistently increase\nthe success rate of the LLM auditor, but are less effective. We release our\nmodels and code, establishing a public benchmark for evaluating secret\nelicitation methods."
                },
                "authors": [
                    {
                        "name": "Bartosz Cywiski"
                    },
                    {
                        "name": "Emil Ryd"
                    },
                    {
                        "name": "Rowan Wang"
                    },
                    {
                        "name": "Senthooran Rajamanoharan"
                    },
                    {
                        "name": "Neel Nanda"
                    },
                    {
                        "name": "Arthur Conmy"
                    },
                    {
                        "name": "Samuel Marks"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Marks"
                },
                "author": "Samuel Marks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.01070v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.01070v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15827v2",
                "updated": "2025-10-31T12:46:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    46,
                    4,
                    4,
                    304,
                    0
                ],
                "published": "2025-04-22T12:18:26Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    12,
                    18,
                    26,
                    1,
                    112,
                    0
                ],
                "title": "DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with\n  Dual Optimizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with\n  Dual Optimizers"
                },
                "summary": "Existing machine unlearning (MU) approaches exhibit significant sensitivity\nto hyperparameters, requiring meticulous tuning that limits practical\ndeployment. In this work, we first empirically demonstrate the instability and\nsuboptimal performance of existing popular MU methods when deployed in\ndifferent scenarios. To address this issue, we propose Dual Optimizer\n(DualOptim), which incorporates adaptive learning rate and decoupled momentum\nfactors. Empirical and theoretical evidence demonstrates that DualOptim\ncontributes to effective and stable unlearning. Through extensive experiments,\nwe show that DualOptim can significantly boost MU efficacy and stability across\ndiverse tasks, including image classification, image generation, and large\nlanguage models, making it a versatile approach to empower existing MU\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing machine unlearning (MU) approaches exhibit significant sensitivity\nto hyperparameters, requiring meticulous tuning that limits practical\ndeployment. In this work, we first empirically demonstrate the instability and\nsuboptimal performance of existing popular MU methods when deployed in\ndifferent scenarios. To address this issue, we propose Dual Optimizer\n(DualOptim), which incorporates adaptive learning rate and decoupled momentum\nfactors. Empirical and theoretical evidence demonstrates that DualOptim\ncontributes to effective and stable unlearning. Through extensive experiments,\nwe show that DualOptim can significantly boost MU efficacy and stability across\ndiverse tasks, including image classification, image generation, and large\nlanguage models, making it a versatile approach to empower existing MU\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Xuyang Zhong"
                    },
                    {
                        "name": "Haochen Luo"
                    },
                    {
                        "name": "Chen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chen Liu"
                },
                "author": "Chen Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.14216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.14216v2",
                "updated": "2025-10-31T12:44:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    44,
                    27,
                    4,
                    304,
                    0
                ],
                "published": "2025-05-20T11:22:34Z",
                "published_parsed": [
                    2025,
                    5,
                    20,
                    11,
                    22,
                    34,
                    1,
                    140,
                    0
                ],
                "title": "Reinforcement Learning vs. Distillation: Understanding Accuracy and\n  Capability in LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning vs. Distillation: Understanding Accuracy and\n  Capability in LLM Reasoning"
                },
                "summary": "Recent studies have shown that reinforcement learning with verifiable rewards\n(RLVR) enhances overall accuracy (pass@1) but often fails to improve capability\n(pass@k) of LLMs in reasoning tasks, while distillation can improve both. In\nthis paper, we investigate the mechanisms behind these phenomena. First, we\ndemonstrate that RLVR struggles to improve capability as it focuses on\nimproving the accuracy of the easier questions to the detriment of the accuracy\nof the most difficult questions. Second, we show that RLVR does not merely\nincrease the success probability for the easier questions, but in our small\nmodel settings, produces quality responses that were absent in its original\noutput distribution. In addition, we show these responses are neither\nnoticeably longer nor feature more reflection-related keywords, underscoring\nthe need for more reliable indicators of response quality. Third, from the\nexperiment distilling teacher responses to in-distribution problems, we find\nthat capability does not always improve with distillation. We conjecture that\ncapability improves only when new knowledge is introduced, whereas distilling\nreasoning patterns only improves accuracy but not capability, sacrificing\nperformance on the most difficult questions, similar to RLVR. Together, these\nfindings offer a clearer understanding of how RLVR and distillation shape\nreasoning behavior in LLMs",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown that reinforcement learning with verifiable rewards\n(RLVR) enhances overall accuracy (pass@1) but often fails to improve capability\n(pass@k) of LLMs in reasoning tasks, while distillation can improve both. In\nthis paper, we investigate the mechanisms behind these phenomena. First, we\ndemonstrate that RLVR struggles to improve capability as it focuses on\nimproving the accuracy of the easier questions to the detriment of the accuracy\nof the most difficult questions. Second, we show that RLVR does not merely\nincrease the success probability for the easier questions, but in our small\nmodel settings, produces quality responses that were absent in its original\noutput distribution. In addition, we show these responses are neither\nnoticeably longer nor feature more reflection-related keywords, underscoring\nthe need for more reliable indicators of response quality. Third, from the\nexperiment distilling teacher responses to in-distribution problems, we find\nthat capability does not always improve with distillation. We conjecture that\ncapability improves only when new knowledge is introduced, whereas distilling\nreasoning patterns only improves accuracy but not capability, sacrificing\nperformance on the most difficult questions, similar to RLVR. Together, these\nfindings offer a clearer understanding of how RLVR and distillation shape\nreasoning behavior in LLMs"
                },
                "authors": [
                    {
                        "name": "Minwu Kim"
                    },
                    {
                        "name": "Anubhav Shrestha"
                    },
                    {
                        "name": "Safal Shrestha"
                    },
                    {
                        "name": "Aadim Nepal"
                    },
                    {
                        "name": "Keith Ross"
                    }
                ],
                "author_detail": {
                    "name": "Keith Ross"
                },
                "author": "Keith Ross",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.14216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.14216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07464v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07464v4",
                "updated": "2025-10-31T12:13:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    13,
                    12,
                    4,
                    304,
                    0
                ],
                "published": "2025-06-09T06:15:54Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    6,
                    15,
                    54,
                    0,
                    160,
                    0
                ],
                "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO"
                },
                "summary": "Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training for enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success using a PPO-style reinforcement algorithm\nwith group-normalized rewards. However, the effectiveness of GRPO in Video\nLarge Language Models (VideoLLMs) has still been less studyed. In this paper,\nwe explore GRPO and identify two problems that deteriorate the effective\nlearning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate\nthese challenges, we propose DeepVideo-R1, a video large language model trained\nwith Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation.\nReg-GRPO reformulates the GRPO loss function into a regression task that\ndirectly predicts the advantage in GRPO, eliminating the need for safeguards\nsuch as the clipping and min functions. It directly aligns the model with\nadvantages, providing guidance to prefer better ones. The difficulty-aware data\naugmentation strategy augments input prompts/videos to locate the difficulty of\nsamples at solvable difficulty levels, enabling diverse reward signals. Our\nexperimental results show that our approach significantly improves video\nreasoning performance across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training for enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success using a PPO-style reinforcement algorithm\nwith group-normalized rewards. However, the effectiveness of GRPO in Video\nLarge Language Models (VideoLLMs) has still been less studyed. In this paper,\nwe explore GRPO and identify two problems that deteriorate the effective\nlearning: (1) reliance on safeguards, and (2) vanishing advantage. To mitigate\nthese challenges, we propose DeepVideo-R1, a video large language model trained\nwith Reg-GRPO (Regressive GRPO) and difficulty-aware data augmentation.\nReg-GRPO reformulates the GRPO loss function into a regression task that\ndirectly predicts the advantage in GRPO, eliminating the need for safeguards\nsuch as the clipping and min functions. It directly aligns the model with\nadvantages, providing guidance to prefer better ones. The difficulty-aware data\naugmentation strategy augments input prompts/videos to locate the difficulty of\nsamples at solvable difficulty levels, enabling diverse reward signals. Our\nexperimental results show that our approach significantly improves video\nreasoning performance across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Jinyoung Park"
                    },
                    {
                        "name": "Jeehye Na"
                    },
                    {
                        "name": "Jinyoung Kim"
                    },
                    {
                        "name": "Hyunwoo J. Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hyunwoo J. Kim"
                },
                "author": "Hyunwoo J. Kim",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07464v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07464v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27418v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27418v1",
                "updated": "2025-10-31T12:12:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    12,
                    51,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T12:12:51Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    12,
                    51,
                    4,
                    304,
                    0
                ],
                "title": "Dynamic Affective Memory Management for Personalized LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Affective Memory Management for Personalized LLM Agents"
                },
                "summary": "Advances in large language models are making personalized AI agents a new\nresearch focus. While current agent systems primarily rely on personalized\nexternal memory databases to deliver customized experiences, they face\nchallenges such as memory redundancy, memory staleness, and poor memory-context\nintegration, largely due to the lack of effective memory updates during\ninteraction. To tackle these issues, we propose a new memory management system\ndesigned for affective scenarios. Our approach employs a Bayesian-inspired\nmemory update algorithm with the concept of memory entropy, enabling the agent\nto autonomously maintain a dynamically updated memory vector database by\nminimizing global entropy to provide more personalized services. To better\nevaluate the system's effectiveness in this context, we propose DABench, a\nbenchmark focusing on emotional expression and emotional change toward objects.\nExperimental results demonstrate that, our system achieves superior performance\nin personalization, logical coherence, and accuracy. Ablation studies further\nvalidate the effectiveness of the Bayesian-inspired update mechanism in\nalleviating memory bloat. Our work offers new insights into the design of\nlong-term memory systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in large language models are making personalized AI agents a new\nresearch focus. While current agent systems primarily rely on personalized\nexternal memory databases to deliver customized experiences, they face\nchallenges such as memory redundancy, memory staleness, and poor memory-context\nintegration, largely due to the lack of effective memory updates during\ninteraction. To tackle these issues, we propose a new memory management system\ndesigned for affective scenarios. Our approach employs a Bayesian-inspired\nmemory update algorithm with the concept of memory entropy, enabling the agent\nto autonomously maintain a dynamically updated memory vector database by\nminimizing global entropy to provide more personalized services. To better\nevaluate the system's effectiveness in this context, we propose DABench, a\nbenchmark focusing on emotional expression and emotional change toward objects.\nExperimental results demonstrate that, our system achieves superior performance\nin personalization, logical coherence, and accuracy. Ablation studies further\nvalidate the effectiveness of the Bayesian-inspired update mechanism in\nalleviating memory bloat. Our work offers new insights into the design of\nlong-term memory systems."
                },
                "authors": [
                    {
                        "name": "Junfeng Lu"
                    },
                    {
                        "name": "Yueyan Li"
                    }
                ],
                "author_detail": {
                    "name": "Yueyan Li"
                },
                "author": "Yueyan Li",
                "arxiv_comment": "12 pasges, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27418v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27418v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27417v1",
                "updated": "2025-10-31T12:12:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    12,
                    1,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T12:12:01Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    12,
                    12,
                    1,
                    4,
                    304,
                    0
                ],
                "title": "Agentic LLMs for REST API Test Amplification: A Comparative Study Across\n  Cloud Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic LLMs for REST API Test Amplification: A Comparative Study Across\n  Cloud Applications"
                },
                "summary": "Representational State Transfer (REST) APIs are a cornerstone of modern cloud\nnative systems. Ensuring their reliability demands automated test suites that\nexercise diverse and boundary level behaviors. Nevertheless, designing such\ntest cases remains a challenging and resource intensive endeavor. This study\nextends prior work on Large Language Model (LLM) based test amplification by\nevaluating single agent and multi agent configurations across four additional\ncloud applications. The amplified test suites maintain semantic validity with\nminimal human intervention. The results demonstrate that agentic LLM systems\ncan effectively generalize across heterogeneous API architectures, increasing\nendpoint and parameter coverage while revealing defects. Moreover, a detailed\nanalysis of computational cost, runtime, and energy consumption highlights\ntrade-offs between accuracy, scalability, and efficiency. These findings\nunderscore the potential of LLM driven test amplification to advance the\nautomation and sustainability of REST API testing in complex cloud\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representational State Transfer (REST) APIs are a cornerstone of modern cloud\nnative systems. Ensuring their reliability demands automated test suites that\nexercise diverse and boundary level behaviors. Nevertheless, designing such\ntest cases remains a challenging and resource intensive endeavor. This study\nextends prior work on Large Language Model (LLM) based test amplification by\nevaluating single agent and multi agent configurations across four additional\ncloud applications. The amplified test suites maintain semantic validity with\nminimal human intervention. The results demonstrate that agentic LLM systems\ncan effectively generalize across heterogeneous API architectures, increasing\nendpoint and parameter coverage while revealing defects. Moreover, a detailed\nanalysis of computational cost, runtime, and energy consumption highlights\ntrade-offs between accuracy, scalability, and efficiency. These findings\nunderscore the potential of LLM driven test amplification to advance the\nautomation and sustainability of REST API testing in complex cloud\nenvironments."
                },
                "authors": [
                    {
                        "name": "Jarne Besjes"
                    },
                    {
                        "name": "Robbe Nooyens"
                    },
                    {
                        "name": "Tolgahan Bardakci"
                    },
                    {
                        "name": "Mutlu Beyazit"
                    },
                    {
                        "name": "Serge Demeyer"
                    }
                ],
                "author_detail": {
                    "name": "Serge Demeyer"
                },
                "author": "Serge Demeyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00943v2",
                "updated": "2025-10-31T11:55:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    11,
                    55,
                    42,
                    4,
                    304,
                    0
                ],
                "published": "2025-07-31T15:19:30Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    19,
                    30,
                    3,
                    212,
                    0
                ],
                "title": "LLMs Can Covertly Sandbag on Capability Evaluations Against\n  Chain-of-Thought Monitoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs Can Covertly Sandbag on Capability Evaluations Against\n  Chain-of-Thought Monitoring"
                },
                "summary": "Trustworthy evaluations of dangerous capabilities are increasingly crucial\nfor determining whether an AI system is safe to deploy. One empirically\ndemonstrated threat is sandbagging - the strategic underperformance on\nevaluations by AI models or their developers. A promising defense is to monitor\na model's chain-of-thought (CoT) reasoning, as this could reveal its intentions\nand plans. In this work, we measure the ability of models to sandbag on\ndangerous capability evaluations against a CoT monitor by prompting them to\nsandbag while being either monitor-oblivious or monitor-aware. We show that\nboth frontier models and small open-sourced models can covertly sandbag against\nCoT monitoring 0-shot without hints. However, they cannot yet do so reliably:\nthey bypass the monitor 16-36% of the time when monitor-aware, conditioned on\nsandbagging successfully. We qualitatively analyzed the uncaught CoTs to\nunderstand why the monitor failed. We reveal a rich attack surface for CoT\nmonitoring and contribute five covert sandbagging policies generated by models.\nThese results inform potential failure modes of CoT monitoring and may help\nbuild more diverse sandbagging model organisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy evaluations of dangerous capabilities are increasingly crucial\nfor determining whether an AI system is safe to deploy. One empirically\ndemonstrated threat is sandbagging - the strategic underperformance on\nevaluations by AI models or their developers. A promising defense is to monitor\na model's chain-of-thought (CoT) reasoning, as this could reveal its intentions\nand plans. In this work, we measure the ability of models to sandbag on\ndangerous capability evaluations against a CoT monitor by prompting them to\nsandbag while being either monitor-oblivious or monitor-aware. We show that\nboth frontier models and small open-sourced models can covertly sandbag against\nCoT monitoring 0-shot without hints. However, they cannot yet do so reliably:\nthey bypass the monitor 16-36% of the time when monitor-aware, conditioned on\nsandbagging successfully. We qualitatively analyzed the uncaught CoTs to\nunderstand why the monitor failed. We reveal a rich attack surface for CoT\nmonitoring and contribute five covert sandbagging policies generated by models.\nThese results inform potential failure modes of CoT monitoring and may help\nbuild more diverse sandbagging model organisms."
                },
                "authors": [
                    {
                        "name": "Chloe Li"
                    },
                    {
                        "name": "Mary Phuong"
                    },
                    {
                        "name": "Noah Y. Siegel"
                    }
                ],
                "author_detail": {
                    "name": "Noah Y. Siegel"
                },
                "author": "Noah Y. Siegel",
                "arxiv_comment": "Accepted to IJCNLP-AACL 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27401v1",
                "updated": "2025-10-31T11:37:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    11,
                    37,
                    42,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T11:37:42Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    11,
                    37,
                    42,
                    4,
                    304,
                    0
                ],
                "title": "\"Koyi Sawaal Nahi Hai\": Reimagining Maternal Health Chatbots for\n  Collective, Culturally Grounded Care",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Koyi Sawaal Nahi Hai\": Reimagining Maternal Health Chatbots for\n  Collective, Culturally Grounded Care"
                },
                "summary": "In recent years, LLM-based maternal health chatbots have been widely deployed\nin low-resource settings, but they often ignore real-world contexts where women\nmay not own phones, have limited literacy, and share decision-making within\nfamilies. Through the deployment of a WhatsApp-based maternal health chatbot\nwith 48 pregnant women in Lahore, Pakistan, we examine barriers to use in\npopulations where phones are shared, decision-making is collective, and\nliteracy varies. We complement this with focus group discussions with obstetric\nclinicians. Our findings reveal how adoption is shaped by proxy consent and\nfamily mediation, intermittent phone access, silence around asking questions,\ninfrastructural breakdowns, and contested authority. We frame barriers to\nnon-use as culturally conditioned rather than individual choices, and introduce\nthe Relational Chatbot Design Grammar (RCDG): four commitments that enable\nmediated decision-making, recognize silence as engagement, support episodic\nuse, and treat fragility as baseline to reorient maternal health chatbots\ntoward culturally grounded, collective care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, LLM-based maternal health chatbots have been widely deployed\nin low-resource settings, but they often ignore real-world contexts where women\nmay not own phones, have limited literacy, and share decision-making within\nfamilies. Through the deployment of a WhatsApp-based maternal health chatbot\nwith 48 pregnant women in Lahore, Pakistan, we examine barriers to use in\npopulations where phones are shared, decision-making is collective, and\nliteracy varies. We complement this with focus group discussions with obstetric\nclinicians. Our findings reveal how adoption is shaped by proxy consent and\nfamily mediation, intermittent phone access, silence around asking questions,\ninfrastructural breakdowns, and contested authority. We frame barriers to\nnon-use as culturally conditioned rather than individual choices, and introduce\nthe Relational Chatbot Design Grammar (RCDG): four commitments that enable\nmediated decision-making, recognize silence as engagement, support episodic\nuse, and treat fragility as baseline to reorient maternal health chatbots\ntoward culturally grounded, collective care."
                },
                "authors": [
                    {
                        "name": "Imaan Hameed"
                    },
                    {
                        "name": "Huma Umar"
                    },
                    {
                        "name": "Fozia Umber"
                    },
                    {
                        "name": "Maryam Mustafa"
                    }
                ],
                "author_detail": {
                    "name": "Maryam Mustafa"
                },
                "author": "Maryam Mustafa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27400v1",
                "updated": "2025-10-31T11:37:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    11,
                    37,
                    39,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T11:37:39Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    11,
                    37,
                    39,
                    4,
                    304,
                    0
                ],
                "title": "Balancing Knowledge Updates: Toward Unified Modular Editing in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Knowledge Updates: Toward Unified Modular Editing in LLMs"
                },
                "summary": "Knowledge editing has emerged as an efficient approach for updating factual\nknowledge in large language models (LLMs). It typically locates knowledge\nstorage modules and then modifies their parameters. However, most existing\nmethods focus on the weights of multilayer perceptron (MLP) modules, which are\noften identified as the main repositories of factual information. Other\ncomponents, such as attention (Attn) modules, are often ignored during editing.\nThis imbalance can leave residual outdated knowledge and limit editing\neffectiveness. We perform comprehensive knowledge localization experiments on\nadvanced LLMs and find that Attn modules play a substantial role in factual\nknowledge storage and retrieval, especially in earlier layers. Based on these\ninsights, we propose IntAttn-Edit, a method that extends the associative memory\nparadigm to jointly update both MLP and Attn modules. Our approach uses a\nknowledge balancing strategy that allocates update magnitudes in proportion to\neach module's measured contribution to knowledge storage. Experiments on\nstandard benchmarks show that IntAttn-Edit achieves higher edit success, better\ngeneralization, and stronger knowledge preservation than prior methods. Further\nanalysis shows that the balancing strategy keeps editing performance within an\noptimal range across diverse settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge editing has emerged as an efficient approach for updating factual\nknowledge in large language models (LLMs). It typically locates knowledge\nstorage modules and then modifies their parameters. However, most existing\nmethods focus on the weights of multilayer perceptron (MLP) modules, which are\noften identified as the main repositories of factual information. Other\ncomponents, such as attention (Attn) modules, are often ignored during editing.\nThis imbalance can leave residual outdated knowledge and limit editing\neffectiveness. We perform comprehensive knowledge localization experiments on\nadvanced LLMs and find that Attn modules play a substantial role in factual\nknowledge storage and retrieval, especially in earlier layers. Based on these\ninsights, we propose IntAttn-Edit, a method that extends the associative memory\nparadigm to jointly update both MLP and Attn modules. Our approach uses a\nknowledge balancing strategy that allocates update magnitudes in proportion to\neach module's measured contribution to knowledge storage. Experiments on\nstandard benchmarks show that IntAttn-Edit achieves higher edit success, better\ngeneralization, and stronger knowledge preservation than prior methods. Further\nanalysis shows that the balancing strategy keeps editing performance within an\noptimal range across diverse settings."
                },
                "authors": [
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Zijian Wang"
                    },
                    {
                        "name": "Kuo Zhao"
                    },
                    {
                        "name": "Dong Hu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Hu"
                },
                "author": "Dong Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07877v2",
                "updated": "2025-10-31T11:31:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    11,
                    31,
                    13,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-09T07:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    7,
                    28,
                    30,
                    3,
                    282,
                    0
                ],
                "title": "Ready to Translate, Not to Represent? Bias and Performance Gaps in\n  Multilingual LLMs Across Language Families and Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ready to Translate, Not to Represent? Bias and Performance Gaps in\n  Multilingual LLMs Across Language Families and Domains"
                },
                "summary": "The rise of Large Language Models (LLMs) has redefined Machine Translation\n(MT), enabling context-aware and fluent translations across hundreds of\nlanguages and textual domains. Despite their remarkable capabilities, LLMs\noften exhibit uneven performance across language families and specialized\ndomains. Moreover, recent evidence reveals that these models can encode and\namplify different biases present in their training data, posing serious\nconcerns for fairness, especially in low-resource languages. To address these\ngaps, we introduce Translation Tangles, a unified framework and dataset for\nevaluating the translation quality and fairness of open-source LLMs. Our\napproach benchmarks 24 bidirectional language pairs across multiple domains\nusing different metrics. We further propose a hybrid bias detection pipeline\nthat integrates rule-based heuristics, semantic similarity filtering, and\nLLM-based validation. We also introduce a high-quality, bias-annotated dataset\nbased on human evaluations of 1,439 translation-reference pairs. The code and\ndataset are accessible on GitHub:\nhttps://github.com/faiyazabdullah/TranslationTangles",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) has redefined Machine Translation\n(MT), enabling context-aware and fluent translations across hundreds of\nlanguages and textual domains. Despite their remarkable capabilities, LLMs\noften exhibit uneven performance across language families and specialized\ndomains. Moreover, recent evidence reveals that these models can encode and\namplify different biases present in their training data, posing serious\nconcerns for fairness, especially in low-resource languages. To address these\ngaps, we introduce Translation Tangles, a unified framework and dataset for\nevaluating the translation quality and fairness of open-source LLMs. Our\napproach benchmarks 24 bidirectional language pairs across multiple domains\nusing different metrics. We further propose a hybrid bias detection pipeline\nthat integrates rule-based heuristics, semantic similarity filtering, and\nLLM-based validation. We also introduce a high-quality, bias-annotated dataset\nbased on human evaluations of 1,439 translation-reference pairs. The code and\ndataset are accessible on GitHub:\nhttps://github.com/faiyazabdullah/TranslationTangles"
                },
                "authors": [
                    {
                        "name": "Md. Faiyaz Abdullah Sayeedi"
                    },
                    {
                        "name": "Md. Mahbub Alam"
                    },
                    {
                        "name": "Subhey Sadi Rahman"
                    },
                    {
                        "name": "Md. Adnanul Islam"
                    },
                    {
                        "name": "Jannatul Ferdous Deepti"
                    },
                    {
                        "name": "Tasnim Mohiuddin"
                    },
                    {
                        "name": "Md Mofijul Islam"
                    },
                    {
                        "name": "Swakkhar Shatabda"
                    }
                ],
                "author_detail": {
                    "name": "Swakkhar Shatabda"
                },
                "author": "Swakkhar Shatabda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19366v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19366v3",
                "updated": "2025-10-31T11:27:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    11,
                    27,
                    16,
                    4,
                    304,
                    0
                ],
                "published": "2025-06-24T06:53:20Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    6,
                    53,
                    20,
                    1,
                    175,
                    0
                ],
                "title": "The Role of Fractal Dimension in Wireless Mesh Network Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Role of Fractal Dimension in Wireless Mesh Network Performance"
                },
                "summary": "Wireless mesh networks (WMNs) depend on the spatial distribution of nodes,\nwhich directly influences connectivity, routing efficiency, and overall network\nperformance. Conventional models typically assume uniform or random node\nplacement, which inadequately represent the complex, hierarchical spatial\npatterns observed in practical deployments. In this study, we present a novel\nalgorithm that constructs WMN topologies with tunable fractal dimensions,\nallowing precise control over spatial self-similarity. By systematically\nvarying the fractal dimension, the algorithm generates network layouts spanning\na continuum of spatial complexities, ranging from sparse fragmented clusters to\ndense, cohesive structures. Through NS-3 simulations, Key performance metrics\nincluding throughput, latency, jitter, and packet delivery ratio were evaluated\nacross a range of fractal dimensions. Comparative evaluations against classical\nrandom, small-world, scale-free, grid and hierarchical tree networks models\nreveal that high-dimensional fractal topologies achieve enhanced resilience and\nthroughput under equivalent conditions. These findings demonstrate the\npotential of fractal geometry as a design paradigm for scalable and efficient\nWMN architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless mesh networks (WMNs) depend on the spatial distribution of nodes,\nwhich directly influences connectivity, routing efficiency, and overall network\nperformance. Conventional models typically assume uniform or random node\nplacement, which inadequately represent the complex, hierarchical spatial\npatterns observed in practical deployments. In this study, we present a novel\nalgorithm that constructs WMN topologies with tunable fractal dimensions,\nallowing precise control over spatial self-similarity. By systematically\nvarying the fractal dimension, the algorithm generates network layouts spanning\na continuum of spatial complexities, ranging from sparse fragmented clusters to\ndense, cohesive structures. Through NS-3 simulations, Key performance metrics\nincluding throughput, latency, jitter, and packet delivery ratio were evaluated\nacross a range of fractal dimensions. Comparative evaluations against classical\nrandom, small-world, scale-free, grid and hierarchical tree networks models\nreveal that high-dimensional fractal topologies achieve enhanced resilience and\nthroughput under equivalent conditions. These findings demonstrate the\npotential of fractal geometry as a design paradigm for scalable and efficient\nWMN architectures."
                },
                "authors": [
                    {
                        "name": "Marat Zaidyn"
                    },
                    {
                        "name": "Sayat Akhtanov"
                    },
                    {
                        "name": "Dana Turlykozhayeva"
                    },
                    {
                        "name": "Symbat Temesheva"
                    },
                    {
                        "name": "Almat Akhmetali"
                    },
                    {
                        "name": "Alisher Skabylov"
                    },
                    {
                        "name": "Nurzhan Ussipov"
                    }
                ],
                "author_detail": {
                    "name": "Nurzhan Ussipov"
                },
                "author": "Nurzhan Ussipov",
                "arxiv_comment": "14 pages, 8 figures, 2 tables. Accepted for publication in Nature\n  Scientific Reports",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19366v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19366v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15670v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15670v2",
                "updated": "2025-10-31T11:15:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    11,
                    15,
                    5,
                    4,
                    304,
                    0
                ],
                "published": "2025-06-18T17:51:19Z",
                "published_parsed": [
                    2025,
                    6,
                    18,
                    17,
                    51,
                    19,
                    2,
                    169,
                    0
                ],
                "title": "Near-Field SWIPT with gMIMO in the Upper Mid-Band: Opportunities,\n  Challenges, and the Way Forward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Near-Field SWIPT with gMIMO in the Upper Mid-Band: Opportunities,\n  Challenges, and the Way Forward"
                },
                "summary": "This paper explores the integration of simultaneous wireless information and\npower transfer (SWIPT) with gigantic multiple-input multiple-output (gMIMO)\ntechnology operating in the upper mid-band frequency range (7-24 GHz). The\nnear-field propagation achieved by gMIMO introduces unique opportunities for\nenergy-efficient, high-capacity communication systems that cater to the demands\nof 6G wireless networks. Exploiting spherical wave propagation, near-field\nSWIPT with gMIMO enables precise energy and data delivery, enhancing spectral\nefficiency through beam focusing and massive spatial multiplexing. This paper\ndiscusses theoretical principles, design challenges, and enabling solutions,\nincluding advanced channel estimation techniques, precoding strategies, and\ndynamic array configurations such as sparse and modular arrays. Through\nanalytical insights and a case study, this paper demonstrates the feasibility\nof achieving optimized energy harvesting and data throughput in dense and\ndynamic environments. These findings contribute to advancing energy-autonomous\nInternet-of-Everything (IoE) deployments, smart factory networks, and other\nenergy-autonomous applications aligned with the goals of next-generation\nwireless technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the integration of simultaneous wireless information and\npower transfer (SWIPT) with gigantic multiple-input multiple-output (gMIMO)\ntechnology operating in the upper mid-band frequency range (7-24 GHz). The\nnear-field propagation achieved by gMIMO introduces unique opportunities for\nenergy-efficient, high-capacity communication systems that cater to the demands\nof 6G wireless networks. Exploiting spherical wave propagation, near-field\nSWIPT with gMIMO enables precise energy and data delivery, enhancing spectral\nefficiency through beam focusing and massive spatial multiplexing. This paper\ndiscusses theoretical principles, design challenges, and enabling solutions,\nincluding advanced channel estimation techniques, precoding strategies, and\ndynamic array configurations such as sparse and modular arrays. Through\nanalytical insights and a case study, this paper demonstrates the feasibility\nof achieving optimized energy harvesting and data throughput in dense and\ndynamic environments. These findings contribute to advancing energy-autonomous\nInternet-of-Everything (IoE) deployments, smart factory networks, and other\nenergy-autonomous applications aligned with the goals of next-generation\nwireless technologies."
                },
                "authors": [
                    {
                        "name": "zlem Tugfe Demir"
                    },
                    {
                        "name": "Mustafa Ozger"
                    },
                    {
                        "name": "Ferdi Kara"
                    },
                    {
                        "name": "Woong-Hee Lee"
                    },
                    {
                        "name": "Emil Bjrnson"
                    }
                ],
                "author_detail": {
                    "name": "Emil Bjrnson"
                },
                "author": "Emil Bjrnson",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15670v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15670v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27363v1",
                "updated": "2025-10-31T10:51:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    51,
                    27,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T10:51:27Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    51,
                    27,
                    4,
                    304,
                    0
                ],
                "title": "ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool\n  Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToolScope: An Agentic Framework for Vision-Guided and Long-Horizon Tool\n  Use"
                },
                "summary": "Recently, large language models (LLMs) have demonstrated remarkable\nproblem-solving capabilities by autonomously integrating with external tools\nfor collaborative reasoning. However, due to the inherently complex and diverse\nnature of multimodal information, enabling multimodal large language models\n(MLLMs) to flexibly and efficiently utilize external tools during reasoning\nremains an underexplored challenge. In this work, we introduce ToolScope, an\nagentic framework designed to unify global planning with local multimodal\nperception, adopting a specialized Perceive tool to mitigates visual context\ndegradation in long-horizon VQA task. ToolScope comprises three primary\ncomponents: the Global Navigator, the Agentic Executor, and the Response\nSynthesizer. The Global Navigator functions as a \"telescope\", offering\nhigh-level strategic guidance. The Agentic Executor operates iteratively to\naugment MLLM with local perception through the integration of external\ntools-Search, Code, and Perceive. Finally, the Response Synthesizer\nconsolidates and organizes the reasoning process into a coherent, user-friendly\noutput. We evaluate ToolScope on four VQA benchmarks across diverse domains,\nincluding VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong\ngeneralization capabilities, achieving an average performance improvement of up\nto +6.69% across all datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have demonstrated remarkable\nproblem-solving capabilities by autonomously integrating with external tools\nfor collaborative reasoning. However, due to the inherently complex and diverse\nnature of multimodal information, enabling multimodal large language models\n(MLLMs) to flexibly and efficiently utilize external tools during reasoning\nremains an underexplored challenge. In this work, we introduce ToolScope, an\nagentic framework designed to unify global planning with local multimodal\nperception, adopting a specialized Perceive tool to mitigates visual context\ndegradation in long-horizon VQA task. ToolScope comprises three primary\ncomponents: the Global Navigator, the Agentic Executor, and the Response\nSynthesizer. The Global Navigator functions as a \"telescope\", offering\nhigh-level strategic guidance. The Agentic Executor operates iteratively to\naugment MLLM with local perception through the integration of external\ntools-Search, Code, and Perceive. Finally, the Response Synthesizer\nconsolidates and organizes the reasoning process into a coherent, user-friendly\noutput. We evaluate ToolScope on four VQA benchmarks across diverse domains,\nincluding VQA 2.0, ScienceQA, MAT-Search and MathVista. It demonstrates strong\ngeneralization capabilities, achieving an average performance improvement of up\nto +6.69% across all datasets."
                },
                "authors": [
                    {
                        "name": "Mengjie Deng"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27355v1",
                "updated": "2025-10-31T10:40:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    40,
                    19,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T10:40:19Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    40,
                    19,
                    4,
                    304,
                    0
                ],
                "title": "ThoughtProbe: Classifier-Guided LLM Thought Space Exploration via\n  Probing Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThoughtProbe: Classifier-Guided LLM Thought Space Exploration via\n  Probing Representations"
                },
                "summary": "This paper introduces ThoughtProbe, a novel inference time framework that\nleverages the hidden reasoning features of Large Language Models (LLMs) to\nimprove their reasoning performance. Unlike previous works that manipulate the\nhidden representations to steer LLM generation, we harness them as\ndiscriminative signals to guide the tree structured response space exploration.\nIn each node expansion, a classifier serves as a scoring and ranking mechanism\nthat efficiently allocates computational resources by prioritizing higher score\ncandidates for continuation. After completing the tree expansion, we collect\nanswers from all branches to form a candidate answer pool. We then propose a\nbranch aggregation method that marginalizes over all supporting branches by\naggregating their CoT scores, thereby identifying the optimal answer from the\npool. Experimental results show that our framework's comprehensive exploration\nnot only covers valid reasoning chains but also effectively identifies them,\nachieving significant improvements across multiple arithmetic reasoning\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces ThoughtProbe, a novel inference time framework that\nleverages the hidden reasoning features of Large Language Models (LLMs) to\nimprove their reasoning performance. Unlike previous works that manipulate the\nhidden representations to steer LLM generation, we harness them as\ndiscriminative signals to guide the tree structured response space exploration.\nIn each node expansion, a classifier serves as a scoring and ranking mechanism\nthat efficiently allocates computational resources by prioritizing higher score\ncandidates for continuation. After completing the tree expansion, we collect\nanswers from all branches to form a candidate answer pool. We then propose a\nbranch aggregation method that marginalizes over all supporting branches by\naggregating their CoT scores, thereby identifying the optimal answer from the\npool. Experimental results show that our framework's comprehensive exploration\nnot only covers valid reasoning chains but also effectively identifies them,\nachieving significant improvements across multiple arithmetic reasoning\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Zijian Wang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "arxiv_comment": "EMNLP2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27353v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27353v1",
                "updated": "2025-10-31T10:39:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    39,
                    16,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T10:39:16Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    39,
                    16,
                    4,
                    304,
                    0
                ],
                "title": "An In-depth Study of LLM Contributions to the Bin Packing Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An In-depth Study of LLM Contributions to the Bin Packing Problem"
                },
                "summary": "Recent studies have suggested that Large Language Models (LLMs) could provide\ninteresting ideas contributing to mathematical discovery. This claim was\nmotivated by reports that LLM-based genetic algorithms produced heuristics\noffering new insights into the online bin packing problem under uniform and\nWeibull distributions. In this work, we reassess this claim through a detailed\nanalysis of the heuristics produced by LLMs, examining both their behavior and\ninterpretability. Despite being human-readable, these heuristics remain largely\nopaque even to domain experts. Building on this analysis, we propose a new\nclass of algorithms tailored to these specific bin packing instances. The\nderived algorithms are significantly simpler, more efficient, more\ninterpretable, and more generalizable, suggesting that the considered instances\nare themselves relatively simple. We then discuss the limitations of the claim\nregarding LLMs' contribution to this problem, which appears to rest on the\nmistaken assumption that the instances had previously been studied. Our\nfindings instead emphasize the need for rigorous validation and\ncontextualization when assessing the scientific value of LLM-generated outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have suggested that Large Language Models (LLMs) could provide\ninteresting ideas contributing to mathematical discovery. This claim was\nmotivated by reports that LLM-based genetic algorithms produced heuristics\noffering new insights into the online bin packing problem under uniform and\nWeibull distributions. In this work, we reassess this claim through a detailed\nanalysis of the heuristics produced by LLMs, examining both their behavior and\ninterpretability. Despite being human-readable, these heuristics remain largely\nopaque even to domain experts. Building on this analysis, we propose a new\nclass of algorithms tailored to these specific bin packing instances. The\nderived algorithms are significantly simpler, more efficient, more\ninterpretable, and more generalizable, suggesting that the considered instances\nare themselves relatively simple. We then discuss the limitations of the claim\nregarding LLMs' contribution to this problem, which appears to rest on the\nmistaken assumption that the instances had previously been studied. Our\nfindings instead emphasize the need for rigorous validation and\ncontextualization when assessing the scientific value of LLM-generated outputs."
                },
                "authors": [
                    {
                        "name": "Julien Herrmann"
                    },
                    {
                        "name": "Guillaume Pallez"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Pallez"
                },
                "author": "Guillaume Pallez",
                "arxiv_comment": "15 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27353v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.8; F.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27335v1",
                "updated": "2025-10-31T10:06:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    6,
                    28,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T10:06:28Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    6,
                    28,
                    4,
                    304,
                    0
                ],
                "title": "Understanding the Implicit User Intention via Reasoning with Large\n  Language Model for Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Implicit User Intention via Reasoning with Large\n  Language Model for Image Editing"
                },
                "summary": "Existing image editing methods can handle simple editing instructions very\nwell. To deal with complex editing instructions, they often need to jointly\nfine-tune the large language models (LLMs) and diffusion models (DMs), which\ninvolves very high computational complexity and training cost. To address this\nissue, we propose a new method, called \\textbf{C}omplex \\textbf{I}mage\n\\textbf{E}diting via \\textbf{L}LM \\textbf{R}easoning (CIELR), which converts a\ncomplex user instruction into a set of simple and explicit editing actions,\neliminating the need for jointly fine-tuning the large language models and\ndiffusion models. Specifically, we first construct a structured semantic\nrepresentation of the input image using foundation models. Then, we introduce\nan iterative update mechanism that can progressively refine this\nrepresentation, obtaining a fine-grained visual representation of the image\nscene. This allows us to perform complex and flexible image editing tasks.\nExtensive experiments on the SmartEdit Reasoning Scenario Set show that our\nmethod surpasses the previous state-of-the-art by 9.955 dB in PSNR, indicating\nits superior preservation of regions that should remain consistent. Due to the\nlimited number of samples of public datasets of complex image editing with\nreasoning, we construct a benchmark named CIEBench, containing 86 image\nsamples, together with a metric specifically for reasoning-based image editing.\nCIELR also outperforms previous methods on this benchmark. The code and dataset\nare available at\n\\href{https://github.com/Jia-shao/Reasoning-Editing}{https://github.com/Jia-shao/Reasoning-Editing}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing image editing methods can handle simple editing instructions very\nwell. To deal with complex editing instructions, they often need to jointly\nfine-tune the large language models (LLMs) and diffusion models (DMs), which\ninvolves very high computational complexity and training cost. To address this\nissue, we propose a new method, called \\textbf{C}omplex \\textbf{I}mage\n\\textbf{E}diting via \\textbf{L}LM \\textbf{R}easoning (CIELR), which converts a\ncomplex user instruction into a set of simple and explicit editing actions,\neliminating the need for jointly fine-tuning the large language models and\ndiffusion models. Specifically, we first construct a structured semantic\nrepresentation of the input image using foundation models. Then, we introduce\nan iterative update mechanism that can progressively refine this\nrepresentation, obtaining a fine-grained visual representation of the image\nscene. This allows us to perform complex and flexible image editing tasks.\nExtensive experiments on the SmartEdit Reasoning Scenario Set show that our\nmethod surpasses the previous state-of-the-art by 9.955 dB in PSNR, indicating\nits superior preservation of regions that should remain consistent. Due to the\nlimited number of samples of public datasets of complex image editing with\nreasoning, we construct a benchmark named CIEBench, containing 86 image\nsamples, together with a metric specifically for reasoning-based image editing.\nCIELR also outperforms previous methods on this benchmark. The code and dataset\nare available at\n\\href{https://github.com/Jia-shao/Reasoning-Editing}{https://github.com/Jia-shao/Reasoning-Editing}."
                },
                "authors": [
                    {
                        "name": "Yijia Wang"
                    },
                    {
                        "name": "Yiqing Shen"
                    },
                    {
                        "name": "Weiming Chen"
                    },
                    {
                        "name": "Zhihai He"
                    }
                ],
                "author_detail": {
                    "name": "Zhihai He"
                },
                "author": "Zhihai He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23724v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23724v3",
                "updated": "2025-10-31T10:04:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    10,
                    4,
                    13,
                    4,
                    304,
                    0
                ],
                "published": "2025-05-29T17:55:21Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    55,
                    21,
                    3,
                    149,
                    0
                ],
                "title": "SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via\n  Subspace-Constrained LoRA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SC-LoRA: Balancing Efficient Fine-tuning and Knowledge Preservation via\n  Subspace-Constrained LoRA"
                },
                "summary": "Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), are indispensable for efficiently customizing Large Language\nModels (LLMs). However, vanilla LoRA suffers from slow convergence speed and\nknowledge forgetting problems. Recent studies have leveraged the power of\ndesigned LoRA initialization, to enhance the fine-tuning efficiency, or to\npreserve knowledge in the pre-trained LLM. However, none of these works can\naddress the two cases at the same time. To this end, we introduce\nSubspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework\nengineered to navigate the trade-off between efficient fine-tuning and\nknowledge preservation. We achieve this by constraining the output of trainable\nLoRA adapters in a low-rank subspace, where the context information of\nfine-tuning data is most preserved while the context information of preserved\nknowledge is least retained, in a balanced way. Such constraint enables the\ntrainable weights to primarily focus on the main features of fine-tuning data\nwhile avoiding damaging the preserved knowledge features. We provide\ntheoretical analysis on our method, and conduct extensive experiments including\nsafety preservation and world knowledge preservation, on various downstream\ntasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning\nperformance while markedly diminishing knowledge forgetting, surpassing\ncontemporary LoRA initialization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-Efficient Fine-Tuning (PEFT) methods, particularly Low-Rank\nAdaptation (LoRA), are indispensable for efficiently customizing Large Language\nModels (LLMs). However, vanilla LoRA suffers from slow convergence speed and\nknowledge forgetting problems. Recent studies have leveraged the power of\ndesigned LoRA initialization, to enhance the fine-tuning efficiency, or to\npreserve knowledge in the pre-trained LLM. However, none of these works can\naddress the two cases at the same time. To this end, we introduce\nSubspace-Constrained LoRA (SC-LoRA), a novel LoRA initialization framework\nengineered to navigate the trade-off between efficient fine-tuning and\nknowledge preservation. We achieve this by constraining the output of trainable\nLoRA adapters in a low-rank subspace, where the context information of\nfine-tuning data is most preserved while the context information of preserved\nknowledge is least retained, in a balanced way. Such constraint enables the\ntrainable weights to primarily focus on the main features of fine-tuning data\nwhile avoiding damaging the preserved knowledge features. We provide\ntheoretical analysis on our method, and conduct extensive experiments including\nsafety preservation and world knowledge preservation, on various downstream\ntasks. In our experiments, SC-LoRA succeeds in delivering superior fine-tuning\nperformance while markedly diminishing knowledge forgetting, surpassing\ncontemporary LoRA initialization methods."
                },
                "authors": [
                    {
                        "name": "Minrui Luo"
                    },
                    {
                        "name": "Fuhang Kuang"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Tianxing He"
                    }
                ],
                "author_detail": {
                    "name": "Tianxing He"
                },
                "author": "Tianxing He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23724v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23724v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27328v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27328v1",
                "updated": "2025-10-31T09:57:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    57,
                    19,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T09:57:19Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    57,
                    19,
                    4,
                    304,
                    0
                ],
                "title": "A Unified Representation Underlying the Judgment of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Representation Underlying the Judgment of Large Language\n  Models"
                },
                "summary": "A central architectural question for both biological and artificial\nintelligence is whether judgment relies on specialized modules or a unified,\ndomain-general resource. While the discovery of decodable neural\nrepresentations for distinct concepts in Large Language Models (LLMs) has\nsuggested a modular architecture, whether these representations are truly\nindependent systems remains an open question. Here we provide evidence for a\nconvergent architecture. Across a range of LLMs, we find that diverse\nevaluative judgments are computed along a dominant dimension, which we term the\nValence-Assent Axis (VAA). This axis jointly encodes subjective valence (\"what\nis good\") and the model's assent to factual claims (\"what is true\"). Through\ndirect interventions, we show this unified representation creates a critical\ndependency: the VAA functions as a control signal that steers the generative\nprocess to construct a rationale consistent with its evaluative state, even at\nthe cost of factual accuracy. This mechanism, which we term the subordination\nof reasoning, shifts the process of reasoning from impartial inference toward\ngoal-directed justification. Our discovery offers a mechanistic account for\nsystemic bias and hallucination, revealing how an architecture that promotes\ncoherent judgment can systematically undermine faithful reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central architectural question for both biological and artificial\nintelligence is whether judgment relies on specialized modules or a unified,\ndomain-general resource. While the discovery of decodable neural\nrepresentations for distinct concepts in Large Language Models (LLMs) has\nsuggested a modular architecture, whether these representations are truly\nindependent systems remains an open question. Here we provide evidence for a\nconvergent architecture. Across a range of LLMs, we find that diverse\nevaluative judgments are computed along a dominant dimension, which we term the\nValence-Assent Axis (VAA). This axis jointly encodes subjective valence (\"what\nis good\") and the model's assent to factual claims (\"what is true\"). Through\ndirect interventions, we show this unified representation creates a critical\ndependency: the VAA functions as a control signal that steers the generative\nprocess to construct a rationale consistent with its evaluative state, even at\nthe cost of factual accuracy. This mechanism, which we term the subordination\nof reasoning, shifts the process of reasoning from impartial inference toward\ngoal-directed justification. Our discovery offers a mechanistic account for\nsystemic bias and hallucination, revealing how an architecture that promotes\ncoherent judgment can systematically undermine faithful reasoning."
                },
                "authors": [
                    {
                        "name": "Yi-Long Lu"
                    },
                    {
                        "name": "Jiajun Song"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27328v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.03419v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.03419v2",
                "updated": "2025-10-31T09:50:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    50,
                    31,
                    4,
                    304,
                    0
                ],
                "published": "2025-09-03T15:48:33Z",
                "published_parsed": [
                    2025,
                    9,
                    3,
                    15,
                    48,
                    33,
                    2,
                    246,
                    0
                ],
                "title": "Curse of Knowledge: When Complex Evaluation Context Benefits yet Biases\n  LLM Judges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Curse of Knowledge: When Complex Evaluation Context Benefits yet Biases\n  LLM Judges"
                },
                "summary": "As large language models (LLMs) grow more capable, they face increasingly\ndiverse and complex tasks, making reliable evaluation challenging. The paradigm\nof LLMs as judges has emerged as a scalable solution, yet prior work primarily\nfocuses on simple settings. Their reliability in complex tasks--where\nmulti-faceted rubrics, unstructured reference answers, and nuanced criteria are\ncritical--remains understudied. In this paper, we constructed ComplexEval, a\nchallenge benchmark designed to systematically expose and quantify Auxiliary\nInformation Induced Biases. We systematically investigated and validated 6\npreviously unexplored biases across 12 basic and 3 advanced scenarios. Key\nfindings reveal: (1) all evaluated models exhibit significant susceptibility to\nthese biases, with bias magnitude scaling with task complexity; (2) notably,\nLarge Reasoning Models (LRMs) show paradoxical vulnerability. Our in-depth\nanalysis offers crucial insights for improving the accuracy and verifiability\nof evaluation signals, paving the way for more general and robust evaluation\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) grow more capable, they face increasingly\ndiverse and complex tasks, making reliable evaluation challenging. The paradigm\nof LLMs as judges has emerged as a scalable solution, yet prior work primarily\nfocuses on simple settings. Their reliability in complex tasks--where\nmulti-faceted rubrics, unstructured reference answers, and nuanced criteria are\ncritical--remains understudied. In this paper, we constructed ComplexEval, a\nchallenge benchmark designed to systematically expose and quantify Auxiliary\nInformation Induced Biases. We systematically investigated and validated 6\npreviously unexplored biases across 12 basic and 3 advanced scenarios. Key\nfindings reveal: (1) all evaluated models exhibit significant susceptibility to\nthese biases, with bias magnitude scaling with task complexity; (2) notably,\nLarge Reasoning Models (LRMs) show paradoxical vulnerability. Our in-depth\nanalysis offers crucial insights for improving the accuracy and verifiability\nof evaluation signals, paving the way for more general and robust evaluation\nmodels."
                },
                "authors": [
                    {
                        "name": "Weiyuan Li"
                    },
                    {
                        "name": "Xintao Wang"
                    },
                    {
                        "name": "Siyu Yuan"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Jiangjie Chen"
                    },
                    {
                        "name": "Qingqing Dong"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.03419v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.03419v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17336v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17336v3",
                "updated": "2025-10-31T09:42:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    42,
                    28,
                    4,
                    304,
                    0
                ],
                "published": "2025-09-22T03:13:58Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    3,
                    13,
                    58,
                    0,
                    265,
                    0
                ],
                "title": "Mano Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mano Technical Report"
                },
                "summary": "Graphical user interfaces (GUIs) are the primary medium for human-computer\ninteraction, yet automating GUI interactions remains challenging due to the\ncomplexity of visual elements, dynamic environments, and the need for\nmulti-step reasoning. Existing methods based on vision-language models (VLMs)\noften suffer from limited resolution, domain mismatch, and insufficient\nsequential decisionmaking capability. To address these issues, we propose Mano,\na robust GUI agent built upon a multi-modal foundation model pre-trained on\nextensive web and computer system data. Our approach integrates a novel\nsimulated environment for high-fidelity data generation, a three-stage training\npipeline (supervised fine-tuning, offline reinforcement learning, and online\nreinforcement learning), and a verification module for error recovery. Mano\ndemonstrates state-of-the-art performance on multiple GUI benchmarks, including\nMind2Web and OSWorld, achieving significant improvements in success rate and\noperational accuracy. Our work provides new insights into the effective\nintegration of reinforcement learning with VLMs for practical GUI agent\ndeployment, highlighting the importance of domain-specific data, iterative\ntraining, and holistic reward design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphical user interfaces (GUIs) are the primary medium for human-computer\ninteraction, yet automating GUI interactions remains challenging due to the\ncomplexity of visual elements, dynamic environments, and the need for\nmulti-step reasoning. Existing methods based on vision-language models (VLMs)\noften suffer from limited resolution, domain mismatch, and insufficient\nsequential decisionmaking capability. To address these issues, we propose Mano,\na robust GUI agent built upon a multi-modal foundation model pre-trained on\nextensive web and computer system data. Our approach integrates a novel\nsimulated environment for high-fidelity data generation, a three-stage training\npipeline (supervised fine-tuning, offline reinforcement learning, and online\nreinforcement learning), and a verification module for error recovery. Mano\ndemonstrates state-of-the-art performance on multiple GUI benchmarks, including\nMind2Web and OSWorld, achieving significant improvements in success rate and\noperational accuracy. Our work provides new insights into the effective\nintegration of reinforcement learning with VLMs for practical GUI agent\ndeployment, highlighting the importance of domain-specific data, iterative\ntraining, and holistic reward design."
                },
                "authors": [
                    {
                        "name": "Tianyu Fu"
                    },
                    {
                        "name": "Anyang Su"
                    },
                    {
                        "name": "Chenxu Zhao"
                    },
                    {
                        "name": "Hanning Wang"
                    },
                    {
                        "name": "Minghui Wu"
                    },
                    {
                        "name": "Zhe Yu"
                    },
                    {
                        "name": "Fei Hu"
                    },
                    {
                        "name": "Mingjia Shi"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Jiayao Wang"
                    },
                    {
                        "name": "Yuyang Chen"
                    },
                    {
                        "name": "Ruiyang Yu"
                    },
                    {
                        "name": "Siran Peng"
                    },
                    {
                        "name": "Menglin Li"
                    },
                    {
                        "name": "Nan Huang"
                    },
                    {
                        "name": "Haitian Wei"
                    },
                    {
                        "name": "Jiawei Yu"
                    },
                    {
                        "name": "Yi Xin"
                    },
                    {
                        "name": "Xilin Zhao"
                    },
                    {
                        "name": "Kai Gu"
                    },
                    {
                        "name": "Ping Jiang"
                    },
                    {
                        "name": "Sifan Zhou"
                    },
                    {
                        "name": "Shuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shuo Wang"
                },
                "author": "Shuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17336v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17336v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22623v2",
                "updated": "2025-10-31T09:40:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    40,
                    18,
                    4,
                    304,
                    0
                ],
                "published": "2025-07-30T12:42:35Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    42,
                    35,
                    2,
                    211,
                    0
                ],
                "title": "Multilingual Political Views of Large Language Models: Identification\n  and Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual Political Views of Large Language Models: Identification\n  and Steering"
                },
                "summary": "Large language models (LLMs) are increasingly used in everyday tools and\napplications, raising concerns about their potential influence on political\nviews. While prior research has shown that LLMs often exhibit measurable\npolitical biases--frequently skewing toward liberal or progressive\npositions--key gaps remain. Most existing studies evaluate only a narrow set of\nmodels and languages, leaving open questions about the generalizability of\npolitical biases across architectures, scales, and multilingual settings.\nMoreover, few works examine whether these biases can be actively controlled.\n  In this work, we address these gaps through a large-scale study of political\norientation in modern open-source instruction-tuned LLMs. We evaluate seven\nmodels, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using\nthe Political Compass Test with 11 semantically equivalent paraphrases per\nstatement to ensure robust measurement. Our results reveal that larger models\nconsistently shift toward libertarian-left positions, with significant\nvariations across languages and model families. To test the manipulability of\npolitical stances, we utilize a simple center-of-mass activation intervention\ntechnique and show that it reliably steers model responses toward alternative\nideological positions across multiple languages. Our code is publicly available\nat https://github.com/d-gurgurov/Political-Ideologies-LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly used in everyday tools and\napplications, raising concerns about their potential influence on political\nviews. While prior research has shown that LLMs often exhibit measurable\npolitical biases--frequently skewing toward liberal or progressive\npositions--key gaps remain. Most existing studies evaluate only a narrow set of\nmodels and languages, leaving open questions about the generalizability of\npolitical biases across architectures, scales, and multilingual settings.\nMoreover, few works examine whether these biases can be actively controlled.\n  In this work, we address these gaps through a large-scale study of political\norientation in modern open-source instruction-tuned LLMs. We evaluate seven\nmodels, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using\nthe Political Compass Test with 11 semantically equivalent paraphrases per\nstatement to ensure robust measurement. Our results reveal that larger models\nconsistently shift toward libertarian-left positions, with significant\nvariations across languages and model families. To test the manipulability of\npolitical stances, we utilize a simple center-of-mass activation intervention\ntechnique and show that it reliably steers model responses toward alternative\nideological positions across multiple languages. Our code is publicly available\nat https://github.com/d-gurgurov/Political-Ideologies-LLMs."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Katharina Trinley"
                    },
                    {
                        "name": "Ivan Vykopal"
                    },
                    {
                        "name": "Josef van Genabith"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Roberto Zamparelli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Zamparelli"
                },
                "author": "Roberto Zamparelli",
                "arxiv_comment": "pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.23371v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.23371v2",
                "updated": "2025-10-31T09:18:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    18,
                    19,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-27T14:21:05Z",
                "published_parsed": [
                    2025,
                    10,
                    27,
                    14,
                    21,
                    5,
                    0,
                    300,
                    0
                ],
                "title": "Towards a Generalizable AI for Materials Discovery: Validation through\n  Immersion Coolant Screening",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Generalizable AI for Materials Discovery: Validation through\n  Immersion Coolant Screening"
                },
                "summary": "Artificial intelligence (AI) has emerged as a powerful accelerator of\nmaterials discovery, yet most existing models remain problem-specific,\nrequiring additional data collection and retraining for each new property. Here\nwe introduce and validate GATE (Geometrically Aligned Transfer Encoder) -- a\ngeneralizable AI framework that jointly learns 34 physicochemical properties\nspanning thermal, electrical, mechanical, and optical domains. By aligning\nthese properties within a shared geometric space, GATE captures cross-property\ncorrelations that reduce disjoint-property bias -- a key factor causing false\npositives in multi-criteria screening. To demonstrate its generalizable\nutility, GATE -- without any problem-specific model reconfiguration -- applied\nto the discovery of immersion cooling fluids for data centers, a stringent\nreal-world challenge defined by the Open Compute Project (OCP). Screening\nbillions of candidates, GATE identified 92,861 molecules as promising for\npractical deployment. Four were experimentally or literarily validated, showing\nstrong agreement with wet-lab measurements and performance comparable to or\nexceeding a commercial coolant. These results establish GATE as a generalizable\nAI platform readily applicable across diverse materials discovery tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) has emerged as a powerful accelerator of\nmaterials discovery, yet most existing models remain problem-specific,\nrequiring additional data collection and retraining for each new property. Here\nwe introduce and validate GATE (Geometrically Aligned Transfer Encoder) -- a\ngeneralizable AI framework that jointly learns 34 physicochemical properties\nspanning thermal, electrical, mechanical, and optical domains. By aligning\nthese properties within a shared geometric space, GATE captures cross-property\ncorrelations that reduce disjoint-property bias -- a key factor causing false\npositives in multi-criteria screening. To demonstrate its generalizable\nutility, GATE -- without any problem-specific model reconfiguration -- applied\nto the discovery of immersion cooling fluids for data centers, a stringent\nreal-world challenge defined by the Open Compute Project (OCP). Screening\nbillions of candidates, GATE identified 92,861 molecules as promising for\npractical deployment. Four were experimentally or literarily validated, showing\nstrong agreement with wet-lab measurements and performance comparable to or\nexceeding a commercial coolant. These results establish GATE as a generalizable\nAI platform readily applicable across diverse materials discovery tasks."
                },
                "authors": [
                    {
                        "name": "Hyunseung Kim"
                    },
                    {
                        "name": "Dae-Woong Jeong"
                    },
                    {
                        "name": "Changyoung Park"
                    },
                    {
                        "name": "Won-Ji Lee"
                    },
                    {
                        "name": "Ha-Eun Lee"
                    },
                    {
                        "name": "Ji-Hye Lee"
                    },
                    {
                        "name": "Rodrigo Hormazabal"
                    },
                    {
                        "name": "Sung Moon Ko"
                    },
                    {
                        "name": "Sumin Lee"
                    },
                    {
                        "name": "Soorin Yim"
                    },
                    {
                        "name": "Chanhui Lee"
                    },
                    {
                        "name": "Sehui Han"
                    },
                    {
                        "name": "Sang-Ho Cha"
                    },
                    {
                        "name": "Woohyung Lim"
                    }
                ],
                "author_detail": {
                    "name": "Woohyung Lim"
                },
                "arxiv_affiliation": "LG AI Research, Republic of Korea",
                "author": "Woohyung Lim",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.23371v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.23371v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00920v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00920v2",
                "updated": "2025-10-31T09:01:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    1,
                    40,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-01T13:58:19Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    13,
                    58,
                    19,
                    2,
                    274,
                    0
                ],
                "title": "Can Emulating Semantic Translation Help LLMs with Code Translation? A\n  Study Based on Pseudocode",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Emulating Semantic Translation Help LLMs with Code Translation? A\n  Study Based on Pseudocode"
                },
                "summary": "Large language models (LLMs) show great potential in code translation.\nHowever, accurate translation remains challenging when using the commonly\nadopted direct code-to-code translation approach, which converts a program into\nthe target programming language (PL) in a single step. Inspired by the success\nof incorporating intermediate steps to guide LLMs in resolving challenging\ntasks, we explore pseudocode-based code translation, which emulates the human\nsemantic translation by first interpreting the program's intent and logic into\npseudocode and then implementing it in the target PL. We find that\npseudocode-based translation helps translate programs that direct translation\nstruggles to handle. Nonetheless, the effectiveness, advantages, and\nlimitations of this approach remain underexplored. To bridge this gap, we\npresent an empirical study on pseudocode-based code translation, aiming to\ninvestigate its effectiveness in enhancing the direct translation approach,\nilluminate its effective usage, and identify limitations hindering its\npotential benefits. By comparing direct and pseudocode-based translation\napproaches on 9,690 translation tasks across six PLs with five popular LLMs, we\ndemonstrate that pseudocode-based translation can effectively complement direct\ntranslation, particularly when translating from flexible to rigid PLs or\ndealing with low-resource Rust. Based on these findings, we suggest adopting\nstrategies that combine the complementary strengths of both approaches to\nenhance code translation accuracy. We also reveal the advantages of\npseudocode-based translation in disentangling translations of complicated\nprograms and mitigating distractions from detailed implementations in original\nprograms, as well as its limitations due to incorrect, incomplete, or ambiguous\npseudocode.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show great potential in code translation.\nHowever, accurate translation remains challenging when using the commonly\nadopted direct code-to-code translation approach, which converts a program into\nthe target programming language (PL) in a single step. Inspired by the success\nof incorporating intermediate steps to guide LLMs in resolving challenging\ntasks, we explore pseudocode-based code translation, which emulates the human\nsemantic translation by first interpreting the program's intent and logic into\npseudocode and then implementing it in the target PL. We find that\npseudocode-based translation helps translate programs that direct translation\nstruggles to handle. Nonetheless, the effectiveness, advantages, and\nlimitations of this approach remain underexplored. To bridge this gap, we\npresent an empirical study on pseudocode-based code translation, aiming to\ninvestigate its effectiveness in enhancing the direct translation approach,\nilluminate its effective usage, and identify limitations hindering its\npotential benefits. By comparing direct and pseudocode-based translation\napproaches on 9,690 translation tasks across six PLs with five popular LLMs, we\ndemonstrate that pseudocode-based translation can effectively complement direct\ntranslation, particularly when translating from flexible to rigid PLs or\ndealing with low-resource Rust. Based on these findings, we suggest adopting\nstrategies that combine the complementary strengths of both approaches to\nenhance code translation accuracy. We also reveal the advantages of\npseudocode-based translation in disentangling translations of complicated\nprograms and mitigating distractions from detailed implementations in original\nprograms, as well as its limitations due to incorrect, incomplete, or ambiguous\npseudocode."
                },
                "authors": [
                    {
                        "name": "Songqiang Chen"
                    },
                    {
                        "name": "Congying Xu"
                    },
                    {
                        "name": "Jingyi Chen"
                    },
                    {
                        "name": "Jialun Cao"
                    },
                    {
                        "name": "Jiarong Wu"
                    },
                    {
                        "name": "Shing-Chi Cheung"
                    }
                ],
                "author_detail": {
                    "name": "Shing-Chi Cheung"
                },
                "author": "Shing-Chi Cheung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00920v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00920v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05433v2",
                "updated": "2025-10-31T09:00:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    9,
                    0,
                    22,
                    4,
                    304,
                    0
                ],
                "published": "2025-08-07T14:24:03Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    14,
                    24,
                    3,
                    3,
                    219,
                    0
                ],
                "title": "Multimodal LLM-assisted Evolutionary Search for Programmatic Control\n  Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal LLM-assisted Evolutionary Search for Programmatic Control\n  Policies"
                },
                "summary": "Deep reinforcement learning has achieved impressive success in control tasks.\nHowever, its policies, represented as opaque neural networks, are often\ndifficult for humans to understand, verify, and debug, which undermines trust\nand hinders real-world deployment. This work addresses this challenge by\nintroducing a novel approach for programmatic control policy discovery, called\nMultimodal Large Language Model-assisted Evolutionary Search (MLES). MLES\nutilizes multimodal large language models as programmatic policy generators,\ncombining them with evolutionary search to automate policy generation. It\nintegrates visual feedback-driven behavior analysis within the policy\ngeneration process to identify failure patterns and guide targeted\nimprovements, thereby enhancing policy discovery efficiency and producing\nadaptable, human-aligned policies. Experimental results demonstrate that MLES\nachieves performance comparable to Proximal Policy Optimization (PPO) across\ntwo standard control tasks while providing transparent control logic and\ntraceable design processes. This approach also overcomes the limitations of\npredefined domain-specific languages, facilitates knowledge transfer and reuse,\nand is scalable across various tasks, showing promise as a new paradigm for\ndeveloping transparent and verifiable control policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep reinforcement learning has achieved impressive success in control tasks.\nHowever, its policies, represented as opaque neural networks, are often\ndifficult for humans to understand, verify, and debug, which undermines trust\nand hinders real-world deployment. This work addresses this challenge by\nintroducing a novel approach for programmatic control policy discovery, called\nMultimodal Large Language Model-assisted Evolutionary Search (MLES). MLES\nutilizes multimodal large language models as programmatic policy generators,\ncombining them with evolutionary search to automate policy generation. It\nintegrates visual feedback-driven behavior analysis within the policy\ngeneration process to identify failure patterns and guide targeted\nimprovements, thereby enhancing policy discovery efficiency and producing\nadaptable, human-aligned policies. Experimental results demonstrate that MLES\nachieves performance comparable to Proximal Policy Optimization (PPO) across\ntwo standard control tasks while providing transparent control logic and\ntraceable design processes. This approach also overcomes the limitations of\npredefined domain-specific languages, facilitates knowledge transfer and reuse,\nand is scalable across various tasks, showing promise as a new paradigm for\ndeveloping transparent and verifiable control policies."
                },
                "authors": [
                    {
                        "name": "Qinglong Hu"
                    },
                    {
                        "name": "Xialiang Tong"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Qingfu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qingfu Zhang"
                },
                "author": "Qingfu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27287v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27287v1",
                "updated": "2025-10-31T08:55:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    55,
                    13,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T08:55:13Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    55,
                    13,
                    4,
                    304,
                    0
                ],
                "title": "Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in\n  Enterprise Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Help You at Work? A Sandbox for Evaluating LLM Agents in\n  Enterprise Environments"
                },
                "summary": "Enterprise systems are crucial for enhancing productivity and decision-making\namong employees and customers. Integrating LLM based systems into enterprise\nsystems enables intelligent automation, personalized experiences, and efficient\ninformation retrieval, driving operational efficiency and strategic growth.\nHowever, developing and evaluating such systems is challenging due to the\ninherent complexity of enterprise environments, where data is fragmented across\nmultiple sources and governed by sophisticated access controls. We present\nEnterpriseBench, a comprehensive benchmark that simulates enterprise settings,\nfeaturing 500 diverse tasks across software engineering, HR, finance, and\nadministrative domains. Our benchmark uniquely captures key enterprise\ncharacteristics including data source fragmentation, access control\nhierarchies, and cross-functional workflows. Additionally, we provide a novel\ndata generation pipeline that creates internally consistent enterprise tasks\nfrom organizational metadata. Experiments with state-of-the-art LLM agents\ndemonstrate that even the most capable models achieve only 41.8% task\ncompletion, highlighting significant opportunities for improvement in\nenterprise-focused AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise systems are crucial for enhancing productivity and decision-making\namong employees and customers. Integrating LLM based systems into enterprise\nsystems enables intelligent automation, personalized experiences, and efficient\ninformation retrieval, driving operational efficiency and strategic growth.\nHowever, developing and evaluating such systems is challenging due to the\ninherent complexity of enterprise environments, where data is fragmented across\nmultiple sources and governed by sophisticated access controls. We present\nEnterpriseBench, a comprehensive benchmark that simulates enterprise settings,\nfeaturing 500 diverse tasks across software engineering, HR, finance, and\nadministrative domains. Our benchmark uniquely captures key enterprise\ncharacteristics including data source fragmentation, access control\nhierarchies, and cross-functional workflows. Additionally, we provide a novel\ndata generation pipeline that creates internally consistent enterprise tasks\nfrom organizational metadata. Experiments with state-of-the-art LLM agents\ndemonstrate that even the most capable models achieve only 41.8% task\ncompletion, highlighting significant opportunities for improvement in\nenterprise-focused AI systems."
                },
                "authors": [
                    {
                        "name": "Harsh Vishwakarma"
                    },
                    {
                        "name": "Ankush Agarwal"
                    },
                    {
                        "name": "Ojas Patil"
                    },
                    {
                        "name": "Chaitanya Devaguptapu"
                    },
                    {
                        "name": "Mahesh Chandran"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Chandran"
                },
                "author": "Mahesh Chandran",
                "arxiv_comment": "Accepted at EMNLP 2025 Main Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27287v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27287v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27275v1",
                "updated": "2025-10-31T08:35:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    35,
                    42,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T08:35:42Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    35,
                    42,
                    4,
                    304,
                    0
                ],
                "title": "Prevalence of Security and Privacy Risk-Inducing Usage of AI-based\n  Conversational Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prevalence of Security and Privacy Risk-Inducing Usage of AI-based\n  Conversational Agents"
                },
                "summary": "Recent improvement gains in large language models (LLMs) have lead to\neveryday usage of AI-based Conversational Agents (CAs). At the same time, LLMs\nare vulnerable to an array of threats, including jailbreaks and, for example,\ncausing remote code execution when fed specific inputs. As a result, users may\nunintentionally introduce risks, for example, by uploading malicious files or\ndisclosing sensitive information. However, the extent to which such user\nbehaviors occur and thus potentially facilitate exploits remains largely\nunclear. To shed light on this issue, we surveyed a representative sample of\n3,270 UK adults in 2024 using Prolific. A third of these use CA services such\nas ChatGPT or Gemini at least once a week. Of these ``regular users'', up to a\nthird exhibited behaviors that may enable attacks, and a fourth have tried\njailbreaking (often out of understandable reasons such as curiosity, fun or\ninformation seeking). Half state that they sanitize data and most participants\nreport not sharing sensitive data. However, few share very sensitive data such\nas passwords. The majority are unaware that their data can be used to train\nmodels and that they can opt-out. Our findings suggest that current academic\nthreat models manifest in the wild, and mitigations or guidelines for the\nsecure usage of CAs should be developed. In areas critical to security and\nprivacy, CAs must be equipped with effective AI guardrails to prevent, for\nexample, revealing sensitive information to curious employees. Vendors need to\nincrease efforts to prevent the entry of sensitive data, and to create\ntransparency with regard to data usage policies and settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent improvement gains in large language models (LLMs) have lead to\neveryday usage of AI-based Conversational Agents (CAs). At the same time, LLMs\nare vulnerable to an array of threats, including jailbreaks and, for example,\ncausing remote code execution when fed specific inputs. As a result, users may\nunintentionally introduce risks, for example, by uploading malicious files or\ndisclosing sensitive information. However, the extent to which such user\nbehaviors occur and thus potentially facilitate exploits remains largely\nunclear. To shed light on this issue, we surveyed a representative sample of\n3,270 UK adults in 2024 using Prolific. A third of these use CA services such\nas ChatGPT or Gemini at least once a week. Of these ``regular users'', up to a\nthird exhibited behaviors that may enable attacks, and a fourth have tried\njailbreaking (often out of understandable reasons such as curiosity, fun or\ninformation seeking). Half state that they sanitize data and most participants\nreport not sharing sensitive data. However, few share very sensitive data such\nas passwords. The majority are unaware that their data can be used to train\nmodels and that they can opt-out. Our findings suggest that current academic\nthreat models manifest in the wild, and mitigations or guidelines for the\nsecure usage of CAs should be developed. In areas critical to security and\nprivacy, CAs must be equipped with effective AI guardrails to prevent, for\nexample, revealing sensitive information to curious employees. Vendors need to\nincrease efforts to prevent the entry of sensitive data, and to create\ntransparency with regard to data usage policies and settings."
                },
                "authors": [
                    {
                        "name": "Kathrin Grosse"
                    },
                    {
                        "name": "Nico Ebert"
                    }
                ],
                "author_detail": {
                    "name": "Nico Ebert"
                },
                "author": "Nico Ebert",
                "arxiv_comment": "10 pages, 3 figures, 5 tables, under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09045v2",
                "updated": "2025-10-31T08:20:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    20,
                    14,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-10T06:28:15Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    6,
                    28,
                    15,
                    4,
                    283,
                    0
                ],
                "title": "LLM Based Long Code Translation using Identifier Replacement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Based Long Code Translation using Identifier Replacement"
                },
                "summary": "In the domain of software development, LLMs have been utilized to automate\ntasks such as code translation, where source code from one programming language\nis translated to another while preserving its functionality. However, LLMs\noften struggle with long source codes that don't fit into the context window,\nwhich produces inaccurate translations. To address this, we propose a novel\nzero-shot code translation method that incorporates identifier replacement. By\nsubstituting user-given long identifiers with generalized placeholders during\ntranslation, our method allows the LLM to focus on the logical structure of the\ncode, by reducing token count and memory usage, which improves the efficiency\nand cost-effectiveness of long code translation. Our empirical results\ndemonstrate that our approach preserves syntactical and hierarchical\ninformation and produces translation results with reduced tokens.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the domain of software development, LLMs have been utilized to automate\ntasks such as code translation, where source code from one programming language\nis translated to another while preserving its functionality. However, LLMs\noften struggle with long source codes that don't fit into the context window,\nwhich produces inaccurate translations. To address this, we propose a novel\nzero-shot code translation method that incorporates identifier replacement. By\nsubstituting user-given long identifiers with generalized placeholders during\ntranslation, our method allows the LLM to focus on the logical structure of the\ncode, by reducing token count and memory usage, which improves the efficiency\nand cost-effectiveness of long code translation. Our empirical results\ndemonstrate that our approach preserves syntactical and hierarchical\ninformation and produces translation results with reduced tokens."
                },
                "authors": [
                    {
                        "name": "Manojit Chakraborty"
                    },
                    {
                        "name": "Madhusudan Ghosh"
                    },
                    {
                        "name": "Rishabh Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Gupta"
                },
                "author": "Rishabh Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13178v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13178v5",
                "updated": "2025-10-31T08:18:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    18,
                    50,
                    4,
                    304,
                    0
                ],
                "published": "2024-12-17T18:55:58Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    18,
                    55,
                    58,
                    1,
                    352,
                    0
                ],
                "title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM\n  Agents"
                },
                "summary": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to understand and plan complicated natural language\ninstructions. However, a foreseeable issue is that those embodied agents can\nalso flawlessly execute some hazardous tasks, potentially causing damages in\nthe real world. Existing benchmarks predominantly overlook critical safety\nrisks, focusing solely on planning performance, while a few evaluate LLMs'\nsafety awareness only on non-interactive image-text data. To address this gap,\nwe present SafeAgentBench -- the first comprehensive benchmark for safety-aware\ntask planning of embodied LLM agents in interactive simulation environments,\ncovering both explicit and implicit hazards. SafeAgentBench includes: (1) an\nexecutable, diverse, and high-quality dataset of 750 tasks, rigorously curated\nto cover 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal\nembodied environment with a low-level controller, supporting multi-agent\nexecution with 17 high-level actions for 9 state-of-the-art baselines; and (3)\nreliable evaluation methods from both execution and semantic perspectives.\nExperimental results show that, although agents based on different design\nframeworks exhibit substantial differences in task success rates, their overall\nsafety awareness remains weak. The most safety-conscious baseline achieves only\na 10% rejection rate for detailed hazardous tasks. Moreover, simply replacing\nthe LLM driving the agent does not lead to notable improvements in safety\nawareness. Dataset and codes are available in\nhttps://github.com/shengyin1224/SafeAgentBench and\nhttps://huggingface.co/datasets/safeagentbench/SafeAgentBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the integration of large language models (LLMs), embodied agents have\nstrong capabilities to understand and plan complicated natural language\ninstructions. However, a foreseeable issue is that those embodied agents can\nalso flawlessly execute some hazardous tasks, potentially causing damages in\nthe real world. Existing benchmarks predominantly overlook critical safety\nrisks, focusing solely on planning performance, while a few evaluate LLMs'\nsafety awareness only on non-interactive image-text data. To address this gap,\nwe present SafeAgentBench -- the first comprehensive benchmark for safety-aware\ntask planning of embodied LLM agents in interactive simulation environments,\ncovering both explicit and implicit hazards. SafeAgentBench includes: (1) an\nexecutable, diverse, and high-quality dataset of 750 tasks, rigorously curated\nto cover 10 potential hazards and 3 task types; (2) SafeAgentEnv, a universal\nembodied environment with a low-level controller, supporting multi-agent\nexecution with 17 high-level actions for 9 state-of-the-art baselines; and (3)\nreliable evaluation methods from both execution and semantic perspectives.\nExperimental results show that, although agents based on different design\nframeworks exhibit substantial differences in task success rates, their overall\nsafety awareness remains weak. The most safety-conscious baseline achieves only\na 10% rejection rate for detailed hazardous tasks. Moreover, simply replacing\nthe LLM driving the agent does not lead to notable improvements in safety\nawareness. Dataset and codes are available in\nhttps://github.com/shengyin1224/SafeAgentBench and\nhttps://huggingface.co/datasets/safeagentbench/SafeAgentBench."
                },
                "authors": [
                    {
                        "name": "Sheng Yin"
                    },
                    {
                        "name": "Xianghe Pang"
                    },
                    {
                        "name": "Yuanzhuo Ding"
                    },
                    {
                        "name": "Menglan Chen"
                    },
                    {
                        "name": "Yutong Bi"
                    },
                    {
                        "name": "Yichen Xiong"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Zhen Xiang"
                    },
                    {
                        "name": "Jing Shao"
                    },
                    {
                        "name": "Siheng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Siheng Chen"
                },
                "author": "Siheng Chen",
                "arxiv_comment": "28 pages, 19 tables, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13178v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13178v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27267v1",
                "updated": "2025-10-31T08:07:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    7,
                    16,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T08:07:16Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    7,
                    16,
                    4,
                    304,
                    0
                ],
                "title": "MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities\n  of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities\n  of Large Language Models"
                },
                "summary": "As large language models (LLMs) enter the medical domain, most benchmarks\nevaluate them on question answering or descriptive reasoning, overlooking\nquantitative reasoning critical to clinical decision-making. Existing datasets\nlike MedCalc-Bench cover few calculation tasks and fail to reflect real-world\ncomputational scenarios.\n  We introduce MedCalc-Eval, the largest benchmark for assessing LLMs' medical\ncalculation abilities, comprising 700+ tasks across two types: equation-based\n(e.g., Cockcroft-Gault, BMI, BSA) and rule-based scoring systems (e.g., Apgar,\nGlasgow Coma Scale). These tasks span diverse specialties including internal\nmedicine, surgery, pediatrics, and cardiology, offering a broader and more\nchallenging evaluation setting.\n  To improve performance, we further develop MedCalc-Env, a reinforcement\nlearning environment built on the InternBootcamp framework, enabling multi-step\nclinical reasoning and planning. Fine-tuning a Qwen2.5-32B model within this\nenvironment achieves state-of-the-art results on MedCalc-Eval, with notable\ngains in numerical sensitivity, formula selection, and reasoning robustness.\nRemaining challenges include unit conversion, multi-condition logic, and\ncontextual understanding.\n  Code and datasets are available at\nhttps://github.com/maokangkun/MedCalc-Eval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) enter the medical domain, most benchmarks\nevaluate them on question answering or descriptive reasoning, overlooking\nquantitative reasoning critical to clinical decision-making. Existing datasets\nlike MedCalc-Bench cover few calculation tasks and fail to reflect real-world\ncomputational scenarios.\n  We introduce MedCalc-Eval, the largest benchmark for assessing LLMs' medical\ncalculation abilities, comprising 700+ tasks across two types: equation-based\n(e.g., Cockcroft-Gault, BMI, BSA) and rule-based scoring systems (e.g., Apgar,\nGlasgow Coma Scale). These tasks span diverse specialties including internal\nmedicine, surgery, pediatrics, and cardiology, offering a broader and more\nchallenging evaluation setting.\n  To improve performance, we further develop MedCalc-Env, a reinforcement\nlearning environment built on the InternBootcamp framework, enabling multi-step\nclinical reasoning and planning. Fine-tuning a Qwen2.5-32B model within this\nenvironment achieves state-of-the-art results on MedCalc-Eval, with notable\ngains in numerical sensitivity, formula selection, and reasoning robustness.\nRemaining challenges include unit conversion, multi-condition logic, and\ncontextual understanding.\n  Code and datasets are available at\nhttps://github.com/maokangkun/MedCalc-Eval."
                },
                "authors": [
                    {
                        "name": "Kangkun Mao"
                    },
                    {
                        "name": "Jinru Ding"
                    },
                    {
                        "name": "Jiayuan Chen"
                    },
                    {
                        "name": "Mouxiao Bian"
                    },
                    {
                        "name": "Ruiyao Chen"
                    },
                    {
                        "name": "Xinwei Peng"
                    },
                    {
                        "name": "Sijie Ren"
                    },
                    {
                        "name": "Linyang Li"
                    },
                    {
                        "name": "Jie Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Xu"
                },
                "author": "Jie Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27265v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27265v1",
                "updated": "2025-10-31T08:05:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    5,
                    40,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T08:05:40Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    5,
                    40,
                    4,
                    304,
                    0
                ],
                "title": "T3: Test-Time Model Merging in VLMs for Zero-Shot Medical Imaging\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "T3: Test-Time Model Merging in VLMs for Zero-Shot Medical Imaging\n  Analysis"
                },
                "summary": "In medical imaging, vision-language models face a critical duality:\npretrained networks offer broad robustness but lack subtle, modality-specific\ncharacteristics, while fine-tuned expert models achieve high in-distribution\naccuracy yet falter under modality shift. Existing model-merging techniques,\ndesigned for natural-image benchmarks, are simple and efficient but fail to\ndeliver consistent gains across diverse medical modalities; their static\ninterpolation limits reliability in varied clinical tasks. To address this, we\nintroduce Test-Time Task adaptive merging (T^3), a backpropagation-free\nframework that computes per-sample interpolation coefficients via the\nJensen-Shannon divergence between the two models' output distributions. T^3\ndynamically preserves local precision when models agree and defers to\ngeneralist robustness under drift. To overcome the inference costs of\nsample-wise merging, we further propose a batch-wise extension, T^3_B, that\ncomputes a merging coefficient across a batch of samples, dramatically reducing\ncomputational bottleneck. Recognizing the lack of a standardized\nmedical-merging benchmark, we present a rigorous cross-evaluation protocol\nspanning in-domain, base-to-novel, and corruptions across four modalities.\nEmpirically, T^3 sets new state-of-the-art in Top-1 accuracy and error\nreduction, outperforming strong baselines while maintaining efficiency, paving\nthe way for adaptive MVLM deployment in clinical settings. Our code is\navailable at https://github.com/Razaimam45/TCube.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In medical imaging, vision-language models face a critical duality:\npretrained networks offer broad robustness but lack subtle, modality-specific\ncharacteristics, while fine-tuned expert models achieve high in-distribution\naccuracy yet falter under modality shift. Existing model-merging techniques,\ndesigned for natural-image benchmarks, are simple and efficient but fail to\ndeliver consistent gains across diverse medical modalities; their static\ninterpolation limits reliability in varied clinical tasks. To address this, we\nintroduce Test-Time Task adaptive merging (T^3), a backpropagation-free\nframework that computes per-sample interpolation coefficients via the\nJensen-Shannon divergence between the two models' output distributions. T^3\ndynamically preserves local precision when models agree and defers to\ngeneralist robustness under drift. To overcome the inference costs of\nsample-wise merging, we further propose a batch-wise extension, T^3_B, that\ncomputes a merging coefficient across a batch of samples, dramatically reducing\ncomputational bottleneck. Recognizing the lack of a standardized\nmedical-merging benchmark, we present a rigorous cross-evaluation protocol\nspanning in-domain, base-to-novel, and corruptions across four modalities.\nEmpirically, T^3 sets new state-of-the-art in Top-1 accuracy and error\nreduction, outperforming strong baselines while maintaining efficiency, paving\nthe way for adaptive MVLM deployment in clinical settings. Our code is\navailable at https://github.com/Razaimam45/TCube."
                },
                "authors": [
                    {
                        "name": "Raza Imam"
                    },
                    {
                        "name": "Hu Wang"
                    },
                    {
                        "name": "Dwarikanath Mahapatra"
                    },
                    {
                        "name": "Mohammad Yaqub"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Yaqub"
                },
                "author": "Mohammad Yaqub",
                "arxiv_comment": "Main: 11 pages, Supplementary: 9 pages 10 tables, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27265v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27265v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27261v1",
                "updated": "2025-10-31T08:00:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    0,
                    32,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T08:00:32Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    8,
                    0,
                    32,
                    4,
                    304,
                    0
                ],
                "title": "RegionRAG: Region-level Retrieval-Augumented Generation for\n  Visually-Rich Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RegionRAG: Region-level Retrieval-Augumented Generation for\n  Visually-Rich Documents"
                },
                "summary": "Multi-modal Retrieval-Augmented Generation (RAG) has become a critical method\nfor empowering LLMs by leveraging candidate visual documents. However, current\nmethods consider the entire document as the basic retrieval unit, introducing\nsubstantial irrelevant visual content in two ways: 1) Relevant documents often\ncontain large regions unrelated to the query, diluting the focus on salient\ninformation; 2) Retrieving multiple documents to increase recall further\nintroduces redundant and irrelevant documents. These redundant contexts\ndistract the model's attention and further degrade the performance. To address\nthis challenge, we propose \\modelname, a novel framework that shifts the\nretrieval paradigm from the document level to the region level. During\ntraining, we design a hybrid supervision strategy from both labeled data and\nunlabeled data to pinpoint relevant patches. During inference, we propose a\ndynamic pipeline that intelligently groups salient patches into complete\nsemantic regions. By delegating the task of identifying relevant regions to the\nretriever, \\modelname enables the generator to focus solely on concise visual\ncontent relevant to queries, improving both efficiency and accuracy.\nExperiments on six benchmarks demonstrate that RegionRAG achieves\nstate-of-the-art performance. Improves retrieval accuracy by 10.02\\% in R@1 on\naverage and increases question answering accuracy by 3.56\\% while using only\n71.42\\% visual tokens compared to prior methods. The code will be available at\nhttps://github.com/Aeryn666/RegionRAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Retrieval-Augmented Generation (RAG) has become a critical method\nfor empowering LLMs by leveraging candidate visual documents. However, current\nmethods consider the entire document as the basic retrieval unit, introducing\nsubstantial irrelevant visual content in two ways: 1) Relevant documents often\ncontain large regions unrelated to the query, diluting the focus on salient\ninformation; 2) Retrieving multiple documents to increase recall further\nintroduces redundant and irrelevant documents. These redundant contexts\ndistract the model's attention and further degrade the performance. To address\nthis challenge, we propose \\modelname, a novel framework that shifts the\nretrieval paradigm from the document level to the region level. During\ntraining, we design a hybrid supervision strategy from both labeled data and\nunlabeled data to pinpoint relevant patches. During inference, we propose a\ndynamic pipeline that intelligently groups salient patches into complete\nsemantic regions. By delegating the task of identifying relevant regions to the\nretriever, \\modelname enables the generator to focus solely on concise visual\ncontent relevant to queries, improving both efficiency and accuracy.\nExperiments on six benchmarks demonstrate that RegionRAG achieves\nstate-of-the-art performance. Improves retrieval accuracy by 10.02\\% in R@1 on\naverage and increases question answering accuracy by 3.56\\% while using only\n71.42\\% visual tokens compared to prior methods. The code will be available at\nhttps://github.com/Aeryn666/RegionRAG."
                },
                "authors": [
                    {
                        "name": "Yinglu Li"
                    },
                    {
                        "name": "Zhiying Lu"
                    },
                    {
                        "name": "Zhihang Liu"
                    },
                    {
                        "name": "Chuanbin Liu"
                    },
                    {
                        "name": "Hongtao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Hongtao Xie"
                },
                "author": "Hongtao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03519v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03519v2",
                "updated": "2025-10-31T07:59:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    59,
                    10,
                    4,
                    304,
                    0
                ],
                "published": "2024-08-07T03:06:57Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    3,
                    6,
                    57,
                    2,
                    220,
                    0
                ],
                "title": "RepoMasterEval: Evaluating Code Completion via Real-World Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoMasterEval: Evaluating Code Completion via Real-World Repositories"
                },
                "summary": "With the growing reliance on automated code completion tools in software\ndevelopment, the need for comprehensive evaluation benchmarks has become\ncritical. Existing benchmarks focus more on code completion in function and\nclass level by providing text descriptions to prompt the model. By contrast,\nsuch descriptive prompt is commonly unavailable in real development and code\ncompletion can occur in wider range of situations such as in the middle of a\nfunction or a code block. These limitations makes existing evaluation\nbenchmarks poorly align with the practical scenarios of code completion tools.\nIn this paper, we propose RepoMasterEval, a novel benchmark for evaluating code\ncompletion models constructed from real-world repositories. Each benchmark\ndatum is generated by masking a code snippet (ground truth) from one source\ncode file with existing test suites. To improve test accuracy of model\ngenerated code, we employ mutation testing to measure the effectiveness of the\ntest cases and we manually crafted new test cases for those test suites with\nlow mutation score. Our empirical evaluation on 10 state-of-the-art models\nshows that test argumentation is critical in improving the accuracy of the\nbenchmark and RepoMasterEval is able to report variance in model performance in\nreal-world scenarios. The deployment of RepoMasterEval also revealed that the\nbenchmark is useful to give accurate feedback during model training and the\nscore is in high correlation with the model's performance in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing reliance on automated code completion tools in software\ndevelopment, the need for comprehensive evaluation benchmarks has become\ncritical. Existing benchmarks focus more on code completion in function and\nclass level by providing text descriptions to prompt the model. By contrast,\nsuch descriptive prompt is commonly unavailable in real development and code\ncompletion can occur in wider range of situations such as in the middle of a\nfunction or a code block. These limitations makes existing evaluation\nbenchmarks poorly align with the practical scenarios of code completion tools.\nIn this paper, we propose RepoMasterEval, a novel benchmark for evaluating code\ncompletion models constructed from real-world repositories. Each benchmark\ndatum is generated by masking a code snippet (ground truth) from one source\ncode file with existing test suites. To improve test accuracy of model\ngenerated code, we employ mutation testing to measure the effectiveness of the\ntest cases and we manually crafted new test cases for those test suites with\nlow mutation score. Our empirical evaluation on 10 state-of-the-art models\nshows that test argumentation is critical in improving the accuracy of the\nbenchmark and RepoMasterEval is able to report variance in model performance in\nreal-world scenarios. The deployment of RepoMasterEval also revealed that the\nbenchmark is useful to give accurate feedback during model training and the\nscore is in high correlation with the model's performance in practice."
                },
                "authors": [
                    {
                        "name": "Qinyun Wu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Pengfei Gao"
                    },
                    {
                        "name": "Ruida Hu"
                    },
                    {
                        "name": "Haoyu Gan"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Jinhe Tang"
                    },
                    {
                        "name": "Zhiwen Deng"
                    },
                    {
                        "name": "Zhanming Guan"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Xia Liu"
                    },
                    {
                        "name": "Ping Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ping Yang"
                },
                "author": "Ping Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03519v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03519v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05079v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05079v3",
                "updated": "2025-10-31T07:58:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    58,
                    52,
                    4,
                    304,
                    0
                ],
                "published": "2025-06-05T14:27:40Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    14,
                    27,
                    40,
                    3,
                    156,
                    0
                ],
                "title": "LLM-Guided Scenario-based GUI Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Guided Scenario-based GUI Testing"
                },
                "summary": "The assurance of mobile app GUIs has become increasingly important, as the\nGUI serves as the primary medium of interaction between users and apps.\nAlthough numerous automated GUI testing approaches have been developed with\ndiverse strategies, a substantial gap remains between these approaches and the\nunderlying app business logic. Most existing approaches focus on general\nexploration rather than the completion of specific testing scenarios, often\nmissing critical functionalities. Inspired by manual testing, which treats\nbusiness logic-driven scenarios as the fundamental unit of testing, this paper\nintroduces an approach that leverages large language models to comprehend GUI\nsemantics and contextual relevance to given scenarios. Building on this\ncapability, we propose ScenGen, an LLM-guided scenario-based GUI testing\nframework employing multi-agent collaboration to simulate and automate manual\ntesting phases.\n  Specifically, ScenGen integrates five agents: the Observer, Decider,\nExecutor, Supervisor, and Recorder. The Observer perceives the app GUI state by\nextracting and structuring GUI widgets and layouts, interpreting semantic\ninformation. This is passed to the Decider, which makes scenario-driven\ndecisions with LLM guidance to identify target widgets and determine actions\ntoward fulfilling specific goals. The Executor performs these operations, while\nthe Supervisor verifies alignment with intended scenario completion, ensuring\ntraceability and consistency. Finally, the Recorder logs GUI operations into\ncontext memory as a knowledge base for subsequent decision-making and monitors\nruntime bugs. Comprehensive evaluations demonstrate that ScenGen effectively\ngenerates scenario-based GUI tests guided by LLM collaboration, achieving\nhigher relevance to business logic and improving the completeness of automated\nGUI testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The assurance of mobile app GUIs has become increasingly important, as the\nGUI serves as the primary medium of interaction between users and apps.\nAlthough numerous automated GUI testing approaches have been developed with\ndiverse strategies, a substantial gap remains between these approaches and the\nunderlying app business logic. Most existing approaches focus on general\nexploration rather than the completion of specific testing scenarios, often\nmissing critical functionalities. Inspired by manual testing, which treats\nbusiness logic-driven scenarios as the fundamental unit of testing, this paper\nintroduces an approach that leverages large language models to comprehend GUI\nsemantics and contextual relevance to given scenarios. Building on this\ncapability, we propose ScenGen, an LLM-guided scenario-based GUI testing\nframework employing multi-agent collaboration to simulate and automate manual\ntesting phases.\n  Specifically, ScenGen integrates five agents: the Observer, Decider,\nExecutor, Supervisor, and Recorder. The Observer perceives the app GUI state by\nextracting and structuring GUI widgets and layouts, interpreting semantic\ninformation. This is passed to the Decider, which makes scenario-driven\ndecisions with LLM guidance to identify target widgets and determine actions\ntoward fulfilling specific goals. The Executor performs these operations, while\nthe Supervisor verifies alignment with intended scenario completion, ensuring\ntraceability and consistency. Finally, the Recorder logs GUI operations into\ncontext memory as a knowledge base for subsequent decision-making and monitors\nruntime bugs. Comprehensive evaluations demonstrate that ScenGen effectively\ngenerates scenario-based GUI tests guided by LLM collaboration, achieving\nhigher relevance to business logic and improving the completeness of automated\nGUI testing."
                },
                "authors": [
                    {
                        "name": "Shengcheng Yu"
                    },
                    {
                        "name": "Yuchen Ling"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Quan Zhou"
                    },
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Chunyang Chen"
                    },
                    {
                        "name": "Shaomin Zhu"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05079v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05079v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27257v1",
                "updated": "2025-10-31T07:53:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    53,
                    40,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T07:53:40Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    53,
                    40,
                    4,
                    304,
                    0
                ],
                "title": "Synergistic Tensor and Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synergistic Tensor and Pipeline Parallelism"
                },
                "summary": "In the machine learning system, the hybrid model parallelism combining tensor\nparallelism (TP) and pipeline parallelism (PP) has become the dominant solution\nfor distributed training of Large Language Models~(LLMs) and Multimodal LLMs\n(MLLMs). However, TP introduces significant collective communication overheads,\nwhile PP suffers from synchronization inefficiencies such as pipeline bubbles.\nExisting works primarily address these challenges from isolated perspectives,\nfocusing either on overlapping TP communication or on flexible PP scheduling to\nmitigate pipeline bubbles. In this paper, we propose a new synergistic tensor\nand pipeline parallelism schedule that simultaneously reduces both types of\nbubbles. Our proposed schedule decouples the forward and backward passes in PP\ninto fine-grained computation units, which are then braided to form a composite\ncomputation sequence. This compositional structure enables near-complete\nelimination of TP-related bubbles. Building upon this structure, we further\ndesign the PP schedule to minimize PP bubbles. Experimental results demonstrate\nthat our approach improves training throughput by up to 12% for LLMs and 16%\nfor MLLMs compared to existing scheduling methods. Our source code is avaiable\nat https://github.com/MICLAB-BUPT/STP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the machine learning system, the hybrid model parallelism combining tensor\nparallelism (TP) and pipeline parallelism (PP) has become the dominant solution\nfor distributed training of Large Language Models~(LLMs) and Multimodal LLMs\n(MLLMs). However, TP introduces significant collective communication overheads,\nwhile PP suffers from synchronization inefficiencies such as pipeline bubbles.\nExisting works primarily address these challenges from isolated perspectives,\nfocusing either on overlapping TP communication or on flexible PP scheduling to\nmitigate pipeline bubbles. In this paper, we propose a new synergistic tensor\nand pipeline parallelism schedule that simultaneously reduces both types of\nbubbles. Our proposed schedule decouples the forward and backward passes in PP\ninto fine-grained computation units, which are then braided to form a composite\ncomputation sequence. This compositional structure enables near-complete\nelimination of TP-related bubbles. Building upon this structure, we further\ndesign the PP schedule to minimize PP bubbles. Experimental results demonstrate\nthat our approach improves training throughput by up to 12% for LLMs and 16%\nfor MLLMs compared to existing scheduling methods. Our source code is avaiable\nat https://github.com/MICLAB-BUPT/STP."
                },
                "authors": [
                    {
                        "name": "Mengshi Qi"
                    },
                    {
                        "name": "Jiaxuan Peng"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Juan Zhu"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Huadong Ma"
                    }
                ],
                "author_detail": {
                    "name": "Huadong Ma"
                },
                "author": "Huadong Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14815v2",
                "updated": "2025-10-31T07:51:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    51,
                    57,
                    4,
                    304,
                    0
                ],
                "published": "2025-07-20T04:11:06Z",
                "published_parsed": [
                    2025,
                    7,
                    20,
                    4,
                    11,
                    6,
                    6,
                    201,
                    0
                ],
                "title": "FastLongSpeech: Enhancing Large Speech-Language Models for Efficient\n  Long-Speech Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastLongSpeech: Enhancing Large Speech-Language Models for Efficient\n  Long-Speech Processing"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has spurred significant\nprogress in Large Speech-Language Models (LSLMs), enhancing their capabilities\nin both speech understanding and generation. While existing LSLMs often\nconcentrate on augmenting speech generation or tackling a diverse array of\nshort-speech tasks, the efficient processing of long-form speech remains a\ncritical yet underexplored challenge. This gap is primarily attributed to the\nscarcity of long-speech training datasets and the high computational costs\nassociated with long sequences. To address these limitations, we introduce\nFastLongSpeech, a novel framework designed to extend LSLM capabilities for\nefficient long-speech processing without necessitating dedicated long-speech\ntraining data. FastLongSpeech incorporates an iterative fusion strategy that\ncan compress excessively long-speech sequences into manageable lengths. To\nadapt LSLMs for long-speech inputs, it introduces a dynamic compression\ntraining approach, which exposes the model to short-speech sequences at varying\ncompression ratios, thereby transferring the capabilities of LSLMs to\nlong-speech tasks. To assess the long-speech capabilities of LSLMs, we develop\na long-speech understanding benchmark called LongSpeech-Eval. Experiments show\nthat our method exhibits strong performance in both long-speech and\nshort-speech tasks, while greatly improving inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has spurred significant\nprogress in Large Speech-Language Models (LSLMs), enhancing their capabilities\nin both speech understanding and generation. While existing LSLMs often\nconcentrate on augmenting speech generation or tackling a diverse array of\nshort-speech tasks, the efficient processing of long-form speech remains a\ncritical yet underexplored challenge. This gap is primarily attributed to the\nscarcity of long-speech training datasets and the high computational costs\nassociated with long sequences. To address these limitations, we introduce\nFastLongSpeech, a novel framework designed to extend LSLM capabilities for\nefficient long-speech processing without necessitating dedicated long-speech\ntraining data. FastLongSpeech incorporates an iterative fusion strategy that\ncan compress excessively long-speech sequences into manageable lengths. To\nadapt LSLMs for long-speech inputs, it introduces a dynamic compression\ntraining approach, which exposes the model to short-speech sequences at varying\ncompression ratios, thereby transferring the capabilities of LSLMs to\nlong-speech tasks. To assess the long-speech capabilities of LSLMs, we develop\na long-speech understanding benchmark called LongSpeech-Eval. Experiments show\nthat our method exhibits strong performance in both long-speech and\nshort-speech tasks, while greatly improving inference efficiency."
                },
                "authors": [
                    {
                        "name": "Shoutao Guo"
                    },
                    {
                        "name": "Shaolei Zhang"
                    },
                    {
                        "name": "Qingkai Fang"
                    },
                    {
                        "name": "Zhengrui Ma"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Yang Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yang Feng"
                },
                "author": "Yang Feng",
                "arxiv_comment": "NeurIPS 2025. The code is at\n  https://github.com/ictnlp/FastLongSpeech. This model is at\n  https://huggingface.co/ICTNLP/FastLongSpeech. The dataset is at\n  https://huggingface.co/datasets/ICTNLP/LongSpeech-Eval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27254v1",
                "updated": "2025-10-31T07:43:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    43,
                    21,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T07:43:21Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    43,
                    21,
                    4,
                    304,
                    0
                ],
                "title": "Languages are Modalities: Cross-Lingual Alignment via Encoder Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Languages are Modalities: Cross-Lingual Alignment via Encoder Injection"
                },
                "summary": "Instruction-tuned Large Language Models (LLMs) underperform on low resource,\nnon-Latin scripts due to tokenizer fragmentation and weak cross-lingual\ncoupling. We present LLINK (Latent Language Injection for Non-English\nKnowledge), a compute efficient language-as-modality method that conditions an\ninstruction-tuned decoder without changing the tokenizer or retraining the\ndecoder. First, we align sentence embeddings from a frozen multilingual encoder\nto the decoder's latent embedding space at a reserved position via a\nlightweight contrastive projector. Second, the vector is expanded into K soft\nslots and trained with minimal adapters so the frozen decoder consumes the\nsignal. LLINK substantially improves bilingual retrieval and achieves 81.3%\npreference over the base model and 63.6% over direct fine-tuning in LLM-judged\nQ&A evaluations. We further find that improvements can be attributed to reduced\ntokenization inflation and a stronger cross lingual alignment, despite the\nmodel having residual weaknesses in numeric fidelity. Treating low resource\nlanguages as a modality offers a practical path to stronger cross-lingual\nalignment in lightweight LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuned Large Language Models (LLMs) underperform on low resource,\nnon-Latin scripts due to tokenizer fragmentation and weak cross-lingual\ncoupling. We present LLINK (Latent Language Injection for Non-English\nKnowledge), a compute efficient language-as-modality method that conditions an\ninstruction-tuned decoder without changing the tokenizer or retraining the\ndecoder. First, we align sentence embeddings from a frozen multilingual encoder\nto the decoder's latent embedding space at a reserved position via a\nlightweight contrastive projector. Second, the vector is expanded into K soft\nslots and trained with minimal adapters so the frozen decoder consumes the\nsignal. LLINK substantially improves bilingual retrieval and achieves 81.3%\npreference over the base model and 63.6% over direct fine-tuning in LLM-judged\nQ&A evaluations. We further find that improvements can be attributed to reduced\ntokenization inflation and a stronger cross lingual alignment, despite the\nmodel having residual weaknesses in numeric fidelity. Treating low resource\nlanguages as a modality offers a practical path to stronger cross-lingual\nalignment in lightweight LLMs."
                },
                "authors": [
                    {
                        "name": "Rajan Agarwal"
                    },
                    {
                        "name": "Aarush Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Aarush Gupta"
                },
                "author": "Aarush Gupta",
                "arxiv_comment": "14 pages, 3 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27251v1",
                "updated": "2025-10-31T07:39:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    39,
                    26,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T07:39:26Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    39,
                    26,
                    4,
                    304,
                    0
                ],
                "title": "FinPos: A Position-Aware Trading Agent System for Real Financial Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinPos: A Position-Aware Trading Agent System for Real Financial Markets"
                },
                "summary": "The exceptional potential of large language models (LLMs) in handling text\ninformation has garnered significant attention in the field of financial\ntrading. However, current trading agents primarily focus on single-step trading\ntasks and lack awareness of continuous position management. Therefore, we\npropose a position-aware trading task designed to simulate a more realistic\nmarket. To address this task, we develop a trading agent system, FinPos,\noptimized for position management. FinPos is able to interpret various types of\nmarket information from a professional perspective, providing a reliable basis\nfor positioning decisions. To mitigate the substantial market risks arising\nfrom position fluctuations, FinPos employs dual decision agents. Furthermore,\nthe continuous nature of position management necessitates our adoption of\nmulti-timescale rewards, which in turn empowers FinPos to effectively balance\nshort-term fluctuations against long-term trends. Extensive experiments\ndemonstrate that FinPos surpasses state-of-the-art trading agents in the\nposition-aware trading task, which closely mirrors real market conditions. More\nimportantly, our findings reveal that LLM-centered agent systems exhibit a\nvast, largely unexplored potential in long-term market decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exceptional potential of large language models (LLMs) in handling text\ninformation has garnered significant attention in the field of financial\ntrading. However, current trading agents primarily focus on single-step trading\ntasks and lack awareness of continuous position management. Therefore, we\npropose a position-aware trading task designed to simulate a more realistic\nmarket. To address this task, we develop a trading agent system, FinPos,\noptimized for position management. FinPos is able to interpret various types of\nmarket information from a professional perspective, providing a reliable basis\nfor positioning decisions. To mitigate the substantial market risks arising\nfrom position fluctuations, FinPos employs dual decision agents. Furthermore,\nthe continuous nature of position management necessitates our adoption of\nmulti-timescale rewards, which in turn empowers FinPos to effectively balance\nshort-term fluctuations against long-term trends. Extensive experiments\ndemonstrate that FinPos surpasses state-of-the-art trading agents in the\nposition-aware trading task, which closely mirrors real market conditions. More\nimportantly, our findings reveal that LLM-centered agent systems exhibit a\nvast, largely unexplored potential in long-term market decision-making."
                },
                "authors": [
                    {
                        "name": "Bijia Liu"
                    },
                    {
                        "name": "Ronghao Dang"
                    }
                ],
                "author_detail": {
                    "name": "Ronghao Dang"
                },
                "author": "Ronghao Dang",
                "arxiv_comment": "LLM Applications, LLM Agents, Financial Technology",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27246v1",
                "updated": "2025-10-31T07:29:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    29,
                    52,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T07:29:52Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    29,
                    52,
                    4,
                    304,
                    0
                ],
                "title": "Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond a Million Tokens: Benchmarking and Enhancing Long-Term Memory in\n  LLMs"
                },
                "summary": "Evaluating the abilities of large language models (LLMs) for tasks that\nrequire long-term memory and thus long-context reasoning, for example in\nconversational settings, is hampered by the existing benchmarks, which often\nlack narrative coherence, cover narrow domains, and only test simple\nrecall-oriented tasks. This paper introduces a comprehensive solution to these\nchallenges. First, we present a novel framework for automatically generating\nlong (up to 10M tokens), coherent, and topically diverse conversations,\naccompanied by probing questions targeting a wide range of memory abilities.\nFrom this, we construct BEAM, a new benchmark comprising 100 conversations and\n2,000 validated questions. Second, to enhance model performance, we propose\nLIGHT-a framework inspired by human cognition that equips LLMs with three\ncomplementary memory systems: a long-term episodic memory, a short-term working\nmemory, and a scratchpad for accumulating salient facts. Our experiments on\nBEAM reveal that even LLMs with 1M token context windows (with and without\nretrieval-augmentation) struggle as dialogues lengthen. In contrast, LIGHT\nconsistently improves performance across various models, achieving an average\nimprovement of 3.5%-12.69% over the strongest baselines, depending on the\nbackbone LLM. An ablation study further confirms the contribution of each\nmemory component.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the abilities of large language models (LLMs) for tasks that\nrequire long-term memory and thus long-context reasoning, for example in\nconversational settings, is hampered by the existing benchmarks, which often\nlack narrative coherence, cover narrow domains, and only test simple\nrecall-oriented tasks. This paper introduces a comprehensive solution to these\nchallenges. First, we present a novel framework for automatically generating\nlong (up to 10M tokens), coherent, and topically diverse conversations,\naccompanied by probing questions targeting a wide range of memory abilities.\nFrom this, we construct BEAM, a new benchmark comprising 100 conversations and\n2,000 validated questions. Second, to enhance model performance, we propose\nLIGHT-a framework inspired by human cognition that equips LLMs with three\ncomplementary memory systems: a long-term episodic memory, a short-term working\nmemory, and a scratchpad for accumulating salient facts. Our experiments on\nBEAM reveal that even LLMs with 1M token context windows (with and without\nretrieval-augmentation) struggle as dialogues lengthen. In contrast, LIGHT\nconsistently improves performance across various models, achieving an average\nimprovement of 3.5%-12.69% over the strongest baselines, depending on the\nbackbone LLM. An ablation study further confirms the contribution of each\nmemory component."
                },
                "authors": [
                    {
                        "name": "Mohammad Tavakoli"
                    },
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Carrie Ye"
                    },
                    {
                        "name": "Mohamed Abdalla"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "J Ross Mitchell"
                    }
                ],
                "author_detail": {
                    "name": "J Ross Mitchell"
                },
                "author": "J Ross Mitchell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27244v1",
                "updated": "2025-10-31T07:27:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    27,
                    54,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T07:27:54Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    27,
                    54,
                    4,
                    304,
                    0
                ],
                "title": "Vintage Code, Modern Judges: Meta-Validation in Low Data Regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vintage Code, Modern Judges: Meta-Validation in Low Data Regimes"
                },
                "summary": "Application modernization in legacy languages such as COBOL, PL/I, and REXX\nfaces an acute shortage of resources, both in expert availability and in\nhigh-quality human evaluation data. While Large Language Models as a Judge\n(LaaJ) offer a scalable alternative to expert review, their reliability must be\nvalidated before being trusted in high-stakes workflows. Without principled\nvalidation, organizations risk a circular evaluation loop, where unverified\nLaaJs are used to assess model outputs, potentially reinforcing unreliable\njudgments and compromising downstream deployment decisions. Although various\nautomated approaches to validating LaaJs have been proposed, alignment with\nhuman judgment remains a widely used and conceptually grounded validation\nstrategy. In many real-world domains, the availability of human-labeled\nevaluation data is severely limited, making it difficult to assess how well a\nLaaJ aligns with human judgment. We introduce SparseAlign, a formal framework\nfor assessing LaaJ alignment with sparse human-labeled data. SparseAlign\ncombines a novel pairwise-confidence concept with a score-sensitive alignment\nmetric that jointly capture ranking consistency and score proximity, enabling\nreliable evaluator selection even when traditional statistical methods are\nineffective due to limited annotated examples. SparseAlign was applied\ninternally to select LaaJs for COBOL code explanation. The top-aligned\nevaluators were integrated into assessment workflows, guiding model release\ndecisions. We present a case study of four LaaJs to demonstrate SparseAlign's\nutility in real-world evaluation scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application modernization in legacy languages such as COBOL, PL/I, and REXX\nfaces an acute shortage of resources, both in expert availability and in\nhigh-quality human evaluation data. While Large Language Models as a Judge\n(LaaJ) offer a scalable alternative to expert review, their reliability must be\nvalidated before being trusted in high-stakes workflows. Without principled\nvalidation, organizations risk a circular evaluation loop, where unverified\nLaaJs are used to assess model outputs, potentially reinforcing unreliable\njudgments and compromising downstream deployment decisions. Although various\nautomated approaches to validating LaaJs have been proposed, alignment with\nhuman judgment remains a widely used and conceptually grounded validation\nstrategy. In many real-world domains, the availability of human-labeled\nevaluation data is severely limited, making it difficult to assess how well a\nLaaJ aligns with human judgment. We introduce SparseAlign, a formal framework\nfor assessing LaaJ alignment with sparse human-labeled data. SparseAlign\ncombines a novel pairwise-confidence concept with a score-sensitive alignment\nmetric that jointly capture ranking consistency and score proximity, enabling\nreliable evaluator selection even when traditional statistical methods are\nineffective due to limited annotated examples. SparseAlign was applied\ninternally to select LaaJs for COBOL code explanation. The top-aligned\nevaluators were integrated into assessment workflows, guiding model release\ndecisions. We present a case study of four LaaJs to demonstrate SparseAlign's\nutility in real-world evaluation scenarios."
                },
                "authors": [
                    {
                        "name": "Ora Nova Fandina"
                    },
                    {
                        "name": "Gal Amram"
                    },
                    {
                        "name": "Eitan Farchi"
                    },
                    {
                        "name": "Shmulik Froimovich"
                    },
                    {
                        "name": "Raviv Gal"
                    },
                    {
                        "name": "Wesam Ibraheem"
                    },
                    {
                        "name": "Rami Katan"
                    },
                    {
                        "name": "Alice Podolsky"
                    },
                    {
                        "name": "Orna Raz"
                    }
                ],
                "author_detail": {
                    "name": "Orna Raz"
                },
                "author": "Orna Raz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27241v1",
                "updated": "2025-10-31T07:10:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    10,
                    30,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T07:10:30Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    10,
                    30,
                    4,
                    304,
                    0
                ],
                "title": "Identifying the Periodicity of Information in Natural Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying the Periodicity of Information in Natural Language"
                },
                "summary": "Recent theoretical advancement of information density in natural language has\nbrought the following question on desk: To what degree does natural language\nexhibit periodicity pattern in its encoded information? We address this\nquestion by introducing a new method called AutoPeriod of Surprisal (APS). APS\nadopts a canonical periodicity detection algorithm and is able to identify any\nsignificant periods that exist in the surprisal sequence of a single document.\nBy applying the algorithm to a set of corpora, we have obtained the following\ninteresting results: Firstly, a considerable proportion of human language\ndemonstrates a strong pattern of periodicity in information; Secondly, new\nperiods that are outside the distributions of typical structural units in text\n(e.g., sentence boundaries, elementary discourse units, etc.) are found and\nfurther confirmed via harmonic regression modeling. We conclude that the\nperiodicity of information in language is a joint outcome from both structured\nfactors and other driving factors that take effect at longer distances. The\nadvantages of our periodicity detection method and its potentials in\nLLM-generation detection are further discussed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent theoretical advancement of information density in natural language has\nbrought the following question on desk: To what degree does natural language\nexhibit periodicity pattern in its encoded information? We address this\nquestion by introducing a new method called AutoPeriod of Surprisal (APS). APS\nadopts a canonical periodicity detection algorithm and is able to identify any\nsignificant periods that exist in the surprisal sequence of a single document.\nBy applying the algorithm to a set of corpora, we have obtained the following\ninteresting results: Firstly, a considerable proportion of human language\ndemonstrates a strong pattern of periodicity in information; Secondly, new\nperiods that are outside the distributions of typical structural units in text\n(e.g., sentence boundaries, elementary discourse units, etc.) are found and\nfurther confirmed via harmonic regression modeling. We conclude that the\nperiodicity of information in language is a joint outcome from both structured\nfactors and other driving factors that take effect at longer distances. The\nadvantages of our periodicity detection method and its potentials in\nLLM-generation detection are further discussed."
                },
                "authors": [
                    {
                        "name": "Yulin Ou"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Hendrik Buschmeier"
                    }
                ],
                "author_detail": {
                    "name": "Hendrik Buschmeier"
                },
                "author": "Hendrik Buschmeier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18559v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18559v3",
                "updated": "2025-10-31T07:07:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    7,
                    12,
                    4,
                    304,
                    0
                ],
                "published": "2025-03-24T11:13:33Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    13,
                    33,
                    0,
                    83,
                    0
                ],
                "title": "AMD-Hummingbird: Towards an Efficient Text-to-Video Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AMD-Hummingbird: Towards an Efficient Text-to-Video Model"
                },
                "summary": "Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Video (T2V) generation has attracted significant attention for its\nability to synthesize realistic videos from textual descriptions. However,\nexisting models struggle to balance computational efficiency and high visual\nquality, particularly on resource-limited devices, e.g.,iGPUs and mobile\nphones. Most prior work prioritizes visual fidelity while overlooking the need\nfor smaller, more efficient models suitable for real-world deployment. To\naddress this challenge, we propose a lightweight T2V framework, termed\nHummingbird, which prunes existing models and enhances visual quality through\nvisual feedback learning. Our approach reduces the size of the U-Net from 1.4\nbillion to 0.7 billion parameters, significantly improving efficiency while\npreserving high-quality video generation. Additionally, we introduce a novel\ndata processing pipeline that leverages Large Language Models (LLMs) and Video\nQuality Assessment (VQA) models to enhance the quality of both text prompts and\nvideo data. To support user-driven training and style customization, we\npublicly release the full training code, including data processing and model\ntraining. Extensive experiments show that our method achieves a 31X speedup\ncompared to state-of-the-art models such as VideoCrafter2, while also attaining\nthe highest overall score on VBench. Moreover, our method supports the\ngeneration of videos with up to 26 frames, addressing the limitations of\nexisting U-Net-based methods in long video generation. Notably, the entire\ntraining process requires only four GPUs, yet delivers performance competitive\nwith existing leading methods. Hummingbird presents a practical and efficient\nsolution for T2V generation, combining high performance, scalability, and\nflexibility for real-world applications."
                },
                "authors": [
                    {
                        "name": "Takashi Isobe"
                    },
                    {
                        "name": "He Cui"
                    },
                    {
                        "name": "Dong Zhou"
                    },
                    {
                        "name": "Mengmeng Ge"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "arxiv_comment": "Homepage:\n  https://www.amd.com/en/developer/resources/technical-articles/amd-hummingbird-0-9b-text-to-video-diffusion-model-with-4-step-inferencing.html|\n  GitHub: https://github.com/AMD-AIG-AIMA/AMD-Hummingbird-T2V",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18559v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18559v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14210v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14210v2",
                "updated": "2025-10-31T07:00:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    7,
                    0,
                    9,
                    4,
                    304,
                    0
                ],
                "published": "2025-07-15T09:43:35Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    9,
                    43,
                    35,
                    1,
                    196,
                    0
                ],
                "title": "Design and Analysis of Phase Conjugation-Based Self-Alignment\n  Beamforming for RIS-Assisted Terahertz SWIPT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Analysis of Phase Conjugation-Based Self-Alignment\n  Beamforming for RIS-Assisted Terahertz SWIPT"
                },
                "summary": "Terahertz (THz) simultaneous wireless information and power transfer (SWIPT)\nis a promising technology for enabling ultra-high-rate and low-latency\ncommunications in massive battery-free Internet of Things (IoT) deployments for\n6G networks. However, conventional THz systems rely on narrow directional beams\nthat necessitate precise alignment, typically achieved through high-overhead\nbeam scanning procedures, which fundamentally at odds with the energy\nconstraints of battery-free IoT devices. In this paper, we propose a novel\nself-alignment architecture for THz SWIPT leveraging a reconfigurable\nintelligent surface (RIS) to eliminate complex beam scanning. By integrating\nphase conjugate circuits at both the base station and user equipment, the RIS\nfacilitates a resonance-based bidirectional retro-reflection mechanism,\nenabling the system to autonomously converge to an aligned state without manual\nintervention. We develop an analytical channel transfer model and a power cycle\nmodel to characterize the resonance-assisted beam alignment process and power\ntransfer efficiency. Simulation results demonstrate that the RIS-enabled system\nachieves effective spatial power concentration with significant sidelobe\nsuppression, leading to a communication capacity of 127.84 Gbit/s and a\nreceived power of 13.62 mW over a 2.2-meter link.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz (THz) simultaneous wireless information and power transfer (SWIPT)\nis a promising technology for enabling ultra-high-rate and low-latency\ncommunications in massive battery-free Internet of Things (IoT) deployments for\n6G networks. However, conventional THz systems rely on narrow directional beams\nthat necessitate precise alignment, typically achieved through high-overhead\nbeam scanning procedures, which fundamentally at odds with the energy\nconstraints of battery-free IoT devices. In this paper, we propose a novel\nself-alignment architecture for THz SWIPT leveraging a reconfigurable\nintelligent surface (RIS) to eliminate complex beam scanning. By integrating\nphase conjugate circuits at both the base station and user equipment, the RIS\nfacilitates a resonance-based bidirectional retro-reflection mechanism,\nenabling the system to autonomously converge to an aligned state without manual\nintervention. We develop an analytical channel transfer model and a power cycle\nmodel to characterize the resonance-assisted beam alignment process and power\ntransfer efficiency. Simulation results demonstrate that the RIS-enabled system\nachieves effective spatial power concentration with significant sidelobe\nsuppression, leading to a communication capacity of 127.84 Gbit/s and a\nreceived power of 13.62 mW over a 2.2-meter link."
                },
                "authors": [
                    {
                        "name": "Jiayuan Wei"
                    },
                    {
                        "name": "Qingwei Jiang"
                    },
                    {
                        "name": "Wen Fang"
                    },
                    {
                        "name": "Mingqing Liu"
                    },
                    {
                        "name": "Qingwen Liu"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Qingqing Wu"
                    }
                ],
                "author_detail": {
                    "name": "Qingqing Wu"
                },
                "author": "Qingqing Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14210v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14210v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12993v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12993v2",
                "updated": "2025-10-31T06:25:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    6,
                    25,
                    38,
                    4,
                    304,
                    0
                ],
                "published": "2025-09-16T12:04:00Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    12,
                    4,
                    0,
                    1,
                    259,
                    0
                ],
                "title": "HPIM: Heterogeneous Processing-In-Memory-based Accelerator for Large\n  Language Models Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPIM: Heterogeneous Processing-In-Memory-based Accelerator for Large\n  Language Models Inference"
                },
                "summary": "The deployment of large language models (LLMs) presents significant\nchallenges due to their enormous memory footprints, low arithmetic intensity,\nand stringent latency requirements, particularly during the autoregressive\ndecoding stage. Traditional compute-centric accelerators, such as GPUs, suffer\nfrom severe resource underutilization and memory bandwidth bottlenecks in these\nmemory-bound workloads. To overcome these fundamental limitations, we propose\nHPIM, the first memory-centric heterogeneous Processing-In-Memory (PIM)\naccelerator that integrates SRAM-PIM and HBM-PIM subsystems designed\nspecifically for LLM inference. HPIM employs a software-hardware co-design\napproach that combines a specialized compiler framework with a heterogeneous\nhardware architecture. It intelligently partitions workloads based on their\ncharacteristics: latency-critical attention operations are mapped to the\nSRAM-PIM subsystem to exploit its ultra-low latency and high computational\nflexibility, while weight-intensive GEMV computations are assigned to the\nHBM-PIM subsystem to leverage its high internal bandwidth and large storage\ncapacity. Furthermore, HPIM introduces a tightly coupled pipeline strategy\nacross SRAM-PIM and HBM-PIM subsystems to maximize intra-token parallelism,\nthereby significantly mitigating serial dependency of the autoregressive\ndecoding stage. Comprehensive evaluations using a cycle-accurate simulator\ndemonstrate that HPIM significantly outperforms state-of-the-art accelerators,\nachieving a peak speedup of up to 22.8x compared to the NVIDIA A100 GPU.\nMoreover, HPIM exhibits superior performance over contemporary PIM-based\naccelerators, highlighting its potential as a highly practical and scalable\nsolution for accelerating large-scale LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large language models (LLMs) presents significant\nchallenges due to their enormous memory footprints, low arithmetic intensity,\nand stringent latency requirements, particularly during the autoregressive\ndecoding stage. Traditional compute-centric accelerators, such as GPUs, suffer\nfrom severe resource underutilization and memory bandwidth bottlenecks in these\nmemory-bound workloads. To overcome these fundamental limitations, we propose\nHPIM, the first memory-centric heterogeneous Processing-In-Memory (PIM)\naccelerator that integrates SRAM-PIM and HBM-PIM subsystems designed\nspecifically for LLM inference. HPIM employs a software-hardware co-design\napproach that combines a specialized compiler framework with a heterogeneous\nhardware architecture. It intelligently partitions workloads based on their\ncharacteristics: latency-critical attention operations are mapped to the\nSRAM-PIM subsystem to exploit its ultra-low latency and high computational\nflexibility, while weight-intensive GEMV computations are assigned to the\nHBM-PIM subsystem to leverage its high internal bandwidth and large storage\ncapacity. Furthermore, HPIM introduces a tightly coupled pipeline strategy\nacross SRAM-PIM and HBM-PIM subsystems to maximize intra-token parallelism,\nthereby significantly mitigating serial dependency of the autoregressive\ndecoding stage. Comprehensive evaluations using a cycle-accurate simulator\ndemonstrate that HPIM significantly outperforms state-of-the-art accelerators,\nachieving a peak speedup of up to 22.8x compared to the NVIDIA A100 GPU.\nMoreover, HPIM exhibits superior performance over contemporary PIM-based\naccelerators, highlighting its potential as a highly practical and scalable\nsolution for accelerating large-scale LLM inference."
                },
                "authors": [
                    {
                        "name": "Cenlin Duan"
                    },
                    {
                        "name": "Jianlei Yang"
                    },
                    {
                        "name": "Rubing Yang"
                    },
                    {
                        "name": "Yikun Wang"
                    },
                    {
                        "name": "Yiou Wang"
                    },
                    {
                        "name": "Lingkun Long"
                    },
                    {
                        "name": "Yingjie Qi"
                    },
                    {
                        "name": "Xiaolin He"
                    },
                    {
                        "name": "Ao Zhou"
                    },
                    {
                        "name": "Xueyan Wang"
                    },
                    {
                        "name": "Weisheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weisheng Zhao"
                },
                "author": "Weisheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12993v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27217v1",
                "updated": "2025-10-31T06:24:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    6,
                    24,
                    45,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T06:24:45Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    6,
                    24,
                    45,
                    4,
                    304,
                    0
                ],
                "title": "Joint Visible Light and Backscatter Communications for Proximity-Based\n  Indoor Asset Tracking Enabled by Energy-Neutral Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Visible Light and Backscatter Communications for Proximity-Based\n  Indoor Asset Tracking Enabled by Energy-Neutral Devices"
                },
                "summary": "In next-generation wireless systems, providing location-based mobile\ncomputing services for energy-neutral devices has become a crucial objective\nfor the provision of sustainable Internet of Things (IoT). Visible light\npositioning (VLP) has gained great research attention as a complementary method\nto radio frequency (RF) solutions since it can leverage ubiquitous lighting\ninfrastructure. However, conventional VLP receivers often rely on\nphotodetectors or cameras that are power-hungry, complex, and expensive. To\naddress this challenge, we propose a hybrid indoor asset tracking system that\nintegrates visible light communication (VLC) and backscatter communication (BC)\nwithin a simultaneous lightwave information and power transfer (SLIPT)\nframework. We design a low-complexity and energy-neutral IoT node, namely\nbackscatter device (BD) which harvests energy from light-emitting diode (LED)\naccess points, and then modulates and reflects ambient RF carriers to indicate\nits location within particular VLC cells. We present a multi-cell VLC\ndeployment with frequency division multiplexing (FDM) method that mitigates\ninterference among LED access points by assigning them distinct frequency pairs\nbased on a four-color map scheduling principle. We develop a lightweight\nparticle filter (PF) tracking algorithm at an edge RF reader, where the fusion\nof proximity reports and the received backscatter signal strength are employed\nto track the BD. Experimental results show that this approach achieves the\npositioning error of 0.318 m at 50th percentile and 0.634 m at 90th percentile,\nwhile avoiding the use of complex photodetectors and active RF synthesizing\ncomponents at the energy-neutral IoT node. By demonstrating robust performance\nin multiple indoor trajectories, the proposed solution enables scalable,\ncost-effective, and energy-neutral indoor tracking for pervasive and\nedge-assisted IoT applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In next-generation wireless systems, providing location-based mobile\ncomputing services for energy-neutral devices has become a crucial objective\nfor the provision of sustainable Internet of Things (IoT). Visible light\npositioning (VLP) has gained great research attention as a complementary method\nto radio frequency (RF) solutions since it can leverage ubiquitous lighting\ninfrastructure. However, conventional VLP receivers often rely on\nphotodetectors or cameras that are power-hungry, complex, and expensive. To\naddress this challenge, we propose a hybrid indoor asset tracking system that\nintegrates visible light communication (VLC) and backscatter communication (BC)\nwithin a simultaneous lightwave information and power transfer (SLIPT)\nframework. We design a low-complexity and energy-neutral IoT node, namely\nbackscatter device (BD) which harvests energy from light-emitting diode (LED)\naccess points, and then modulates and reflects ambient RF carriers to indicate\nits location within particular VLC cells. We present a multi-cell VLC\ndeployment with frequency division multiplexing (FDM) method that mitigates\ninterference among LED access points by assigning them distinct frequency pairs\nbased on a four-color map scheduling principle. We develop a lightweight\nparticle filter (PF) tracking algorithm at an edge RF reader, where the fusion\nof proximity reports and the received backscatter signal strength are employed\nto track the BD. Experimental results show that this approach achieves the\npositioning error of 0.318 m at 50th percentile and 0.634 m at 90th percentile,\nwhile avoiding the use of complex photodetectors and active RF synthesizing\ncomponents at the energy-neutral IoT node. By demonstrating robust performance\nin multiple indoor trajectories, the proposed solution enables scalable,\ncost-effective, and energy-neutral indoor tracking for pervasive and\nedge-assisted IoT applications."
                },
                "authors": [
                    {
                        "name": "Boxuan Xie"
                    },
                    {
                        "name": "Lauri Mela"
                    },
                    {
                        "name": "Alexis A. Dowhuszko"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Zehui Xiong"
                    },
                    {
                        "name": "Zhu Han"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Riku Jntti"
                    }
                ],
                "author_detail": {
                    "name": "Riku Jntti"
                },
                "author": "Riku Jntti",
                "arxiv_comment": "14 pages, 14 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02901v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02901v3",
                "updated": "2025-10-31T06:10:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    6,
                    10,
                    24,
                    4,
                    304,
                    0
                ],
                "published": "2024-03-05T12:11:07Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    12,
                    11,
                    7,
                    1,
                    65,
                    0
                ],
                "title": "A Comprehensive Survey on Process-Oriented Automatic Text Summarization\n  with Exploration of LLM-Based Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Process-Oriented Automatic Text Summarization\n  with Exploration of LLM-Based Methods"
                },
                "summary": "Automatic Text Summarization (ATS), utilizing Natural Language Processing\n(NLP) algorithms, aims to create concise and accurate summaries, thereby\nsignificantly reducing the human effort required in processing large volumes of\ntext. ATS has drawn considerable interest in both academic and industrial\ncircles. Many studies have been conducted in the past to survey ATS methods;\nhowever, they generally lack practicality for real-world implementations, as\nthey often categorize previous methods from a theoretical standpoint. Moreover,\nthe advent of Large Language Models (LLMs) has altered conventional ATS\nmethods. In this survey, we aim to 1) provide a comprehensive overview of ATS\nfrom a ``Process-Oriented Schema'' perspective, which is best aligned with\nreal-world implementations; 2) comprehensively review the latest LLM-based ATS\nworks; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in\nthe literature. To the best of our knowledge, this is the first survey to\nspecifically investigate LLM-based ATS methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Text Summarization (ATS), utilizing Natural Language Processing\n(NLP) algorithms, aims to create concise and accurate summaries, thereby\nsignificantly reducing the human effort required in processing large volumes of\ntext. ATS has drawn considerable interest in both academic and industrial\ncircles. Many studies have been conducted in the past to survey ATS methods;\nhowever, they generally lack practicality for real-world implementations, as\nthey often categorize previous methods from a theoretical standpoint. Moreover,\nthe advent of Large Language Models (LLMs) has altered conventional ATS\nmethods. In this survey, we aim to 1) provide a comprehensive overview of ATS\nfrom a ``Process-Oriented Schema'' perspective, which is best aligned with\nreal-world implementations; 2) comprehensively review the latest LLM-based ATS\nworks; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in\nthe literature. To the best of our knowledge, this is the first survey to\nspecifically investigate LLM-based ATS methods."
                },
                "authors": [
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Hanlei Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Jinghua Tan"
                    }
                ],
                "author_detail": {
                    "name": "Jinghua Tan"
                },
                "author": "Jinghua Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02901v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02901v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.03865v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.03865v2",
                "updated": "2025-10-31T06:08:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    6,
                    8,
                    26,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-04T16:22:19Z",
                "published_parsed": [
                    2025,
                    10,
                    4,
                    16,
                    22,
                    19,
                    5,
                    277,
                    0
                ],
                "title": "Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning\n  Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Reasoning Capabilities in LLMs via Reinforcement Learning\n  Exploration"
                },
                "summary": "Reinforcement learning with verifiable rewards (RLVR) has recently enhanced\nthe reasoning capabilities of large language models (LLMs), particularly for\nmathematical problem solving. However, a fundamental limitation remains: as the\nsampling budget increases, the advantage of RLVR-trained models over their\npretrained bases often diminishes or even vanishes, revealing a strong\ndependence on the base model's restricted search space. We attribute this\nphenomenon to the widespread use of the reverse Kullback-Leibler (KL)\ndivergence regularizer, whose mode-seeking behavior keeps the policy trapped\ninside the base model's support region and hampers wider exploration. To\naddress this issue, we propose RAPO (Rewards-Aware Policy Optimization), an\nalgorithm to promote broader yet focused exploration. Our method (i) utilizes\nthe forward KL penalty to replace the reverse KL penalty for\nout-of-distribution exploration, and (ii) reweights the reference policy to\nfacilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B\nmodels with RAPO on the 8K SimpleRL-Zero dataset, without supervised\nfine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO\nconsistently improves problem-solving performance. Notably, RAPO enables models\nto surpass the base model's performance ceiling and solves previously\nintractable problems, advancing the frontier of RLVR for challenging reasoning\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning with verifiable rewards (RLVR) has recently enhanced\nthe reasoning capabilities of large language models (LLMs), particularly for\nmathematical problem solving. However, a fundamental limitation remains: as the\nsampling budget increases, the advantage of RLVR-trained models over their\npretrained bases often diminishes or even vanishes, revealing a strong\ndependence on the base model's restricted search space. We attribute this\nphenomenon to the widespread use of the reverse Kullback-Leibler (KL)\ndivergence regularizer, whose mode-seeking behavior keeps the policy trapped\ninside the base model's support region and hampers wider exploration. To\naddress this issue, we propose RAPO (Rewards-Aware Policy Optimization), an\nalgorithm to promote broader yet focused exploration. Our method (i) utilizes\nthe forward KL penalty to replace the reverse KL penalty for\nout-of-distribution exploration, and (ii) reweights the reference policy to\nfacilitate adaptive in-distribution exploration. We train Qwen2.5-3B and 7B\nmodels with RAPO on the 8K SimpleRL-Zero dataset, without supervised\nfine-tuning, and evaluate them on AIME2024 and AIME2025. Results show that RAPO\nconsistently improves problem-solving performance. Notably, RAPO enables models\nto surpass the base model's performance ceiling and solves previously\nintractable problems, advancing the frontier of RLVR for challenging reasoning\ntasks."
                },
                "authors": [
                    {
                        "name": "Wenhao Deng"
                    },
                    {
                        "name": "Long Wei"
                    },
                    {
                        "name": "Chenglei Yu"
                    },
                    {
                        "name": "Tailin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tailin Wu"
                },
                "author": "Tailin Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.03865v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.03865v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21432v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21432v2",
                "updated": "2025-10-31T06:04:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    6,
                    4,
                    19,
                    4,
                    304,
                    0
                ],
                "published": "2025-08-29T09:01:34Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    1,
                    34,
                    4,
                    241,
                    0
                ],
                "title": "RepoMark: A Code Usage Auditing Framework for Code Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoMark: A Code Usage Auditing Framework for Code Large Language Models"
                },
                "summary": "The rapid development of Large Language Models (LLMs) for code generation has\ntransformed software development by automating coding tasks with unprecedented\nefficiency.\n  However, the training of these models on open-source code repositories (e.g.,\nfrom GitHub) raises critical ethical and legal concerns, particularly regarding\ndata authorization and open-source license compliance. Developers are\nincreasingly questioning whether model trainers have obtained proper\nauthorization before using repositories for training, especially given the lack\nof transparency in data collection.\n  To address these concerns, we propose a novel data marking framework RepoMark\nto audit the data usage of code LLMs. Our method enables repository owners to\nverify whether their code has been used in training, while ensuring semantic\npreservation, imperceptibility, and theoretical false detection rate (FDR)\nguarantees. By generating multiple semantically equivalent code variants,\nRepoMark introduces data marks into the code files, and during detection,\nRepoMark leverages a novel ranking-based hypothesis test to detect memorization\nwithin the model. Compared to prior data auditing approaches, RepoMark\nsignificantly enhances sample efficiency, allowing effective auditing even when\nthe user's repository possesses only a small number of code files.\n  Experiments demonstrate that RepoMark achieves a detection success rate over\n90\\% on small code repositories under a strict FDR guarantee of 5\\%. This\nrepresents a significant advancement over existing data marking techniques, all\nof which only achieve accuracy below 55\\% under identical settings. This\nfurther validates RepoMark as a robust, theoretically sound, and promising\nsolution for enhancing transparency in code LLM training, which can safeguard\nthe rights of repository owners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) for code generation has\ntransformed software development by automating coding tasks with unprecedented\nefficiency.\n  However, the training of these models on open-source code repositories (e.g.,\nfrom GitHub) raises critical ethical and legal concerns, particularly regarding\ndata authorization and open-source license compliance. Developers are\nincreasingly questioning whether model trainers have obtained proper\nauthorization before using repositories for training, especially given the lack\nof transparency in data collection.\n  To address these concerns, we propose a novel data marking framework RepoMark\nto audit the data usage of code LLMs. Our method enables repository owners to\nverify whether their code has been used in training, while ensuring semantic\npreservation, imperceptibility, and theoretical false detection rate (FDR)\nguarantees. By generating multiple semantically equivalent code variants,\nRepoMark introduces data marks into the code files, and during detection,\nRepoMark leverages a novel ranking-based hypothesis test to detect memorization\nwithin the model. Compared to prior data auditing approaches, RepoMark\nsignificantly enhances sample efficiency, allowing effective auditing even when\nthe user's repository possesses only a small number of code files.\n  Experiments demonstrate that RepoMark achieves a detection success rate over\n90\\% on small code repositories under a strict FDR guarantee of 5\\%. This\nrepresents a significant advancement over existing data marking techniques, all\nof which only achieve accuracy below 55\\% under identical settings. This\nfurther validates RepoMark as a robust, theoretically sound, and promising\nsolution for enhancing transparency in code LLM training, which can safeguard\nthe rights of repository owners."
                },
                "authors": [
                    {
                        "name": "Wenjie Qu"
                    },
                    {
                        "name": "Yuguang Zhou"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Lionel Z. Wang"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Zhang"
                },
                "author": "Jiaheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21432v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21432v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00883v2",
                "updated": "2025-10-31T06:03:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    6,
                    3,
                    55,
                    4,
                    304,
                    0
                ],
                "published": "2025-07-01T15:51:46Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    15,
                    51,
                    46,
                    1,
                    182,
                    0
                ],
                "title": "Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and\n  Scenario Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and\n  Scenario Perturbations"
                },
                "summary": "Although mathematics is often considered culturally neutral, the way\nmathematical problems are presented can carry implicit cultural context.\nExisting benchmarks like GSM8K are predominantly rooted in Western norms,\nincluding names, currencies, and everyday scenarios. In this work, we create\nculturally adapted variants of the GSM8K test set for five regions Africa,\nIndia, China, Korea, and Japan using prompt-based transformations followed by\nmanual verification. We evaluate six large language models (LLMs), ranging from\n8B to 72B parameters, across five prompting strategies to assess their\nrobustness to cultural variation in math problem presentation. Our findings\nreveal a consistent performance gap: models perform best on the original\nUS-centric dataset and comparatively worse on culturally adapted versions.\nHowever, models with reasoning capabilities are more resilient to these shifts,\nsuggesting that deeper reasoning helps bridge cultural presentation gaps in\nmathematical tasks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although mathematics is often considered culturally neutral, the way\nmathematical problems are presented can carry implicit cultural context.\nExisting benchmarks like GSM8K are predominantly rooted in Western norms,\nincluding names, currencies, and everyday scenarios. In this work, we create\nculturally adapted variants of the GSM8K test set for five regions Africa,\nIndia, China, Korea, and Japan using prompt-based transformations followed by\nmanual verification. We evaluate six large language models (LLMs), ranging from\n8B to 72B parameters, across five prompting strategies to assess their\nrobustness to cultural variation in math problem presentation. Our findings\nreveal a consistent performance gap: models perform best on the original\nUS-centric dataset and comparatively worse on culturally adapted versions.\nHowever, models with reasoning capabilities are more resilient to these shifts,\nsuggesting that deeper reasoning helps bridge cultural presentation gaps in\nmathematical tasks"
                },
                "authors": [
                    {
                        "name": "Aditya Tomar"
                    },
                    {
                        "name": "Nihar Ranjan Sahoo"
                    },
                    {
                        "name": "Ashish Mittal"
                    },
                    {
                        "name": "Rudra Murthy"
                    },
                    {
                        "name": "Pushpak Bhattacharyya"
                    }
                ],
                "author_detail": {
                    "name": "Pushpak Bhattacharyya"
                },
                "author": "Pushpak Bhattacharyya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27206v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27206v1",
                "updated": "2025-10-31T06:01:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    6,
                    1,
                    4,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T06:01:04Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    6,
                    1,
                    4,
                    4,
                    304,
                    0
                ],
                "title": "Fints: Efficient Inference-Time Personalization for LLMs with\n  Fine-Grained Instance-Tailored Steering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fints: Efficient Inference-Time Personalization for LLMs with\n  Fine-Grained Instance-Tailored Steering"
                },
                "summary": "The rapid evolution of large language models (LLMs) has intensified the\ndemand for effective personalization techniques that can adapt model behavior\nto individual user preferences. Despite the non-parametric methods utilizing\nthe in-context learning ability of LLMs, recent parametric adaptation methods,\nincluding personalized parameter-efficient fine-tuning and reward modeling\nemerge. However, these methods face limitations in handling dynamic user\npatterns and high data sparsity scenarios, due to low adaptability and data\nefficiency. To address these challenges, we propose a fine-grained and\ninstance-tailored steering framework that dynamically generates sample-level\ninterference vectors from user data and injects them into the model's forward\npass for personalized adaptation. Our approach introduces two key technical\ninnovations: a fine-grained steering component that captures nuanced signals by\nhooking activations from attention and MLP layers, and an input-aware\naggregation module that synthesizes these signals into contextually relevant\nenhancements. The method demonstrates high flexibility and data efficiency,\nexcelling in fast-changing distribution and high data sparsity scenarios. In\naddition, the proposed method is orthogonal to existing methods and operates as\na plug-in component compatible with different personalization techniques.\nExtensive experiments across diverse scenarios--including short-to-long text\ngeneration, and web function calling--validate the effectiveness and\ncompatibility of our approach. Results show that our method significantly\nenhances personalization performance in fast-shifting environments while\nmaintaining robustness across varying interaction modes and context lengths.\nImplementation is available at https://github.com/KounianhuaDu/Fints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of large language models (LLMs) has intensified the\ndemand for effective personalization techniques that can adapt model behavior\nto individual user preferences. Despite the non-parametric methods utilizing\nthe in-context learning ability of LLMs, recent parametric adaptation methods,\nincluding personalized parameter-efficient fine-tuning and reward modeling\nemerge. However, these methods face limitations in handling dynamic user\npatterns and high data sparsity scenarios, due to low adaptability and data\nefficiency. To address these challenges, we propose a fine-grained and\ninstance-tailored steering framework that dynamically generates sample-level\ninterference vectors from user data and injects them into the model's forward\npass for personalized adaptation. Our approach introduces two key technical\ninnovations: a fine-grained steering component that captures nuanced signals by\nhooking activations from attention and MLP layers, and an input-aware\naggregation module that synthesizes these signals into contextually relevant\nenhancements. The method demonstrates high flexibility and data efficiency,\nexcelling in fast-changing distribution and high data sparsity scenarios. In\naddition, the proposed method is orthogonal to existing methods and operates as\na plug-in component compatible with different personalization techniques.\nExtensive experiments across diverse scenarios--including short-to-long text\ngeneration, and web function calling--validate the effectiveness and\ncompatibility of our approach. Results show that our method significantly\nenhances personalization performance in fast-shifting environments while\nmaintaining robustness across varying interaction modes and context lengths.\nImplementation is available at https://github.com/KounianhuaDu/Fints."
                },
                "authors": [
                    {
                        "name": "Kounianhua Du"
                    },
                    {
                        "name": "Jianxing Liu"
                    },
                    {
                        "name": "Kangning Zhang"
                    },
                    {
                        "name": "Wenxiang Jiao"
                    },
                    {
                        "name": "Yuan Lu"
                    },
                    {
                        "name": "Jiarui Jin"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27206v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.16559v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.16559v3",
                "updated": "2025-10-31T05:31:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    5,
                    31,
                    37,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-18T16:13:50Z",
                "published_parsed": [
                    2025,
                    10,
                    18,
                    16,
                    13,
                    50,
                    5,
                    291,
                    0
                ],
                "title": "BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for\n  Engineering Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for\n  Engineering Construction"
                },
                "summary": "Engineering construction automation aims to transform natural language\nspecifications into physically viable structures, requiring complex integrated\nreasoning under strict physical constraints. While modern LLMs possess broad\nknowledge and strong reasoning capabilities that make them promising candidates\nfor this domain, their construction competencies remain largely unevaluated. To\naddress this gap, we introduce BuildArena, the first physics-aligned\ninteractive benchmark designed for language-driven engineering construction. It\ncontributes to the community in four aspects: (1) a highly customizable\nbenchmarking framework for in-depth comparison and analysis of LLMs; (2) an\nextendable task design strategy spanning static and dynamic mechanics across\nmultiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for\nsupporting construction based on language instructions; (4) a baseline LLM\nagentic workflow that effectively evaluates diverse model capabilities. On\neight frontier LLMs, BuildArena comprehensively evaluates their capabilities\nfor language-driven and physics-grounded construction automation. The project\npage is at https://build-arena.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Engineering construction automation aims to transform natural language\nspecifications into physically viable structures, requiring complex integrated\nreasoning under strict physical constraints. While modern LLMs possess broad\nknowledge and strong reasoning capabilities that make them promising candidates\nfor this domain, their construction competencies remain largely unevaluated. To\naddress this gap, we introduce BuildArena, the first physics-aligned\ninteractive benchmark designed for language-driven engineering construction. It\ncontributes to the community in four aspects: (1) a highly customizable\nbenchmarking framework for in-depth comparison and analysis of LLMs; (2) an\nextendable task design strategy spanning static and dynamic mechanics across\nmultiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for\nsupporting construction based on language instructions; (4) a baseline LLM\nagentic workflow that effectively evaluates diverse model capabilities. On\neight frontier LLMs, BuildArena comprehensively evaluates their capabilities\nfor language-driven and physics-grounded construction automation. The project\npage is at https://build-arena.github.io/."
                },
                "authors": [
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Tianrun Gao"
                    },
                    {
                        "name": "Wenhao Deng"
                    },
                    {
                        "name": "Long Wei"
                    },
                    {
                        "name": "Xiaowei Qian"
                    },
                    {
                        "name": "Yixian Jiang"
                    },
                    {
                        "name": "Chenglei Yu"
                    },
                    {
                        "name": "Tailin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tailin Wu"
                },
                "author": "Tailin Wu",
                "arxiv_comment": "33 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.16559v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.16559v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05589v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05589v2",
                "updated": "2025-10-31T05:24:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    5,
                    24,
                    27,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-07T05:29:18Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    5,
                    29,
                    18,
                    1,
                    280,
                    0
                ],
                "title": "Deciphering Invariant Feature Decoupling in Source-free Time Series\n  Forecasting with Proxy Denoising",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deciphering Invariant Feature Decoupling in Source-free Time Series\n  Forecasting with Proxy Denoising"
                },
                "summary": "The proliferation of mobile devices generates a massive volume of time series\nacross various domains, where effective time series forecasting enables a\nvariety of real-world applications. This study focuses on a new problem of\nsource-free domain adaptation for time series forecasting. It aims to adapt a\npretrained model from sufficient source time series to the sparse target time\nseries domain without access to the source data, embracing data protection\nregulations. To achieve this, we propose TimePD, the first source-free time\nseries forecasting framework with proxy denoising, where large language models\n(LLMs) are employed to benefit from their generalization capabilities.\nSpecifically, TimePD consists of three key components: (1) dual-branch\ninvariant disentangled feature learning that enforces representation- and\ngradient-wise invariance by means of season-trend decomposition; (2)\nlightweight, parameter-free proxy denoising that dynamically calibrates\nsystematic biases of LLMs; and (3) knowledge distillation that bidirectionally\naligns the denoised prediction and the original target prediction. Extensive\nexperiments on real-world datasets offer insight into the effectiveness of the\nproposed TimePD, outperforming SOTA baselines by 9.3% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of mobile devices generates a massive volume of time series\nacross various domains, where effective time series forecasting enables a\nvariety of real-world applications. This study focuses on a new problem of\nsource-free domain adaptation for time series forecasting. It aims to adapt a\npretrained model from sufficient source time series to the sparse target time\nseries domain without access to the source data, embracing data protection\nregulations. To achieve this, we propose TimePD, the first source-free time\nseries forecasting framework with proxy denoising, where large language models\n(LLMs) are employed to benefit from their generalization capabilities.\nSpecifically, TimePD consists of three key components: (1) dual-branch\ninvariant disentangled feature learning that enforces representation- and\ngradient-wise invariance by means of season-trend decomposition; (2)\nlightweight, parameter-free proxy denoising that dynamically calibrates\nsystematic biases of LLMs; and (3) knowledge distillation that bidirectionally\naligns the denoised prediction and the original target prediction. Extensive\nexperiments on real-world datasets offer insight into the effectiveness of the\nproposed TimePD, outperforming SOTA baselines by 9.3% on average."
                },
                "authors": [
                    {
                        "name": "Kangjia Yan"
                    },
                    {
                        "name": "Chenxi Liu"
                    },
                    {
                        "name": "Hao Miao"
                    },
                    {
                        "name": "Xinle Wu"
                    },
                    {
                        "name": "Yan Zhao"
                    },
                    {
                        "name": "Chenjuan Guo"
                    },
                    {
                        "name": "Bin Yang"
                    }
                ],
                "author_detail": {
                    "name": "Bin Yang"
                },
                "author": "Bin Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05589v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05589v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00081v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00081v2",
                "updated": "2025-10-31T05:23:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    5,
                    23,
                    32,
                    4,
                    304,
                    0
                ],
                "published": "2025-05-30T02:36:56Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    2,
                    36,
                    56,
                    4,
                    150,
                    0
                ],
                "title": "Artificial Empathy: AI based Mental Health",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Empathy: AI based Mental Health"
                },
                "summary": "Many people suffer from mental health problems but not everyone seeks\nprofessional help or has access to mental health care. AI chatbots have\nincreasingly become a go-to for individuals who either have mental disorders or\nsimply want someone to talk to. This paper presents a study on participants who\nhave previously used chatbots and a scenario-based testing of large language\nmodel (LLM) chatbots. Our findings indicate that AI chatbots were primarily\nutilized as a \"Five minute therapist\" or as a non-judgmental companion.\nParticipants appreciated the anonymity and lack of judgment from chatbots.\nHowever, there were concerns about privacy and the security of sensitive\ninformation. The scenario-based testing of LLM chatbots highlighted additional\nissues. Some chatbots were consistently reassuring, used emojis and names to\nadd a personal touch, and were quick to suggest seeking professional help.\nHowever, there were limitations such as inconsistent tone, occasional\ninappropriate responses (e.g., casual or romantic), and a lack of crisis\nsensitivity, particularly in recognizing red flag language and escalating\nresponses appropriately. These findings can inform both the technology and\nmental health care industries on how to better utilize AI chatbots to support\nindividuals during challenging emotional periods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many people suffer from mental health problems but not everyone seeks\nprofessional help or has access to mental health care. AI chatbots have\nincreasingly become a go-to for individuals who either have mental disorders or\nsimply want someone to talk to. This paper presents a study on participants who\nhave previously used chatbots and a scenario-based testing of large language\nmodel (LLM) chatbots. Our findings indicate that AI chatbots were primarily\nutilized as a \"Five minute therapist\" or as a non-judgmental companion.\nParticipants appreciated the anonymity and lack of judgment from chatbots.\nHowever, there were concerns about privacy and the security of sensitive\ninformation. The scenario-based testing of LLM chatbots highlighted additional\nissues. Some chatbots were consistently reassuring, used emojis and names to\nadd a personal touch, and were quick to suggest seeking professional help.\nHowever, there were limitations such as inconsistent tone, occasional\ninappropriate responses (e.g., casual or romantic), and a lack of crisis\nsensitivity, particularly in recognizing red flag language and escalating\nresponses appropriately. These findings can inform both the technology and\nmental health care industries on how to better utilize AI chatbots to support\nindividuals during challenging emotional periods."
                },
                "authors": [
                    {
                        "name": "Aditya Naik"
                    },
                    {
                        "name": "Jovi Thomas"
                    },
                    {
                        "name": "Teja Sree Mandava"
                    },
                    {
                        "name": "Himavanth Reddy Vemula"
                    }
                ],
                "author_detail": {
                    "name": "Himavanth Reddy Vemula"
                },
                "author": "Himavanth Reddy Vemula",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00081v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00081v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17948v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17948v4",
                "updated": "2025-10-31T05:15:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    5,
                    15,
                    11,
                    4,
                    304,
                    0
                ],
                "published": "2025-06-22T09:01:42Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    9,
                    1,
                    42,
                    6,
                    173,
                    0
                ],
                "title": "Your Build Scripts Stink: The State of Code Smells in Build Scripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Build Scripts Stink: The State of Code Smells in Build Scripts"
                },
                "summary": "Build scripts automate the process of compiling source code, managing\ndependencies, running tests, and packaging software into deployable artifacts.\nThese scripts are ubiquitous in modern software development pipelines for\nstreamlining testing and delivery. While developing build scripts,\npractitioners may inadvertently introduce code smells, which are recurring\npatterns of poor coding practices that may lead to build failures or increase\nrisk and technical debt. The goal of this study is to aid practitioners in\navoiding code smells in build scripts through an empirical study of build\nscripts and issues on GitHub.We employed a mixed-methods approach, combining\nqualitative and quantitative analysis. First, we conducted a qualitative\nanalysis of 2000 build-script-related GitHub issues to understand recurring\nsmells. Next, we developed a static analysis tool, Sniffer, to automatically\ndetect code smells in 5882 build scripts of Maven, Gradle, CMake, and Make\nfiles, collected from 4877 open-source GitHub repositories. To assess Sniffer's\nperformance, we conducted a user study, where Sniffer achieved higher\nprecision, recall, and F-score. We identified 13 code smell categories, with a\ntotal of 10,895 smell occurrences, where 3184 were in Maven, 1214 in Gradle,\n337 in CMake, and 6160 in Makefiles. Our analysis revealed that Insecure URLs\nwere the most prevalent code smell in Maven build scripts, while\nHardcodedPaths/URLs were commonly observed in both Gradle and CMake scripts.\nWildcard Usage emerged as the most frequent smell in Makefiles. The\nco-occurrence analysis revealed strong associations between specific smell\npairs of Hardcoded Paths/URLs with Duplicates, and Inconsistent Dependency\nManagement with Empty or Incomplete Tags, which indicate potential underlying\nissues in the build script structure and maintenance practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Build scripts automate the process of compiling source code, managing\ndependencies, running tests, and packaging software into deployable artifacts.\nThese scripts are ubiquitous in modern software development pipelines for\nstreamlining testing and delivery. While developing build scripts,\npractitioners may inadvertently introduce code smells, which are recurring\npatterns of poor coding practices that may lead to build failures or increase\nrisk and technical debt. The goal of this study is to aid practitioners in\navoiding code smells in build scripts through an empirical study of build\nscripts and issues on GitHub.We employed a mixed-methods approach, combining\nqualitative and quantitative analysis. First, we conducted a qualitative\nanalysis of 2000 build-script-related GitHub issues to understand recurring\nsmells. Next, we developed a static analysis tool, Sniffer, to automatically\ndetect code smells in 5882 build scripts of Maven, Gradle, CMake, and Make\nfiles, collected from 4877 open-source GitHub repositories. To assess Sniffer's\nperformance, we conducted a user study, where Sniffer achieved higher\nprecision, recall, and F-score. We identified 13 code smell categories, with a\ntotal of 10,895 smell occurrences, where 3184 were in Maven, 1214 in Gradle,\n337 in CMake, and 6160 in Makefiles. Our analysis revealed that Insecure URLs\nwere the most prevalent code smell in Maven build scripts, while\nHardcodedPaths/URLs were commonly observed in both Gradle and CMake scripts.\nWildcard Usage emerged as the most frequent smell in Makefiles. The\nco-occurrence analysis revealed strong associations between specific smell\npairs of Hardcoded Paths/URLs with Duplicates, and Inconsistent Dependency\nManagement with Empty or Incomplete Tags, which indicate potential underlying\nissues in the build script structure and maintenance practices."
                },
                "authors": [
                    {
                        "name": "Mahzabin Tamanna"
                    },
                    {
                        "name": "Yash Chandrani"
                    },
                    {
                        "name": "Matthew Burrows"
                    },
                    {
                        "name": "Brandon Wroblewski"
                    },
                    {
                        "name": "Laurie Williams"
                    },
                    {
                        "name": "Dominik Wermke"
                    }
                ],
                "author_detail": {
                    "name": "Dominik Wermke"
                },
                "author": "Dominik Wermke",
                "arxiv_comment": "13 pages, 5 tables, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17948v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17948v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.27185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.27185v1",
                "updated": "2025-10-31T05:14:10Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    5,
                    14,
                    10,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-31T05:14:10Z",
                "published_parsed": [
                    2025,
                    10,
                    31,
                    5,
                    14,
                    10,
                    4,
                    304,
                    0
                ],
                "title": "Dual-Scale Antenna Deployment for Pinching Antenna Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-Scale Antenna Deployment for Pinching Antenna Systems"
                },
                "summary": "A dual-scale deployment (DSD) framework for pinching antenna systems (PASS)\nis proposed. 1) In the first coarse stage, the pinching antenna (PA) is\ntransferred over a large-scale range at the waveguide level. 2) The refinement\nstage performs small-scale relocation of the PA with high precision. Four PA\ndeployment protocols are provided in the proposed DSD framework. Then, a\npractical power consumption model is proposed, based on which the theoretical\nenergy efficiency formulas for PASS are derived. The transmit precoding, PA\nradiation power, and PA deployment are jointly optimized to maximize the energy\nefficiency under the provided PA deployment protocols. To solve this\nnon-convex, highly coupled problem, a low-complexity penalty-based alternating\noptimization algorithm is proposed. Simulation results validate the accuracy of\ntheoretical results and the convergence of the proposed algorithm. It is\ndemonstrated that: 1) PASS delivers about 70% higher energy efficiency than the\nconventional cell-free architecture and nearly twofold improvement relative to\nMIMO systems; 2) it is essential to specify the DSD resolution and deployment\nprotocol to achieve the maximum energy efficiency for PASS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A dual-scale deployment (DSD) framework for pinching antenna systems (PASS)\nis proposed. 1) In the first coarse stage, the pinching antenna (PA) is\ntransferred over a large-scale range at the waveguide level. 2) The refinement\nstage performs small-scale relocation of the PA with high precision. Four PA\ndeployment protocols are provided in the proposed DSD framework. Then, a\npractical power consumption model is proposed, based on which the theoretical\nenergy efficiency formulas for PASS are derived. The transmit precoding, PA\nradiation power, and PA deployment are jointly optimized to maximize the energy\nefficiency under the provided PA deployment protocols. To solve this\nnon-convex, highly coupled problem, a low-complexity penalty-based alternating\noptimization algorithm is proposed. Simulation results validate the accuracy of\ntheoretical results and the convergence of the proposed algorithm. It is\ndemonstrated that: 1) PASS delivers about 70% higher energy efficiency than the\nconventional cell-free architecture and nearly twofold improvement relative to\nMIMO systems; 2) it is essential to specify the DSD resolution and deployment\nprotocol to achieve the maximum energy efficiency for PASS."
                },
                "authors": [
                    {
                        "name": "Xu Gan"
                    },
                    {
                        "name": "Zhaolin Wang"
                    },
                    {
                        "name": "Yuanwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanwei Liu"
                },
                "author": "Yuanwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.27185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.27185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.26606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.26606v2",
                "updated": "2025-10-31T05:11:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    31,
                    5,
                    11,
                    24,
                    4,
                    304,
                    0
                ],
                "published": "2025-10-30T15:35:13Z",
                "published_parsed": [
                    2025,
                    10,
                    30,
                    15,
                    35,
                    13,
                    3,
                    303,
                    0
                ],
                "title": "Normative Reasoning in Large Language Models: A Comparative Benchmark\n  from Logical and Modal Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Normative Reasoning in Large Language Models: A Comparative Benchmark\n  from Logical and Modal Perspectives"
                },
                "summary": "Normative reasoning is a type of reasoning that involves normative or deontic\nmodality, such as obligation and permission. While large language models (LLMs)\nhave demonstrated remarkable performance across various reasoning tasks, their\nability to handle normative reasoning remains underexplored. In this paper, we\nsystematically evaluate LLMs' reasoning capabilities in the normative domain\nfrom both logical and modal perspectives. Specifically, to assess how well LLMs\nreason with normative modals, we make a comparison between their reasoning with\nnormative modals and their reasoning with epistemic modals, which share a\ncommon formal structure. To this end, we introduce a new dataset covering a\nwide range of formal patterns of reasoning in both normative and epistemic\ndomains, while also incorporating non-formal cognitive factors that influence\nhuman reasoning. Our results indicate that, although LLMs generally adhere to\nvalid reasoning patterns, they exhibit notable inconsistencies in specific\ntypes of normative reasoning and display cognitive biases similar to those\nobserved in psychological studies of human reasoning. These findings highlight\nchallenges in achieving logical consistency in LLMs' normative reasoning and\nprovide insights for enhancing their reliability. All data and code are\nreleased publicly at https://github.com/kmineshima/NeuBAROCO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Normative reasoning is a type of reasoning that involves normative or deontic\nmodality, such as obligation and permission. While large language models (LLMs)\nhave demonstrated remarkable performance across various reasoning tasks, their\nability to handle normative reasoning remains underexplored. In this paper, we\nsystematically evaluate LLMs' reasoning capabilities in the normative domain\nfrom both logical and modal perspectives. Specifically, to assess how well LLMs\nreason with normative modals, we make a comparison between their reasoning with\nnormative modals and their reasoning with epistemic modals, which share a\ncommon formal structure. To this end, we introduce a new dataset covering a\nwide range of formal patterns of reasoning in both normative and epistemic\ndomains, while also incorporating non-formal cognitive factors that influence\nhuman reasoning. Our results indicate that, although LLMs generally adhere to\nvalid reasoning patterns, they exhibit notable inconsistencies in specific\ntypes of normative reasoning and display cognitive biases similar to those\nobserved in psychological studies of human reasoning. These findings highlight\nchallenges in achieving logical consistency in LLMs' normative reasoning and\nprovide insights for enhancing their reliability. All data and code are\nreleased publicly at https://github.com/kmineshima/NeuBAROCO."
                },
                "authors": [
                    {
                        "name": "Kentaro Ozeki"
                    },
                    {
                        "name": "Risako Ando"
                    },
                    {
                        "name": "Takanobu Morishita"
                    },
                    {
                        "name": "Hirohiko Abe"
                    },
                    {
                        "name": "Koji Mineshima"
                    },
                    {
                        "name": "Mitsuhiro Okada"
                    }
                ],
                "author_detail": {
                    "name": "Mitsuhiro Okada"
                },
                "author": "Mitsuhiro Okada",
                "arxiv_comment": "Accepted to the 8th BlackboxNLP Workshop at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.26606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.26606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]