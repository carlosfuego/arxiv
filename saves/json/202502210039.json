[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2410.17897v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v4",
                "updated": "2025-02-19T17:53:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    11,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning"
                },
                "summary": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer models have achieved remarkable success in various domains,\nthe effectiveness of information propagation through deep networks remains a\ncritical challenge. Standard hidden state residuals often fail to adequately\npreserve initial token-level information in deeper layers. This paper\nintroduces ResFormer, a novel architecture that enhances information flow by\nincorporating value residual connections in addition to hidden state residuals.\nAnd a variant is the SVFormer, where all layers share the first layer's value\nembedding. Comprehensive empirical evidence demonstrates ResFormer achieves\nequivalent validation loss with 13.3\\% fewer model parameters and 15.4\\% less\ntraining data compared to Transformer, while maintaining similar memory usage\nand computational cost. Besides, SVFormer reduces KV cache size by nearly half\nwith only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13873v1",
                "updated": "2025-02-19T16:54:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:54:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    54,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "NVR: Vector Runahead on NPUs for Sparse Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVR: Vector Runahead on NPUs for Sparse Memory Access"
                },
                "summary": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Neural Networks are increasingly leveraging sparsity to reduce the\nscaling up of model parameter size. However, reducing wall-clock time through\nsparsity and pruning remains challenging due to irregular memory access\npatterns, leading to frequent cache misses. In this paper, we present NPU\nVector Runahead (NVR), a prefetching mechanism tailored for NPUs to address\ncache miss problems in sparse DNN workloads. Rather than optimising memory\npatterns with high overhead and poor portability, NVR adapts runahead execution\nto the unique architecture of NPUs. NVR provides a general micro-architectural\nsolution for sparse DNN workloads without requiring compiler or algorithmic\nsupport, operating as a decoupled, speculative, lightweight hardware sub-thread\nalongside the NPU, with minimal hardware overhead (under 5%). NVR achieves an\naverage 90% reduction in cache misses compared to SOTA prefetching in\ngeneral-purpose processors, delivering 4x average speedup on sparse workloads\nversus NPUs without prefetching. Moreover, we investigate the advantages of\nincorporating a small cache (16KB) into the NPU combined with NVR. Our\nevaluation shows that expanding this modest cache delivers 5x higher\nperformance benefits than increasing the L2 cache size by the same amount."
                },
                "authors": [
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Zhengpeng Zhao"
                    },
                    {
                        "name": "Jing Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Yuan Cheng"
                    },
                    {
                        "name": "Bing Guo"
                    },
                    {
                        "name": "He Xiao"
                    },
                    {
                        "name": "Chenhao Ma"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v2",
                "updated": "2025-02-19T11:10:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    10,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v4",
                "updated": "2025-02-19T10:39:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    39,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "ToCa is honored to be accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13575v1",
                "updated": "2025-02-19T09:30:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T09:30:38Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    30,
                    38,
                    2,
                    50,
                    0
                ],
                "title": "ETS: Efficient Tree Search for Inference-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETS: Efficient Tree Search for Inference-Time Scaling"
                },
                "summary": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute scaling has emerged as a new axis along which to improve\nmodel accuracy, where additional computation is used at inference time to allow\nthe model to think longer for more challenging problems. One promising approach\nfor test-time compute scaling is search against a process reward model, where a\nmodel generates multiple potential candidates at each step of the search, and\nthese partial trajectories are then scored by a separate reward model in order\nto guide the search process. The diversity of trajectories in the tree search\nprocess affects the accuracy of the search, since increasing diversity promotes\nmore exploration. However, this diversity comes at a cost, as divergent\ntrajectories have less KV sharing, which means they consume more memory and\nslow down the search process. Previous search methods either do not perform\nsufficient exploration, or else explore diverse trajectories but have high\nlatency. We address this challenge by proposing Efficient Tree Search (ETS),\nwhich promotes KV sharing by pruning redundant trajectories while maintaining\nnecessary diverse trajectories. ETS incorporates a linear programming cost\nmodel to promote KV cache sharing by penalizing the number of nodes retained,\nwhile incorporating a semantic coverage term into the cost model to ensure that\nwe retain trajectories which are semantically different. We demonstrate how ETS\ncan achieve 1.8$\\times$ reduction in average KV cache size during the search\nprocess, leading to 1.4$\\times$ increased throughput relative to prior\nstate-of-the-art methods, with minimal accuracy degradation and without\nrequiring any custom kernel implementation. Code is available at:\nhttps://github.com/SqueezeAILab/ETS."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Suhong Moon"
                    },
                    {
                        "name": "Kerem Dilmen"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "Nicholas Lee"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13542v1",
                "updated": "2025-02-19T08:50:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T08:50:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference"
                },
                "summary": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiaqi Tang"
                    },
                    {
                        "name": "Shuangyin Li"
                    },
                    {
                        "name": "Yongqi Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13502v1",
                "updated": "2025-02-19T07:43:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    43,
                    36,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T07:43:36Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    7,
                    43,
                    36,
                    2,
                    50,
                    0
                ],
                "title": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PLDR-LLMs Learn A Generalizable Tensor Operator That Can Replace Its Own\n  Deep Neural Net At Inference"
                },
                "summary": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that Large Language Model from Power Law Decoder Representations\n(PLDR-LLM) is a foundational model whose deductive outputs are invariant\ntensors up to a small perturbation. PLDR-LLM learns a singularity condition for\nthe deductive outputs that enable the once-inferred energy-curvature tensor\n$\\mathbf{G}_{LM}$ to replace the deep neural network of power law graph\nattention (PLGA) generating the deductive outputs at inference. We demonstrate\nthat a cache for $\\mathbf{G}_{LM}$ (G-cache) and KV-cache can be implemented in\na straightforward manner to improve the inference time. The invariance and\ngeneralizable nature of deductive outputs is at a very high fidelity where\ndeductive outputs have same RMSE and determinant values up to 15 decimal places\nafter caching, and zero-shot benchmark scores remain unchanged. Ablation\nstudies show that learned deductive outputs have distinct loss and accuracy\ncharacteristics from models pretrained with transferred, randomly initialized\nor identity tensors as a constant tensor operator and an LLM with scaled-dot\nproduct attention (SDPA) is a special case of PLDR-LLM where $\\mathbf{G}_{LM}$\nis predefined as identity. The observed invariance characteristic introduces a\nnovel asymmetry between training and inference phases with caching. We outline\nobserved common characteristics of the deductive outputs for the learned\nsingularity condition. We provide an implementation of a training and inference\nframework for PLDR-LLM with KV-cache and G-cache."
                },
                "authors": [
                    {
                        "name": "Burc Gokden"
                    }
                ],
                "author_detail": {
                    "name": "Burc Gokden"
                },
                "author": "Burc Gokden",
                "arxiv_comment": "15 pages, 1 figure, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v1",
                "updated": "2025-02-18T19:22:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13145v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13145v1",
                "updated": "2025-02-18T18:59:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T18:59:57Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    18,
                    59,
                    57,
                    1,
                    49,
                    0
                ],
                "title": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Mamba: Decoder-only Multimodal State Space Model via\n  Quadratic to Linear Distillation"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have achieved remarkable\nperformance but face deployment challenges due to their quadratic computational\ncomplexity, growing Key-Value cache requirements, and reliance on separate\nvision encoders. We propose mmMamba, a framework for developing\nlinear-complexity native multimodal state space models through progressive\ndistillation from existing MLLMs using moderate academic computational\nresources. Our approach enables the direct conversion of trained decoder-only\nMLLMs to linear-complexity architectures without requiring pre-trained\nRNN-based LLM or vision encoders. We propose an seeding strategy to carve Mamba\nfrom trained Transformer and a three-stage distillation recipe, which can\neffectively transfer the knowledge from Transformer to Mamba while preserving\nmultimodal capabilities. Our method also supports flexible hybrid architectures\nthat combine Transformer and Mamba layers for customizable\nefficiency-performance trade-offs. Distilled from the Transformer-based\ndecoder-only HoVLE, mmMamba-linear achieves competitive performance against\nexisting linear and quadratic-complexity VLMs, while mmMamba-hybrid further\nimproves performance significantly, approaching HoVLE's capabilities. At 103K\ntokens, mmMamba-linear demonstrates 20.6$\\times$ speedup and 75.8% GPU memory\nreduction compared to HoVLE, while mmMamba-hybrid achieves 13.5$\\times$ speedup\nand 60.2% memory savings. Code and models are released at\nhttps://github.com/hustvl/mmMamba"
                },
                "authors": [
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Yingyue Li"
                    },
                    {
                        "name": "Haoran Yin"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang",
                "arxiv_comment": "Code and model are available at https://github.com/hustvl/mmMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13145v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13145v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13063v1",
                "updated": "2025-02-18T17:08:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T17:08:45Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    17,
                    8,
                    45,
                    1,
                    49,
                    0
                ],
                "title": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cramming 1568 Tokens into a Single Vector and Back Again: Exploring the\n  Limits of Embedding Space Capacity"
                },
                "summary": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A range of recent works addresses the problem of compression of sequence of\ntokens into a shorter sequence of real-valued vectors to be used as inputs\ninstead of token embeddings or key-value cache. These approaches allow to\nreduce the amount of compute in existing language models. Despite relying on\npowerful models as encoders, the maximum attainable lossless compression ratio\nis typically not higher than x10. This fact is highly intriguing because, in\ntheory, the maximum information capacity of large real-valued vectors is far\nbeyond the presented rates even for 16-bit precision and a modest vector size.\nIn this work, we explore the limits of compression by replacing the encoder\nwith a per-sample optimization procedure. We show that vectors with compression\nratios up to x1500 exist, which highlights two orders of magnitude gap between\nexisting and practically attainable solutions. Furthermore, we empirically show\nthat the compression limits are determined not by the length of the input but\nby the amount of uncertainty to be reduced, namely, the cross-entropy loss on\nthis sequence without any conditioning. The obtained limits highlight the\nsubstantial gap between the theoretical capacity of input embeddings and their\npractical utilization, suggesting significant room for optimization in model\ndesign."
                },
                "authors": [
                    {
                        "name": "Yuri Kuratov"
                    },
                    {
                        "name": "Mikhail Arkhipov"
                    },
                    {
                        "name": "Aydar Bulatov"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Burtsev"
                },
                "author": "Mikhail Burtsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12875v1",
                "updated": "2025-02-18T14:05:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T14:05:12Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    14,
                    5,
                    12,
                    1,
                    49,
                    0
                ],
                "title": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on DRL based UAV Communications and Networking: DRL\n  Fundamentals, Applications and Implementations"
                },
                "summary": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicles (UAVs) are playing an increasingly pivotal role in\nmodern communication networks,offering flexibility and enhanced coverage for a\nvariety of applica-tions. However, UAV networks pose significant challenges due\nto their dynamic and distributed nature, particularly when dealing with tasks\nsuch as power allocation, channel assignment, caching,and task offloading.\nTraditional optimization techniques often struggle to handle the complexity and\nunpredictability of these environments, leading to suboptimal performance. This\nsurvey provides a comprehensive examination of how deep reinforcement learning\n(DRL) can be applied to solve these mathematical optimization problems in UAV\ncommunications and networking.Rather than simply introducing DRL methods, the\nfocus is on demonstrating how these methods can be utilized to solve complex\nmathematical models of the underlying problems. We begin by reviewing the\nfundamental concepts of DRL, including value-based, policy-based, and\nactor-critic approaches. Then,we illustrate how DRL algorithms are applied to\nspecific UAV network tasks by discussing from problem formulations to DRL\nimplementation. By framing UAV communication challenges as optimization\nproblems, this survey emphasizes the practical value of DRL in dynamic and\nuncertain environments. We also explore the strengths of DRL in handling\nlarge-scale network scenarios and the ability to continuously adapt to changes\nin the environment. In addition, future research directions are outlined,\nhighlighting the potential for DRL to further enhance UAV communications and\nexpand its applicability to more complex,multi-agent settings."
                },
                "authors": [
                    {
                        "name": "Wei Zhao"
                    },
                    {
                        "name": "Shaoxin Cui"
                    },
                    {
                        "name": "Wen Qiu"
                    },
                    {
                        "name": "Zhiqiang He"
                    },
                    {
                        "name": "Zhi Liu"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Bomin Mao"
                    },
                    {
                        "name": "Nei Kato"
                    }
                ],
                "author_detail": {
                    "name": "Nei Kato"
                },
                "author": "Nei Kato",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12665v1",
                "updated": "2025-02-18T09:11:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T09:11:51Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    9,
                    11,
                    51,
                    1,
                    49,
                    0
                ],
                "title": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A$^2$ATS: Retrieval-Based KV Cache Reduction via Windowed Rotary\n  Position Embedding and Query-Aware Vector Quantization"
                },
                "summary": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context large language models (LLMs) pose significant challenges for\nefficient serving due to the large memory footprint and high access overhead of\nKV cache. Retrieval-based KV cache reduction methods can mitigate these\nchallenges, typically by offloading the complete KV cache to CPU and retrieving\nnecessary tokens on demand during inference. However, these methods still\nsuffer from unsatisfactory accuracy degradation and extra retrieval overhead.\nTo address these limitations, this paper proposes A$^2$ATS, a novel\nretrieval-based KV cache reduction method. A$^2$ATS aims to obtain an accurate\napproximation of attention scores by applying the vector quantization technique\nto key states, thereby enabling efficient and precise retrieval of the top-K\ntokens. First, we propose Windowed Rotary Position Embedding, which decouples\nthe positional dependency from query and key states after position embedding.\nThen, we propose query-aware vector quantization that optimizes the objective\nof attention score approximation directly. Finally, we design the heterogeneous\ninference architecture for KV cache offloading, enabling long context serving\nwith larger batch sizes. Experimental results demonstrate that A$^2$ATS can\nachieve a lower performance degradation with similar or lower overhead compared\nto existing methods, thereby increasing long context serving throughput by up\nto $2.7 \\times$."
                },
                "authors": [
                    {
                        "name": "Junhui He"
                    },
                    {
                        "name": "Junna Xing"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Rui Xu"
                    },
                    {
                        "name": "Shangyu Wu"
                    },
                    {
                        "name": "Peng Zhou"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Chun Jason Xue"
                    },
                    {
                        "name": "Qingan Li"
                    }
                ],
                "author_detail": {
                    "name": "Qingan Li"
                },
                "author": "Qingan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v2",
                "updated": "2025-02-18T07:58:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    7,
                    58,
                    29,
                    1,
                    49,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Vehicular Networks: An\n  Operator's Perspective"
                },
                "summary": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Access to sensing data (SD) is crucial for vehicular networks to ensure safe\nand efficient transportation services. Given the vast volume of data involved,\nproactive caching required SD is a pivotal strategy for alleviating network\ncongestion and improving data accessibility. Despite merits, existing studies\npredominantly address SD caching within a single slot. Therefore, these\napproaches lack scalability for scenarios involving multi-slots and are not\nwell-suited for network operators who manage resources within a long-term cost\nbudget. Moreover, the oversight of service capacity at caching nodes may result\nin substantial queuing delays for SD reception. To tackle these limitations, we\njointly consider the problem of anchoring SD caching and allocating from an\noperator's perspective. A value model incorporating both temporal and spacial\ncharacteristics is given to estimate the significance of various caching\ndecisions. Subsequently, a stochastic programming model is proposed to optimize\nthe long-term system performance, which is converted into a series of online\noptimization problem by leveraging the Lyapunov method and linearized via\nintroducing auxiliary variables. To expedite the solution, we provide a binary\nquantum particle swarm optimization based algorithm with quadratic time\ncomplexity. Numerical investigations demonstrate the superiority of proposed\nalgorithms compared with other schemes in terms of energy consumption, response\nlatency, and cache-hit ratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12574v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12574v1",
                "updated": "2025-02-18T06:26:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T06:26:05Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    26,
                    5,
                    1,
                    49,
                    0
                ],
                "title": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HeadInfer: Memory-Efficient LLM Inference by Head-wise Offloading"
                },
                "summary": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) demonstrate impressive\nperformance in long context generation. Extending the context length has\ndisproportionately shifted the memory footprint of LLMs during inference to the\nkey-value cache (KV cache). In this paper, we propose HEADINFER, which offloads\nthe KV cache to CPU RAM while avoiding the need to fully store the KV cache for\nany transformer layer on the GPU. HEADINFER employs a fine-grained, head-wise\noffloading strategy, maintaining only selective attention heads KV cache on the\nGPU while computing attention output dynamically. Through roofline analysis, we\ndemonstrate that HEADINFER maintains computational efficiency while\nsignificantly reducing memory footprint. We evaluate HEADINFER on the\nLlama-3-8B model with a 1-million-token sequence, reducing the GPU memory\nfootprint of the KV cache from 128 GB to 1 GB and the total GPU memory usage\nfrom 207 GB to 17 GB, achieving a 92% reduction compared to BF16 baseline\ninference. Notably, HEADINFER enables 4-million-token inference with an 8B\nmodel on a single consumer GPU with 24GB memory (e.g., NVIDIA RTX 4090) without\napproximation methods."
                },
                "authors": [
                    {
                        "name": "Cheng Luo"
                    },
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Bo Yuan"
                    },
                    {
                        "name": "Wen Xiao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Anima Anandkumar"
                    }
                ],
                "author_detail": {
                    "name": "Anima Anandkumar"
                },
                "author": "Anima Anandkumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12574v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12574v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13176v1",
                "updated": "2025-02-18T04:08:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    18,
                    4,
                    8,
                    29,
                    1,
                    49,
                    0
                ],
                "published": "2025-02-18T04:08:29Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    4,
                    8,
                    29,
                    1,
                    49,
                    0
                ],
                "title": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BaKlaVa -- Budgeted Allocation of KV cache for Long-context Inference"
                },
                "summary": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Large Language Model (LLM) inference, Key-Value (KV) caches (KV-caches)\nare essential for reducing time complexity. However, they result in a linear\nincrease in GPU memory as the context length grows. While recent work explores\nKV-cache eviction and compression policies to reduce memory usage, they often\nconsider uniform KV-caches across all attention heads, leading to suboptimal\nperformance. We introduce BaKlaVa, a method to allocate optimal memory for\nindividual KV-caches across the model by estimating the importance of each\nKV-cache. Our empirical analysis demonstrates that not all KV-caches are\nequally critical for LLM performance. Using a one-time profiling approach,\nBaKlaVa assigns optimal memory budgets to each KV-cache. We evaluated our\nmethod on LLaMA-3-8B, and Qwen2.5-7B models, achieving up to a 70\\% compression\nratio while keeping baseline performance and delivering up to an\norder-of-magnitude accuracy improvement at higher compression levels."
                },
                "authors": [
                    {
                        "name": "Ahmed Burak Gulhan"
                    },
                    {
                        "name": "Krishna Teja Chitty-Venkata"
                    },
                    {
                        "name": "Murali Emani"
                    },
                    {
                        "name": "Mahmut Kandemir"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v1",
                "updated": "2025-02-17T14:54:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate Expert Predictions in MoE Inference via Cross-Layer Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v2",
                "updated": "2025-02-17T14:34:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    34,
                    58,
                    0,
                    48,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12216v1",
                "updated": "2025-02-17T08:39:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    39,
                    43,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T08:39:43Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    8,
                    39,
                    43,
                    0,
                    48,
                    0
                ],
                "title": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tactic: Adaptive Sparse Attention with Clustering and Distribution\n  Fitting for Long-Context LLMs"
                },
                "summary": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context models are essential for many applications but face\ninefficiencies in loading large KV caches during decoding. Prior methods\nenforce fixed token budgets for sparse attention, assuming a set number of\ntokens can approximate full attention. However, these methods overlook\nvariations in the importance of attention across heads, layers, and contexts.\nTo address these limitations, we propose Tactic, a sparsity-adaptive and\ncalibration-free sparse attention mechanism that dynamically selects tokens\nbased on their cumulative attention scores rather than a fixed token budget. By\nsetting a target fraction of total attention scores, Tactic ensures that token\nselection naturally adapts to variations in attention sparsity. To efficiently\napproximate this selection, Tactic leverages clustering-based sorting and\ndistribution fitting, allowing it to accurately estimate token importance with\nminimal computational overhead. We show that Tactic outperforms existing sparse\nattention algorithms, achieving superior accuracy and up to 7.29x decode\nattention speedup. This improvement translates to an overall 1.58x end-to-end\ninference speedup, making Tactic a practical and effective solution for\nlong-context LLM inference in accuracy-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Kan Zhu"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Qinyu Xu"
                    },
                    {
                        "name": "Yile Gu"
                    },
                    {
                        "name": "Zhichen Zeng"
                    },
                    {
                        "name": "Rohan Kadekodi"
                    },
                    {
                        "name": "Liangyu Zhao"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Baris Kasikci"
                    }
                ],
                "author_detail": {
                    "name": "Baris Kasikci"
                },
                "author": "Baris Kasikci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11501v1",
                "updated": "2025-02-17T07:05:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T07:05:36Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    7,
                    5,
                    36,
                    0,
                    48,
                    0
                ],
                "title": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning in Multimodal Large Language Models: Are We Solving the\n  Right Problem?"
                },
                "summary": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have shown remarkable performance\nfor cross-modal understanding and generation, yet still suffer from severe\ninference costs. Recently, abundant works have been proposed to solve this\nproblem with token pruning, which identifies the redundant tokens in MLLMs and\nthen prunes them to reduce the computation and KV storage costs, leading to\nsignificant acceleration without training. While these methods claim efficiency\ngains, critical questions about their fundamental design and evaluation remain\nunanswered: Why do many existing approaches underperform even compared to naive\nrandom token selection? Are attention-based scoring sufficient for reliably\nidentifying redundant tokens? Is language information really helpful during\ntoken pruning? What makes a good trade-off between token importance and\nduplication? Are current evaluation protocols comprehensive and unbiased? The\nignorance of previous research on these problems hinders the long-term\ndevelopment of token pruning. In this paper, we answer these questions one by\none, providing insights into the design of future token pruning methods."
                },
                "authors": [
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yifeng Gao"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11444v1",
                "updated": "2025-02-17T05:02:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    17,
                    5,
                    2,
                    25,
                    0,
                    48,
                    0
                ],
                "published": "2025-02-17T05:02:25Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    5,
                    2,
                    25,
                    0,
                    48,
                    0
                ],
                "title": "Does RAG Really Perform Bad For Long-Context Processing?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does RAG Really Perform Bad For Long-Context Processing?"
                },
                "summary": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient processing of long context poses a serious challenge for large\nlanguage models (LLMs). Recently, retrieval-augmented generation (RAG) has\nemerged as a promising strategy for this problem, as it enables LLMs to make\nselective use of the long context for efficient computation. However, existing\nRAG approaches lag behind other long-context processing methods due to inherent\nlimitations on inaccurate retrieval and fragmented contexts. To address these\nchallenges, we introduce RetroLM, a novel RAG framework for long-context\nprocessing. Unlike traditional methods, RetroLM employs KV-level retrieval\naugmentation, where it partitions the LLM's KV cache into contiguous pages and\nretrieves the most crucial ones for efficient computation. This approach\nenhances robustness to retrieval inaccuracy, facilitates effective utilization\nof fragmented contexts, and saves the cost from repeated computation. Building\non this framework, we further develop a specialized retriever for precise\nretrieval of critical pages and conduct unsupervised post-training to optimize\nthe model's ability to leverage retrieved information. We conduct comprehensive\nevaluations with a variety of benchmarks, including LongBench, InfiniteBench,\nand RULER, where RetroLM significantly outperforms existing long-context LLMs\nand efficient long-context processing methods, particularly in tasks requiring\nintensive reasoning or extremely long-context comprehension."
                },
                "authors": [
                    {
                        "name": "Kun Luo"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09383v2",
                "updated": "2025-02-16T18:31:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    18,
                    31,
                    10,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-13T14:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    14,
                    59,
                    3,
                    3,
                    44,
                    0
                ],
                "title": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capitalizing on a Crisis: A Computational Analysis of all Five Million\n  British Firms During the Covid-19 Pandemic"
                },
                "summary": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Covid-19 pandemic brought unprecedented changes to business ownership in\nthe UK which affects a generation of entrepreneurs and their employees.\nNonetheless, the impact remains poorly understood. This is because research on\ncapital accumulation has typically lacked high-quality, individualized,\npopulation-level data. We overcome these barriers to examine who benefits from\neconomic crises through a computationally orientated lens of firm creation.\nLeveraging a comprehensive cache of administrative data on every UK firm and\nall nine million people running them, combined with probabilistic algorithms,\nwe conduct individual-level analyses to understand who became Covid\nentrepreneurs. Using these techniques, we explore characteristics of\nentrepreneurs--such as age, gender, region, business experience, and\nindustry--which potentially predict Covid entrepreneurship. By employing an\nautomated time series model selection procedure to generate counterfactuals, we\nshow that Covid entrepreneurs were typically aged 35-49 (40.4%), men (73.1%),\nand had previously held roles in existing firms (59.4%). For most industries,\ngrowth was disproportionately concentrated around London. It was therefore\nexisting corporate elites who were most able to capitalize on the Covid crisis\nand not, as some hypothesized, young entrepreneurs who were setting up their\nfirst businesses. In this respect, the pandemic will likely impact future\nwealth inequalities. Our work offers methodological guidance for future\npolicymakers during economic crises and highlights the long-term consequences\nfor capital and wealth inequality."
                },
                "authors": [
                    {
                        "name": "Naomi Muggleton"
                    },
                    {
                        "name": "Charles Rahal"
                    },
                    {
                        "name": "Aaron Reeves"
                    }
                ],
                "author_detail": {
                    "name": "Aaron Reeves"
                },
                "author": "Aaron Reeves",
                "arxiv_doi": "10.1007/s42001-025-00360-4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s42001-025-00360-4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Computational Social Science, 8(2), 1-29 (2025)",
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.07627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.07627v2",
                "updated": "2025-02-16T16:41:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    16,
                    41,
                    43,
                    6,
                    47,
                    0
                ],
                "published": "2024-11-12T08:17:15Z",
                "published_parsed": [
                    2024,
                    11,
                    12,
                    8,
                    17,
                    15,
                    1,
                    317,
                    0
                ],
                "title": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Previous Steps: A Training-free Fast Solver for Flow\n  Diffusion"
                },
                "summary": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow diffusion models (FDMs) have recently shown potential in generation\ntasks due to the high generation quality. However, the current ordinary\ndifferential equation (ODE) solver for FDMs, e.g., the Euler solver, still\nsuffers from slow generation since ODE solvers need many number function\nevaluations (NFE) to keep high-quality generation. In this paper, we propose a\nnovel training-free flow-solver to reduce NFE while maintaining high-quality\ngeneration. The key insight for the flow-solver is to leverage the previous\nsteps to reduce the NFE, where a cache is created to reuse these results from\nthe previous steps. Specifically, the Taylor expansion is first used to\napproximate the ODE. To calculate the high-order derivatives of Taylor\nexpansion, the flow-solver proposes to use the previous steps and a polynomial\ninterpolation to approximate it, where the number of orders we could\napproximate equals the number of previous steps we cached. We also prove that\nthe flow-solver has a more minor approximation error and faster generation\nspeed. Experimental results on the CIFAR-10, CelebA-HQ, LSUN-Bedroom,\nLSUN-Church, ImageNet, and real text-to-image generation prove the efficiency\nof the flow-solver. Specifically, the flow-solver improves the FID-30K from\n13.79 to 6.75, from 46.64 to 19.49 with $\\text{NFE}=10$ on CIFAR-10 and\nLSUN-Church, respectively."
                },
                "authors": [
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.07627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.07627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15332v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15332v2",
                "updated": "2025-02-16T14:50:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    50,
                    0,
                    6,
                    47,
                    0
                ],
                "published": "2024-10-20T08:42:29Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    8,
                    42,
                    29,
                    6,
                    294,
                    0
                ],
                "title": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPIC: Efficient Position-Independent Context Caching for Serving Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are critical for a wide range of applications,\nbut serving them efficiently becomes increasingly challenging as inputs become\nmore complex. Context caching improves serving performance by exploiting\ninter-request dependency and reusing key-value (KV) cache across requests, thus\nimproving time-to-first-token (TTFT). However, existing prefix-based context\ncaching requires exact token prefix matches, limiting cache reuse in few-shot\nlearning, multi-document QA, or retrieval-augmented generation, where prefixes\nmay vary. In this paper, we present EPIC, an LLM serving system that introduces\nposition-independent context caching (PIC), enabling modular KV cache reuse\nregardless of token chunk position (or prefix). EPIC features two key designs:\nAttnLink, which leverages static attention sparsity to minimize recomputation\nfor accuracy recovery, and KVSplit, a customizable chunking method that\npreserves semantic coherence. Our experiments demonstrate that Epic delivers up\nto 8x improvements in TTFT and 7x throughput over existing systems, with\nnegligible or no accuracy loss. By addressing the limitations of traditional\ncaching approaches, Epic enables more scalable and efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Haoyi Wang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Tao Xie"
                    }
                ],
                "author_detail": {
                    "name": "Tao Xie"
                },
                "author": "Tao Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15332v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15332v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11147v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11147v1",
                "updated": "2025-02-16T14:28:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T14:28:52Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    14,
                    28,
                    52,
                    6,
                    47,
                    0
                ],
                "title": "Efficient Long-Decoding Inference with Reasoning-Aware Attention\n  Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Long-Decoding Inference with Reasoning-Aware Attention\n  Sparsity"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong capabilities across\nvarious domains, with recent advancements in challenging reasoning tasks such\nas mathematics and programming. However, solving reasoning tasks often requires\nlong decoding chains (of thoughts), which incur $O(N)$ time and memory\nconsumption, where $N$ is the chain length. To mitigate $O(N)$ time and memory\nconsumption, existing sparsity-based algorithms propose retaining only the most\ncritical token's intermediate data (i.e., key-value cache) and discarding the\nrest. However, these existing algorithms struggle with the ``impossible\ntrinity'' of accuracy, time, and memory. For example, the state-of-the-art\nalgorithm, Quest, achieves high accuracy with $O(L)$ time but $O(N)$ memory\n($L$ is the cache budget, $L \\ll N$). To address this issue, in this paper, we\nidentify a new attention pattern during the decode stage of reasoning tasks,\nwhere milestone tokens (analogous to lemmas in mathematical proofs) emerge, are\nutilized, and then become unimportant afterward. Based on this pattern, we\npropose a new algorithm named RaaS that identifies and retains milestone tokens\nonly until they are no longer needed, achieving high accuracy with $O(L)$ time\nand $O(L)$ memory complexity."
                },
                "authors": [
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Wenrui Huang"
                    },
                    {
                        "name": "Weidong Wang"
                    },
                    {
                        "name": "Zhenwen Li"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11147v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11147v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11101v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11101v1",
                "updated": "2025-02-16T12:33:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    33,
                    16,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T12:33:16Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    12,
                    33,
                    16,
                    6,
                    47,
                    0
                ],
                "title": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFocus: Dynamic Cache Re-Positioning for Efficient\n  Retrieval-Augmented Generation"
                },
                "summary": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel across a variety of language tasks yet are\nconstrained by limited input lengths and high computational costs. Existing\napproaches\\textemdash such as relative positional encodings (e.g., RoPE, ALiBi)\nand sliding window mechanisms\\textemdash partially alleviate these issues but\noften require additional training or suffer from performance degradation with\nlonger inputs. In this paper, we introduce \\textbf{\\textit{CacheFocus}}, a\nmethod that enhances length normalization and reduces inference latency without\nany further training. Our approach leverages query-independent, offline caching\nto efficiently reuse a Context KV Cache Store. We address the amplification of\nabnormal token distributions problem by re-positioning cached keys and\nintroducing Layer-Adaptive Cache Pruning to discard low-relevance caches during\npre-filling. Additionally, our Adaptive Positional Allocation Strategy\ndynamically reassigns cache positions to maximize the use of the available\npositional encoding range. Experiments on the Natural Questions and TriviaQA\ndatasets demonstrate that CacheFocus outperforms alternative methods even when\ninputs exceed the $4$K limit of the \\texttt{LLaMA-2} model, emphasizing its\npractical effectiveness for long-context LLMs. Moreover, even with large\nmaximum input length of \\texttt{Qwen2}, the performance of CacheFocus shows\nthat it maintains consistent performance even as the number of documents\nincreases, effectively managing long-text generation without degradation."
                },
                "authors": [
                    {
                        "name": "Kun-Hui Lee"
                    },
                    {
                        "name": "Eunhwan Park"
                    },
                    {
                        "name": "Donghoon Han"
                    },
                    {
                        "name": "Seung-Hoon Na"
                    }
                ],
                "author_detail": {
                    "name": "Seung-Hoon Na"
                },
                "author": "Seung-Hoon Na",
                "arxiv_comment": "11 pages (Work in progress)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11101v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11101v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11083v1",
                "updated": "2025-02-16T11:37:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    37,
                    14,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T11:37:14Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    11,
                    37,
                    14,
                    6,
                    47,
                    0
                ],
                "title": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining the Collaborative Chain of Models into A Single Forward\n  Pass in Generation-Based Tasks"
                },
                "summary": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Retrieval-Augmented Generation (RAG) and agent-based frameworks, the\n\"Chain of Models\" approach is widely used, where multiple specialized models\nwork sequentially on distinct sub-tasks. This approach is effective but\nincreases resource demands as each model must be deployed separately. Recent\nadvancements attempt to address this by applying prompt tuning, which allows a\nshared base model to adapt to multiple tasks with minimal parameter changes.\nHowever, a key challenge remains: intermediate outputs, passed between models\nas plain text, require recomputation of hidden states (i.e., Key and Value (KV)\nstates in Transformers) during inference. In this paper, we introduce FTHSS, a\nnovel prompt-tuning method that enables models to share KV hidden states,\neliminating redundant forward passes and reducing KV cache storage. By\nmodifying input and attention masks during training, FTHSS allows models to\neffectively utilize KV hidden states from prior models in both single- and\nmulti-round scenarios. Empirical results on four tasks show that FTHSS matches\nthe performance of traditional model chains while improving inference\nefficiency."
                },
                "authors": [
                    {
                        "name": "Yuanjie Lyu"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Yuhao Chen"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Tong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Tong Xu"
                },
                "author": "Tong Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v1",
                "updated": "2025-02-16T09:08:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05231v2",
                "updated": "2025-02-15T23:54:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    23,
                    54,
                    38,
                    5,
                    46,
                    0
                ],
                "published": "2024-05-08T17:27:11Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    17,
                    27,
                    11,
                    2,
                    129,
                    0
                ],
                "title": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskGNN: Bridging I/O Efficiency and Model Accuracy for Out-of-Core GNN\n  Training"
                },
                "summary": "Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural networks (GNNs) are machine learning models specialized for\ngraph data and widely used in many applications. To train GNNs on large graphs\nthat exceed CPU memory, several systems store data on disk and conduct\nout-of-core processing. However, these systems suffer from either read\namplification when reading node features that are usually smaller than a disk\npage or degraded model accuracy by treating the graph as disconnected\npartitions. To close this gap, we build a system called DiskGNN, which achieves\nhigh I/O efficiency and thus fast training without hurting model accuracy. The\nkey technique used by DiskGNN is offline sampling, which helps decouple graph\nsampling from model computation. In particular, by conducting graph sampling\nbeforehand, DiskGNN acquires the node features that will be accessed by model\ncomputation, and such information is utilized to pack the target node features\ncontiguously on disk to avoid read amplification. Besides, \\name{} also adopts\ndesigns including four-level feature store to fully utilize the memory\nhierarchy to cache node features and reduce disk access, batched packing to\naccelerate the feature packing process, and pipelined training to overlap disk\naccess with other operations. We compare DiskGNN with Ginex and MariusGNN,\nwhich are state-of-the-art systems for out-of-core GNN training. The results\nshow that DiskGNN can speed up the baselines by over 8x while matching their\nbest model accuracy."
                },
                "authors": [
                    {
                        "name": "Renjie Liu"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Haitian Jiang"
                    },
                    {
                        "name": "Zhenkun Cai"
                    },
                    {
                        "name": "Minjie Wang"
                    },
                    {
                        "name": "Bo Tang"
                    },
                    {
                        "name": "Jinyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Li"
                },
                "author": "Jinyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.01939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.01939v2",
                "updated": "2025-02-15T18:09:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    18,
                    9,
                    50,
                    5,
                    46,
                    0
                ],
                "published": "2024-06-04T03:48:08Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    3,
                    48,
                    8,
                    1,
                    156,
                    0
                ],
                "title": "Speeding up Policy Simulation in Supply Chain RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speeding up Policy Simulation in Supply Chain RL"
                },
                "summary": "Simulating a single trajectory of a dynamical system under some\nstate-dependent policy is a core bottleneck in policy optimization (PO)\nalgorithms. The many inherently serial policy evaluations that must be\nperformed in a single simulation constitute the bulk of this bottleneck. In\napplying PO to supply chain optimization (SCO) problems, simulating a single\nsample path corresponding to one month of a supply chain can take several\nhours. We present an iterative algorithm to accelerate policy simulation,\ndubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks\nto independent processes. Within an iteration, any given process evaluates the\npolicy only on its assigned tasks while assuming a certain \"cached\" evaluation\nfor other tasks; the cache is updated at the end of the iteration. Implemented\non GPUs, this scheme admits batched evaluation of the policy across a single\ntrajectory. We prove that the structure afforded by many SCO problems allows\nconvergence in a small number of iterations independent of the horizon. We\ndemonstrate practical speedups of 400x on large-scale SCO problems even with a\nsingle GPU, and also demonstrate practical efficacy in other RL environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating a single trajectory of a dynamical system under some\nstate-dependent policy is a core bottleneck in policy optimization (PO)\nalgorithms. The many inherently serial policy evaluations that must be\nperformed in a single simulation constitute the bulk of this bottleneck. In\napplying PO to supply chain optimization (SCO) problems, simulating a single\nsample path corresponding to one month of a supply chain can take several\nhours. We present an iterative algorithm to accelerate policy simulation,\ndubbed Picard Iteration. This scheme carefully assigns policy evaluation tasks\nto independent processes. Within an iteration, any given process evaluates the\npolicy only on its assigned tasks while assuming a certain \"cached\" evaluation\nfor other tasks; the cache is updated at the end of the iteration. Implemented\non GPUs, this scheme admits batched evaluation of the policy across a single\ntrajectory. We prove that the structure afforded by many SCO problems allows\nconvergence in a small number of iterations independent of the horizon. We\ndemonstrate practical speedups of 400x on large-scale SCO problems even with a\nsingle GPU, and also demonstrate practical efficacy in other RL environments."
                },
                "authors": [
                    {
                        "name": "Vivek Farias"
                    },
                    {
                        "name": "Joren Gijsbrechts"
                    },
                    {
                        "name": "Aryan Khojandi"
                    },
                    {
                        "name": "Tianyi Peng"
                    },
                    {
                        "name": "Andrew Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zheng"
                },
                "author": "Andrew Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.01939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.01939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10659v1",
                "updated": "2025-02-15T03:56:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    56,
                    22,
                    5,
                    46,
                    0
                ],
                "published": "2025-02-15T03:56:22Z",
                "published_parsed": [
                    2025,
                    2,
                    15,
                    3,
                    56,
                    22,
                    5,
                    46,
                    0
                ],
                "title": "Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for\n  Efficient LLM Decoding on Embedded FPGA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing up to the Limit of Memory Bandwidth and Capacity Utilization for\n  Efficient LLM Decoding on Embedded FPGA"
                },
                "summary": "The extremely high computational and storage demands of large language models\nhave excluded most edge devices, which were widely used for efficient machine\nlearning, from being viable options. A typical edge device usually only has 4GB\nof memory capacity and a bandwidth of less than 20GB/s, while a large language\nmodel quantized to 4-bit precision with 7B parameters already requires 3.5GB of\ncapacity, and its decoding process is purely bandwidth-bound. In this paper, we\naim to explore these limits by proposing a hardware accelerator for large\nlanguage model (LLM) inference on the Zynq-based KV260 platform, equipped with\n4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,\nachieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory\ncapacity and reaching 85% decoding speed of the theoretical memory bandwidth\nlimit. To fully reserve the memory capacity for model weights and key-value\ncache, we develop the system in a bare-metal environment without an operating\nsystem. To fully reserve the bandwidth for model weight transfers, we implement\na customized dataflow with an operator fusion pipeline and propose a data\narrangement format that can maximize the data transaction efficiency. This\nresearch marks the first attempt to deploy a 7B level LLM on a standalone\nembedded field programmable gate array (FPGA) device. It provides key insights\ninto efficient LLM inference on embedded FPGA devices and provides guidelines\nfor future architecture design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extremely high computational and storage demands of large language models\nhave excluded most edge devices, which were widely used for efficient machine\nlearning, from being viable options. A typical edge device usually only has 4GB\nof memory capacity and a bandwidth of less than 20GB/s, while a large language\nmodel quantized to 4-bit precision with 7B parameters already requires 3.5GB of\ncapacity, and its decoding process is purely bandwidth-bound. In this paper, we\naim to explore these limits by proposing a hardware accelerator for large\nlanguage model (LLM) inference on the Zynq-based KV260 platform, equipped with\n4GB of 64-bit 2400Mbps DDR4 memory. We successfully deploy a LLaMA2-7B model,\nachieving a decoding speed of around 5 token/s, utilizing 93.3% of the memory\ncapacity and reaching 85% decoding speed of the theoretical memory bandwidth\nlimit. To fully reserve the memory capacity for model weights and key-value\ncache, we develop the system in a bare-metal environment without an operating\nsystem. To fully reserve the bandwidth for model weight transfers, we implement\na customized dataflow with an operator fusion pipeline and propose a data\narrangement format that can maximize the data transaction efficiency. This\nresearch marks the first attempt to deploy a 7B level LLM on a standalone\nembedded field programmable gate array (FPGA) device. It provides key insights\ninto efficient LLM inference on embedded FPGA devices and provides guidelines\nfor future architecture design."
                },
                "authors": [
                    {
                        "name": "Jindong Li"
                    },
                    {
                        "name": "Tenglong Li"
                    },
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Qian Zhang"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "arxiv_comment": "Accepted by DATE2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10389v1",
                "updated": "2025-02-14T18:59:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T18:59:36Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    18,
                    59,
                    36,
                    4,
                    45,
                    0
                ],
                "title": "Region-Adaptive Sampling for Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Region-Adaptive Sampling for Diffusion Transformers"
                },
                "summary": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models (DMs) have become the leading choice for generative tasks\nacross diverse domains. However, their reliance on multiple sequential forward\npasses significantly limits real-time performance. Previous acceleration\nmethods have primarily focused on reducing the number of sampling steps or\nreusing intermediate results, failing to leverage variations across spatial\nregions within the image due to the constraints of convolutional U-Net\nstructures. By harnessing the flexibility of Diffusion Transformers (DiTs) in\nhandling variable number of tokens, we introduce RAS, a novel, training-free\nsampling strategy that dynamically assigns different sampling ratios to regions\nwithin an image based on the focus of the DiT model. Our key observation is\nthat during each sampling step, the model concentrates on semantically\nmeaningful regions, and these areas of focus exhibit strong continuity across\nconsecutive steps. Leveraging this insight, RAS updates only the regions\ncurrently in focus, while other regions are updated using cached noise from the\nprevious step. The model's focus is determined based on the output from the\npreceding step, capitalizing on the temporal consistency we observed. We\nevaluate RAS on Stable Diffusion 3 and Lumina-Next-T2I, achieving speedups up\nto 2.36x and 2.51x, respectively, with minimal degradation in generation\nquality. Additionally, a user study reveals that RAS delivers comparable\nqualities under human evaluation while achieving a 1.6x speedup. Our approach\nmakes a significant step towards more efficient diffusion transformers,\nenhancing their potential for real-time applications."
                },
                "authors": [
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yiqi Zhang"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Yuqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yuqing Yang"
                },
                "author": "Yuqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v2",
                "updated": "2025-02-14T17:17:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    17,
                    17,
                    20,
                    4,
                    45,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-Based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) have achieved notable advancements\nin detection accuracy, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "arxiv_doi": "10.1145/3701716.3715192",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3701716.3715192",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.09057v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by WWW 2025 (Demo Track)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10220v1",
                "updated": "2025-02-14T15:14:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T15:14:53Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    15,
                    14,
                    53,
                    4,
                    45,
                    0
                ],
                "title": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal and Coordinated Voltage Control: Case Study on a 132 kV\n  Norwegian Grid Subsystem"
                },
                "summary": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a framework for dynamic performance assessment of the\nhigher layers in the hierarchical voltage regulation scheme, with case studies\napplied to specific areas of the Norwegian grid. Unlike the primary (PVR)\nlevel, the secondary (SVR) and tertiary (TVR) levels are not tuned to a single\ndevice at a time, handling instead several reactive power resources available\nwithin a control zone including generator units, static VAr compensators and\nothers. Proper SVR-TVR coordination for realistic transmission systems is a\nchallenging topic at the core of many ongoing discussions in voltage control\nliterature. Special focus is placed on practical considerations from the system\noperator perspective, since this research is also aimed at simplifying daily\ncontrol centre routines. Dynamic simulation results concern a 21-bus equivalent\nof a 132 kV network model that accurately represents a Norwegian grid\nsubsystem. Case studies address daily grid operation with real-life load demand\nand wind power generation profiles, showing that the proposed strategy is\neffective not only to minimize total active power losses as much as possible\nwithin system-wide limitations, but also to maintain adequate voltage profiles\nand reactive power flows. Findings pertaining to this work showcase the\nbenefits of applying hierarchical voltage regulation layers as an asset to\nday-to-day control center management of a realistic transmission network."
                },
                "authors": [
                    {
                        "name": "Hugo Rodrigues de Brito"
                    },
                    {
                        "name": "Daniel Simon Baltensperger"
                    },
                    {
                        "name": "Kjetil Obstfelder Uhlen"
                    }
                ],
                "author_detail": {
                    "name": "Kjetil Obstfelder Uhlen"
                },
                "author": "Kjetil Obstfelder Uhlen",
                "arxiv_comment": "11 pages, 8 figures, CIGRE Symposium 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10167v1",
                "updated": "2025-02-14T13:55:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T13:55:01Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    13,
                    55,
                    1,
                    4,
                    45,
                    0
                ],
                "title": "Modeling and Simulating Emerging Memory Technologies: A Tutorial",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling and Simulating Emerging Memory Technologies: A Tutorial"
                },
                "summary": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-volatile Memory (NVM) technologies present a promising alternative to\ntraditional volatile memories such as SRAM and DRAM. Due to the limited\navailability of real NVM devices, simulators play a crucial role in\narchitectural exploration and hardware-software co-design. This tutorial\npresents a simulation toolchain through four detailed case studies, showcasing\nits applicability to various domains of system design, including hybrid\nmain-memory and cache, compute-in-memory, and wear-leveling design. These case\nstudies provide the reader with practical insights on customizing the toolchain\nfor their specific research needs. The source code is open-sourced."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Tristan Seidl"
                    },
                    {
                        "name": "Nils Hlscher"
                    },
                    {
                        "name": "Christian Hakert"
                    },
                    {
                        "name": "Minh Duy Truong"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Joo Paulo C. de Lima"
                    },
                    {
                        "name": "Asif Ali Khan"
                    },
                    {
                        "name": "Jeronimo Castrillon"
                    },
                    {
                        "name": "Ali Nezhadi"
                    },
                    {
                        "name": "Lokesh Siddhu"
                    },
                    {
                        "name": "Hassan Nassar"
                    },
                    {
                        "name": "Mahta Mayahinia"
                    },
                    {
                        "name": "Mehdi Baradaran Tahoori"
                    },
                    {
                        "name": "Jrg Henkel"
                    },
                    {
                        "name": "Nils Wilbert"
                    },
                    {
                        "name": "Stefan Wildermann"
                    },
                    {
                        "name": "Jrgen Teich"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Teich"
                },
                "author": "Jrgen Teich",
                "arxiv_comment": "DFG Priority Program 2377 - Disruptive Memory Technologies",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09921v1",
                "updated": "2025-02-14T05:19:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T05:19:46Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    5,
                    19,
                    46,
                    4,
                    45,
                    0
                ],
                "title": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INF^2: High-Throughput Generative Inference of Large Language Models\n  using Near-Storage Processing"
                },
                "summary": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing memory and computational demands of large language models (LLMs)\nfor generative inference present significant challenges for practical\ndeployment. One promising solution to address these challenges is\noffloading-based batched inference, which leverages host memory and disk as an\nextended memory hierarchy for GPUs. While the approach cost-effectively enables\nLLM inference, its performance is limited by substantial I/O overhead,\nprimarily due to the large key-value (KV) cache sizes, which increase with\nbatch size and LLM context window length.\n  In this paper, we introduce INFerence-INFinity (INF^2), a framework that\nboosts generative inference throughput using computational storage devices\n(CSDs). The core of INF^2 is attention-near storage, which offloads\nmemory-intensive self-attention operations to near-storage accelerators,\nsignificantly reducing traffic through the system interconnect. We also propose\ndelayed KV cache writeback to hide storage write latency by delaying newly\ngenerated KV cache writes until the cache reaches sufficient size in system\nmemory. Additionally, we introduce cooperative X-cache, a technique designed to\nfurther trade off the remaining memory capacity for storage bandwidth. Our\nmethods effectively minimize idle time for computation, improving the overall\nthroughput.\n  To demonstrate the effectiveness of our approach, \\thiswork has been\nimplemented on PyTorch and evaluated on a real system. Our experiments show\nthat INF^2 achieves up to 3.46$\\times$ throughput improvement compared to\nstate-of-the-art baselines. We will open-source INF^2 to facilitate broader\nadoption."
                },
                "authors": [
                    {
                        "name": "Hongsun Jang"
                    },
                    {
                        "name": "Siung Noh"
                    },
                    {
                        "name": "Changmin Shin"
                    },
                    {
                        "name": "Jaewon Jung"
                    },
                    {
                        "name": "Jaeyong Song"
                    },
                    {
                        "name": "Jinho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jinho Lee"
                },
                "author": "Jinho Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v1",
                "updated": "2025-02-14T03:25:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Large Recommendation Model: Towards a Resource-Optimal\n  Scaling Law"
                },
                "summary": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pursuit of scaling up recommendation models confronts intrinsic tensions\nbetween expanding model capacity and preserving computational tractability.\nWhile prior studies have explored scaling laws for recommendation systems,\ntheir resource-intensive paradigms -- often requiring tens of thousands of A100\nGPU hours -- remain impractical for most industrial applications. This work\naddresses a critical gap: achieving sustainable model scaling under strict\ncomputational budgets. We propose Climber, a resource-efficient recommendation\nframework comprising two synergistic components: the ASTRO model architecture\nfor algorithmic innovation and the TURBO acceleration framework for engineering\noptimization. ASTRO (Adaptive Scalable Transformer for RecOmmendation) adopts\ntwo core innovations: (1) multi-scale sequence partitioning that reduces\nattention complexity from O(n^2d) to O(n^2d/Nb) via hierarchical blocks,\nenabling more efficient scaling with sequence length; (2) dynamic temperature\nmodulation that adaptively adjusts attention scores for multimodal\ndistributions arising from inherent multi-scenario and multi-behavior\ninteractions. Complemented by TURBO (Two-stage Unified Ranking with Batched\nOutput), a co-designed acceleration framework integrating gradient-aware\nfeature compression and memory-efficient Key-Value caching, Climber achieves\n5.15x throughput gains without performance degradation. Comprehensive offline\nexperiments on multiple datasets validate that Climber exhibits a more ideal\nscaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Fangjian Li"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09888v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09726v1",
                "updated": "2025-02-13T19:16:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:16:39Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    16,
                    39,
                    3,
                    44,
                    0
                ],
                "title": "Analysis of Robust and Secure DNS Protocols for IoT Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Robust and Secure DNS Protocols for IoT Devices"
                },
                "summary": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The DNS (Domain Name System) protocol has been in use since the early days of\nthe Internet. Although DNS as a de facto networking protocol had no security\nconsiderations in its early years, there have been many security enhancements,\nsuch as DNSSec (Domain Name System Security Extensions), DoT (DNS over\nTransport Layer Security), DoH (DNS over HTTPS) and DoQ (DNS over QUIC). With\nall these security improvements, it is not yet clear what resource-constrained\nInternet-of-Things (IoT) devices should be used for robustness. In this paper,\nwe investigate different DNS security approaches using an edge DNS resolver\nimplemented as a Virtual Network Function (VNF) to replicate the impact of the\nprotocol from an IoT perspective and compare their performances under different\nconditions. We present our results for cache-based and non-cached responses and\nevaluate the corresponding security benefits. Our results and framework can\ngreatly help consumers, manufacturers, and the research community decide and\nimplement their DNS protocols depending on the given dynamic network conditions\nand enable robust Internet access via DNS for different devices."
                },
                "authors": [
                    {
                        "name": "Abdullah Aydeger"
                    },
                    {
                        "name": "Sanzida Hoque"
                    },
                    {
                        "name": "Engin Zeydan"
                    },
                    {
                        "name": "Kapal Dev"
                    }
                ],
                "author_detail": {
                    "name": "Kapal Dev"
                },
                "author": "Kapal Dev",
                "arxiv_comment": "6 pages, 2 tables, 2 figures. This paper has been accepted in the\n  2025 IEEE International Conference on Communications (ICC): SAC Cloud\n  Computing, Networking, and Storage Track. The final version will be published\n  in the IEEE Xplore",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v1",
                "updated": "2025-02-13T19:11:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent work have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Meta's SpinQuant (perplexity 7.3). Comparisons on various\nLLM evaluation benchmarks also show a reduction in performance degradation\ninduced by quantization."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07864v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07864v2",
                "updated": "2025-02-13T18:07:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    18,
                    7,
                    4,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-11T18:20:18Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    20,
                    18,
                    1,
                    42,
                    0
                ],
                "title": "TransMLA: Multi-Head Latent Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransMLA: Multi-Head Latent Attention Is All You Need"
                },
                "summary": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) often encounter communication bottlenecks\non current hardware, rather than purely computational constraints. Multi-head\nLatent Attention (MLA) tackles this challenge by using low-rank matrices in the\nkey-value (KV) layers, thereby allowing compressed latent KV states to be\ncached. This approach significantly reduces the KV cache size relative to\ntraditional multi-head attention, leading to faster inference. Moreover, MLA\nemploys an up-projection matrix to increase expressiveness, trading additional\ncomputation for reduced communication overhead. Although MLA has demonstrated\nefficiency and effectiveness in Deepseek V2/V3/R1, many major model providers\nstill rely on Group Query Attention (GQA) and have not announced any plans to\nadopt MLA. In this paper, we show that GQA can always be represented by MLA\nwhile maintaining the same KV cache overhead, but the converse does not hold.\nTo encourage broader use of MLA, we introduce TransMLA, a post-training method\nthat converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,\nMixtral) into MLA-based models. After conversion, the model can undergo\nadditional training to boost expressiveness without increasing the KV cache\nsize. Furthermore, we plan to develop MLA-specific inference acceleration\ntechniques to preserve low latency in transformed models, thus enabling more\nefficient distillation of Deepseek R1."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Zengwei Yao"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/fxmeng/TransMLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07864v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07864v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09541v1",
                "updated": "2025-02-13T17:57:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T17:57:05Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    17,
                    57,
                    5,
                    3,
                    44,
                    0
                ],
                "title": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated\n  Large-Scale Data Analytics"
                },
                "summary": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the high computational throughput of GPUs, limited memory capacity\nand bandwidth-limited CPU-GPU communication via PCIe links remain significant\nbottlenecks for accelerating large-scale data analytics workloads. This paper\nintroduces Vortex, a GPU-accelerated framework designed for data analytics\nworkloads that exceed GPU memory capacity. A key aspect of our framework is an\noptimized IO primitive that leverages all available PCIe links in multi-GPU\nsystems for the IO demand of a single target GPU. It routes data through other\nGPUs to such target GPU that handles IO-intensive analytics tasks. This\napproach is advantageous when other GPUs are occupied with compute-bound\nworkloads, such as popular AI applications that typically underutilize IO\nresources. We also introduce a novel programming model that separates GPU\nkernel development from IO scheduling, reducing programmer burden and enabling\nGPU code reuse. Additionally, we present the design of certain important query\noperators and discuss a late materialization technique based on GPU's zero-copy\nmemory access. Without caching any data in GPU memory, Vortex improves the\nperformance of the state-of-the-art GPU baseline, Proteus, by 5.7$\\times$ on\naverage and enhances price performance by 2.5$\\times$ compared to a CPU-based\nDuckDB baseline."
                },
                "authors": [
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Advait Iyer"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "VLDB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07115v2",
                "updated": "2025-02-13T12:54:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    12,
                    54,
                    36,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-10T23:11:44Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    23,
                    11,
                    44,
                    0,
                    41,
                    0
                ],
                "title": "Online Scheduling for LLM Inference with KV Cache Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Scheduling for LLM Inference with KV Cache Constraints"
                },
                "summary": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment."
                },
                "authors": [
                    {
                        "name": "Patrick Jaillet"
                    },
                    {
                        "name": "Jiashuo Jiang"
                    },
                    {
                        "name": "Chara Podimata"
                    },
                    {
                        "name": "Zijie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zijie Zhou"
                },
                "author": "Zijie Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09003v1",
                "updated": "2025-02-13T06:44:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T06:44:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    6,
                    44,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach\n  for Large Language Models"
                },
                "summary": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning is a standard method for adapting pre-trained large\nlanguage models (LLMs) to downstream tasks. Quantization has been recently\nstudied as a post-training technique for efficient LLM deployment. To obtain\nquantized fine-tuned LLMs, conventional pipelines would first fine-tune the\npre-trained models, followed by post-training quantization. This often yields\nsuboptimal performance as it fails to leverage the synergy between fine-tuning\nand quantization. To effectively realize low-bit quantization of weights,\nactivations, and KV caches in LLMs, we propose an algorithm named Rotated\nStraight-Through-Estimator (RoSTE), which combines quantization-aware\nsupervised fine-tuning (QA-SFT) with an adaptive rotation strategy that\nidentifies an effective rotation configuration to reduce activation outliers.\nWe provide theoretical insights on RoSTE by analyzing its prediction error when\napplied to an overparameterized least square quantized training problem. Our\nfindings reveal that the prediction error is directly proportional to the\nquantization error of the converged weights, which can be effectively managed\nthrough an optimized rotation configuration. Experiments on Pythia and Llama\nmodels of different sizes demonstrate the effectiveness of RoSTE. Compared to\nexisting post-SFT quantization baselines, our method consistently achieves\nsuperior performances across various tasks and different LLM architectures."
                },
                "authors": [
                    {
                        "name": "Quan Wei"
                    },
                    {
                        "name": "Chung-Yiu Yau"
                    },
                    {
                        "name": "Hoi-To Wai"
                    },
                    {
                        "name": "Yang"
                    },
                    {
                        "name": "Zhao"
                    },
                    {
                        "name": "Dongyeop Kang"
                    },
                    {
                        "name": "Youngsuk Park"
                    },
                    {
                        "name": "Mingyi Hong"
                    }
                ],
                "author_detail": {
                    "name": "Mingyi Hong"
                },
                "arxiv_affiliation": "Katie",
                "author": "Mingyi Hong",
                "arxiv_comment": "18 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08982v1",
                "updated": "2025-02-13T05:40:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T05:40:28Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    5,
                    40,
                    28,
                    3,
                    44,
                    0
                ],
                "title": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Outback: Fast and Communication-efficient Index for Key-Value Store on\n  Disaggregated Memory"
                },
                "summary": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory systems achieve resource utilization efficiency and\nsystem scalability by distributing computation and memory resources into\ndistinct pools of nodes. RDMA is an attractive solution to support\nhigh-throughput communication between different disaggregated resource pools.\nHowever, existing RDMA solutions face a dilemma: one-sided RDMA completely\nbypasses computation at memory nodes, but its communication takes multiple\nround trips; two-sided RDMA achieves one-round-trip communication but requires\nnon-trivial computation for index lookups at memory nodes, which violates the\nprinciple of disaggregated memory. This work presents Outback, a novel indexing\nsolution for key-value stores with a one-round-trip RDMA-based network that\ndoes not incur computation-heavy tasks at memory nodes. Outback is the first to\nutilize dynamic minimal perfect hashing and separates its index into two\ncomponents: one memory-efficient and compute-heavy component at compute nodes\nand the other memory-heavy and compute-efficient component at memory nodes. We\nimplement a prototype of Outback and evaluate its performance in a public\ncloud. The experimental results show that Outback achieves higher throughput\nthan both the state-of-the-art one-sided RDMA and two-sided RDMA-based\nin-memory KVS by 1.06-5.03x, due to the unique strength of applying a separated\nperfect hashing index."
                },
                "authors": [
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Minghao Xie"
                    },
                    {
                        "name": "Shouqian Shi"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Heiner Litz"
                    },
                    {
                        "name": "Chen Qian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Qian"
                },
                "author": "Chen Qian",
                "arxiv_doi": "10.14778/3705829.3705849",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3705829.3705849",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.08982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "PVLDB, 18(2): 335-348, 2024",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08910v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08910v1",
                "updated": "2025-02-13T02:52:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "published": "2025-02-13T02:52:01Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    52,
                    1,
                    3,
                    44,
                    0
                ],
                "title": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InfiniteHiP: Extending Language Model Context Up to 3 Million Tokens on\n  a Single GPU"
                },
                "summary": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), handling very long context lengths\npresents significant challenges as it causes slower inference speeds and\nincreased memory costs. Additionally, most existing pre-trained LLMs fail to\ngeneralize beyond their original training sequence lengths. To enable efficient\nand practical long-context utilization, we introduce InfiniteHiP, a novel, and\npractical LLM inference framework that accelerates processing by dynamically\neliminating irrelevant context tokens through a modular hierarchical token\npruning algorithm. Our method also allows generalization to longer sequences by\nselectively applying various RoPE adjustment methods according to the internal\nattention patterns within LLMs. Furthermore, we offload the key-value cache to\nhost memory during inference, significantly reducing GPU memory pressure. As a\nresult, InfiniteHiP enables the processing of up to 3 million tokens on a\nsingle L40s 48GB GPU -- 3x larger -- without any permanent loss of context\ninformation. Our framework achieves an 18.95x speedup in attention decoding for\na 1 million token context without requiring additional training. We implement\nour method in the SGLang framework and demonstrate its effectiveness and\npracticality through extensive evaluations."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "21 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08910v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08910v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02690v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02690v2",
                "updated": "2025-02-12T14:32:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    14,
                    32,
                    46,
                    2,
                    43,
                    0
                ],
                "published": "2024-04-03T12:37:34Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    12,
                    37,
                    34,
                    2,
                    94,
                    0
                ],
                "title": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Sparse Attention Approximates Exact Attention? Your Attention is\n  Naturally $n^C$-Sparse"
                },
                "summary": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention is a technique that approximates standard attention\ncomputation with sub-quadratic complexity. This is achieved by selectively\nignoring smaller entries in the attention matrix during the softmax function\ncomputation. Variations of this technique, such as pruning KV cache,\nsparsity-based fast attention, and Sparse Transformer, have been extensively\nutilized for efficient Large Language Models (LLMs) deployment. Despite its\nwidespread use, a theoretical understanding of the conditions under which\nsparse attention performs on par with traditional attention remains elusive.\nThis work aims to $\\textbf{bridge this gap by examining the inherent sparsity\nof standard attention processes}$. Our theoretical framework reveals several\nbrand-new key insights:\n  $\\bullet$ Attention is $n^{C}$-sparse, implying that considering only the\nlargest $\\Omega(n^{C})$ entries out of all $n$ entries is sufficient for sparse\nattention to approximate the exact attention matrix with decreasing loss. Here,\n$n$ represents the input length and $C \\in (0, 1)$ is a constant.\n  $\\bullet$ Stable $o(\\log(n))$-sparse attention, which approximates attention\ncomputation with $\\log(n)$ or fewer entries, may not be feasible since the\nerror will persist at a minimum of $O(1)$.\n  $\\bullet$ An adaptive strategy ($\\alpha \\cdot n^C, \\alpha \\in \\mathbb{R}$)\nfor the window size of efficient attention methods rather than a fixed one is\nguaranteed to perform more accurately and efficiently in a task for inference\non flexible context lengths."
                },
                "authors": [
                    {
                        "name": "Yichuan Deng"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Chiwun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chiwun Yang"
                },
                "author": "Chiwun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02690v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02690v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05431v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05431v2",
                "updated": "2025-02-12T13:54:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    13,
                    54,
                    1,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-08T03:41:16Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    41,
                    16,
                    5,
                    39,
                    0
                ],
                "title": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APE: Faster and Longer Context-Augmented Generation via Adaptive\n  Parallel Encoding"
                },
                "summary": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-augmented generation (CAG) techniques, including RAG and ICL, require\nthe efficient combination of multiple contexts to generate responses to user\nqueries. Directly inputting these contexts as a sequence introduces a\nconsiderable computational burden by re-encoding the combined selection of\ncontexts for every request. To address this, we explore the promising potential\nof parallel encoding to independently pre-compute and cache each context's KV\nstates. This approach enables the direct loading of cached states during\ninference while accommodating more contexts through position reuse across\ncontexts. However, due to misalignments in attention distribution, directly\napplying parallel encoding results in a significant performance drop. To enable\neffective and efficient CAG, we propose Adaptive Parallel Encoding\n($\\textbf{APE}$), which brings shared prefix, attention temperature, and\nscaling factor to align the distribution of parallel encoding with sequential\nencoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%\nand 93% sequential encoding performance using the same inputs while\noutperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales\nto many-shot CAG, effectively encoding hundreds of contexts in parallel.\nEfficiency evaluation shows that APE can achieve an end-to-end 4.5$\\times$\nspeedup by reducing 28$\\times$ prefilling time for a 128K-length context."
                },
                "authors": [
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05431v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05431v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08363v1",
                "updated": "2025-02-12T12:50:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "published": "2025-02-12T12:50:15Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    12,
                    50,
                    15,
                    2,
                    43,
                    0
                ],
                "title": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-Theta Attention: Sparsifying Transformers by Compensated\n  Thresholding"
                },
                "summary": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention mechanism is essential for the impressive capabilities of\ntransformer-based Large Language Models (LLMs). However, calculating attention\nis computationally intensive due to its quadratic dependency on the sequence\nlength. We introduce a novel approach called Top-Theta Attention, or simply\nTop-$\\theta$, which selectively prunes less essential attention elements by\ncomparing them against carefully calibrated thresholds. This method greatly\nimproves the efficiency of self-attention matrix multiplication while\npreserving model accuracy, reducing the number of required V cache rows by 3x\nduring generative decoding and the number of attention elements by 10x during\nthe prefill phase. Our method does not require model retraining; instead, it\nrequires only a brief calibration phase to be resilient to distribution shifts,\nthus not requiring the thresholds for different datasets to be recalibrated.\nUnlike top-k attention, Top-$\\theta$ eliminates full-vector dependency, making\nit suitable for tiling and scale-out and avoiding costly top-k search. A key\ninnovation of our approach is the development of efficient numerical\ncompensation techniques, which help preserve model accuracy even under\naggressive pruning of attention scores."
                },
                "authors": [
                    {
                        "name": "Konstantin Berestizshevsky"
                    },
                    {
                        "name": "Renzo Andri"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "arxiv_comment": "8 pages, 11 figures, work under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v2",
                "updated": "2025-02-12T11:05:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    11,
                    5,
                    5,
                    2,
                    43,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20002v3",
                "updated": "2025-02-12T07:02:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    12,
                    7,
                    2,
                    6,
                    2,
                    43,
                    0
                ],
                "published": "2024-09-30T06:55:00Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    6,
                    55,
                    0,
                    0,
                    274,
                    0
                ],
                "title": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Early Bird Catches the Leak: Unveiling Timing Side Channels in LLM\n  Serving Systems"
                },
                "summary": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The wide deployment of Large Language Models (LLMs) has given rise to strong\ndemands for optimizing their inference performance. Today's techniques serving\nthis purpose primarily focus on reducing latency and improving throughput\nthrough algorithmic and hardware enhancements, while largely overlooking their\nprivacy side effects, particularly in a multi-user environment. In our\nresearch, for the first time, we discovered a set of new timing side channels\nin LLM systems, arising from shared caches and GPU memory allocations, which\ncan be exploited to infer both confidential system prompts and those issued by\nother users. These vulnerabilities echo security challenges observed in\ntraditional computing systems, highlighting an urgent need to address potential\ninformation leakage in LLM serving infrastructures. In this paper, we report\nnovel attack strategies designed to exploit such timing side channels inherent\nin LLM deployments, specifically targeting the Key-Value (KV) cache and\nsemantic cache widely used to enhance LLM inference performance. Our approach\nleverages timing measurements and classification models to detect cache hits,\nallowing an adversary to infer private prompts with high accuracy. We also\npropose a token-by-token search algorithm to efficiently recover shared prompt\nprefixes in the caches, showing the feasibility of stealing system prompts and\nthose produced by peer users. Our experimental studies on black-box testing of\npopular online LLM services demonstrate that such privacy risks are completely\nrealistic, with significant consequences. Our findings underscore the need for\nrobust mitigation to protect LLM systems against such emerging threats."
                },
                "authors": [
                    {
                        "name": "Linke Song"
                    },
                    {
                        "name": "Zixuan Pang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "XiaoFeng Wang"
                    },
                    {
                        "name": "Hongbo Chen"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Yier Jin"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_comment": "This work was submitted for review on Sept. 5, 2024, and the initial\n  version was uploaded to Arxiv on Sept. 30, 2024. The latest version reflects\n  the up-to-date experimental results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07903v1",
                "updated": "2025-02-11T19:17:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T19:17:35Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    19,
                    17,
                    35,
                    1,
                    42,
                    0
                ],
                "title": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous\n  Environment"
                },
                "summary": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating the prefill and decoding phases represents an effective new\nparadigm for generative inference of large language models (LLM), which\neliminates prefill-decoding interference and optimizes resource allocation.\nHowever, it is still an open problem about how to deploy the disaggregated\ninference paradigm across a group of heterogeneous GPUs, which can be an\neconomical alternative to deployment over homogeneous high-performance GPUs.\nTowards this end, we introduce HexGen-2, a distributed system for efficient and\neconomical LLM serving on heterogeneous GPUs following the disaggregated\nparadigm. Built on top of HexGen, the core component of HexGen-2 is a\nscheduling algorithm that formalizes the allocation of disaggregated LLM\ninference computations and communications over heterogeneous GPUs and network\nconnections as a constraint optimization problem. We leverage the graph\npartitioning and max-flow algorithms to co-optimize resource allocation,\nparallel strategies for distinct inference phases, and the efficiency of\ninter-phase key-value (KV) cache communications. We conduct extensive\nexperiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models\nin various real-world settings, the results reveal that HexGen-2 delivers up to\na 2.0 times and on average a 1.3 times improvement in serving throughput,\nreduces the average inference latency by 1.5 times compared with\nstate-of-the-art systems given the same price budget, and achieves comparable\ninference performance with a 30% lower price budget."
                },
                "authors": [
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Ran Yan"
                    },
                    {
                        "name": "Binhang Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Binhang Yuan"
                },
                "author": "Binhang Yuan",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07776v1",
                "updated": "2025-02-11T18:58:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T18:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    58,
                    4,
                    1,
                    42,
                    0
                ],
                "title": "Auditing Prompt Caching in Language Model APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auditing Prompt Caching in Language Model APIs"
                },
                "summary": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt caching in large language models (LLMs) results in data-dependent\ntiming variations: cached prompts are processed faster than non-cached prompts.\nThese timing differences introduce the risk of side-channel timing attacks. For\nexample, if the cache is shared across users, an attacker could identify cached\nprompts from fast API response times to learn information about other users'\nprompts. Because prompt caching may cause privacy leakage, transparency around\nthe caching policies of API providers is important. To this end, we develop and\nconduct statistical audits to detect prompt caching in real-world LLM API\nproviders. We detect global cache sharing across users in seven API providers,\nincluding OpenAI, resulting in potential privacy leakage about users' prompts.\nTiming variations due to prompt caching can also result in leakage of\ninformation about model architecture. Namely, we find evidence that OpenAI's\nembedding model is a decoder-only Transformer, which was previously not\npublicly known."
                },
                "authors": [
                    {
                        "name": "Chenchen Gu"
                    },
                    {
                        "name": "Xiang Lisa Li"
                    },
                    {
                        "name": "Rohith Kuditipudi"
                    },
                    {
                        "name": "Percy Liang"
                    },
                    {
                        "name": "Tatsunori Hashimoto"
                    }
                ],
                "author_detail": {
                    "name": "Tatsunori Hashimoto"
                },
                "author": "Tatsunori Hashimoto",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v2",
                "updated": "2025-02-11T18:45:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    18,
                    45,
                    12,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03743v2",
                "updated": "2025-02-11T17:48:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    48,
                    15,
                    1,
                    42,
                    0
                ],
                "published": "2024-09-05T17:56:19Z",
                "published_parsed": [
                    2024,
                    9,
                    5,
                    17,
                    56,
                    19,
                    3,
                    249,
                    0
                ],
                "title": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Libra: Architectural Support For Principled, Secure And Efficient\n  Balanced Execution On High-End Processors (Extended Version)"
                },
                "summary": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow leakage (CFL) attacks enable an attacker to expose control-flow\ndecisions of a victim program via side-channel observations. Linearization\n(i.e., elimination) of secret-dependent control flow is the main countermeasure\nagainst these attacks, yet it comes at a non-negligible cost. Conversely,\nbalancing secret-dependent branches often incurs a smaller overhead, but is\nnotoriously insecure on high-end processors. Hence, linearization has been\nwidely believed to be the only effective countermeasure against CFL attacks. In\nthis paper, we challenge this belief and investigate an unexplored alternative:\nhow to securely balance secret-dependent branches on higher-end processors?\n  We propose Libra, a generic and principled hardware-software codesign to\nefficiently address CFL on high-end processors. We perform a systematic\nclassification of hardware primitives leaking control flow from the literature,\nand provide guidelines to handle them with our design. Importantly, Libra\nenables secure control-flow balancing without the need to disable\nperformance-critical hardware such as the instruction cache and the prefetcher.\nWe formalize the semantics of Libra and propose a code transformation algorithm\nfor securing programs, which we prove correct and secure. Finally, we implement\nand evaluate Libra on an out-of-order RISC-V processor, showing performance\noverhead on par with insecure balanced code, and outperforming state-of-the-art\nlinearized code by 19.3%."
                },
                "authors": [
                    {
                        "name": "Hans Winderix"
                    },
                    {
                        "name": "Marton Bognar"
                    },
                    {
                        "name": "Lesly-Ann Daniel"
                    },
                    {
                        "name": "Frank Piessens"
                    }
                ],
                "author_detail": {
                    "name": "Frank Piessens"
                },
                "author": "Frank Piessens",
                "arxiv_doi": "10.1145/3658644.3690319",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3658644.3690319",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.03743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v2",
                "updated": "2025-02-11T17:36:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    36,
                    32,
                    1,
                    42,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for Offline LLM Inference"
                },
                "summary": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Glinthawk, an architecture for offline Large Language Model\n(LLM) inference. By leveraging a two-tiered structure, Glinthawk optimizes the\nutilization of the high-end accelerators (\"Tier 1\") by offloading the attention\nmechanism to lower-end compute tier (\"Tier 2\"). This separation allows the\nmemory demand of the attention, known as the key-value cache, to scale\nindependently from the model weights, enabling larger batch sizes and more\nefficient accelerator usage. Prototyped with NVIDIA T4 GPUs and standard CPU\nVMs, Glinthawk improves throughput by $5.9\\times$ and reduces cost of\ngeneration by $2.8\\times$, compared to paged attention baselines. For long\nsequence lengths, it achieves $16.3\\times$ throughput improvement at\n$2.4\\times$ less cost. Our evaluation shows that this architecture can tolerate\nmoderate network latency with minimal performance degradation, making it highly\neffective for latency-tolerant, throughput-focused applications such as batch\nprocessing. The prototype is publicly available at\nhttps://github.com/microsoft/glinthawk."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07861v1",
                "updated": "2025-02-11T17:18:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    18,
                    17,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T17:18:17Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    17,
                    18,
                    17,
                    1,
                    42,
                    0
                ],
                "title": "BalanceKV: KV Cache Compression through Discrepancy Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BalanceKV: KV Cache Compression through Discrepancy Theory"
                },
                "summary": "Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved impressive success, but their high\nmemory requirements present challenges for long-context token generation. The\nmemory complexity of long-context LLMs is primarily due to the need to store\nKey-Value (KV) embeddings in their KV cache. We present BalanceKV, a KV cache\ncompression method based on geometric sampling process stemming from\nBanaszczyk's vector balancing theory, which introduces dependencies informed by\nthe geometry of keys and value tokens, and improves precision. BalanceKV offers\nboth theoretically proven and empirically validated performance improvements\nover existing methods."
                },
                "authors": [
                    {
                        "name": "Insu Han"
                    },
                    {
                        "name": "Michael Kapralov"
                    },
                    {
                        "name": "Ekaterina Kochetkova"
                    },
                    {
                        "name": "Kshiteej Sheth"
                    },
                    {
                        "name": "Amir Zandieh"
                    }
                ],
                "author_detail": {
                    "name": "Amir Zandieh"
                },
                "author": "Amir Zandieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03736v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03736v3",
                "updated": "2025-02-11T15:42:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    15,
                    42,
                    19,
                    1,
                    42,
                    0
                ],
                "published": "2024-06-06T04:22:11Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    4,
                    22,
                    11,
                    3,
                    158,
                    0
                ],
                "title": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your Absorbing Discrete Diffusion Secretly Models the Conditional\n  Distributions of Clean Data"
                },
                "summary": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete diffusion models with absorbing processes have shown promise in\nlanguage modeling. The key quantities to be estimated are the ratios between\nthe marginal probabilities of two transitive states at all timesteps, called\nthe concrete score. In this paper, we reveal that the concrete score in\nabsorbing diffusion can be expressed as conditional probabilities of clean\ndata, multiplied by a time-dependent scalar in an analytic form. Motivated by\nthis finding, we propose reparameterized absorbing discrete diffusion (RADD), a\ndedicated diffusion model without time-condition that characterizes the\ntime-independent conditional probabilities. Besides its simplicity, RADD can\nreduce the number of function evaluations (NFEs) by caching the output of the\ntime-independent network when the noisy sample remains unchanged in a sampling\ninterval, which enables sampling acceleration. Built upon the new perspective\nof conditional distributions, we further unify absorbing discrete diffusion and\nany-order autoregressive models (AO-ARMs), showing that the upper bound on the\nnegative log-likelihood for the diffusion model can be interpreted as an\nexpected negative log-likelihood for AO-ARMs. Further, our RADD models achieve\nSOTA performance among diffusion models on 5 zero-shot language modeling\nbenchmarks (measured by perplexity) at the GPT-2 scale. Our code is available\nat https://github.com/ML-GSAI/RADD."
                },
                "authors": [
                    {
                        "name": "Jingyang Ou"
                    },
                    {
                        "name": "Shen Nie"
                    },
                    {
                        "name": "Kaiwen Xue"
                    },
                    {
                        "name": "Fengqi Zhu"
                    },
                    {
                        "name": "Jiacheng Sun"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Chongxuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongxuan Li"
                },
                "author": "Chongxuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03736v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03736v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v1",
                "updated": "2025-02-11T14:25:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users to pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v2",
                "updated": "2025-02-10T18:34:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    18,
                    34,
                    53,
                    0,
                    41,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v2",
                "updated": "2025-02-10T17:19:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    17,
                    19,
                    21,
                    0,
                    41,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v2",
                "updated": "2025-02-10T15:17:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    15,
                    17,
                    49,
                    0,
                    41,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06327v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06327v1",
                "updated": "2025-02-10T10:28:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T10:28:11Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    10,
                    28,
                    11,
                    0,
                    41,
                    0
                ],
                "title": "Prompt-Driven Continual Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Driven Continual Graph Learning"
                },
                "summary": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Graph Learning (CGL), which aims to accommodate new tasks over\nevolving graph data without forgetting prior knowledge, is garnering\nsignificant research interest. Mainstream solutions adopt the memory\nreplay-based idea, ie, caching representative data from earlier tasks for\nretraining the graph model. However, this strategy struggles with scalability\nissues for constantly evolving graphs and raises concerns regarding data\nprivacy. Inspired by recent advancements in the prompt-based learning paradigm,\nthis paper introduces a novel prompt-driven continual graph learning\n(PROMPTCGL) framework, which learns a separate prompt for each incoming task\nand maintains the underlying graph neural network model fixed. In this way,\nPROMPTCGL naturally avoids catastrophic forgetting of knowledge from previous\ntasks. More specifically, we propose hierarchical prompting to instruct the\nmodel from both feature- and topology-level to fully address the variability of\ntask graphs in dynamic continual learning. Additionally, we develop a\npersonalized prompt generator to generate tailored prompts for each graph node\nwhile minimizing the number of prompts needed, leading to constant memory\nconsumption regardless of the graph scale. Extensive experiments on four\nbenchmarks show that PROMPTCGL achieves superior performance against existing\nCGL approaches while significantly reducing memory consumption. Our code is\navailable at https://github.com/QiWang98/PromptCGL."
                },
                "authors": [
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Tianfei Zhou"
                    },
                    {
                        "name": "Ye Yuan"
                    },
                    {
                        "name": "Rui Mao"
                    }
                ],
                "author_detail": {
                    "name": "Rui Mao"
                },
                "author": "Rui Mao",
                "arxiv_comment": "12 pages, 7figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06327v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06327v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06166v1",
                "updated": "2025-02-10T05:33:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "published": "2025-02-10T05:33:25Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    5,
                    33,
                    25,
                    0,
                    41,
                    0
                ],
                "title": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Portable, High-Frequency, and High-Voltage Control Circuits for\n  Untethered Miniature Robots Driven by Dielectric Elastomer Actuators"
                },
                "summary": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a high-voltage, high-frequency control circuit for\nthe untethered applications of dielectric elastomer actuators (DEAs). The\ncircuit board leverages low-voltage resistive components connected in series to\ncontrol voltages of up to 1.8 kV within a compact size, suitable for\nfrequencies ranging from 0 to 1 kHz. A single-channel control board weighs only\n2.5 g. We tested the performance of the control circuit under different load\nconditions and power supplies. Based on this control circuit, along with a\ncommercial miniature high-voltage power converter, we construct an untethered\ncrawling robot driven by a cylindrical DEA. The 42-g untethered robots\nsuccessfully obtained crawling locomotion on a bench and within a pipeline at a\ndriving frequency of 15 Hz, while simultaneously transmitting real-time video\ndata via an onboard camera and antenna. Our work provides a practical way to\nuse low-voltage control electronics to achieve the untethered driving of DEAs,\nand therefore portable and wearable devices."
                },
                "authors": [
                    {
                        "name": "Qi Shao"
                    },
                    {
                        "name": "Xin-Jun Liu"
                    },
                    {
                        "name": "Huichan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Huichan Zhao"
                },
                "author": "Huichan Zhao",
                "arxiv_comment": "7 pages, 10 figures, accepted by ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04603v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04603v2",
                "updated": "2025-02-09T20:52:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    52,
                    26,
                    6,
                    40,
                    0
                ],
                "published": "2024-10-06T19:36:34Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    19,
                    36,
                    34,
                    6,
                    280,
                    0
                ],
                "title": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-compensating Light Calorimetry with Liquid Argon Time Projection\n  Chamber for GeV Neutrino Physics"
                },
                "summary": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Liquid Argon Time Projection Chamber (LArTPC) is a powerful dual\ncalorimeter capable of estimating particle energy from both ionization charge\nand scintillation light. Our study shows that, due to the recombination\nluminescence, the LArTPC functions as a self-compensating light calorimeter:\nthe missing energy in the hadronic component is compensated for by the\nincreased luminescence relative to the electromagnetic component. Using 0.5--5\nGeV electron neutrino charged current interactions as a case study, we show\nthat good compensation of the electron-to-hadron response ratio (e/h) from\n1--1.05 can be achieved across a broad range of drift electric fields (0.2--1.8\nkV/cm), with better performance for neutrino energies above 2 GeV. This study\nhighlights the potential of light calorimetry in LArTPCs for GeV neutrino\nenergy reconstruction, complementing traditional charge calorimetry. Under\nideal conditions of uniform light collection, we show that LArTPC light\ncalorimetry can achieve an energy resolution comparable to the charge imaging\ncalorimetry. Challenges arising from nonuniform light collection in large\nLArTPCs can be mitigated with a position-dependent light yield correction\nderived from 3D charge signal imaging."
                },
                "authors": [
                    {
                        "name": "Xuyang Ning"
                    },
                    {
                        "name": "Wei Shi"
                    },
                    {
                        "name": "Chao Zhang"
                    },
                    {
                        "name": "Ciro Riccio"
                    },
                    {
                        "name": "Jay Hyun Jo"
                    }
                ],
                "author_detail": {
                    "name": "Jay Hyun Jo"
                },
                "author": "Jay Hyun Jo",
                "arxiv_doi": "10.1103/PhysRevD.111.032007",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.032007",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.04603v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04603v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "18 pages, 14 figures",
                "arxiv_journal_ref": "Phys. Rev. D 111, 032007 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06901v1",
                "updated": "2025-02-09T20:02:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    2,
                    5,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T20:02:05Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    20,
                    2,
                    5,
                    6,
                    40,
                    0
                ],
                "title": "Enabling Autoregressive Models to Fill In Masked Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Autoregressive Models to Fill In Masked Tokens"
                },
                "summary": "Historically, LLMs have been trained using either autoregressive (AR) or\nmasked language modeling (MLM) objectives, with AR models gaining dominance in\nrecent years. However, AR models are inherently incapable of masked infilling,\nwhich is the ability to predict masked tokens between past and future context.\nIn contrast, MLM models suffer from intrinsic computational inefficiencies\nduring both training and inference that hinder their scalability. This work\nintroduces MARIA (Masked and Autoregressive Infilling Architecture), a novel\napproach that leverages the strengths of both paradigms to achieve\nstate-of-the-art masked infilling performance. MARIA combines a pre-trained MLM\nand AR model by training a linear decoder that takes their concatenated hidden\nstates as input. This minimal modification enables the AR model to perform\ninfilling while retaining its inherent advantages in terms of faster inference\nwith KV caching. Our results demonstrate that MARIA significantly outperforms\nexisting methods, namely discrete diffusion models, on masked infilling tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Historically, LLMs have been trained using either autoregressive (AR) or\nmasked language modeling (MLM) objectives, with AR models gaining dominance in\nrecent years. However, AR models are inherently incapable of masked infilling,\nwhich is the ability to predict masked tokens between past and future context.\nIn contrast, MLM models suffer from intrinsic computational inefficiencies\nduring both training and inference that hinder their scalability. This work\nintroduces MARIA (Masked and Autoregressive Infilling Architecture), a novel\napproach that leverages the strengths of both paradigms to achieve\nstate-of-the-art masked infilling performance. MARIA combines a pre-trained MLM\nand AR model by training a linear decoder that takes their concatenated hidden\nstates as input. This minimal modification enables the AR model to perform\ninfilling while retaining its inherent advantages in terms of faster inference\nwith KV caching. Our results demonstrate that MARIA significantly outperforms\nexisting methods, namely discrete diffusion models, on masked infilling tasks."
                },
                "authors": [
                    {
                        "name": "Daniel Israel"
                    },
                    {
                        "name": "Aditya Grover"
                    },
                    {
                        "name": "Guy Van den Broeck"
                    }
                ],
                "author_detail": {
                    "name": "Guy Van den Broeck"
                },
                "author": "Guy Van den Broeck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05960v1",
                "updated": "2025-02-09T17:09:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    17,
                    9,
                    20,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T17:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    17,
                    9,
                    20,
                    6,
                    40,
                    0
                ],
                "title": "Electric field control of nonlinear Hall effect in Weyl semimetal\n  TaIrTe4",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of nonlinear Hall effect in Weyl semimetal\n  TaIrTe4"
                },
                "summary": "The nonlinear Hall effect (NLHE), as an important probe to reveal the\nsymmetry breaking in topological properties of materials, opens up a new\ndimension for exploring the energy band structure and electron transport\nmechanism of quantum materials. Current studies mainly focus on the observation\nof material intrinsic the NLHE or inducing the NLHE response by artificially\nconstructing corrugated/twisted twodimensionalmaterial systems. Notably, the\nmodulation of NLHE signal strength, a core parameter of device performance, has\nattracted much attention, while theoretical predictions suggest that an applied\nelectric field can achieve the NLHE enhancement through modulation of the Berry\ncurvature dipole (BCD). Here we report effective modulation the magnitude and\nsign of the NLHE by applying additional constant electric fields of different\ndirections and magnitudes in the semimetal TaIrTe4. The NLHE response strength\nis enhanced by 168 times compared to the intrinsic one at 4 K when the\nadditional constant electric field of -0.5 kV/cm is applied to the b-axis of\nTaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling\nlaw analysis suggests that the enhancement may be the result of the combined\neffect of the electric field on the intrinsic BCD and disorder scattering\neffect of TaIrTe4. This work provides a means to study the properties of\nTaIrTe4, as well as a valuable reference for the study of novel electronic\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nonlinear Hall effect (NLHE), as an important probe to reveal the\nsymmetry breaking in topological properties of materials, opens up a new\ndimension for exploring the energy band structure and electron transport\nmechanism of quantum materials. Current studies mainly focus on the observation\nof material intrinsic the NLHE or inducing the NLHE response by artificially\nconstructing corrugated/twisted twodimensionalmaterial systems. Notably, the\nmodulation of NLHE signal strength, a core parameter of device performance, has\nattracted much attention, while theoretical predictions suggest that an applied\nelectric field can achieve the NLHE enhancement through modulation of the Berry\ncurvature dipole (BCD). Here we report effective modulation the magnitude and\nsign of the NLHE by applying additional constant electric fields of different\ndirections and magnitudes in the semimetal TaIrTe4. The NLHE response strength\nis enhanced by 168 times compared to the intrinsic one at 4 K when the\nadditional constant electric field of -0.5 kV/cm is applied to the b-axis of\nTaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling\nlaw analysis suggests that the enhancement may be the result of the combined\neffect of the electric field on the intrinsic BCD and disorder scattering\neffect of TaIrTe4. This work provides a means to study the properties of\nTaIrTe4, as well as a valuable reference for the study of novel electronic\ndevices."
                },
                "authors": [
                    {
                        "name": "Jiaju Yang"
                    },
                    {
                        "name": "Lujun Wei"
                    },
                    {
                        "name": "Yanghui Li"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Feng Li"
                    },
                    {
                        "name": "Ping Liu"
                    },
                    {
                        "name": "Shuang Zhou"
                    },
                    {
                        "name": "Yong Pu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Pu"
                },
                "author": "Yong Pu",
                "arxiv_comment": "19 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05859v1",
                "updated": "2025-02-09T11:36:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    11,
                    36,
                    45,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T11:36:45Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    11,
                    36,
                    45,
                    6,
                    40,
                    0
                ],
                "title": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SphereFusion: Efficient Panorama Depth Estimation via Gated Fusion"
                },
                "summary": "Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the rapid development of panorama cameras, the task of estimating\npanorama depth has attracted significant attention from the computer vision\ncommunity, especially in applications such as robot sensing and autonomous\ndriving. However, existing methods relying on different projection formats\noften encounter challenges, either struggling with distortion and discontinuity\nin the case of equirectangular, cubemap, and tangent projections, or\nexperiencing a loss of texture details with the spherical projection. To tackle\nthese concerns, we present SphereFusion, an end-to-end framework that combines\nthe strengths of various projection methods. Specifically, SphereFusion\ninitially employs 2D image convolution and mesh operations to extract two\ndistinct types of features from the panorama image in both equirectangular and\nspherical projection domains. These features are then projected onto the\nspherical domain, where a gate fusion module selects the most reliable features\nfor fusion. Finally, SphereFusion estimates panorama depth within the spherical\ndomain. Meanwhile, SphereFusion employs a cache strategy to improve the\nefficiency of mesh operation. Extensive experiments on three public panorama\ndatasets demonstrate that SphereFusion achieves competitive results with other\nstate-of-the-art methods, while presenting the fastest inference speed at only\n17 ms on a 512$\\times$1024 panorama image."
                },
                "authors": [
                    {
                        "name": "Qingsong Yan"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Kaiyong Zhao"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Fei Deng"
                    }
                ],
                "author_detail": {
                    "name": "Fei Deng"
                },
                "author": "Fei Deng",
                "arxiv_comment": "3DV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05763v1",
                "updated": "2025-02-09T03:49:52Z",
                "updated_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    49,
                    52,
                    6,
                    40,
                    0
                ],
                "published": "2025-02-09T03:49:52Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    3,
                    49,
                    52,
                    6,
                    40,
                    0
                ],
                "title": "Public DNS Resolvers Meet Content Delivery Networks: A Performance\n  Assessment of the Interplay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public DNS Resolvers Meet Content Delivery Networks: A Performance\n  Assessment of the Interplay"
                },
                "summary": "This paper investigates two key performance aspects of the interplay between\npublic DNS resolution services and content delivery networks -- the latency of\nDNS queries for resolving CDN-accelerated hostnames and the latency between the\nend-user and the CDN's edge server obtained by the user through a given\nresolution service. While these important issues have been considered in the\npast, significant developments, such as the IPv6 finally getting traction, the\nadoption of the ECS extension to DNS by major DNS resolution services, and the\nembracing of anycast by some CDNs warrant a reassessment under these new\nrealities. Among the resolution services we consider, We find Google DNS and\nOpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in\nterms of DNS latency, and trace the cause to drastically lower cache hit rates.\nAt the same time, we find that Google and OpenDNS have largely closed the gap\nwith ISP resolvers in the quality of CDNs'client-to-edge-server mappings as\nmeasured by latency, while the Cloudflare resolver still shows some penalty\nwith Akamai, and Quad9 exhibits a noticeable penalty with three of the four\nCDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to\nmap clients to servers. Finally, in several locations, we observe IPv6 penalty\nin the latency of client-to-CDN-edge-server mappings produced by the resolvers.\nMoreover, this penalty does not rise above typical thresholds employed by the\nHappy Eyeballs algorithm for falling back to IPv4 communication. Thus,\ndual-stacked clients in these locations may experience suboptimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates two key performance aspects of the interplay between\npublic DNS resolution services and content delivery networks -- the latency of\nDNS queries for resolving CDN-accelerated hostnames and the latency between the\nend-user and the CDN's edge server obtained by the user through a given\nresolution service. While these important issues have been considered in the\npast, significant developments, such as the IPv6 finally getting traction, the\nadoption of the ECS extension to DNS by major DNS resolution services, and the\nembracing of anycast by some CDNs warrant a reassessment under these new\nrealities. Among the resolution services we consider, We find Google DNS and\nOpenDNS to lag behind the Cloudflare resolver and, for some CDNs, Quad9 in\nterms of DNS latency, and trace the cause to drastically lower cache hit rates.\nAt the same time, we find that Google and OpenDNS have largely closed the gap\nwith ISP resolvers in the quality of CDNs'client-to-edge-server mappings as\nmeasured by latency, while the Cloudflare resolver still shows some penalty\nwith Akamai, and Quad9 exhibits a noticeable penalty with three of the four\nCDNs in the study, keeping up only for Cloudflare CDN that does not use DNS to\nmap clients to servers. Finally, in several locations, we observe IPv6 penalty\nin the latency of client-to-CDN-edge-server mappings produced by the resolvers.\nMoreover, this penalty does not rise above typical thresholds employed by the\nHappy Eyeballs algorithm for falling back to IPv4 communication. Thus,\ndual-stacked clients in these locations may experience suboptimal performance."
                },
                "authors": [
                    {
                        "name": "Nicholas Kernan"
                    },
                    {
                        "name": "Joey Li"
                    },
                    {
                        "name": "Rami Al-Dalky"
                    },
                    {
                        "name": "Michael Rabinovich"
                    }
                ],
                "author_detail": {
                    "name": "Michael Rabinovich"
                },
                "author": "Michael Rabinovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v2",
                "updated": "2025-02-08T21:44:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    21,
                    44,
                    24,
                    5,
                    39,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Seluk Kse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22134v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22134v2",
                "updated": "2025-02-08T14:11:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    14,
                    11,
                    25,
                    5,
                    39,
                    0
                ],
                "published": "2024-10-29T15:31:27Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    31,
                    27,
                    1,
                    303,
                    0
                ],
                "title": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProMoE: Fast MoE-based LLM Serving using Proactive Caching"
                },
                "summary": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The promising applications of large language models are often limited by the\nconstrained GPU memory capacity available on edge devices. Mixture-of-Experts\n(MoE) models help address this issue by activating only a subset of the model's\nparameters during computation. This approach allows the unused parameters to be\noffloaded to host memory, thereby reducing the overall GPU memory demand.\nHowever, existing cache-based offloading solutions handle cache misses\nreactively, which significantly impacts system performance. In this paper, we\nintroduce ProMoE, a novel proactive caching system that utilizes intermediate\nresults to predict subsequent expert usage. By proactively fetching experts in\nadvance, ProMoE eliminates passive cache misses, removes loading time from the\ncritical path, and reduces the performance overhead associated with offloading.\nOur evaluations demonstrate that ProMoE achieves an average speedup of 2.20x\n(up to 3.21x) and 2.07x (up to 5.02x) in the prefill and decode stages,\nrespectively, compared to existing offloading solutions."
                },
                "authors": [
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Zihang Zhong"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22134v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22134v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v2",
                "updated": "2025-02-08T11:51:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    11,
                    51,
                    57,
                    5,
                    39,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusions in the mathematical\ndomain. We release MRBench - a new evaluation benchmark containing 192\nconversations and 1,596 responses from seven state-of-the-art LLM-based and\nhuman tutors, providing gold annotations for eight pedagogical dimensions. We\nassess reliability of the popular Prometheus2 and Llama-3.1-8B LLMs as\nevaluators and analyze each tutor's pedagogical abilities, highlighting which\nLLMs are good tutors and which ones are more suitable as question-answering\nsystems. We believe that the presented taxonomy, benchmark, and human-annotated\nlabels will streamline the evaluation process and help track the progress in AI\ntutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "9 pages",
                "arxiv_journal_ref": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05511v1",
                "updated": "2025-02-08T10:14:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    10,
                    14,
                    21,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T10:14:21Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    10,
                    14,
                    21,
                    5,
                    39,
                    0
                ],
                "title": "New and Improved Bounds for Markov Paging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "New and Improved Bounds for Markov Paging"
                },
                "summary": "In the Markov paging model, one assumes that page requests are drawn from a\nMarkov chain over the pages in memory, and the goal is to maintain a fast cache\nthat suffers few page faults in expectation. While computing the optimal online\nalgorithm $(\\mathrm{OPT})$ for this problem naively takes time exponential in\nthe size of the cache, the best-known polynomial-time approximation algorithm\nis the dominating distribution algorithm due to Lund, Phillips and Reingold\n(FOCS 1994), who showed that the algorithm is $4$-competitive against\n$\\mathrm{OPT}$. We substantially improve their analysis and show that the\ndominating distribution algorithm is in fact $2$-competitive against\n$\\mathrm{OPT}$. We also show a lower bound of $1.5907$-competitiveness for this\nalgorithm -- to the best of our knowledge, no such lower bound was previously\nknown.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the Markov paging model, one assumes that page requests are drawn from a\nMarkov chain over the pages in memory, and the goal is to maintain a fast cache\nthat suffers few page faults in expectation. While computing the optimal online\nalgorithm $(\\mathrm{OPT})$ for this problem naively takes time exponential in\nthe size of the cache, the best-known polynomial-time approximation algorithm\nis the dominating distribution algorithm due to Lund, Phillips and Reingold\n(FOCS 1994), who showed that the algorithm is $4$-competitive against\n$\\mathrm{OPT}$. We substantially improve their analysis and show that the\ndominating distribution algorithm is in fact $2$-competitive against\n$\\mathrm{OPT}$. We also show a lower bound of $1.5907$-competitiveness for this\nalgorithm -- to the best of our knowledge, no such lower bound was previously\nknown."
                },
                "authors": [
                    {
                        "name": "Chirag Pabbaraju"
                    },
                    {
                        "name": "Ali Vakilian"
                    }
                ],
                "author_detail": {
                    "name": "Ali Vakilian"
                },
                "author": "Ali Vakilian",
                "arxiv_comment": "26 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05433v1",
                "updated": "2025-02-08T03:46:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    46,
                    28,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:46:28Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    46,
                    28,
                    5,
                    39,
                    0
                ],
                "title": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming\n  And Keyframe Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaFlow: Efficient Long Video Editing via Adaptive Attention Slimming\n  And Keyframe Selection"
                },
                "summary": "Despite great progress, text-driven long video editing is still notoriously\nchallenging mainly due to excessive memory overhead. Although recent efforts\nhave simplified this task into a two-step process of keyframe translation and\ninterpolation generation, the token-wise keyframe translation still plagues the\nupper limit of video length. In this paper, we propose a novel and\ntraining-free approach towards efficient and effective long video editing,\ntermed AdaFlow. We first reveal that not all tokens of video frames hold equal\nimportance for keyframe translation, based on which we propose an Adaptive\nAttention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus\nincreasing the number of keyframes for translations by an order of magnitude.\nIn addition, an Adaptive Keyframe Selection scheme is also equipped to select\nthe representative frames for joint editing, further improving generation\nquality. With these innovative designs, AdaFlow achieves high-quality long\nvideo editing of minutes in one inference, i.e., more than 1$k$ frames on one\nA800 GPU, which is about ten times longer than the compared methods, e.g.,\nTokenFlow. To validate AdaFlow, we also build a new benchmark for long video\nediting with high-quality annotations, termed LongV-EVAL. Our code is released\nat: https://github.com/jidantang55/AdaFlow.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite great progress, text-driven long video editing is still notoriously\nchallenging mainly due to excessive memory overhead. Although recent efforts\nhave simplified this task into a two-step process of keyframe translation and\ninterpolation generation, the token-wise keyframe translation still plagues the\nupper limit of video length. In this paper, we propose a novel and\ntraining-free approach towards efficient and effective long video editing,\ntermed AdaFlow. We first reveal that not all tokens of video frames hold equal\nimportance for keyframe translation, based on which we propose an Adaptive\nAttention Slimming scheme for AdaFlow to squeeze the $KV$ sequence, thus\nincreasing the number of keyframes for translations by an order of magnitude.\nIn addition, an Adaptive Keyframe Selection scheme is also equipped to select\nthe representative frames for joint editing, further improving generation\nquality. With these innovative designs, AdaFlow achieves high-quality long\nvideo editing of minutes in one inference, i.e., more than 1$k$ frames on one\nA800 GPU, which is about ten times longer than the compared methods, e.g.,\nTokenFlow. To validate AdaFlow, we also build a new benchmark for long video\nediting with high-quality annotations, termed LongV-EVAL. Our code is released\nat: https://github.com/jidantang55/AdaFlow."
                },
                "authors": [
                    {
                        "name": "Shuheng Zhang"
                    },
                    {
                        "name": "Yuqi Liu"
                    },
                    {
                        "name": "Hongbo Zhou"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Yiyi Zhou"
                    },
                    {
                        "name": "Xiaoshuai Sun"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05429v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05429v1",
                "updated": "2025-02-08T03:35:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    35,
                    55,
                    5,
                    39,
                    0
                ],
                "published": "2025-02-08T03:35:55Z",
                "published_parsed": [
                    2025,
                    2,
                    8,
                    3,
                    35,
                    55,
                    5,
                    39,
                    0
                ],
                "title": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code\n  Conflicts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code\n  Conflicts"
                },
                "summary": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-modifying code (SMC) allows programs to alter their own instructions,\noptimizing performance and functionality on x86 processors. Despite its\nbenefits, SMC introduces unique microarchitectural behaviors that can be\nexploited for malicious purposes. In this paper, we explore the security\nimplications of SMC by examining how specific x86 instructions affecting\ninstruction cache lines lead to measurable timing discrepancies between cache\nhits and misses. These discrepancies facilitate refined cache attacks, making\nthem less noisy and more effective. We introduce novel attack techniques that\nleverage these timing variations to enhance existing methods such as\nPrime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more\nprecisely attack cryptographic keys and create covert channels akin to Spectre\nacross various x86 platforms. Finally, we propose a dynamic detection\nmethodology utilizing hardware performance counters to mitigate these enhanced\nthreats."
                },
                "authors": [
                    {
                        "name": "Seonghun Son"
                    },
                    {
                        "name": "Daniel Moghimi"
                    },
                    {
                        "name": "Berk Gulmezoglu"
                    }
                ],
                "author_detail": {
                    "name": "Berk Gulmezoglu"
                },
                "author": "Berk Gulmezoglu",
                "arxiv_doi": "10.1145/3676641.3716274",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716274",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.05429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05429v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Proceedings of the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS) accepted",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12304v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12304v4",
                "updated": "2025-02-07T23:14:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    23,
                    14,
                    10,
                    4,
                    38,
                    0
                ],
                "published": "2024-05-20T18:11:45Z",
                "published_parsed": [
                    2024,
                    5,
                    20,
                    18,
                    11,
                    45,
                    0,
                    141,
                    0
                ],
                "title": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A\n  Non-Linear Programming Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Hardware Pragma Insertion in High-Level Synthesis: A\n  Non-Linear Programming Approach"
                },
                "summary": "High-Level Synthesis enables the rapid prototyping of hardware accelerators,\nby combining a high-level description of the functional behavior of a kernel\nwith a set of micro-architecture optimizations as inputs. Such optimizations\ncan be described by inserting pragmas e.g. pipelining and replication of units,\nor even higher level transformations for HLS such as automatic data caching\nusing the AMD/Xilinx Merlin compiler. Selecting the best combination of\npragmas, even within a restricted set, remains particularly challenging and the\ntypical state-of-practice uses design-space exploration to navigate this space.\nBut due to the highly irregular performance distribution of pragma\nconfigurations, typical DSE approaches are either extremely time consuming, or\noperating on a severely restricted search space. This work proposes a framework\nto automatically insert HLS pragmas in regular loop-based programs, supporting\npipelining, unit replication, and data caching. We develop an analytical\nperformance and resource model as a function of the input program properties\nand pragmas inserted, using non-linear constraints and objectives. We prove\nthis model provides a lower bound on the actual performance after HLS. We then\nencode this model as a Non-Linear Program, by making the pragma configuration\nunknowns of the system, which is computed optimally by solving this NLP. This\napproach can also be used during DSE, to quickly prune points with a (possibly\npartial) pragma configuration, driven by lower bounds on achievable latency. We\nextensively evaluate our end-to-end, fully implemented system, showing it can\neffectively manipulate spaces of billions of designs in seconds to minutes for\nthe kernels evaluated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Level Synthesis enables the rapid prototyping of hardware accelerators,\nby combining a high-level description of the functional behavior of a kernel\nwith a set of micro-architecture optimizations as inputs. Such optimizations\ncan be described by inserting pragmas e.g. pipelining and replication of units,\nor even higher level transformations for HLS such as automatic data caching\nusing the AMD/Xilinx Merlin compiler. Selecting the best combination of\npragmas, even within a restricted set, remains particularly challenging and the\ntypical state-of-practice uses design-space exploration to navigate this space.\nBut due to the highly irregular performance distribution of pragma\nconfigurations, typical DSE approaches are either extremely time consuming, or\noperating on a severely restricted search space. This work proposes a framework\nto automatically insert HLS pragmas in regular loop-based programs, supporting\npipelining, unit replication, and data caching. We develop an analytical\nperformance and resource model as a function of the input program properties\nand pragmas inserted, using non-linear constraints and objectives. We prove\nthis model provides a lower bound on the actual performance after HLS. We then\nencode this model as a Non-Linear Program, by making the pragma configuration\nunknowns of the system, which is computed optimally by solving this NLP. This\napproach can also be used during DSE, to quickly prune points with a (possibly\npartial) pragma configuration, driven by lower bounds on achievable latency. We\nextensively evaluate our end-to-end, fully implemented system, showing it can\neffectively manipulate spaces of billions of designs in seconds to minutes for\nthe kernels evaluated."
                },
                "authors": [
                    {
                        "name": "Stphane Pouget"
                    },
                    {
                        "name": "Louis-Nol Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "arxiv_doi": "10.1145/3711847",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711847",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.12304v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12304v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05370v1",
                "updated": "2025-02-07T22:51:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T22:51:17Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    51,
                    17,
                    4,
                    38,
                    0
                ],
                "title": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "fMoE: Fine-Grained Expert Offloading for Large Mixture-of-Experts\n  Serving"
                },
                "summary": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained immense success in revolutionizing\nvarious applications, including content generation, search and recommendation,\nand AI-assisted operation. To reduce high training costs, Mixture-of-Experts\n(MoE) architecture has become a popular backbone for modern LLMs. However,\ndespite the benefits, serving MoE-based LLMs experience severe memory\ninefficiency due to sparsely activated experts. Recent studies propose to\noffload inactive experts from GPU memory to CPU memory to improve the serving\nefficiency of MoE models. However, they either incur high inference latency or\nhigh model memory footprints due to coarse-grained designs. To tame the\nlatency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert\noffloading system for MoE serving that achieves low inference latency with\nmemory efficiency. We design fMoE to extract fine-grained expert selection\npatterns from MoE models and semantic hints from input prompts to efficiently\nguide expert prefetching, caching, and offloading decisions. fMoE is prototyped\non top of HuggingFace Transformers and deployed on a six-GPU testbed.\nExperiments with open-source MoE models and real-world workloads show that fMoE\nreduces inference latency by 47% and improves expert hit rate by 36% over\nstate-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Hanfei Yu"
                    },
                    {
                        "name": "Xingqi Cui"
                    },
                    {
                        "name": "Hong Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v2",
                "updated": "2025-02-07T22:00:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    22,
                    0,
                    48,
                    4,
                    38,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04923v1",
                "updated": "2025-02-07T13:41:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T13:41:51Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    41,
                    51,
                    4,
                    38,
                    0
                ],
                "title": "Cached Multi-Lora Composition for Multi-Concept Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Multi-Lora Composition for Multi-Concept Image Generation"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a widely adopted technique in\ntext-to-image models, enabling precise rendering of multiple distinct elements,\nsuch as characters and styles, in multi-concept image generation. However,\ncurrent approaches face significant challenges when composing these LoRAs for\nmulti-concept image generation, resulting in diminished generated image\nquality. In this paper, we initially investigate the role of LoRAs in the\ndenoising process through the lens of the Fourier frequency domain. Based on\nthe hypothesis that applying multiple LoRAs could lead to \"semantic conflicts\",\nwe find that certain LoRAs amplify high-frequency features such as edges and\ntextures, whereas others mainly focus on low-frequency elements, including the\noverall structure and smooth color gradients. Building on these insights, we\ndevise a frequency domain based sequencing strategy to determine the optimal\norder in which LoRAs should be integrated during inference. This strategy\noffers a methodical and generalizable solution compared to the naive\nintegration commonly found in existing LoRA fusion techniques. To fully\nleverage our proposed LoRA order sequence determination method in multi-LoRA\ncomposition tasks, we introduce a novel, training-free framework, Cached\nMulti-LoRA (CMLoRA), designed to efficiently integrate multiple LoRAs while\nmaintaining cohesive image generation. With its flexible backbone for\nmulti-LoRA fusion and a non-uniform caching strategy tailored to individual\nLoRAs, CMLoRA has the potential to reduce semantic conflicts in LoRA\ncomposition and improve computational efficiency. Our experimental evaluations\ndemonstrate that CMLoRA outperforms state-of-the-art training-free LoRA fusion\nmethods by a significant margin -- it achieves an average improvement of\n$2.19\\%$ in CLIPScore, and $11.25\\%$ in MLLM win rate compared to LoraHub, LoRA\nComposite, and LoRA Switch."
                },
                "authors": [
                    {
                        "name": "Xiandong Zou"
                    },
                    {
                        "name": "Mingzhu Shen"
                    },
                    {
                        "name": "Christos-Savvas Bouganis"
                    },
                    {
                        "name": "Yiren Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yiren Zhao"
                },
                "author": "Yiren Zhao",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14846v2",
                "updated": "2025-02-07T13:09:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    13,
                    9,
                    17,
                    4,
                    38,
                    0
                ],
                "published": "2024-09-23T09:22:59Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    9,
                    22,
                    59,
                    0,
                    267,
                    0
                ],
                "title": "A-VL: Adaptive Attention for Large Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A-VL: Adaptive Attention for Large Vision-Language Models"
                },
                "summary": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Large Vision-Language Model (LVLM) integrates computer vision and natural\nlanguage processing techniques, offering substantial application potential.\nHowever, these models demand extensive resources during inference. Adaptive\nattention techniques can dynamically reduce computational redundancy and thus\nimprove efficiency. Although current adaptive attention methods significantly\nreduce the memory requirements of Transformer-based language models, they are\nnot tailored for LVLMs. We observe that LVLMs generate responses from both\nremote image tokens and local text tokens, and different modalities have\ndifferent attention patterns. This observation inspires us to manage the\nattention for each modality separately. Specifically, for visual input, we\nstore the cache of potentially useful information but only compute the most\ncritical parts. For language input, we care more about local information. Based\non our observation and analysis of vision-language attention patterns, we\ndevelop A-VL, a plug-and-play adaptive attention tailored for LVLM inference.\nExtensive evaluations on three vision-language tasks and five datasets show the\neffectiveness of our designs. Our approach A-VL outperforms existing adaptive\nattention methods in reducing memory usage and computational load without\ncompromising performance."
                },
                "authors": [
                    {
                        "name": "Junyang Zhang"
                    },
                    {
                        "name": "Mu Yuan"
                    },
                    {
                        "name": "Ruiguang Zhong"
                    },
                    {
                        "name": "Puhan Luo"
                    },
                    {
                        "name": "Huiyou Zhan"
                    },
                    {
                        "name": "Ningkang Zhang"
                    },
                    {
                        "name": "Chengchen Hu"
                    },
                    {
                        "name": "Xiangyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Li"
                },
                "author": "Xiangyang Li",
                "arxiv_comment": "AAAI 2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v1",
                "updated": "2025-02-07T08:48:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21035v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21035v2",
                "updated": "2025-02-06T20:26:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    20,
                    26,
                    24,
                    3,
                    37,
                    0
                ],
                "published": "2024-10-28T13:56:30Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    13,
                    56,
                    30,
                    0,
                    302,
                    0
                ],
                "title": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Autoregression: Fast LLMs via Self-Distillation Through Time"
                },
                "summary": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive (AR) Large Language Models (LLMs) have demonstrated\nsignificant success across numerous tasks. However, the AR modeling paradigm\npresents certain limitations; for instance, contemporary autoregressive LLMs\nare trained to generate one token at a time, which can result in noticeable\nlatency. Recent advances have indicated that search and repeated sampling can\nenhance performance in various applications, such as theorem proving, code\ngeneration, and alignment, by utilizing greater computational resources during\ninference. In this study, we demonstrate that diffusion language models are\ncapable of generating at least 32 tokens simultaneously, while exceeding the\nperformance of AR models in text quality and on the LAMBADA natural language\nunderstanding benchmark. This outcome is achieved through a novel distillation\nmethod for discrete diffusion models, which reduces the number of inference\nsteps by a factor of 32-64. Practically, at the 1.3B parameters scale,\ndiffusion models, even without caching, can generate tokens at a rate that is\nup to 8 times faster than AR models employing KV-caching, and we anticipate\nfurther improvements with the inclusion of caching. Moreover, we demonstrate\nthe efficacy of our approach for diffusion language models with up to 860M\nparameters."
                },
                "authors": [
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21035v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21035v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04420v1",
                "updated": "2025-02-06T15:26:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T15:26:26Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    15,
                    26,
                    26,
                    3,
                    37,
                    0
                ],
                "title": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVTuner: Sensitivity-Aware Layer-wise Mixed Precision KV Cache\n  Quantization for Efficient and Nearly Lossless LLM Inference"
                },
                "summary": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache quantization can improve Large Language Models (LLMs) inference\nthroughput and latency in long contexts and large batch-size scenarios while\npreserving LLMs effectiveness. However, current methods have three unsolved\nissues: overlooking layer-wise sensitivity to KV cache quantization, high\noverhead of online fine-grained decision-making, and low flexibility to\ndifferent LLMs and constraints. Therefore, we thoroughly analyze the inherent\ncorrelation of layer-wise transformer attention patterns to KV cache\nquantization errors and study why key cache is more important than value cache\nfor quantization error reduction. We further propose a simple yet effective\nframework KVTuner to adaptively search for the optimal hardware-friendly\nlayer-wise KV quantization precision pairs for coarse-grained KV cache with\nmulti-objective optimization and directly utilize the offline searched\nconfigurations during online inference. To reduce the computational cost of\noffline calibration, we utilize the intra-layer KV precision pair pruning and\ninter-layer clustering to reduce the search space. Experimental results show\nthat we can achieve nearly lossless 3.25-bit mixed precision KV cache\nquantization for LLMs like Llama-3.1-8B-Instruct and 4.0-bit for sensitive\nmodels like Qwen2.5-7B-Instruct on mathematical reasoning tasks. The maximum\ninference throughput can be improved by 38.3% compared with KV8 quantization\nover various context lengths."
                },
                "authors": [
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zeyu Xing"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Linping Qu"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Sinno Jialin Pan"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04077v1",
                "updated": "2025-02-06T13:41:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T13:41:46Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    13,
                    41,
                    46,
                    3,
                    37,
                    0
                ],
                "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference"
                },
                "summary": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of large language models (LLMs), efficient inference\nthrough Key-Value (KV) cache compression has attracted considerable attention,\nespecially for long-context generation. To compress the KV cache, recent\nmethods identify critical KV tokens through heuristic ranking with attention\nscores. However, these methods often struggle to accurately determine critical\ntokens as they neglect the \\textit{temporal patterns} in attention scores,\nresulting in a noticeable degradation in LLM performance. To address this\nchallenge, we propose AttentionPredictor, which is the first learning-based\ncritical token identification approach. Specifically, AttentionPredictor learns\na lightweight convolution model to capture spatiotemporal patterns and predict\nthe next-token attention score. An appealing feature of AttentionPredictor is\nthat it accurately predicts the attention score while consuming negligible\nmemory. Moreover, we propose a cross-token critical cache prefetching framework\nthat hides the token estimation time overhead to accelerate the decoding stage.\nBy retaining most of the attention information, AttentionPredictor achieves\n16$\\times$ KV cache compression with comparable LLM performance, significantly\noutperforming the state-of-the-art."
                },
                "authors": [
                    {
                        "name": "Qingyue Yang"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhihai Wang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Jianye Hao"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06893v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06893v4",
                "updated": "2025-02-06T12:32:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    32,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2023-12-11T23:34:23Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    23,
                    34,
                    23,
                    0,
                    345,
                    0
                ],
                "title": "Styx: Transactional Stateful Functions on Streaming Dataflows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Styx: Transactional Stateful Functions on Streaming Dataflows"
                },
                "summary": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing stateful cloud applications, such as low-latency workflows and\nmicroservices with strict consistency requirements, remains arduous for\nprogrammers. The Stateful Functions-as-a-Service (SFaaS) paradigm aims to serve\nthese use cases. However, existing approaches provide weak transactional\nguarantees or perform expensive external state accesses requiring inefficient\ntransactional protocols that increase execution latency.\n  In this paper, we present Styx, a novel dataflow-based SFaaS runtime that\nexecutes serializable transactions consisting of stateful functions that form\narbitrary call-graphs with exactly-once guarantees. Styx extends a\ndeterministic transactional protocol by contributing: i) a function\nacknowledgment scheme to determine transaction boundaries required in SFaaS\nworkloads, ii) a function-execution caching mechanism, and iii) an early-commit\nreply mechanism that substantially reduces transaction execution latency.\nExperiments with the YCSB, TPC-C, and Deathstar benchmarks show that Styx\noutperforms state-of-the-art approaches by achieving at least one order of\nmagnitude higher throughput while exhibiting near-linear scalability and low\nlatency."
                },
                "authors": [
                    {
                        "name": "Kyriakos Psarakis"
                    },
                    {
                        "name": "George Christodoulou"
                    },
                    {
                        "name": "George Siachamis"
                    },
                    {
                        "name": "Marios Fragkoulis"
                    },
                    {
                        "name": "Asterios Katsifodimos"
                    }
                ],
                "author_detail": {
                    "name": "Asterios Katsifodimos"
                },
                "author": "Asterios Katsifodimos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06893v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06893v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v1",
                "updated": "2025-02-06T12:19:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keon Vin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01449v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01449v2",
                "updated": "2025-02-06T08:36:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    8,
                    36,
                    44,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-03T15:38:53Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    38,
                    53,
                    0,
                    34,
                    0
                ],
                "title": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlaceIT: Placement-based Inter-Chiplet Interconnect Topologies"
                },
                "summary": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5D integration technology is gaining traction as it copes with the\nexponentially growing design cost of modern integrated circuits. A crucial part\nof a 2.5D stacked chip is a low-latency and high-throughput inter-chiplet\ninterconnect (ICI). Two major factors affecting the latency and throughput are\nthe topology of links between chiplets and the chiplet placement. In this work,\nwe present PlaceIT, a novel methodology to jointly optimize the ICI topology\nand the chiplet placement. While state-of-the-art methods optimize the chiplet\nplacement for a predetermined ICI topology, or they select one topology out of\na set of candidates, we generate a completely new topology for each placement.\nOur process of inferring placement-based ICI topologies connects chiplets that\nare in close proximity to each other, making it particularly attractive for\nchips with silicon bridges or passive silicon interposers with severely limited\nlink lengths. We provide an open-source implementation of our method that\noptimizes the placement of homogeneously or heterogeneously shaped chiplets and\nthe ICI topology connecting them for a user-defined mix of four different\ntraffic types. We evaluate our methodology using synthetic traffic and traces,\nand we compare our results to a 2D mesh baseline. PlaceIT reduces the latency\nof synthetic L1-to-L2 and L2-to-memory traffic, the two most important types\nfor cache coherency traffic, by up to 28% and 62%, respectively. It also\nachieve an average packet latency reduction of up to 18% on traffic traces.\nPlaceIT enables the construction of 2.5D stacked chips with low-latency ICIs."
                },
                "authors": [
                    {
                        "name": "Patrick Iff"
                    },
                    {
                        "name": "Benigna Bruggmann"
                    },
                    {
                        "name": "Maciej Besta"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01449v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01449v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03805v1",
                "updated": "2025-02-06T06:31:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T06:31:47Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    6,
                    31,
                    47,
                    3,
                    37,
                    0
                ],
                "title": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identify Critical KV Cache in LLM Inference from an Output Perturbation\n  Perspective"
                },
                "summary": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized natural language processing but\nface significant challenges of high storage and runtime costs, due to the\ntransformer architecture's reliance on self-attention, particularly the large\nKey-Value (KV) cache for long-sequence inference. Recent efforts to reduce KV\ncache size by pruning less critical entries based on attention weights remain\nempirical and lack formal grounding. This paper presents a formal study on\nidentifying critical KV cache entries by analyzing attention output\nperturbation. Our analysis reveals that, beyond attention weights, the value\nstates within KV entries and pretrained parameter matrices are also crucial.\nBased on this, we propose a perturbation-constrained selection algorithm that\noptimizes the worst-case output perturbation to identify critical entries.\nEvaluations on the Needle-in-a-Haystack test and Longbench benchmark show our\nalgorithm enhances state-of-the-art cache eviction methods. Further empirical\nanalysis confirms that our algorithm achieves lower output perturbations in\nover 92% attention heads in Llama model, thereby providing a significant\nimprovement over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S Kevin Zhou"
                },
                "author": "S Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v1",
                "updated": "2025-02-06T04:16:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different prompts. We\npropose VectorQ, a framework to learn embedding-specific threshold regions that\nadapt to the complexity and uncertainty of an embedding. Through evaluations on\na combination of four diverse datasets, we show that VectorQ consistently\noutperforms state-of-the-art systems across all static thresholds, achieving up\nto 12x increases in cache hit rate and error rate reductions up to 92%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04393v1",
                "updated": "2025-02-06T03:56:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    56,
                    11,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-06T03:56:11Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    56,
                    11,
                    3,
                    37,
                    0
                ],
                "title": "UniCP: A Unified Caching and Pruning Framework for Efficient Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCP: A Unified Caching and Pruning Framework for Efficient Video\n  Generation"
                },
                "summary": "Diffusion Transformers (DiT) excel in video generation but encounter\nsignificant computational challenges due to the quadratic complexity of\nattention. Notably, attention differences between adjacent diffusion steps\nfollow a U-shaped pattern. Current methods leverage this property by caching\nattention blocks, however, they still struggle with sudden error spikes and\nlarge discrepancies. To address these issues, we propose UniCP a unified\ncaching and pruning framework for efficient video generation. UniCP optimizes\nboth temporal and spatial dimensions through. Error Aware Dynamic Cache Window\n(EDCW): Dynamically adjusts cache window sizes for different blocks at various\ntimesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and\nDynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS\nintegrates caching and pruning by enabling dynamic switching between pruned and\ncached outputs. By adjusting cache windows and pruning redundant components,\nUniCP enhances computational efficiency and maintains video detail fidelity.\nExperimental results show that UniCP outperforms existing methods in both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) excel in video generation but encounter\nsignificant computational challenges due to the quadratic complexity of\nattention. Notably, attention differences between adjacent diffusion steps\nfollow a U-shaped pattern. Current methods leverage this property by caching\nattention blocks, however, they still struggle with sudden error spikes and\nlarge discrepancies. To address these issues, we propose UniCP a unified\ncaching and pruning framework for efficient video generation. UniCP optimizes\nboth temporal and spatial dimensions through. Error Aware Dynamic Cache Window\n(EDCW): Dynamically adjusts cache window sizes for different blocks at various\ntimesteps, adapting to abrupt error changes. PCA based Slicing (PCAS) and\nDynamic Weight Shift (DWS): PCAS prunes redundant attention components, and DWS\nintegrates caching and pruning by enabling dynamic switching between pruned and\ncached outputs. By adjusting cache windows and pruning redundant components,\nUniCP enhances computational efficiency and maintains video detail fidelity.\nExperimental results show that UniCP outperforms existing methods in both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Wenzhang Sun"
                    },
                    {
                        "name": "Qirui Hou"
                    },
                    {
                        "name": "Donglin Di"
                    },
                    {
                        "name": "Jiahui Yang"
                    },
                    {
                        "name": "Yongjia Ma"
                    },
                    {
                        "name": "Jianxun Cui"
                    }
                ],
                "author_detail": {
                    "name": "Jianxun Cui"
                },
                "author": "Jianxun Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02770v2",
                "updated": "2025-02-06T03:16:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    6,
                    3,
                    16,
                    0,
                    3,
                    37,
                    0
                ],
                "published": "2025-02-04T23:26:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    23,
                    26,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twilight: Adaptive Attention Sparsity with Hierarchical Top-$p$ Pruning"
                },
                "summary": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging attention sparsity to accelerate long-context large language\nmodels (LLMs) has been a hot research topic. However, current algorithms such\nas sparse attention or key-value (KV) cache compression tend to use a fixed\nbudget, which presents a significant challenge during deployment because it\nfails to account for the dynamic nature of real-world scenarios, where the\noptimal balance between accuracy and efficiency can vary greatly. In this\npaper, we find that borrowing top-$p$ sampling (nucleus sampling) to sparse\nattention can surprisingly achieve adaptive budgeting. Based on this, we\npropose Twilight, a framework to bring adaptive sparsity to any existing sparse\nattention algorithm without sacrificing their accuracy. Empirical results show\nthat Twilight can adaptively prune at most 98% of redundant tokens, leading to\n$15.4\\times$ acceleration in self-attention operations and $3.9\\times$\nacceleration in end-to-end per token latency in long context LLM decoding."
                },
                "authors": [
                    {
                        "name": "Chaofan Lin"
                    },
                    {
                        "name": "Jiaming Tang"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Hanshuo Wang"
                    },
                    {
                        "name": "Tian Tang"
                    },
                    {
                        "name": "Boyu Tian"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v2",
                "updated": "2025-02-05T22:55:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    22,
                    55,
                    47,
                    2,
                    36,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently Serving Large Multimodal Models Using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step negatively impacting key Service Level Objectives (SLOs)\nlike time to first token (TTFT) and end-to-end throughput (E2ETP). We introduce\nEncode-Prefill-Decode (EPD) Disaggregation, a novel framework that separates\nthe encoding, prefill, and decode stages onto dedicated resources. Unlike\ncurrent systems, which bundle encoding and prefill together, our approach\ndecouple these steps unlocking new opportunities and optimizations. These\ninclude a new mechanism to cache multimedia tokens for efficient transfer, a\nnovel way to parallelize encoding load within a request, a module to find the\noptimal resource allocation for disaggregated serving, and a novel role\nswitching method to handle changing workload characteristics. Experimental\nevaluations with popular LMMs show substantial gains in memory efficiency (up\nto 15$\\times$ less utilization), batch sizes (up to 22$\\times$ larger),\n10$\\times$ more images/request, and 2.2$\\times$ larger KV caches. Further, it\nleads to significant improvements in latency metrics (TTFT up to 71\\%\nreduction) and end-to-end throughput (up to 57\\% reduction), compared to\nsystems that do not disaggregate."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Yifan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v2",
                "updated": "2025-02-05T21:44:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    21,
                    44,
                    56,
                    2,
                    36,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "ZACK: Zero-Overhead LLM Inference Acceleration via Dimensionality\n  Compression of the Key-Value Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZACK: Zero-Overhead LLM Inference Acceleration via Dimensionality\n  Compression of the Key-Value Cache"
                },
                "summary": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose ZACK, the first KV\ndimensionality compression system that achieves zero-overhead compression and\ndecompression and also reduces attention computation time. It complements and\ncan be combined with eviction-based and quantization-based methods to further\nenhance KV compression. Moreover, ZACK employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, ZACK enhances the self-attention kernel to balance\nthe uneven workloads caused by the adaptive compression approach to further\nreduce attention computation latency. Comprehensive experiments demonstrate\nthat when combined with ZACK, state-of-the-art eviction-based and\nquantization-based methods for KV compression further reduce KV size by up to\n68%, Time-To-First-Token (TTFT) by up to 44%, and Time-Between-Tokens (TBT) by\nup to 55% and achieve up to 1.72X throughput under the same latency, while\nmaintaining 99% of the baseline accuracy. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the Key-Value Cache (KVC)\npose a challenge during inference. In this work, we propose ZACK, the first KV\ndimensionality compression system that achieves zero-overhead compression and\ndecompression and also reduces attention computation time. It complements and\ncan be combined with eviction-based and quantization-based methods to further\nenhance KV compression. Moreover, ZACK employs adaptive compression, tailoring\nKV compression rates across heads and layers based on their contributions to\ninference to maximize overall compression while maintaining an accuracy loss\nconstraint. Additionally, ZACK enhances the self-attention kernel to balance\nthe uneven workloads caused by the adaptive compression approach to further\nreduce attention computation latency. Comprehensive experiments demonstrate\nthat when combined with ZACK, state-of-the-art eviction-based and\nquantization-based methods for KV compression further reduce KV size by up to\n68%, Time-To-First-Token (TTFT) by up to 44%, and Time-Between-Tokens (TBT) by\nup to 55% and achieve up to 1.72X throughput under the same latency, while\nmaintaining 99% of the baseline accuracy. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.10424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.10424v1",
                "updated": "2025-02-05T20:43:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    43,
                    48,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T20:43:48Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    43,
                    48,
                    2,
                    36,
                    0
                ],
                "title": "QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV\n  Cache"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed on edge devices\nfor long-context settings, creating a growing need for fast and efficient\nlong-context inference. In these scenarios, the Key-Value (KV) cache is the\nprimary bottleneck in terms of both GPU memory and latency, as the full KV\ncache must be loaded for each decoding step. While speculative decoding is a\nwidely accepted technique to accelerate autoregressive decoding, existing\nmethods often struggle to achieve significant speedups due to inefficient KV\ncache optimization strategies and result in low acceptance rates. To address\nthese challenges, we propose a novel self-speculative decoding framework,\nQuantSpec, where the draft model shares the architecture of the target model\nbut employs a hierarchical 4-bit quantized KV cache and 4-bit quantized weights\nfor acceleration. QuantSpec maintains high acceptance rates ($>$90%) and\nreliably provides consistent end-to-end speedups upto $\\sim2.5\\times$,\noutperforming other self-speculative decoding methods that use sparse KV cache\nfor long-context LLM inference. QuantSpec also reduces the memory requirements\nby $\\sim 1.3\\times$ compared to these alternatives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed on edge devices\nfor long-context settings, creating a growing need for fast and efficient\nlong-context inference. In these scenarios, the Key-Value (KV) cache is the\nprimary bottleneck in terms of both GPU memory and latency, as the full KV\ncache must be loaded for each decoding step. While speculative decoding is a\nwidely accepted technique to accelerate autoregressive decoding, existing\nmethods often struggle to achieve significant speedups due to inefficient KV\ncache optimization strategies and result in low acceptance rates. To address\nthese challenges, we propose a novel self-speculative decoding framework,\nQuantSpec, where the draft model shares the architecture of the target model\nbut employs a hierarchical 4-bit quantized KV cache and 4-bit quantized weights\nfor acceleration. QuantSpec maintains high acceptance rates ($>$90%) and\nreliably provides consistent end-to-end speedups upto $\\sim2.5\\times$,\noutperforming other self-speculative decoding methods that use sparse KV cache\nfor long-context LLM inference. QuantSpec also reduces the memory requirements\nby $\\sim 1.3\\times$ compared to these alternatives."
                },
                "authors": [
                    {
                        "name": "Rishabh Tiwari"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Aditya Tomar"
                    },
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Maxwell Horton"
                    },
                    {
                        "name": "Mahyar Najibi"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.10424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.10424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03589v1",
                "updated": "2025-02-05T20:09:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    9,
                    51,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T20:09:51Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    20,
                    9,
                    51,
                    2,
                    36,
                    0
                ],
                "title": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache\n  for Disaggregated LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HACK: Homomorphic Acceleration via Compression of the Key-Value Cache\n  for Disaggregated LLM Inference"
                },
                "summary": "Disaggregated Large Language Model (LLM) inference has gained popularity as\nit separates the computation-intensive prefill stage from the memory-intensive\ndecode stage, avoiding the prefill-decode interference and improving resource\nutilization. However, transmitting Key-Value (KV) data between the two stages\ncan be a bottleneck, especially for long prompts. Additionally, the computation\ntime overhead for prefill and decode is key for optimizing Job Completion Time\n(JCT), and KV data size can become prohibitive for long prompts and sequences.\nExisting KV quantization methods can alleviate the transmission bottleneck and\nreduce memory requirements, but they introduce significant dequantization\noverhead, exacerbating the computation time.\n  We propose Homomorphic Acceleration via Compression of the KV cache (HACK)\nfor disaggregated LLM inference. HACK eliminates the heavy KV dequantization\nstep, and directly performs computations on quantized KV data to approximate\nand reduce the cost of the expensive matrix-multiplication step. Extensive\ntrace-driven experiments show that HACK reduces JCT by up to 70.9% compared to\ndisaggregated LLM inference baseline and by up to 52.3% compared to\nstate-of-the-art KV quantization methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated Large Language Model (LLM) inference has gained popularity as\nit separates the computation-intensive prefill stage from the memory-intensive\ndecode stage, avoiding the prefill-decode interference and improving resource\nutilization. However, transmitting Key-Value (KV) data between the two stages\ncan be a bottleneck, especially for long prompts. Additionally, the computation\ntime overhead for prefill and decode is key for optimizing Job Completion Time\n(JCT), and KV data size can become prohibitive for long prompts and sequences.\nExisting KV quantization methods can alleviate the transmission bottleneck and\nreduce memory requirements, but they introduce significant dequantization\noverhead, exacerbating the computation time.\n  We propose Homomorphic Acceleration via Compression of the KV cache (HACK)\nfor disaggregated LLM inference. HACK eliminates the heavy KV dequantization\nstep, and directly performs computations on quantized KV data to approximate\nand reduce the cost of the expensive matrix-multiplication step. Extensive\ntrace-driven experiments show that HACK reduces JCT by up to 70.9% compared to\ndisaggregated LLM inference baseline and by up to 52.3% compared to\nstate-of-the-art KV quantization methods."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    },
                    {
                        "name": "Shay Vargaftik"
                    },
                    {
                        "name": "Ran Ben Basat"
                    },
                    {
                        "name": "Michael Mitzenmacher"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v2",
                "updated": "2025-02-05T09:35:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    9,
                    35,
                    38,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14442v2",
                "updated": "2025-02-05T08:22:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    22,
                    5,
                    2,
                    36,
                    0
                ],
                "published": "2024-10-18T13:01:14Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    13,
                    1,
                    14,
                    4,
                    292,
                    0
                ],
                "title": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference"
                },
                "summary": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, sharing key-value (KV) cache across layers has been found effective\nin efficient inference of large language models (LLMs). To systematically\ninvestigate different techniques of cross-layer KV sharing, we propose a\nunified framework that covers several recent methods and their novel variants.\nWe conduct comprehensive experiments on all the configurations of the\nframework, evaluating their generation throughput and performance in language\nmodeling and downstream tasks. We find that when reducing the size of the KV\ncache by 2$\\times$, most configurations can achieve higher throughput than\nstandard transformers while maintaining competitive performance. When further\nreducing the size of the KV cache, however, pairing queries of all layers with\nKVs of upper layers performs better, at the expense of additional training cost\nand prefilling latency. We hope that this work will help users make more\ninformed choices of cross-layer KV sharing approaches and facilitate future\nresearch on efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "You Wu"
                    },
                    {
                        "name": "Haoyi Wu"
                    },
                    {
                        "name": "Kewei Tu"
                    }
                ],
                "author_detail": {
                    "name": "Kewei Tu"
                },
                "author": "Kewei Tu",
                "arxiv_comment": "Accepted to NAACL2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v2",
                "updated": "2025-02-05T08:10:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    8,
                    10,
                    45,
                    2,
                    36,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and Effortless 4-bit LLM Quantization by Significant\n  Data Razoring"
                },
                "summary": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) excel in language processing tasks but\nface deployment challenges due to high memory and computational demands. While\nlow-bit quantization, such as 4-bit techniques, offers a potential solution,\nthese methods often suffer from significant accuracy loss or require\nconsiderable effort for implementation such as reordering, rotation, etc. To\naddress these challenges, we propose QRazor, a simple yet effective\nquantization scheme that enables 4-bit quantization of weights, activations,\nand KV cache in transformer-based LLMs. QRazor operates in two stages: first,\nquantizing data using 8 or 16-bit integers as a basis with absolute max scaling\nto preserve accuracy close to full-precision models, and second, compressing\nthe quantized data to 4-bit using our significant data razoring (SDR)\ntechnique, which retains only the four most salient bits. Without any\nadditional requirment of fine-tuning or additional training, QRazor achieves\nperformance similar or better compared to state-of-the-art in 4-bit\nquantization method, surpassing Smoothquant and QLLM by over 12 points and\nQuarot(RTN) by more than 2.9 points in zero-shot reasoning task accuracy on the\nLLaMA2-7B model. Additionally, we introduce an integer-based arithmetic unit\noptimized for QRazor, allowing direct low-precision operations on SDR data\nwithout decompression."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02818v1",
                "updated": "2025-02-05T01:36:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "published": "2025-02-05T01:36:40Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    1,
                    36,
                    40,
                    2,
                    36,
                    0
                ],
                "title": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessible and Portable LLM Inference by Compiling Computational Graphs\n  into SQL"
                },
                "summary": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) often demands specialized hardware,\ndedicated frameworks, and substantial development efforts, which restrict their\naccessibility, especially for edge devices and organizations with limited\ntechnical resources. We propose a novel compiler that translates LLM inference\ngraphs into SQL queries, enabling relational databases, one of the most widely\nused and mature software systems globally, to serve as the runtime. By mapping\nneural operators such as matrix multiplication and attention into relational\nprimitives like joins and aggregations, our approach leverages database\ncapabilities, including disk-based data management and native caching.\nSupporting key transformer components, such as attention mechanisms and\nkey-value caching, our system generates SQL pipelines for end-to-end LLM\ninference. Using the Llama3 family as a case study, we demonstrate up to 30x\nspeedup in token generation for memory-constrained scenarios comparable to\ncompetitive CPU-based frameworks. Our work offers an accessible, portable, and\nefficient solution, facilitating the serving of LLMs across diverse deployment\nenvironments."
                },
                "authors": [
                    {
                        "name": "Wenbo Sun"
                    },
                    {
                        "name": "Qiming Guo"
                    },
                    {
                        "name": "Wenlu Wang"
                    },
                    {
                        "name": "Rihan Hai"
                    }
                ],
                "author_detail": {
                    "name": "Rihan Hai"
                },
                "author": "Rihan Hai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02750v1",
                "updated": "2025-02-04T22:37:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T22:37:17Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    22,
                    37,
                    17,
                    1,
                    35,
                    0
                ],
                "title": "Cache is King: Smart Page Eviction with eBPF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache is King: Smart Page Eviction with eBPF"
                },
                "summary": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The page cache is a central part of an OS. It reduces repeated accesses to\nstorage by deciding which pages to retain in memory. As a result, the page\ncache has a significant impact on the performance of many applications.\nHowever, its one-size-fits-all eviction policy performs poorly in many\nworkloads. While the systems community has experimented with a plethora of new\nand adaptive eviction policies in non-OS settings (e.g., key-value stores,\nCDNs), it is very difficult to implement such policies in the page cache, due\nto the complexity of modifying kernel code. To address these shortcomings, we\ndesign a novel eBPF-based framework for the Linux page cache, called\n$\\texttt{cachebpf}$, that allows developers to customize the page cache without\nmodifying the kernel. $\\texttt{cachebpf}$ enables applications to customize the\npage cache policy for their specific needs, while also ensuring that different\napplications' policies do not interfere with each other and preserving the page\ncache's ability to share memory across different processes. We demonstrate the\nflexibility of $\\texttt{cachebpf}$'s interface by using it to implement several\neviction policies. Our evaluation shows that it is indeed beneficial for\napplications to customize the page cache to match their workloads' unique\nproperties, and that they can achieve up to 70% higher throughput and 58% lower\ntail latency."
                },
                "authors": [
                    {
                        "name": "Tal Zussman"
                    },
                    {
                        "name": "Ioannis Zarkadas"
                    },
                    {
                        "name": "Jeremy Carin"
                    },
                    {
                        "name": "Andrew Cheng"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Jonas Pfefferle"
                    },
                    {
                        "name": "Asaf Cidon"
                    }
                ],
                "author_detail": {
                    "name": "Asaf Cidon"
                },
                "author": "Asaf Cidon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02564v1",
                "updated": "2025-02-04T18:39:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "published": "2025-02-04T18:39:10Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    18,
                    39,
                    10,
                    1,
                    35,
                    0
                ],
                "title": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CReIS: Computation Reuse through Image Similarity in ICN-Based Edge\n  Computing"
                },
                "summary": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "At the edge, there is a high level of similarity in computing. One approach\nthat has been proposed to enhance the efficiency of edge computing is\ncomputation reuse, which eliminates redundant computations. Edge computing is\nintegrated with the ICN architecture, capitalizing on its inherent intelligence\nto facilitate computation reuse and reduce redundancies in computing\noperations. In many past works, ICN's ability to enable computation reuse\nthrough caching has been limited. In this context, a new approach is proposed\nthat considers computation requests with similar input data, which yield\nidentical results, as equivalent. This method facilitates computation reuse\nthrough caching in ICN. The use of approximate results to reduce redundant\ncomputations without requiring high accuracy in input matching is provided.\nThis concept is termed the Similarity Index, which effectively considers images\nto be similar despite minor changes in the angle of photography. The Similarity\nIndex is determined through an algorithm known as HNSW and utilizes the SIFT\ndescriptor to identify similar data. This approach helps reduce user latency\ntimes by providing quick access to results. The evaluation, simulated using the\nndnSIM tool, showed an 86% improvement in completion time compared to scenarios\nwithout computation reuse, whereas previous works reported only a 70%\nimprovement. To strengthen this method, an analytical model for computing\nrequest transfer considering computation reuse in ICN-based edge computing is\nprovided. To assess the accuracy of the model, several evaluations have been\nconducted in the simulator by varying the parameters, resulting in a maximum\nerror percentage of approximately 16%."
                },
                "authors": [
                    {
                        "name": "Atiyeh Javaheri"
                    },
                    {
                        "name": "Ali Bohlooli"
                    },
                    {
                        "name": "Kamal Jamshidi"
                    }
                ],
                "author_detail": {
                    "name": "Kamal Jamshidi"
                },
                "author": "Kamal Jamshidi",
                "arxiv_comment": "18 pages, 14 figures, submit to Digital Communications and Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14001v2",
                "updated": "2025-02-04T17:14:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    4,
                    17,
                    14,
                    22,
                    1,
                    35,
                    0
                ],
                "published": "2024-08-26T03:58:20Z",
                "published_parsed": [
                    2024,
                    8,
                    26,
                    3,
                    58,
                    20,
                    0,
                    239,
                    0
                ],
                "title": "Decentralized Federated Learning with Model Caching on Mobile Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Federated Learning with Model Caching on Mobile Agents"
                },
                "summary": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains a shared model using data and computation\npower on distributed agents coordinated by a central server. Decentralized FL\n(DFL) utilizes local model exchange and aggregation between agents to reduce\nthe communication and computation overheads on the central server. However,\nwhen agents are mobile, the communication opportunity between agents can be\nsporadic, largely hindering the convergence and accuracy of DFL. In this paper,\nwe propose Cached Decentralized Federated Learning (Cached-DFL) to investigate\ndelay-tolerant model spreading and aggregation enabled by model caching on\nmobile agents. Each agent stores not only its own model, but also models of\nagents encountered in the recent past. When two agents meet, they exchange\ntheir own models as well as the cached models. Local model aggregation utilizes\nall models stored in the cache. We theoretically analyze the convergence of\nCached-DFL, explicitly taking into account the model staleness introduced by\ncaching. We design and compare different model caching algorithms for different\nDFL and mobility scenarios. We conduct detailed case studies in a vehicular\nnetwork to systematically investigate the interplay between agent mobility,\ncache staleness, and model convergence. In our experiments, Cached-DFL\nconverges quickly, and significantly outperforms DFL without caching."
                },
                "authors": [
                    {
                        "name": "Xiaoyu Wang"
                    },
                    {
                        "name": "Guojun Xiong"
                    },
                    {
                        "name": "Houwei Cao"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "Oral Presentation at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.13966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13966v1",
                "updated": "2025-02-19T18:59:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    59,
                    32,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:59:32Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    59,
                    32,
                    2,
                    50,
                    0
                ],
                "title": "Where's the Bug? Attention Probing for Scalable Fault Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where's the Bug? Attention Probing for Scalable Fault Localization"
                },
                "summary": "Ensuring code correctness remains a challenging problem even as large\nlanguage models (LLMs) become increasingly capable at code-related tasks. While\nLLM-based program repair systems can propose bug fixes using only a user's bug\nreport, their effectiveness is fundamentally limited by their ability to\nperform fault localization (FL), a challenging problem for both humans and\nLLMs. Existing FL approaches rely on executable test cases, require training on\ncostly and often noisy line-level annotations, or demand resource-intensive\nLLMs. In this paper, we present Bug Attention Probe (BAP), a method which\nlearns state-of-the-art fault localization without any direct localization\nlabels, outperforming traditional FL baselines and prompting of large-scale\nLLMs. We evaluate our approach across a variety of code settings, including\nreal-world Java bugs from the standard Defects4J dataset as well as seven other\ndatasets which span a diverse set of bug types and languages. Averaged across\nall eight datasets, BAP improves by 34.6% top-1 accuracy compared to the\nstrongest baseline and 93.4% over zero-shot prompting GPT-4o. BAP is also\nsignificantly more efficient than prompting, outperforming large open-weight\nmodels at a small fraction of the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring code correctness remains a challenging problem even as large\nlanguage models (LLMs) become increasingly capable at code-related tasks. While\nLLM-based program repair systems can propose bug fixes using only a user's bug\nreport, their effectiveness is fundamentally limited by their ability to\nperform fault localization (FL), a challenging problem for both humans and\nLLMs. Existing FL approaches rely on executable test cases, require training on\ncostly and often noisy line-level annotations, or demand resource-intensive\nLLMs. In this paper, we present Bug Attention Probe (BAP), a method which\nlearns state-of-the-art fault localization without any direct localization\nlabels, outperforming traditional FL baselines and prompting of large-scale\nLLMs. We evaluate our approach across a variety of code settings, including\nreal-world Java bugs from the standard Defects4J dataset as well as seven other\ndatasets which span a diverse set of bug types and languages. Averaged across\nall eight datasets, BAP improves by 34.6% top-1 accuracy compared to the\nstrongest baseline and 93.4% over zero-shot prompting GPT-4o. BAP is also\nsignificantly more efficient than prompting, outperforming large open-weight\nmodels at a small fraction of the computational cost."
                },
                "authors": [
                    {
                        "name": "Adam Stein"
                    },
                    {
                        "name": "Arthur Wayne"
                    },
                    {
                        "name": "Aaditya Naik"
                    },
                    {
                        "name": "Mayur Naik"
                    },
                    {
                        "name": "Eric Wong"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wong"
                },
                "author": "Eric Wong",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13965v1",
                "updated": "2025-02-19T18:59:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    59,
                    30,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:59:30Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    59,
                    30,
                    2,
                    50,
                    0
                ],
                "title": "Autellix: An Efficient Serving Engine for LLM Agents as General Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autellix: An Efficient Serving Engine for LLM Agents as General Programs"
                },
                "summary": "Large language model (LLM) applications are evolving beyond simple chatbots\ninto dynamic, general-purpose agentic programs, which scale LLM calls and\noutput tokens to help AI agents reason, explore, and solve complex tasks.\nHowever, existing LLM serving systems ignore dependencies between programs and\ncalls, missing significant opportunities for optimization. Our analysis reveals\nthat programs submitted to LLM serving engines experience long cumulative wait\ntimes, primarily due to head-of-line blocking at both the individual LLM\nrequest and the program. To address this, we introduce Autellix, an LLM serving\nsystem that treats programs as first-class citizens to minimize their\nend-to-end latencies. Autellix intercepts LLM calls submitted by programs,\nenriching schedulers with program-level context. We propose two scheduling\nalgorithms-for single-threaded and distributed programs-that preempt and\nprioritize LLM calls based on their programs' previously completed calls. Our\nevaluation demonstrates that across diverse LLMs and agentic workloads,\nAutellix improves throughput of programs by 4-15x at the same latency compared\nto state-of-the-art systems, such as vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) applications are evolving beyond simple chatbots\ninto dynamic, general-purpose agentic programs, which scale LLM calls and\noutput tokens to help AI agents reason, explore, and solve complex tasks.\nHowever, existing LLM serving systems ignore dependencies between programs and\ncalls, missing significant opportunities for optimization. Our analysis reveals\nthat programs submitted to LLM serving engines experience long cumulative wait\ntimes, primarily due to head-of-line blocking at both the individual LLM\nrequest and the program. To address this, we introduce Autellix, an LLM serving\nsystem that treats programs as first-class citizens to minimize their\nend-to-end latencies. Autellix intercepts LLM calls submitted by programs,\nenriching schedulers with program-level context. We propose two scheduling\nalgorithms-for single-threaded and distributed programs-that preempt and\nprioritize LLM calls based on their programs' previously completed calls. Our\nevaluation demonstrates that across diverse LLMs and agentic workloads,\nAutellix improves throughput of programs by 4-15x at the same latency compared\nto state-of-the-art systems, such as vLLM."
                },
                "authors": [
                    {
                        "name": "Michael Luo"
                    },
                    {
                        "name": "Xiaoxiang Shi"
                    },
                    {
                        "name": "Colin Cai"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Justin Wong"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Chi Wang"
                    },
                    {
                        "name": "Yanping Huang"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13963v1",
                "updated": "2025-02-19T18:59:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    59,
                    15,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:59:15Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    59,
                    15,
                    2,
                    50,
                    0
                ],
                "title": "MuDAF: Long-Context Multi-Document Attention Focusing through\n  Contrastive Learning on Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuDAF: Long-Context Multi-Document Attention Focusing through\n  Contrastive Learning on Attention Heads"
                },
                "summary": "Large Language Models (LLMs) frequently show distracted attention due to\nirrelevant information in the input, which severely impairs their long-context\ncapabilities. Inspired by recent studies on the effectiveness of retrieval\nheads in long-context factutality, we aim at addressing this distraction issue\nthrough improving such retrieval heads directly. We propose Multi-Document\nAttention Focusing (MuDAF), a novel method that explicitly optimizes the\nattention distribution at the head level through contrastive learning.\nAccording to the experimental results, MuDAF can significantly improve the\nlong-context question answering performance of LLMs, especially in\nmulti-document question answering. Extensive evaluations on retrieval scores\nand attention visualizations show that MuDAF possesses great potential in\nmaking attention heads more focused on relevant information and reducing\nattention distractions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) frequently show distracted attention due to\nirrelevant information in the input, which severely impairs their long-context\ncapabilities. Inspired by recent studies on the effectiveness of retrieval\nheads in long-context factutality, we aim at addressing this distraction issue\nthrough improving such retrieval heads directly. We propose Multi-Document\nAttention Focusing (MuDAF), a novel method that explicitly optimizes the\nattention distribution at the head level through contrastive learning.\nAccording to the experimental results, MuDAF can significantly improve the\nlong-context question answering performance of LLMs, especially in\nmulti-document question answering. Extensive evaluations on retrieval scores\nand attention visualizations show that MuDAF possesses great potential in\nmaking attention heads more focused on relevant information and reducing\nattention distractions."
                },
                "authors": [
                    {
                        "name": "Weihao Liu"
                    },
                    {
                        "name": "Ning Wu"
                    },
                    {
                        "name": "Shiping Yang"
                    },
                    {
                        "name": "Wenbiao Ding"
                    },
                    {
                        "name": "Shining Liang"
                    },
                    {
                        "name": "Ming Gong"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13962v1",
                "updated": "2025-02-19T18:58:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    58,
                    31,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:58:31Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    58,
                    31,
                    2,
                    50,
                    0
                ],
                "title": "Is That Your Final Answer? Test-Time Scaling Improves Selective Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is That Your Final Answer? Test-Time Scaling Improves Selective Question\n  Answering"
                },
                "summary": "Scaling the test-time compute of large language models has demonstrated\nimpressive performance on reasoning benchmarks. However, existing evaluations\nof test-time scaling make the strong assumption that a reasoning system should\nalways give an answer to any question provided. This overlooks concerns about\nwhether a model is confident in its answer, and whether it is appropriate to\nalways provide a response. To address these concerns, we extract confidence\nscores during reasoning for thresholding model responses. We find that\nincreasing compute budget at inference time not only helps models answer more\nquestions correctly, but also increases confidence in correct responses. We\nthen extend the current paradigm of zero-risk responses during evaluation by\nconsidering settings with non-zero levels of response risk, and suggest a\nrecipe for reporting evaluations under these settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the test-time compute of large language models has demonstrated\nimpressive performance on reasoning benchmarks. However, existing evaluations\nof test-time scaling make the strong assumption that a reasoning system should\nalways give an answer to any question provided. This overlooks concerns about\nwhether a model is confident in its answer, and whether it is appropriate to\nalways provide a response. To address these concerns, we extract confidence\nscores during reasoning for thresholding model responses. We find that\nincreasing compute budget at inference time not only helps models answer more\nquestions correctly, but also increases confidence in correct responses. We\nthen extend the current paradigm of zero-risk responses during evaluation by\nconsidering settings with non-zero levels of response risk, and suggest a\nrecipe for reporting evaluations under these settings."
                },
                "authors": [
                    {
                        "name": "William Jurayj"
                    },
                    {
                        "name": "Jeffrey Cheng"
                    },
                    {
                        "name": "Benjamin Van Durme"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Van Durme"
                },
                "author": "Benjamin Van Durme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13957v1",
                "updated": "2025-02-19T18:56:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    56,
                    3,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:56:03Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    56,
                    3,
                    2,
                    50,
                    0
                ],
                "title": "RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision"
                },
                "summary": "Retrieval-augmented generation (RAG) has shown great potential for\nknowledge-intensive tasks, but its traditional architectures rely on static\nretrieval, limiting their effectiveness for complex questions that require\nsequential information-seeking. While agentic reasoning and search offer a more\nadaptive approach, most existing methods depend heavily on prompt engineering.\nIn this work, we introduce RAG-Gym, a unified optimization framework that\nenhances information-seeking agents through fine-grained process supervision at\neach search step. We also propose ReSearch, a novel agent architecture that\nsynergizes answer reasoning and search query generation within the RAG-Gym\nframework. Experiments on four challenging datasets show that RAG-Gym improves\nperformance by up to 25.6\\% across various agent architectures, with ReSearch\nconsistently outperforming existing baselines. Further analysis highlights the\neffectiveness of advanced LLMs as process reward judges and the transferability\nof trained reward models as verifiers for different LLMs. Additionally, we\nexamine the scaling properties of training and inference in agentic RAG. The\nproject homepage is available at https://rag-gym.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has shown great potential for\nknowledge-intensive tasks, but its traditional architectures rely on static\nretrieval, limiting their effectiveness for complex questions that require\nsequential information-seeking. While agentic reasoning and search offer a more\nadaptive approach, most existing methods depend heavily on prompt engineering.\nIn this work, we introduce RAG-Gym, a unified optimization framework that\nenhances information-seeking agents through fine-grained process supervision at\neach search step. We also propose ReSearch, a novel agent architecture that\nsynergizes answer reasoning and search query generation within the RAG-Gym\nframework. Experiments on four challenging datasets show that RAG-Gym improves\nperformance by up to 25.6\\% across various agent architectures, with ReSearch\nconsistently outperforming existing baselines. Further analysis highlights the\neffectiveness of advanced LLMs as process reward judges and the transferability\nof trained reward models as verifiers for different LLMs. Additionally, we\nexamine the scaling properties of training and inference in agentic RAG. The\nproject homepage is available at https://rag-gym.github.io/."
                },
                "authors": [
                    {
                        "name": "Guangzhi Xiong"
                    },
                    {
                        "name": "Qiao Jin"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Yin Fang"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Fangyuan Chen"
                    },
                    {
                        "name": "Zhixing Song"
                    },
                    {
                        "name": "Dengyu Wang"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Zhiyong Lu"
                    },
                    {
                        "name": "Aidong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Aidong Zhang"
                },
                "author": "Aidong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13953v1",
                "updated": "2025-02-19T18:53:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    53,
                    16,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:53:16Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    53,
                    16,
                    2,
                    50,
                    0
                ],
                "title": "Neurosymbolic artificial intelligence via large language models and\n  coherence-driven inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neurosymbolic artificial intelligence via large language models and\n  coherence-driven inference"
                },
                "summary": "We devise an algorithm to generate sets of propositions that objectively\ninstantiate graphs that support coherence-driven inference. We then benchmark\nthe ability of large language models (LLMs) to reconstruct coherence graphs\nfrom (a straightforward transformation of) propositions expressed in natural\nlanguage, with promising results from a single prompt to models optimized for\nreasoning. Combining coherence-driven inference with consistency evaluations by\nneural models may advance the state of the art in machine cognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We devise an algorithm to generate sets of propositions that objectively\ninstantiate graphs that support coherence-driven inference. We then benchmark\nthe ability of large language models (LLMs) to reconstruct coherence graphs\nfrom (a straightforward transformation of) propositions expressed in natural\nlanguage, with promising results from a single prompt to models optimized for\nreasoning. Combining coherence-driven inference with consistency evaluations by\nneural models may advance the state of the art in machine cognition."
                },
                "authors": [
                    {
                        "name": "Steve Huntsman"
                    },
                    {
                        "name": "Jewell Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Jewell Thomas"
                },
                "author": "Jewell Thomas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13950v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13950v1",
                "updated": "2025-02-19T18:48:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    48,
                    26,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:48:26Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    48,
                    26,
                    2,
                    50,
                    0
                ],
                "title": "Asteroseismology with TESS: Emergence of Dipole Mode Suppression From\n  Subgiants?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asteroseismology with TESS: Emergence of Dipole Mode Suppression From\n  Subgiants?"
                },
                "summary": "Dipole mode suppression is an observed behavior of solar-like oscillations in\nevolved stars. This study aims to search for depressed dipole modes in giant\nstars using data from the Transiting Exoplanet Survey Satellite (TESS) and\ninvestigate when the suppression starts to emerge. We study a sample of 8,651\nTESS-evolved stars and find 179 stars with significant dipole mode depression\nby comparing the oscillation amplitudes of radial and dipole modes. Notably, 11\nof them are located near the base of the red-giant branch, indicating that mode\nsuppression appears earlier than the point inferred in previous studies with\nthe Kepler data. These findings provide new evidence for the dipole mode\nsuppression in giant stars, particularly in subgiants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dipole mode suppression is an observed behavior of solar-like oscillations in\nevolved stars. This study aims to search for depressed dipole modes in giant\nstars using data from the Transiting Exoplanet Survey Satellite (TESS) and\ninvestigate when the suppression starts to emerge. We study a sample of 8,651\nTESS-evolved stars and find 179 stars with significant dipole mode depression\nby comparing the oscillation amplitudes of radial and dipole modes. Notably, 11\nof them are located near the base of the red-giant branch, indicating that mode\nsuppression appears earlier than the point inferred in previous studies with\nthe Kepler data. These findings provide new evidence for the dipole mode\nsuppression in giant stars, particularly in subgiants."
                },
                "authors": [
                    {
                        "name": "Shurui Lin"
                    },
                    {
                        "name": "Tanda Li"
                    },
                    {
                        "name": "Shude Mao"
                    },
                    {
                        "name": "Jim Fuller"
                    }
                ],
                "author_detail": {
                    "name": "Jim Fuller"
                },
                "author": "Jim Fuller",
                "arxiv_doi": "10.3847/1538-4357/adae9e",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-4357/adae9e",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.13950v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13950v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "11 pages, 9 figures, and 2 Tables. The catalog is available online in\n  the published version of APJ",
                "arxiv_journal_ref": "ApJ 980 217 (2025)",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13946v1",
                "updated": "2025-02-19T18:42:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    42,
                    45,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:42:45Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    42,
                    45,
                    2,
                    50,
                    0
                ],
                "title": "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety\n  Mechanisms Tend to Be Anchored in The Template Region",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety\n  Mechanisms Tend to Be Anchored in The Template Region"
                },
                "summary": "The safety alignment of large language models (LLMs) remains vulnerable, as\ntheir initial behavior can be easily jailbroken by even relatively simple\nattacks. Since infilling a fixed template between the input instruction and\ninitial model output is a common practice for existing LLMs, we hypothesize\nthat this template is a key factor behind their vulnerabilities: LLMs'\nsafety-related decision-making overly relies on the aggregated information from\nthe template region, which largely influences these models' safety behavior. We\nrefer to this issue as template-anchored safety alignment. In this paper, we\nconduct extensive experiments and verify that template-anchored safety\nalignment is widespread across various aligned LLMs. Our mechanistic analyses\ndemonstrate how it leads to models' susceptibility when encountering\ninference-time jailbreak attacks. Furthermore, we show that detaching safety\nmechanisms from the template region is promising in mitigating vulnerabilities\nto jailbreak attacks. We encourage future research to develop more robust\nsafety alignment techniques that reduce reliance on the template region.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The safety alignment of large language models (LLMs) remains vulnerable, as\ntheir initial behavior can be easily jailbroken by even relatively simple\nattacks. Since infilling a fixed template between the input instruction and\ninitial model output is a common practice for existing LLMs, we hypothesize\nthat this template is a key factor behind their vulnerabilities: LLMs'\nsafety-related decision-making overly relies on the aggregated information from\nthe template region, which largely influences these models' safety behavior. We\nrefer to this issue as template-anchored safety alignment. In this paper, we\nconduct extensive experiments and verify that template-anchored safety\nalignment is widespread across various aligned LLMs. Our mechanistic analyses\ndemonstrate how it leads to models' susceptibility when encountering\ninference-time jailbreak attacks. Furthermore, we show that detaching safety\nmechanisms from the template region is promising in mitigating vulnerabilities\nto jailbreak attacks. We encourage future research to develop more robust\nsafety alignment techniques that reduce reliance on the template region."
                },
                "authors": [
                    {
                        "name": "Chak Tou Leong"
                    },
                    {
                        "name": "Qingyu Yin"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03663v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03663v3",
                "updated": "2025-02-19T18:34:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    34,
                    19,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-04T17:59:41Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    17,
                    59,
                    41,
                    4,
                    278,
                    0
                ],
                "title": "Learning from Committee: Reasoning Distillation from a Mixture of\n  Teachers with Peer-Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Committee: Reasoning Distillation from a Mixture of\n  Teachers with Peer-Review"
                },
                "summary": "While reasoning capabilities typically emerge in large language models (LLMs)\nwith tens of billions of parameters, recent research focuses on improving\nsmaller open-source models through knowledge distillation (KD) from commercial\nLLMs. However, many of these studies rely solely on responses from a single LLM\nas the gold rationale, unlike the natural human learning process, which\ninvolves understanding both the correct answers and the reasons behind\nmistakes. In this paper, we introduce a novel Fault-Aware DistIllation via\nPeer-Review (FAIR) approach: 1) Instead of merely obtaining rationales from\nteachers, our method asks teachers to identify and explain the student's\nmistakes, providing customized instruction learning data. 2) We design a\nsimulated peer-review process between teacher LLMs, which selects only the\ngenerated rationales above the acceptance threshold. This reduces the chance of\nteachers guessing correctly with flawed rationale, improving instructional data\nquality. Comprehensive experiments and analysis on mathematical, commonsense,\nand logical reasoning tasks demonstrate the effectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reasoning capabilities typically emerge in large language models (LLMs)\nwith tens of billions of parameters, recent research focuses on improving\nsmaller open-source models through knowledge distillation (KD) from commercial\nLLMs. However, many of these studies rely solely on responses from a single LLM\nas the gold rationale, unlike the natural human learning process, which\ninvolves understanding both the correct answers and the reasons behind\nmistakes. In this paper, we introduce a novel Fault-Aware DistIllation via\nPeer-Review (FAIR) approach: 1) Instead of merely obtaining rationales from\nteachers, our method asks teachers to identify and explain the student's\nmistakes, providing customized instruction learning data. 2) We design a\nsimulated peer-review process between teacher LLMs, which selects only the\ngenerated rationales above the acceptance threshold. This reduces the chance of\nteachers guessing correctly with flawed rationale, improving instructional data\nquality. Comprehensive experiments and analysis on mathematical, commonsense,\nand logical reasoning tasks demonstrate the effectiveness of our method."
                },
                "authors": [
                    {
                        "name": "Zhuochun Li"
                    },
                    {
                        "name": "Yuelyu Ji"
                    },
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Daqing He"
                    }
                ],
                "author_detail": {
                    "name": "Daqing He"
                },
                "author": "Daqing He",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03663v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03663v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02890v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02890v4",
                "updated": "2025-02-19T18:18:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    18,
                    11,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-03T18:28:10Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    18,
                    28,
                    10,
                    3,
                    277,
                    0
                ],
                "title": "Theoretically Grounded Framework for LLM Watermarking: A\n  Distribution-Adaptive Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretically Grounded Framework for LLM Watermarking: A\n  Distribution-Adaptive Approach"
                },
                "summary": "Watermarking has emerged as a crucial method to distinguish AI-generated text\nfrom human-created text. In this paper, we present a novel theoretical\nframework for watermarking Large Language Models (LLMs) that jointly optimizes\nboth the watermarking scheme and the detection process. Our approach focuses on\nmaximizing detection performance while maintaining control over the worst-case\nType-I error and text distortion. We characterize \\emph{the universally minimum\nType-II error}, showing a fundamental trade-off between watermark detectability\nand text distortion. Importantly, we identify that the optimal watermarking\nschemes are adaptive to the LLM generative distribution. Building on our\ntheoretical insights, we propose an efficient, model-agnostic,\ndistribution-adaptive watermarking algorithm, utilizing a surrogate model\nalongside the Gumbel-max trick. Experiments conducted on Llama2-13B and\nMistral-8$\\times$7B models confirm the effectiveness of our approach.\nAdditionally, we examine incorporating robustness into our framework, paving a\nway to future watermarking systems that withstand adversarial attacks more\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking has emerged as a crucial method to distinguish AI-generated text\nfrom human-created text. In this paper, we present a novel theoretical\nframework for watermarking Large Language Models (LLMs) that jointly optimizes\nboth the watermarking scheme and the detection process. Our approach focuses on\nmaximizing detection performance while maintaining control over the worst-case\nType-I error and text distortion. We characterize \\emph{the universally minimum\nType-II error}, showing a fundamental trade-off between watermark detectability\nand text distortion. Importantly, we identify that the optimal watermarking\nschemes are adaptive to the LLM generative distribution. Building on our\ntheoretical insights, we propose an efficient, model-agnostic,\ndistribution-adaptive watermarking algorithm, utilizing a surrogate model\nalongside the Gumbel-max trick. Experiments conducted on Llama2-13B and\nMistral-8$\\times$7B models confirm the effectiveness of our approach.\nAdditionally, we examine incorporating robustness into our framework, paving a\nway to future watermarking systems that withstand adversarial attacks more\neffectively."
                },
                "authors": [
                    {
                        "name": "Haiyun He"
                    },
                    {
                        "name": "Yepeng Liu"
                    },
                    {
                        "name": "Ziqiao Wang"
                    },
                    {
                        "name": "Yongyi Mao"
                    },
                    {
                        "name": "Yuheng Bu"
                    }
                ],
                "author_detail": {
                    "name": "Yuheng Bu"
                },
                "author": "Yuheng Bu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02890v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02890v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13925v1",
                "updated": "2025-02-19T18:04:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    4,
                    44,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:04:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    4,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual\n  Narratives in Image Sequences?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual\n  Narratives in Image Sequences?"
                },
                "summary": "Large Multimodal Models (LMMs) have achieved remarkable success across\nvarious visual-language tasks. However, existing benchmarks predominantly focus\non single-image understanding, leaving the analysis of image sequences largely\nunexplored. To address this limitation, we introduce StripCipher, a\ncomprehensive benchmark designed to evaluate capabilities of LMMs to comprehend\nand reason over sequential images. StripCipher comprises a human-annotated\ndataset and three challenging subtasks: visual narrative comprehension,\ncontextual frame prediction, and temporal narrative reordering. Our evaluation\nof $16$ state-of-the-art LMMs, including GPT-4o and Qwen2.5VL, reveals a\nsignificant performance gap compared to human capabilities, particularly in\ntasks that require reordering shuffled sequential images. For instance, GPT-4o\nachieves only 23.93% accuracy in the reordering subtask, which is 56.07% lower\nthan human performance. Further quantitative analysis discuss several factors,\nsuch as input format of images, affecting the performance of LLMs in sequential\nunderstanding, underscoring the fundamental challenges that remain in the\ndevelopment of LMMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) have achieved remarkable success across\nvarious visual-language tasks. However, existing benchmarks predominantly focus\non single-image understanding, leaving the analysis of image sequences largely\nunexplored. To address this limitation, we introduce StripCipher, a\ncomprehensive benchmark designed to evaluate capabilities of LMMs to comprehend\nand reason over sequential images. StripCipher comprises a human-annotated\ndataset and three challenging subtasks: visual narrative comprehension,\ncontextual frame prediction, and temporal narrative reordering. Our evaluation\nof $16$ state-of-the-art LMMs, including GPT-4o and Qwen2.5VL, reveals a\nsignificant performance gap compared to human capabilities, particularly in\ntasks that require reordering shuffled sequential images. For instance, GPT-4o\nachieves only 23.93% accuracy in the reordering subtask, which is 56.07% lower\nthan human performance. Further quantitative analysis discuss several factors,\nsuch as input format of images, affecting the performance of LLMs in sequential\nunderstanding, underscoring the fundamental challenges that remain in the\ndevelopment of LMMs."
                },
                "authors": [
                    {
                        "name": "Xiaochen Wang"
                    },
                    {
                        "name": "Heming Xia"
                    },
                    {
                        "name": "Jialin Song"
                    },
                    {
                        "name": "Longyu Guan"
                    },
                    {
                        "name": "Yixin Yang"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Weiyao Luo"
                    },
                    {
                        "name": "Yifan Pu"
                    },
                    {
                        "name": "Yiru Wang"
                    },
                    {
                        "name": "Xiangdi Meng"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13924v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13924v1",
                "updated": "2025-02-19T18:01:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    1,
                    48,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:01:48Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    1,
                    48,
                    2,
                    50,
                    0
                ],
                "title": "PINN ME: A Physics-Informed Neural Network Framework for Accurate\n  Milne-Eddington Inversions of Solar Magnetic Fields",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINN ME: A Physics-Informed Neural Network Framework for Accurate\n  Milne-Eddington Inversions of Solar Magnetic Fields"
                },
                "summary": "Spectropolarimetric inversions of solar observations are fundamental for the\nestimation of the magnetic field in the solar atmosphere. However, instrumental\nnoise, computational requirements, and varying levels of physical realism make\nit challenging to derive reliable solar magnetic field estimates. In this\nstudy, we present a novel approach for spectropolarimetric inversions based on\nPhysics Informed Neural Networks (PINNs) to infer the photospheric magnetic\nfield under the Milne-Eddington approximation (PINN ME). Our model acts as a\nrepresentation of the parameter space, mapping input coordinates (t, x, y) to\nthe respective spectropolarimetric parameters, which are used to synthesize the\ncorresponding stokes profiles. By iteratively sampling coordinate points,\nsynthesizing profiles, and minimizing the deviation from the observed stokes\nprofiles, our method can find the set of Milne-Eddington parameters that best\nfit the observations. In addition, we directly include the\npoint-spread-function to account for instrumental effects. We use a predefined\nparameter space as well as synthetic profiles from a radiative MHD simulation\nto evaluate the performance of our method and to estimate the impact of\ninstrumental noise. Our results demonstrate that PINN ME achieves an intrinsic\nspatio-temporal coupling, which can largely mitigate observational noise and\nprovides a memory-efficient inversion even for extended fields-of-view.\nFinally, we apply our method to observations and show that our method provides\na high spatial coherence and can resolve small-scale features both in strong-\nand weak-field regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectropolarimetric inversions of solar observations are fundamental for the\nestimation of the magnetic field in the solar atmosphere. However, instrumental\nnoise, computational requirements, and varying levels of physical realism make\nit challenging to derive reliable solar magnetic field estimates. In this\nstudy, we present a novel approach for spectropolarimetric inversions based on\nPhysics Informed Neural Networks (PINNs) to infer the photospheric magnetic\nfield under the Milne-Eddington approximation (PINN ME). Our model acts as a\nrepresentation of the parameter space, mapping input coordinates (t, x, y) to\nthe respective spectropolarimetric parameters, which are used to synthesize the\ncorresponding stokes profiles. By iteratively sampling coordinate points,\nsynthesizing profiles, and minimizing the deviation from the observed stokes\nprofiles, our method can find the set of Milne-Eddington parameters that best\nfit the observations. In addition, we directly include the\npoint-spread-function to account for instrumental effects. We use a predefined\nparameter space as well as synthetic profiles from a radiative MHD simulation\nto evaluate the performance of our method and to estimate the impact of\ninstrumental noise. Our results demonstrate that PINN ME achieves an intrinsic\nspatio-temporal coupling, which can largely mitigate observational noise and\nprovides a memory-efficient inversion even for extended fields-of-view.\nFinally, we apply our method to observations and show that our method provides\na high spatial coherence and can resolve small-scale features both in strong-\nand weak-field regions."
                },
                "authors": [
                    {
                        "name": "Robert Jarolim"
                    },
                    {
                        "name": "Momchil E. Molnar"
                    },
                    {
                        "name": "Benoit Tremblay"
                    },
                    {
                        "name": "Rebecca Centeno"
                    },
                    {
                        "name": "Matthias Rempel"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Rempel"
                },
                "author": "Matthias Rempel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13924v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13924v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13923v1",
                "updated": "2025-02-19T18:00:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    0,
                    14,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:00:14Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    0,
                    14,
                    2,
                    50,
                    0
                ],
                "title": "Qwen2.5-VL Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qwen2.5-VL Technical Report"
                },
                "summary": "We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language\nseries, which demonstrates significant advancements in both foundational\ncapabilities and innovative functionalities. Qwen2.5-VL achieves a major leap\nforward in understanding and interacting with the world through enhanced visual\nrecognition, precise object localization, robust document parsing, and\nlong-video comprehension. A standout feature of Qwen2.5-VL is its ability to\nlocalize objects using bounding boxes or points accurately. It provides robust\nstructured data extraction from invoices, forms, and tables, as well as\ndetailed analysis of charts, diagrams, and layouts. To handle complex inputs,\nQwen2.5-VL introduces dynamic resolution processing and absolute time encoding,\nenabling it to process images of varying sizes and videos of extended durations\n(up to hours) with second-level event localization. This allows the model to\nnatively perceive spatial scales and temporal dynamics without relying on\ntraditional normalization techniques. By training a native dynamic-resolution\nVision Transformer (ViT) from scratch and incorporating Window Attention, we\nreduce computational overhead while maintaining native resolution. As a result,\nQwen2.5-VL excels not only in static image and document understanding but also\nas an interactive visual agent capable of reasoning, tool usage, and task\nexecution in real-world scenarios such as operating computers and mobile\ndevices. Qwen2.5-VL is available in three sizes, addressing diverse use cases\nfrom edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model\nmatches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly\nexcelling in document and diagram understanding. Additionally, Qwen2.5-VL\nmaintains robust linguistic performance, preserving the core language\ncompetencies of the Qwen2.5 LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language\nseries, which demonstrates significant advancements in both foundational\ncapabilities and innovative functionalities. Qwen2.5-VL achieves a major leap\nforward in understanding and interacting with the world through enhanced visual\nrecognition, precise object localization, robust document parsing, and\nlong-video comprehension. A standout feature of Qwen2.5-VL is its ability to\nlocalize objects using bounding boxes or points accurately. It provides robust\nstructured data extraction from invoices, forms, and tables, as well as\ndetailed analysis of charts, diagrams, and layouts. To handle complex inputs,\nQwen2.5-VL introduces dynamic resolution processing and absolute time encoding,\nenabling it to process images of varying sizes and videos of extended durations\n(up to hours) with second-level event localization. This allows the model to\nnatively perceive spatial scales and temporal dynamics without relying on\ntraditional normalization techniques. By training a native dynamic-resolution\nVision Transformer (ViT) from scratch and incorporating Window Attention, we\nreduce computational overhead while maintaining native resolution. As a result,\nQwen2.5-VL excels not only in static image and document understanding but also\nas an interactive visual agent capable of reasoning, tool usage, and task\nexecution in real-world scenarios such as operating computers and mobile\ndevices. Qwen2.5-VL is available in three sizes, addressing diverse use cases\nfrom edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model\nmatches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly\nexcelling in document and diagram understanding. Additionally, Qwen2.5-VL\nmaintains robust linguistic performance, preserving the core language\ncompetencies of the Qwen2.5 LLM."
                },
                "authors": [
                    {
                        "name": "Shuai Bai"
                    },
                    {
                        "name": "Keqin Chen"
                    },
                    {
                        "name": "Xuejing Liu"
                    },
                    {
                        "name": "Jialin Wang"
                    },
                    {
                        "name": "Wenbin Ge"
                    },
                    {
                        "name": "Sibo Song"
                    },
                    {
                        "name": "Kai Dang"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Shijie Wang"
                    },
                    {
                        "name": "Jun Tang"
                    },
                    {
                        "name": "Humen Zhong"
                    },
                    {
                        "name": "Yuanzhi Zhu"
                    },
                    {
                        "name": "Mingkun Yang"
                    },
                    {
                        "name": "Zhaohai Li"
                    },
                    {
                        "name": "Jianqiang Wan"
                    },
                    {
                        "name": "Pengfei Wang"
                    },
                    {
                        "name": "Wei Ding"
                    },
                    {
                        "name": "Zheren Fu"
                    },
                    {
                        "name": "Yiheng Xu"
                    },
                    {
                        "name": "Jiabo Ye"
                    },
                    {
                        "name": "Xi Zhang"
                    },
                    {
                        "name": "Tianbao Xie"
                    },
                    {
                        "name": "Zesen Cheng"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Zhibo Yang"
                    },
                    {
                        "name": "Haiyang Xu"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13922v1",
                "updated": "2025-02-19T17:59:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    59,
                    3,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T17:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    59,
                    3,
                    2,
                    50,
                    0
                ],
                "title": "LongPO: Long Context Self-Evolution of Large Language Models through\n  Short-to-Long Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongPO: Long Context Self-Evolution of Large Language Models through\n  Short-to-Long Preference Optimization"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities\nthrough pretraining and alignment. However, superior short-context LLMs may\nunderperform in long-context scenarios due to insufficient long-context\nalignment. This alignment process remains challenging due to the impracticality\nof human annotation for extended contexts and the difficulty in balancing\nshort- and long-context performance. To address these challenges, we introduce\nLongPO, that enables short-context LLMs to self-evolve to excel on long-context\ntasks by internally transferring short-context capabilities. LongPO harnesses\nLLMs to learn from self-generated short-to-long preference data, comprising\npaired responses generated for identical instructions with long-context inputs\nand their compressed short-context counterparts, respectively. This preference\nreveals capabilities and potentials of LLMs cultivated during short-context\nalignment that may be diminished in under-aligned long-context scenarios.\nAdditionally, LongPO incorporates a short-to-long KL constraint to mitigate\nshort-context performance decline during long-context alignment. When applied\nto Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully\nretains short-context performance and largely outperforms naive SFT and DPO in\nboth long- and short-context tasks. Specifically, \\ourMethod-trained models can\nachieve results on long-context benchmarks comparable to, or even surpassing,\nthose of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context\nannotation and larger parameter scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities\nthrough pretraining and alignment. However, superior short-context LLMs may\nunderperform in long-context scenarios due to insufficient long-context\nalignment. This alignment process remains challenging due to the impracticality\nof human annotation for extended contexts and the difficulty in balancing\nshort- and long-context performance. To address these challenges, we introduce\nLongPO, that enables short-context LLMs to self-evolve to excel on long-context\ntasks by internally transferring short-context capabilities. LongPO harnesses\nLLMs to learn from self-generated short-to-long preference data, comprising\npaired responses generated for identical instructions with long-context inputs\nand their compressed short-context counterparts, respectively. This preference\nreveals capabilities and potentials of LLMs cultivated during short-context\nalignment that may be diminished in under-aligned long-context scenarios.\nAdditionally, LongPO incorporates a short-to-long KL constraint to mitigate\nshort-context performance decline during long-context alignment. When applied\nto Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully\nretains short-context performance and largely outperforms naive SFT and DPO in\nboth long- and short-context tasks. Specifically, \\ourMethod-trained models can\nachieve results on long-context benchmarks comparable to, or even surpassing,\nthose of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context\nannotation and larger parameter scales."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13921v1",
                "updated": "2025-02-19T17:53:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    59,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T17:53:59Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    59,
                    2,
                    50,
                    0
                ],
                "title": "Exploring Code Language Models for Automated HLS-based Hardware\n  Generation: Benchmark, Infrastructure and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Code Language Models for Automated HLS-based Hardware\n  Generation: Benchmark, Infrastructure and Analysis"
                },
                "summary": "Recent advances in code generation have illuminated the potential of\nemploying large language models (LLMs) for general-purpose programming\nlanguages such as Python and C++, opening new opportunities for automating\nsoftware development and enhancing programmer productivity. The potential of\nLLMs in software programming has sparked significant interest in exploring\nautomated hardware generation and automation. Although preliminary endeavors\nhave been made to adopt LLMs in generating hardware description languages\n(HDLs), several challenges persist in this direction. First, the volume of\navailable HDL training data is substantially smaller compared to that for\nsoftware programming languages. Second, the pre-trained LLMs, mainly tailored\nfor software code, tend to produce HDL designs that are more error-prone.\nThird, the generation of HDL requires a significantly higher number of tokens\ncompared to software programming, leading to inefficiencies in cost and energy\nconsumption. To tackle these challenges, this paper explores leveraging LLMs to\ngenerate High-Level Synthesis (HLS)-based hardware design. Although code\ngeneration for domain-specific programming languages is not new in the\nliterature, we aim to provide experimental results, insights, benchmarks, and\nevaluation infrastructure to investigate the suitability of HLS over low-level\nHDLs for LLM-assisted hardware design generation. To achieve this, we first\nfinetune pre-trained models for HLS-based hardware generation, using a\ncollected dataset with text prompts and corresponding reference HLS designs. An\nLLM-assisted framework is then proposed to automate end-to-end hardware code\ngeneration, which also investigates the impact of chain-of-thought and feedback\nloops promoting techniques on HLS-design generation. Limited by the timeframe\nof this research, we plan to evaluate more advanced reasoning models in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in code generation have illuminated the potential of\nemploying large language models (LLMs) for general-purpose programming\nlanguages such as Python and C++, opening new opportunities for automating\nsoftware development and enhancing programmer productivity. The potential of\nLLMs in software programming has sparked significant interest in exploring\nautomated hardware generation and automation. Although preliminary endeavors\nhave been made to adopt LLMs in generating hardware description languages\n(HDLs), several challenges persist in this direction. First, the volume of\navailable HDL training data is substantially smaller compared to that for\nsoftware programming languages. Second, the pre-trained LLMs, mainly tailored\nfor software code, tend to produce HDL designs that are more error-prone.\nThird, the generation of HDL requires a significantly higher number of tokens\ncompared to software programming, leading to inefficiencies in cost and energy\nconsumption. To tackle these challenges, this paper explores leveraging LLMs to\ngenerate High-Level Synthesis (HLS)-based hardware design. Although code\ngeneration for domain-specific programming languages is not new in the\nliterature, we aim to provide experimental results, insights, benchmarks, and\nevaluation infrastructure to investigate the suitability of HLS over low-level\nHDLs for LLM-assisted hardware design generation. To achieve this, we first\nfinetune pre-trained models for HLS-based hardware generation, using a\ncollected dataset with text prompts and corresponding reference HLS designs. An\nLLM-assisted framework is then proposed to automate end-to-end hardware code\ngeneration, which also investigates the impact of chain-of-thought and feedback\nloops promoting techniques on HLS-design generation. Limited by the timeframe\nof this research, we plan to evaluate more advanced reasoning models in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiahao Gai"
                    },
                    {
                        "name": "Hao"
                    },
                    {
                        "name": "Chen"
                    },
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Wanru Zhao"
                    },
                    {
                        "name": "Nicholas Lane"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "arxiv_affiliation": "Mark",
                "author": "Hongxiang Fan",
                "arxiv_comment": "Paper accepted by ASP-DAC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13920v1",
                "updated": "2025-02-19T17:53:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    43,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T17:53:43Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    43,
                    2,
                    50,
                    0
                ],
                "title": "Exploring Personalized Health Support through Data-Driven, Theory-Guided\n  LLMs: A Case Study in Sleep Health",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Personalized Health Support through Data-Driven, Theory-Guided\n  LLMs: A Case Study in Sleep Health"
                },
                "summary": "Despite the prevalence of sleep-tracking devices, many individuals struggle\nto translate data into actionable improvements in sleep health. Current methods\noften provide data-driven suggestions but may not be feasible and adaptive to\nreal-life constraints and individual contexts. We present HealthGuru, a novel\nlarge language model-powered chatbot to enhance sleep health through\ndata-driven, theory-guided, and adaptive recommendations with conversational\nbehavior change support. HealthGuru's multi-agent framework integrates wearable\ndevice data, contextual information, and a contextual multi-armed bandit model\nto suggest tailored sleep-enhancing activities. The system facilitates natural\nconversations while incorporating data-driven insights and theoretical behavior\nchange techniques. Our eight-week in-the-wild deployment study with 16\nparticipants compared HealthGuru to a baseline chatbot. Results show improved\nmetrics like sleep duration and activity scores, higher quality responses, and\nincreased user motivation for behavior change with HealthGuru. We also identify\nchallenges and design considerations for personalization and user engagement in\nhealth chatbots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the prevalence of sleep-tracking devices, many individuals struggle\nto translate data into actionable improvements in sleep health. Current methods\noften provide data-driven suggestions but may not be feasible and adaptive to\nreal-life constraints and individual contexts. We present HealthGuru, a novel\nlarge language model-powered chatbot to enhance sleep health through\ndata-driven, theory-guided, and adaptive recommendations with conversational\nbehavior change support. HealthGuru's multi-agent framework integrates wearable\ndevice data, contextual information, and a contextual multi-armed bandit model\nto suggest tailored sleep-enhancing activities. The system facilitates natural\nconversations while incorporating data-driven insights and theoretical behavior\nchange techniques. Our eight-week in-the-wild deployment study with 16\nparticipants compared HealthGuru to a baseline chatbot. Results show improved\nmetrics like sleep duration and activity scores, higher quality responses, and\nincreased user motivation for behavior change with HealthGuru. We also identify\nchallenges and design considerations for personalization and user engagement in\nhealth chatbots."
                },
                "authors": [
                    {
                        "name": "Xingbo Wang"
                    },
                    {
                        "name": "Janessa Griffith"
                    },
                    {
                        "name": "Daniel A. Adler"
                    },
                    {
                        "name": "Joey Castillo"
                    },
                    {
                        "name": "Tanzeem Choudhury"
                    },
                    {
                        "name": "Fei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wang"
                },
                "author": "Fei Wang",
                "arxiv_doi": "10.1145/3706598.3713852",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713852",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.13920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to CHI Conference on Human Factors in Computing Systems (CHI\n  2025)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18549v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18549v3",
                "updated": "2025-02-19T17:52:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    52,
                    45,
                    2,
                    50,
                    0
                ],
                "published": "2024-11-27T17:51:08Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    17,
                    51,
                    8,
                    2,
                    332,
                    0
                ],
                "title": "Finite population inference for skewness measures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finite population inference for skewness measures"
                },
                "summary": "In this article we consider Bowley's skewness measure and the\nGroeneveld-Meeden $b_{3}$ index in the context of finite population sampling.\nWe employ the functional delta method to obtain asymptotic variance formulae\nfor plug-in estimators and propose corresponding variance estimators. We then\nconsider plug-in estimators based on the H\\'{a}jek cdf-estimator and on a\nDeville-S\\\"arndal type calibration estimator and test the performance of normal\nconfidence intervals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this article we consider Bowley's skewness measure and the\nGroeneveld-Meeden $b_{3}$ index in the context of finite population sampling.\nWe employ the functional delta method to obtain asymptotic variance formulae\nfor plug-in estimators and propose corresponding variance estimators. We then\nconsider plug-in estimators based on the H\\'{a}jek cdf-estimator and on a\nDeville-S\\\"arndal type calibration estimator and test the performance of normal\nconfidence intervals."
                },
                "authors": [
                    {
                        "name": "Leo Pasquazzi"
                    }
                ],
                "author_detail": {
                    "name": "Leo Pasquazzi"
                },
                "author": "Leo Pasquazzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18549v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18549v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62D05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13917v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13917v1",
                "updated": "2025-02-19T17:50:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    50,
                    31,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T17:50:31Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    50,
                    31,
                    2,
                    50,
                    0
                ],
                "title": "TESS 2: A Large-Scale Generalist Diffusion Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TESS 2: A Large-Scale Generalist Diffusion Language Model"
                },
                "summary": "We introduce TESS 2, a general instruction-following diffusion language model\nthat outperforms contemporary instruction-tuned diffusion models, as well as\nmatches and sometimes exceeds strong autoregressive (AR) models. We train TESS\n2 by first adapting a strong AR model via continued pretraining with the usual\ncross-entropy as diffusion loss, and then performing further instruction\ntuning. We find that adaptation training as well as the choice of the base\nmodel is crucial for training good instruction-following diffusion models. We\nfurther propose reward guidance, a novel and modular inference-time guidance\nprocedure to align model outputs without needing to train the underlying model.\nFinally, we show that TESS 2 further improves with increased inference-time\ncompute, highlighting the utility of diffusion LMs in having fine-grained\ncontrollability over the amount of compute used at inference time. Code and\nmodels are available at https://github.com/hamishivi/tess-2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce TESS 2, a general instruction-following diffusion language model\nthat outperforms contemporary instruction-tuned diffusion models, as well as\nmatches and sometimes exceeds strong autoregressive (AR) models. We train TESS\n2 by first adapting a strong AR model via continued pretraining with the usual\ncross-entropy as diffusion loss, and then performing further instruction\ntuning. We find that adaptation training as well as the choice of the base\nmodel is crucial for training good instruction-following diffusion models. We\nfurther propose reward guidance, a novel and modular inference-time guidance\nprocedure to align model outputs without needing to train the underlying model.\nFinally, we show that TESS 2 further improves with increased inference-time\ncompute, highlighting the utility of diffusion LMs in having fine-grained\ncontrollability over the amount of compute used at inference time. Code and\nmodels are available at https://github.com/hamishivi/tess-2."
                },
                "authors": [
                    {
                        "name": "Jaesung Tae"
                    },
                    {
                        "name": "Hamish Ivison"
                    },
                    {
                        "name": "Sachin Kumar"
                    },
                    {
                        "name": "Arman Cohan"
                    }
                ],
                "author_detail": {
                    "name": "Arman Cohan"
                },
                "author": "Arman Cohan",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13917v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13917v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13913v1",
                "updated": "2025-02-19T17:46:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    46,
                    30,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T17:46:30Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    46,
                    30,
                    2,
                    50,
                    0
                ],
                "title": "How Do LLMs Perform Two-Hop Reasoning in Context?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Do LLMs Perform Two-Hop Reasoning in Context?"
                },
                "summary": "\"Socrates is human. All humans are mortal. Therefore, Socrates is mortal.\"\nThis classical example demonstrates two-hop reasoning, where a conclusion\nlogically follows from two connected premises. While transformer-based Large\nLanguage Models (LLMs) can make two-hop reasoning, they tend to collapse to\nrandom guessing when faced with distracting premises. To understand the\nunderlying mechanism, we train a three-layer transformer on synthetic two-hop\nreasoning tasks. The training dynamics show two stages: a slow learning phase,\nwhere the 3-layer transformer performs random guessing like LLMs, followed by\nan abrupt phase transitions, where the 3-layer transformer suddenly reaches\n$100%$ accuracy. Through reverse engineering, we explain the inner mechanisms\nfor how models learn to randomly guess between distractions initially, and how\nthey learn to ignore distractions eventually. We further propose a\nthree-parameter model that supports the causal claims for the mechanisms to the\ntraining dynamics of the transformer. Finally, experiments on LLMs suggest that\nthe discovered mechanisms generalize across scales. Our methodologies provide\nnew perspectives for scientific understandings of LLMs and our findings provide\nnew insights into how reasoning emerges during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Socrates is human. All humans are mortal. Therefore, Socrates is mortal.\"\nThis classical example demonstrates two-hop reasoning, where a conclusion\nlogically follows from two connected premises. While transformer-based Large\nLanguage Models (LLMs) can make two-hop reasoning, they tend to collapse to\nrandom guessing when faced with distracting premises. To understand the\nunderlying mechanism, we train a three-layer transformer on synthetic two-hop\nreasoning tasks. The training dynamics show two stages: a slow learning phase,\nwhere the 3-layer transformer performs random guessing like LLMs, followed by\nan abrupt phase transitions, where the 3-layer transformer suddenly reaches\n$100%$ accuracy. Through reverse engineering, we explain the inner mechanisms\nfor how models learn to randomly guess between distractions initially, and how\nthey learn to ignore distractions eventually. We further propose a\nthree-parameter model that supports the causal claims for the mechanisms to the\ntraining dynamics of the transformer. Finally, experiments on LLMs suggest that\nthe discovered mechanisms generalize across scales. Our methodologies provide\nnew perspectives for scientific understandings of LLMs and our findings provide\nnew insights into how reasoning emerges during training."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Hanlin Zhu"
                    },
                    {
                        "name": "Ruiqi Zhang"
                    },
                    {
                        "name": "Jiantao Jiao"
                    },
                    {
                        "name": "Song Mei"
                    },
                    {
                        "name": "Michael I. Jordan"
                    },
                    {
                        "name": "Stuart Russell"
                    }
                ],
                "author_detail": {
                    "name": "Stuart Russell"
                },
                "author": "Stuart Russell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13909v1",
                "updated": "2025-02-19T17:41:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    41,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T17:41:09Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    41,
                    9,
                    2,
                    50,
                    0
                ],
                "title": "Lost in Sequence: Do Large Language Models Understand Sequential\n  Recommendation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Sequence: Do Large Language Models Understand Sequential\n  Recommendation?"
                },
                "summary": "Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec."
                },
                "authors": [
                    {
                        "name": "Sein Kim"
                    },
                    {
                        "name": "Hongseok Kang"
                    },
                    {
                        "name": "Kibum Kim"
                    },
                    {
                        "name": "Jiwan Kim"
                    },
                    {
                        "name": "Donghyun Kim"
                    },
                    {
                        "name": "Minchul Yang"
                    },
                    {
                        "name": "Kwangjin Oh"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Chanyoung Park"
                    }
                ],
                "author_detail": {
                    "name": "Chanyoung Park"
                },
                "author": "Chanyoung Park",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13908v1",
                "updated": "2025-02-19T17:40:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    40,
                    32,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T17:40:32Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    40,
                    32,
                    2,
                    50,
                    0
                ],
                "title": "Judging the Judges: A Collection of LLM-Generated Relevance Judgements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judging the Judges: A Collection of LLM-Generated Relevance Judgements"
                },
                "summary": "Using Large Language Models (LLMs) for relevance assessments offers promising\nopportunities to improve Information Retrieval (IR), Natural Language\nProcessing (NLP), and related fields. Indeed, LLMs hold the promise of allowing\nIR experimenters to build evaluation collections with a fraction of the manual\nhuman labor currently required. This could help with fresh topics on which\nthere is still limited knowledge and could mitigate the challenges of\nevaluating ranking systems in low-resource scenarios, where it is challenging\nto find human annotators. Given the fast-paced recent developments in the\ndomain, many questions concerning LLMs as assessors are yet to be answered.\nAmong the aspects that require further investigation, we can list the impact of\nvarious components in a relevance judgment generation pipeline, such as the\nprompt used or the LLM chosen.\n  This paper benchmarks and reports on the results of a large-scale automatic\nrelevance judgment evaluation, the LLMJudge challenge at SIGIR 2024, where\ndifferent relevance assessment approaches were proposed. In detail, we release\nand benchmark 42 LLM-generated labels of the TREC 2023 Deep Learning track\nrelevance judgments produced by eight international teams who participated in\nthe challenge. Given their diverse nature, these automatically generated\nrelevance judgments can help the community not only investigate systematic\nbiases caused by LLMs but also explore the effectiveness of ensemble models,\nanalyze the trade-offs between different models and human assessors, and\nadvance methodologies for improving automated evaluation techniques. The\nreleased resource is available at the following link:\nhttps://llm4eval.github.io/LLMJudge-benchmark/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models (LLMs) for relevance assessments offers promising\nopportunities to improve Information Retrieval (IR), Natural Language\nProcessing (NLP), and related fields. Indeed, LLMs hold the promise of allowing\nIR experimenters to build evaluation collections with a fraction of the manual\nhuman labor currently required. This could help with fresh topics on which\nthere is still limited knowledge and could mitigate the challenges of\nevaluating ranking systems in low-resource scenarios, where it is challenging\nto find human annotators. Given the fast-paced recent developments in the\ndomain, many questions concerning LLMs as assessors are yet to be answered.\nAmong the aspects that require further investigation, we can list the impact of\nvarious components in a relevance judgment generation pipeline, such as the\nprompt used or the LLM chosen.\n  This paper benchmarks and reports on the results of a large-scale automatic\nrelevance judgment evaluation, the LLMJudge challenge at SIGIR 2024, where\ndifferent relevance assessment approaches were proposed. In detail, we release\nand benchmark 42 LLM-generated labels of the TREC 2023 Deep Learning track\nrelevance judgments produced by eight international teams who participated in\nthe challenge. Given their diverse nature, these automatically generated\nrelevance judgments can help the community not only investigate systematic\nbiases caused by LLMs but also explore the effectiveness of ensemble models,\nanalyze the trade-offs between different models and human assessors, and\nadvance methodologies for improving automated evaluation techniques. The\nreleased resource is available at the following link:\nhttps://llm4eval.github.io/LLMJudge-benchmark/"
                },
                "authors": [
                    {
                        "name": "Hossein A. Rahmani"
                    },
                    {
                        "name": "Clemencia Siro"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Nick Craswell"
                    },
                    {
                        "name": "Charles L. A. Clarke"
                    },
                    {
                        "name": "Guglielmo Faggioli"
                    },
                    {
                        "name": "Bhaskar Mitra"
                    },
                    {
                        "name": "Paul Thomas"
                    },
                    {
                        "name": "Emine Yilmaz"
                    }
                ],
                "author_detail": {
                    "name": "Emine Yilmaz"
                },
                "author": "Emine Yilmaz",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13905v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13905v1",
                "updated": "2025-02-19T17:39:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    39,
                    46,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T17:39:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    39,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "Partially Observable Gaussian Process Network and Doubly Stochastic\n  Variational Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partially Observable Gaussian Process Network and Doubly Stochastic\n  Variational Inference"
                },
                "summary": "To reduce the curse of dimensionality for Gaussian processes (GP), they can\nbe decomposed into a Gaussian Process Network (GPN) of coupled subprocesses\nwith lower dimensionality. In some cases, intermediate observations are\navailable within the GPN. However, intermediate observations are often\nindirect, noisy, and incomplete in most real-world systems. This work\nintroduces the Partially Observable Gaussian Process Network (POGPN) to model\nreal-world process networks. We model a joint distribution of latent functions\nof subprocesses and make inferences using observations from all subprocesses.\nPOGPN incorporates observation lenses (observation likelihoods) into the\nwell-established inference method of deep Gaussian processes. We also introduce\ntwo training methods for POPGN to make inferences on the whole network using\nnode observations. The application to benchmark problems demonstrates how\nincorporating partial observations during training and inference can improve\nthe predictive performance of the overall network, offering a promising outlook\nfor its practical application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To reduce the curse of dimensionality for Gaussian processes (GP), they can\nbe decomposed into a Gaussian Process Network (GPN) of coupled subprocesses\nwith lower dimensionality. In some cases, intermediate observations are\navailable within the GPN. However, intermediate observations are often\nindirect, noisy, and incomplete in most real-world systems. This work\nintroduces the Partially Observable Gaussian Process Network (POGPN) to model\nreal-world process networks. We model a joint distribution of latent functions\nof subprocesses and make inferences using observations from all subprocesses.\nPOGPN incorporates observation lenses (observation likelihoods) into the\nwell-established inference method of deep Gaussian processes. We also introduce\ntwo training methods for POPGN to make inferences on the whole network using\nnode observations. The application to benchmark problems demonstrates how\nincorporating partial observations during training and inference can improve\nthe predictive performance of the overall network, offering a promising outlook\nfor its practical application."
                },
                "authors": [
                    {
                        "name": "Saksham Kiroriwal"
                    },
                    {
                        "name": "Julius Pfrommer"
                    },
                    {
                        "name": "Jrgen Beyerer"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Beyerer"
                },
                "author": "Jrgen Beyerer",
                "arxiv_comment": "8 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13905v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13905v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13897v1",
                "updated": "2025-02-19T17:31:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    31,
                    51,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T17:31:51Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    31,
                    51,
                    2,
                    50,
                    0
                ],
                "title": "DataSciBench: An LLM Agent Benchmark for Data Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataSciBench: An LLM Agent Benchmark for Data Science"
                },
                "summary": "This paper presents DataSciBench, a comprehensive benchmark for evaluating\nLarge Language Model (LLM) capabilities in data science. Recent related\nbenchmarks have primarily focused on single tasks, easily obtainable ground\ntruth, and straightforward evaluation metrics, which limits the scope of tasks\nthat can be evaluated. In contrast, DataSciBench is constructed based on a more\ncomprehensive and curated collection of natural and challenging prompts for\nuncertain ground truth and evaluation metrics. We develop a semi-automated\npipeline for generating ground truth (GT) and validating evaluation metrics.\nThis pipeline utilizes and implements an LLM-based self-consistency and human\nverification strategy to produce accurate GT by leveraging collected prompts,\npredefined task types, and aggregate functions (metrics). Furthermore, we\npropose an innovative Task - Function - Code (TFC) framework to assess each\ncode execution outcome based on precisely defined metrics and programmatic\nrules. Our experimental framework involves testing 6 API-based models, 8\nopen-source general models, and 9 open-source code generation models using the\ndiverse set of prompts we have gathered. This approach aims to provide a more\ncomprehensive and rigorous evaluation of LLMs in data science, revealing their\nstrengths and weaknesses. Experimental results demonstrate that API-based\nmodels outperform open-sourced models on all metrics and\nDeepseek-Coder-33B-Instruct achieves the highest score among open-sourced\nmodels. We release all code and data at https://github.com/THUDM/DataSciBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents DataSciBench, a comprehensive benchmark for evaluating\nLarge Language Model (LLM) capabilities in data science. Recent related\nbenchmarks have primarily focused on single tasks, easily obtainable ground\ntruth, and straightforward evaluation metrics, which limits the scope of tasks\nthat can be evaluated. In contrast, DataSciBench is constructed based on a more\ncomprehensive and curated collection of natural and challenging prompts for\nuncertain ground truth and evaluation metrics. We develop a semi-automated\npipeline for generating ground truth (GT) and validating evaluation metrics.\nThis pipeline utilizes and implements an LLM-based self-consistency and human\nverification strategy to produce accurate GT by leveraging collected prompts,\npredefined task types, and aggregate functions (metrics). Furthermore, we\npropose an innovative Task - Function - Code (TFC) framework to assess each\ncode execution outcome based on precisely defined metrics and programmatic\nrules. Our experimental framework involves testing 6 API-based models, 8\nopen-source general models, and 9 open-source code generation models using the\ndiverse set of prompts we have gathered. This approach aims to provide a more\ncomprehensive and rigorous evaluation of LLMs in data science, revealing their\nstrengths and weaknesses. Experimental results demonstrate that API-based\nmodels outperform open-sourced models on all metrics and\nDeepseek-Coder-33B-Instruct achieves the highest score among open-sourced\nmodels. We release all code and data at https://github.com/THUDM/DataSciBench."
                },
                "authors": [
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Sining Zhoubian"
                    },
                    {
                        "name": "Min Cai"
                    },
                    {
                        "name": "Fengzu Li"
                    },
                    {
                        "name": "Lekang Yang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Tianjiao Dong"
                    },
                    {
                        "name": "Ziniu Hu"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yisong Yue"
                    }
                ],
                "author_detail": {
                    "name": "Yisong Yue"
                },
                "author": "Yisong Yue",
                "arxiv_comment": "40 pages, 7 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01563v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01563v2",
                "updated": "2025-02-19T17:23:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    23,
                    26,
                    2,
                    50,
                    0
                ],
                "published": "2024-08-02T20:31:51Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    20,
                    31,
                    51,
                    4,
                    215,
                    0
                ],
                "title": "Discriminating among cosmological models by data-driven methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discriminating among cosmological models by data-driven methods"
                },
                "summary": "We explores the Pantheon+SH0ES dataset to identify patterns that can\ndiscriminate between different cosmological models. We focus on determining\nwhether the behaviour of dark energy is consistent with the standard\n$\\Lambda$CDM model or suggests novel cosmological features. The central goal is\nto evaluate the robustness of the $\\Lambda$CDM model compared with other dark\nenergy models, and to investigate whether there are deviations that might\nindicate new cosmological insights. The study takes into account a data-driven\napproach, using both traditional statistical methods and machine learning\ntechniques. Initially, we evaluate six different dark energy models using\ntraditional statistical methods like Markov Chain Monte Carlo (MCMC), Static\nand Dynamic Nested Sampling to infer the cosmological parameters. Subsequently,\nwe adopt a machine learning approach, developing a regression model to compute\nthe distance modulus of each supernova, expanding the feature set to 74\nstatistical features. Traditional statistical analysis confirms that the\n$\\Lambda$CDM model is robust, yielding expected parameter values. Other models\nshow deviations, with the Generalised and Modified Chaplygin Gas models\nperforming poorly. In the machine learning analysis, feature selection\ntechniques, particularly Boruta, significantly improve model performance. In\nparticular, models initially considered weak (Generalised/Modified Chaplygin\nGas) show significant improvement after feature selection. The study\ndemonstrates the effectiveness of a data-driven approach to cosmological model\nevaluation. The $\\Lambda$CDM model remains robust, while machine learning\ntechniques, in particular feature selection, reveal potential improvements in\nalternative models which could be relevant for new observational campaigns like\nthe recent DESI survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explores the Pantheon+SH0ES dataset to identify patterns that can\ndiscriminate between different cosmological models. We focus on determining\nwhether the behaviour of dark energy is consistent with the standard\n$\\Lambda$CDM model or suggests novel cosmological features. The central goal is\nto evaluate the robustness of the $\\Lambda$CDM model compared with other dark\nenergy models, and to investigate whether there are deviations that might\nindicate new cosmological insights. The study takes into account a data-driven\napproach, using both traditional statistical methods and machine learning\ntechniques. Initially, we evaluate six different dark energy models using\ntraditional statistical methods like Markov Chain Monte Carlo (MCMC), Static\nand Dynamic Nested Sampling to infer the cosmological parameters. Subsequently,\nwe adopt a machine learning approach, developing a regression model to compute\nthe distance modulus of each supernova, expanding the feature set to 74\nstatistical features. Traditional statistical analysis confirms that the\n$\\Lambda$CDM model is robust, yielding expected parameter values. Other models\nshow deviations, with the Generalised and Modified Chaplygin Gas models\nperforming poorly. In the machine learning analysis, feature selection\ntechniques, particularly Boruta, significantly improve model performance. In\nparticular, models initially considered weak (Generalised/Modified Chaplygin\nGas) show significant improvement after feature selection. The study\ndemonstrates the effectiveness of a data-driven approach to cosmological model\nevaluation. The $\\Lambda$CDM model remains robust, while machine learning\ntechniques, in particular feature selection, reveal potential improvements in\nalternative models which could be relevant for new observational campaigns like\nthe recent DESI survey."
                },
                "authors": [
                    {
                        "name": "Simone Vilardi"
                    },
                    {
                        "name": "Salvatore Capozziello"
                    },
                    {
                        "name": "Massimo Brescia"
                    }
                ],
                "author_detail": {
                    "name": "Massimo Brescia"
                },
                "author": "Massimo Brescia",
                "arxiv_comment": "23 pages, 20 figures, accepted for publication in Astronomyy &\n  Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01563v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01563v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02098v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02098v4",
                "updated": "2025-02-19T17:22:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    22,
                    17,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-02T23:39:10Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    23,
                    39,
                    10,
                    2,
                    276,
                    0
                ],
                "title": "EC-DIT: Scaling Diffusion Transformers with Adaptive Expert-Choice\n  Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EC-DIT: Scaling Diffusion Transformers with Adaptive Expert-Choice\n  Routing"
                },
                "summary": "Diffusion transformers have been widely adopted for text-to-image synthesis.\nWhile scaling these models up to billions of parameters shows promise, the\neffectiveness of scaling beyond current sizes remains underexplored and\nchallenging. By explicitly exploiting the computational heterogeneity of image\ngenerations, we develop a new family of Mixture-of-Experts (MoE) models\n(EC-DIT) for diffusion transformers with expert-choice routing. EC-DIT learns\nto adaptively optimize the compute allocated to understand the input texts and\ngenerate the respective image patches, enabling heterogeneous computation\naligned with varying text-image complexities. This heterogeneity provides an\nefficient way of scaling EC-DIT up to 97 billion parameters and achieving\nsignificant improvements in training convergence, text-to-image alignment, and\noverall generation quality over dense models and conventional MoE models.\nThrough extensive ablations, we show that EC-DIT demonstrates superior\nscalability and adaptive compute allocation by recognizing varying textual\nimportance through end-to-end training. Notably, in text-to-image alignment\nevaluation, our largest models achieve a state-of-the-art GenEval score of\n71.68% and still maintain competitive inference speed with intuitive\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have been widely adopted for text-to-image synthesis.\nWhile scaling these models up to billions of parameters shows promise, the\neffectiveness of scaling beyond current sizes remains underexplored and\nchallenging. By explicitly exploiting the computational heterogeneity of image\ngenerations, we develop a new family of Mixture-of-Experts (MoE) models\n(EC-DIT) for diffusion transformers with expert-choice routing. EC-DIT learns\nto adaptively optimize the compute allocated to understand the input texts and\ngenerate the respective image patches, enabling heterogeneous computation\naligned with varying text-image complexities. This heterogeneity provides an\nefficient way of scaling EC-DIT up to 97 billion parameters and achieving\nsignificant improvements in training convergence, text-to-image alignment, and\noverall generation quality over dense models and conventional MoE models.\nThrough extensive ablations, we show that EC-DIT demonstrates superior\nscalability and adaptive compute allocation by recognizing varying textual\nimportance through end-to-end training. Notably, in text-to-image alignment\nevaluation, our largest models achieve a state-of-the-art GenEval score of\n71.68% and still maintain competitive inference speed with intuitive\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Haotian Sun"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Bo Dai"
                    },
                    {
                        "name": "Nan Du"
                    }
                ],
                "author_detail": {
                    "name": "Nan Du"
                },
                "author": "Nan Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02098v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02098v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.08193v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.08193v3",
                "updated": "2025-02-19T17:16:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    16,
                    18,
                    2,
                    50,
                    0
                ],
                "published": "2023-05-14T16:06:52Z",
                "published_parsed": [
                    2023,
                    5,
                    14,
                    16,
                    6,
                    52,
                    6,
                    134,
                    0
                ],
                "title": "Estimation and inference for Deep Neuronal Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation and inference for Deep Neuronal Networks"
                },
                "summary": "Nonlinear regression problem is one of the most popular and important\nstatistical tasks. The first methods like least squares estimation go back to\nGauss and Legendre. Recent models and developments in statistics and machine\nlearning like Deep Neuronal Networks (DNN) or nonlinear PDE stimulate new\nresearch in this direction which has to address the important issues and\nchallenges of modern statistical inference such as huge complexity and\nparameter dimension of the model, limited sample size, lack of convexity and\nidentifiability, among many others. Classical results of nonparametric\nstatistics in terms of rate of convergence do not really address the mentioned\nissues. This paper offers a general approach to studying a nonlinear regression\nproblem based on the notion of effective dimension. First, a special case of\nmodels with stochastically linear structure (SLS) is studied. The results\nprovide finite sample expansions for the loss of the penalized maximum\nlikelihood estimation (MLE). The leading term of such expansions as well as the\ncorresponding remainder are given via the effective dimension and the effective\nsample size. The obtained expansions can be used to obtain sharp risk bounds\nand for statistical inference. Despite generality, all the presented bounds are\nnearly sharp and the classical asymptotic results can be obtained as simple\ncorollaries. Although the basic SLS assumptions are not fulfilled for nonlinear\nsmooth regression, we explain how the stochastic linearity can be achieved by\nextending the parameter space. The obtained general results are specified to\nnonlinear smooth regression and to a DNN with one hidden layer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlinear regression problem is one of the most popular and important\nstatistical tasks. The first methods like least squares estimation go back to\nGauss and Legendre. Recent models and developments in statistics and machine\nlearning like Deep Neuronal Networks (DNN) or nonlinear PDE stimulate new\nresearch in this direction which has to address the important issues and\nchallenges of modern statistical inference such as huge complexity and\nparameter dimension of the model, limited sample size, lack of convexity and\nidentifiability, among many others. Classical results of nonparametric\nstatistics in terms of rate of convergence do not really address the mentioned\nissues. This paper offers a general approach to studying a nonlinear regression\nproblem based on the notion of effective dimension. First, a special case of\nmodels with stochastically linear structure (SLS) is studied. The results\nprovide finite sample expansions for the loss of the penalized maximum\nlikelihood estimation (MLE). The leading term of such expansions as well as the\ncorresponding remainder are given via the effective dimension and the effective\nsample size. The obtained expansions can be used to obtain sharp risk bounds\nand for statistical inference. Despite generality, all the presented bounds are\nnearly sharp and the classical asymptotic results can be obtained as simple\ncorollaries. Although the basic SLS assumptions are not fulfilled for nonlinear\nsmooth regression, we explain how the stochastic linearity can be achieved by\nextending the parameter space. The obtained general results are specified to\nnonlinear smooth regression and to a DNN with one hidden layer."
                },
                "authors": [
                    {
                        "name": "Vladimir Spokoiny"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir Spokoiny"
                },
                "author": "Vladimir Spokoiny",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.08193v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.08193v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F10, 62F25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11589v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11589v3",
                "updated": "2025-02-19T17:12:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    12,
                    45,
                    2,
                    50,
                    0
                ],
                "published": "2024-06-17T14:34:14Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    14,
                    34,
                    14,
                    0,
                    169,
                    0
                ],
                "title": "CoSQA+: Pioneering the Multi-Choice Code Search Benchmark with\n  Test-Driven Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSQA+: Pioneering the Multi-Choice Code Search Benchmark with\n  Test-Driven Agents"
                },
                "summary": "Semantic code search, retrieving code that matches a given natural language\nquery, is an important task to improve productivity in software engineering.\nExisting code search datasets face limitations: they rely on human annotators\nwho assess code primarily through semantic understanding rather than functional\nverification, leading to potential inaccuracies and scalability issues.\nAdditionally, current evaluation metrics often overlook the multi-choice nature\nof code search. This paper introduces CoSQA+, pairing high-quality queries from\nCoSQA with multiple suitable codes. We develop an automated pipeline featuring\nmultiple model-based candidate selections and the novel test-driven agent\nannotation system. Among a single Large Language Model (LLM) annotator and\nPython expert annotators (without test-based verification), agents leverage\ntest-based verification and achieve the highest accuracy of 96.4%. Through\nextensive experiments, CoSQA+ has demonstrated superior quality over CoSQA.\nModels trained on CoSQA+ exhibit improved performance. We provide the code and\ndata at https://github.com/DeepSoftwareAnalytics/CoSQA_Plus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic code search, retrieving code that matches a given natural language\nquery, is an important task to improve productivity in software engineering.\nExisting code search datasets face limitations: they rely on human annotators\nwho assess code primarily through semantic understanding rather than functional\nverification, leading to potential inaccuracies and scalability issues.\nAdditionally, current evaluation metrics often overlook the multi-choice nature\nof code search. This paper introduces CoSQA+, pairing high-quality queries from\nCoSQA with multiple suitable codes. We develop an automated pipeline featuring\nmultiple model-based candidate selections and the novel test-driven agent\nannotation system. Among a single Large Language Model (LLM) annotator and\nPython expert annotators (without test-based verification), agents leverage\ntest-based verification and achieve the highest accuracy of 96.4%. Through\nextensive experiments, CoSQA+ has demonstrated superior quality over CoSQA.\nModels trained on CoSQA+ exhibit improved performance. We provide the code and\ndata at https://github.com/DeepSoftwareAnalytics/CoSQA_Plus."
                },
                "authors": [
                    {
                        "name": "Jing Gong"
                    },
                    {
                        "name": "Yanghui Wu"
                    },
                    {
                        "name": "Linxi Liang"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "15 pages, 4 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11589v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11589v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.01412v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.01412v3",
                "updated": "2025-02-19T17:09:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    9,
                    47,
                    2,
                    50,
                    0
                ],
                "published": "2023-11-02T17:26:49Z",
                "published_parsed": [
                    2023,
                    11,
                    2,
                    17,
                    26,
                    49,
                    3,
                    306,
                    0
                ],
                "title": "Causal Temporal Regime Structure Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Temporal Regime Structure Learning"
                },
                "summary": "Understanding causal relationships in multivariate time series is essential\nfor predicting and controlling dynamic systems in fields like economics,\nneuroscience, and climate science. However, existing causal discovery methods\noften assume stationarity, limiting their effectiveness when time series\nconsist of sequential regimes, consecutive temporal segments with unknown\nboundaries and changing causal structures. In this work, we firstly introduce a\nframework to describe and model such time series. Then, we present CASTOR, a\nnovel method that concurrently learns the Directed Acyclic Graph (DAG) for each\nregime while determining the number of regimes and their sequential\narrangement. CASTOR optimizes the data log-likelihood using an\nexpectation-maximization algorithm, alternating between assigning regime\nindices (expectation step) and inferring causal relationships in each regime\n(maximization step). We establish the identifiability of the regimes and DAGs\nwithin our framework. Extensive experiments show that CASTOR consistently\noutperforms existing causal discovery models in detecting different regimes and\nlearning their DAGs across various settings, including linear and nonlinear\ncausal relationships, on both synthetic and real world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding causal relationships in multivariate time series is essential\nfor predicting and controlling dynamic systems in fields like economics,\nneuroscience, and climate science. However, existing causal discovery methods\noften assume stationarity, limiting their effectiveness when time series\nconsist of sequential regimes, consecutive temporal segments with unknown\nboundaries and changing causal structures. In this work, we firstly introduce a\nframework to describe and model such time series. Then, we present CASTOR, a\nnovel method that concurrently learns the Directed Acyclic Graph (DAG) for each\nregime while determining the number of regimes and their sequential\narrangement. CASTOR optimizes the data log-likelihood using an\nexpectation-maximization algorithm, alternating between assigning regime\nindices (expectation step) and inferring causal relationships in each regime\n(maximization step). We establish the identifiability of the regimes and DAGs\nwithin our framework. Extensive experiments show that CASTOR consistently\noutperforms existing causal discovery models in detecting different regimes and\nlearning their DAGs across various settings, including linear and nonlinear\ncausal relationships, on both synthetic and real world datasets."
                },
                "authors": [
                    {
                        "name": "Abdellah Rahmani"
                    },
                    {
                        "name": "Pascal Frossard"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Frossard"
                },
                "author": "Pascal Frossard",
                "arxiv_journal_ref": "Proceedings of the 28th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2025, Mai Khao, Thailand. PMLR: Volume\n  258. Copyright 2025 by the author(s)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.01412v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.01412v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13875v1",
                "updated": "2025-02-19T16:58:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    58,
                    42,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:58:42Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    58,
                    42,
                    2,
                    50,
                    0
                ],
                "title": "MEX: Memory-efficient Approach to Referring Multi-Object Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEX: Memory-efficient Approach to Referring Multi-Object Tracking"
                },
                "summary": "Referring Multi-Object Tracking (RMOT) is a relatively new concept that has\nrapidly gained traction as a promising research direction at the intersection\nof computer vision and natural language processing. Unlike traditional\nmulti-object tracking, RMOT identifies and tracks objects and incorporates\ntextual descriptions for object class names, making the approach more\nintuitive. Various techniques have been proposed to address this challenging\nproblem; however, most require the training of the entire network due to their\nend-to-end nature. Among these methods, iKUN has emerged as a particularly\npromising solution. Therefore, we further explore its pipeline and enhance its\nperformance. In this paper, we introduce a practical module dubbed\nMemory-Efficient Cross-modality -- MEX. This memory-efficient technique can be\ndirectly applied to off-the-shelf trackers like iKUN, resulting in significant\narchitectural improvements. Our method proves effective during inference on a\nsingle GPU with 4 GB of memory. Among the various benchmarks, the Refer-KITTI\ndataset, which offers diverse autonomous driving scenes with relevant language\nexpressions, is particularly useful for studying this problem. Empirically, our\nmethod demonstrates effectiveness and efficiency regarding HOTA tracking\nscores, substantially improving memory allocation and processing speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Referring Multi-Object Tracking (RMOT) is a relatively new concept that has\nrapidly gained traction as a promising research direction at the intersection\nof computer vision and natural language processing. Unlike traditional\nmulti-object tracking, RMOT identifies and tracks objects and incorporates\ntextual descriptions for object class names, making the approach more\nintuitive. Various techniques have been proposed to address this challenging\nproblem; however, most require the training of the entire network due to their\nend-to-end nature. Among these methods, iKUN has emerged as a particularly\npromising solution. Therefore, we further explore its pipeline and enhance its\nperformance. In this paper, we introduce a practical module dubbed\nMemory-Efficient Cross-modality -- MEX. This memory-efficient technique can be\ndirectly applied to off-the-shelf trackers like iKUN, resulting in significant\narchitectural improvements. Our method proves effective during inference on a\nsingle GPU with 4 GB of memory. Among the various benchmarks, the Refer-KITTI\ndataset, which offers diverse autonomous driving scenes with relevant language\nexpressions, is particularly useful for studying this problem. Empirically, our\nmethod demonstrates effectiveness and efficiency regarding HOTA tracking\nscores, substantially improving memory allocation and processing speed."
                },
                "authors": [
                    {
                        "name": "Huu-Thien Tran"
                    },
                    {
                        "name": "Phuoc-Sang Pham"
                    },
                    {
                        "name": "Thai-Son Tran"
                    },
                    {
                        "name": "Khoa Luu"
                    }
                ],
                "author_detail": {
                    "name": "Khoa Luu"
                },
                "author": "Khoa Luu",
                "arxiv_comment": "6 pages, 6 figures, 2024 International Conference on Advanced\n  Technologies for Communications (ATC), Signal Processing Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13870v1",
                "updated": "2025-02-19T16:49:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    49,
                    55,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:49:55Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    49,
                    55,
                    2,
                    50,
                    0
                ],
                "title": "SPEX: Scaling Feature Interaction Explanations for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPEX: Scaling Feature Interaction Explanations for LLMs"
                },
                "summary": "Large language models (LLMs) have revolutionized machine learning due to\ntheir ability to capture complex interactions between input features. Popular\npost-hoc explanation methods like SHAP provide marginal feature attributions,\nwhile their extensions to interaction importances only scale to small input\nlengths ($\\approx 20$). We propose Spectral Explainer (SPEX), a model-agnostic\ninteraction attribution algorithm that efficiently scales to large input\nlengths ($\\approx 1000)$. SPEX exploits underlying natural sparsity among\ninteractions -- common in real-world data -- and applies a sparse Fourier\ntransform using a channel decoding algorithm to efficiently identify important\ninteractions. We perform experiments across three difficult long-context\ndatasets that require LLMs to utilize interactions between inputs to complete\nthe task. For large inputs, SPEX outperforms marginal attribution methods by up\nto 20% in terms of faithfully reconstructing LLM outputs. Further, SPEX\nsuccessfully identifies key features and interactions that strongly influence\nmodel output. For one of our datasets, HotpotQA, SPEX provides interactions\nthat align with human annotations. Finally, we use our model-agnostic approach\nto generate explanations to demonstrate abstract reasoning in closed-source\nLLMs (GPT-4o mini) and compositional reasoning in vision-language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized machine learning due to\ntheir ability to capture complex interactions between input features. Popular\npost-hoc explanation methods like SHAP provide marginal feature attributions,\nwhile their extensions to interaction importances only scale to small input\nlengths ($\\approx 20$). We propose Spectral Explainer (SPEX), a model-agnostic\ninteraction attribution algorithm that efficiently scales to large input\nlengths ($\\approx 1000)$. SPEX exploits underlying natural sparsity among\ninteractions -- common in real-world data -- and applies a sparse Fourier\ntransform using a channel decoding algorithm to efficiently identify important\ninteractions. We perform experiments across three difficult long-context\ndatasets that require LLMs to utilize interactions between inputs to complete\nthe task. For large inputs, SPEX outperforms marginal attribution methods by up\nto 20% in terms of faithfully reconstructing LLM outputs. Further, SPEX\nsuccessfully identifies key features and interactions that strongly influence\nmodel output. For one of our datasets, HotpotQA, SPEX provides interactions\nthat align with human annotations. Finally, we use our model-agnostic approach\nto generate explanations to demonstrate abstract reasoning in closed-source\nLLMs (GPT-4o mini) and compositional reasoning in vision-language models."
                },
                "authors": [
                    {
                        "name": "Justin Singh Kang"
                    },
                    {
                        "name": "Landon Butler"
                    },
                    {
                        "name": "Abhineet Agarwal"
                    },
                    {
                        "name": "Yigit Efe Erginbas"
                    },
                    {
                        "name": "Ramtin Pedarsani"
                    },
                    {
                        "name": "Kannan Ramchandran"
                    },
                    {
                        "name": "Bin Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Yu"
                },
                "author": "Bin Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12633v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12633v2",
                "updated": "2025-02-19T16:45:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    45,
                    48,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-18T08:24:52Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    8,
                    24,
                    52,
                    1,
                    49,
                    0
                ],
                "title": "One Size doesn't Fit All: A Personalized Conversational Tutoring Agent\n  for Mathematics Instruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Size doesn't Fit All: A Personalized Conversational Tutoring Agent\n  for Mathematics Instruction"
                },
                "summary": "Large language models (LLMs) have been increasingly employed in various\nintelligent educational systems, simulating human tutors to facilitate\neffective human-machine interaction. However, previous studies often overlook\nthe significance of recognizing and adapting to individual learner\ncharacteristics. Such adaptation is crucial for enhancing student engagement\nand learning efficiency, particularly in mathematics instruction, where diverse\nlearning styles require personalized strategies to promote comprehension and\nenthusiasm. In this paper, we propose a \\textbf{P}erson\\textbf{A}lized\n\\textbf{C}onversational tutoring ag\\textbf{E}nt (PACE) for mathematics\ninstruction. PACE simulates students' learning styles based on the Felder and\nSilverman learning style model, aligning with each student's persona. In this\nway, our PACE can effectively assess the personality of students, allowing to\ndevelop individualized teaching strategies that resonate with their unique\nlearning styles. To further enhance students' comprehension, PACE employs the\nSocratic teaching method to provide instant feedback and encourage deep\nthinking. By constructing personalized teaching data and training models, PACE\ndemonstrates the ability to identify and adapt to the unique needs of each\nstudent, significantly improving the overall learning experience and outcomes.\nMoreover, we establish multi-aspect evaluation criteria and conduct extensive\nanalysis to assess the performance of personalized teaching. Experimental\nresults demonstrate the superiority of our model in personalizing the\neducational experience and motivating students compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been increasingly employed in various\nintelligent educational systems, simulating human tutors to facilitate\neffective human-machine interaction. However, previous studies often overlook\nthe significance of recognizing and adapting to individual learner\ncharacteristics. Such adaptation is crucial for enhancing student engagement\nand learning efficiency, particularly in mathematics instruction, where diverse\nlearning styles require personalized strategies to promote comprehension and\nenthusiasm. In this paper, we propose a \\textbf{P}erson\\textbf{A}lized\n\\textbf{C}onversational tutoring ag\\textbf{E}nt (PACE) for mathematics\ninstruction. PACE simulates students' learning styles based on the Felder and\nSilverman learning style model, aligning with each student's persona. In this\nway, our PACE can effectively assess the personality of students, allowing to\ndevelop individualized teaching strategies that resonate with their unique\nlearning styles. To further enhance students' comprehension, PACE employs the\nSocratic teaching method to provide instant feedback and encourage deep\nthinking. By constructing personalized teaching data and training models, PACE\ndemonstrates the ability to identify and adapt to the unique needs of each\nstudent, significantly improving the overall learning experience and outcomes.\nMoreover, we establish multi-aspect evaluation criteria and conduct extensive\nanalysis to assess the performance of personalized teaching. Experimental\nresults demonstrate the superiority of our model in personalizing the\neducational experience and motivating students compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Ben Liu"
                    },
                    {
                        "name": "Jihan Zhang"
                    },
                    {
                        "name": "Fangquan Lin"
                    },
                    {
                        "name": "Xu Jia"
                    },
                    {
                        "name": "Min Peng"
                    }
                ],
                "author_detail": {
                    "name": "Min Peng"
                },
                "author": "Min Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12633v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12633v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09575v2",
                "updated": "2025-02-19T16:32:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    32,
                    42,
                    2,
                    50,
                    0
                ],
                "published": "2024-09-15T01:32:57Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    1,
                    32,
                    57,
                    6,
                    259,
                    0
                ],
                "title": "Traffic Scene Generation from Natural Language Description for\n  Autonomous Vehicles with Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic Scene Generation from Natural Language Description for\n  Autonomous Vehicles with Large Language Model"
                },
                "summary": "Text-to-scene generation typically limits environmental diversity by\ngenerating key scenarios along predetermined paths. To address these\nconstraints, we propose a novel text-to-traffic scene framework that leverages\na large language model (LLM) to autonomously generate diverse traffic scenarios\nfor the CARLA simulator based on natural language descriptions. Our pipeline\ncomprises several key stages: (1) Prompt Analysis, where natural language\ninputs are decomposed; (2) Road Retrieval, selecting optimal roads from a\ndatabase; (3) Agent Planning, detailing agent types and behaviors; (4) Road\nRanking, scoring roads to match scenario requirements; and (5) Scene\nGeneration, rendering the planned scenarios in the simulator. This framework\nsupports both routine and critical traffic scenarios, enhancing its\napplicability. We demonstrate that our approach not only diversifies agent\nplanning and road selection but also significantly reduces the average\ncollision rate from 8% to 3.5% in SafeBench. Additionally, our framework\nimproves narration and reasoning for driving captioning tasks. Our\ncontributions and resources are publicly available at\nhttps://basiclab.github.io/TTSG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-scene generation typically limits environmental diversity by\ngenerating key scenarios along predetermined paths. To address these\nconstraints, we propose a novel text-to-traffic scene framework that leverages\na large language model (LLM) to autonomously generate diverse traffic scenarios\nfor the CARLA simulator based on natural language descriptions. Our pipeline\ncomprises several key stages: (1) Prompt Analysis, where natural language\ninputs are decomposed; (2) Road Retrieval, selecting optimal roads from a\ndatabase; (3) Agent Planning, detailing agent types and behaviors; (4) Road\nRanking, scoring roads to match scenario requirements; and (5) Scene\nGeneration, rendering the planned scenarios in the simulator. This framework\nsupports both routine and critical traffic scenarios, enhancing its\napplicability. We demonstrate that our approach not only diversifies agent\nplanning and road selection but also significantly reduces the average\ncollision rate from 8% to 3.5% in SafeBench. Additionally, our framework\nimproves narration and reasoning for driving captioning tasks. Our\ncontributions and resources are publicly available at\nhttps://basiclab.github.io/TTSG."
                },
                "authors": [
                    {
                        "name": "Bo-Kai Ruan"
                    },
                    {
                        "name": "Hao-Tang Tsui"
                    },
                    {
                        "name": "Yung-Hui Li"
                    },
                    {
                        "name": "Hong-Han Shuai"
                    }
                ],
                "author_detail": {
                    "name": "Hong-Han Shuai"
                },
                "author": "Hong-Han Shuai",
                "arxiv_comment": "update to the newest version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20727v2",
                "updated": "2025-02-19T16:26:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    26,
                    44,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-28T04:47:39Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    4,
                    47,
                    39,
                    0,
                    302,
                    0
                ],
                "title": "Faster WIND: Accelerating Iterative Best-of-$N$ Distillation for LLM\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster WIND: Accelerating Iterative Best-of-$N$ Distillation for LLM\n  Alignment"
                },
                "summary": "Recent advances in aligning large language models with human preferences have\ncorroborated the growing importance of best-of-N distillation (BOND). However,\nthe iterative BOND algorithm is prohibitively expensive in practice due to the\nsample and computation inefficiency. This paper addresses the problem by\nrevealing a unified game-theoretic connection between iterative BOND and\nself-play alignment, which unifies seemingly disparate algorithmic paradigms.\nBased on the connection, we establish a novel framework, WIN rate Dominance\n(WIND), with a series of efficient algorithms for regularized win rate\ndominance optimization that approximates iterative BOND in the parameter space.\nWe provides provable sample efficiency guarantee for one of the WIND variant\nwith the square loss objective. The experimental results confirm that our\nalgorithm not only accelerates the computation, but also achieves superior\nsample efficiency compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in aligning large language models with human preferences have\ncorroborated the growing importance of best-of-N distillation (BOND). However,\nthe iterative BOND algorithm is prohibitively expensive in practice due to the\nsample and computation inefficiency. This paper addresses the problem by\nrevealing a unified game-theoretic connection between iterative BOND and\nself-play alignment, which unifies seemingly disparate algorithmic paradigms.\nBased on the connection, we establish a novel framework, WIN rate Dominance\n(WIND), with a series of efficient algorithms for regularized win rate\ndominance optimization that approximates iterative BOND in the parameter space.\nWe provides provable sample efficiency guarantee for one of the WIND variant\nwith the square loss objective. The experimental results confirm that our\nalgorithm not only accelerates the computation, but also achieves superior\nsample efficiency compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Jincheng Mei"
                    },
                    {
                        "name": "Hanjun Dai"
                    },
                    {
                        "name": "Zixin Wen"
                    },
                    {
                        "name": "Shicong Cen"
                    },
                    {
                        "name": "Dale Schuurmans"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Bo Dai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Dai"
                },
                "author": "Bo Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09713v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09713v3",
                "updated": "2025-02-19T16:24:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    24,
                    30,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-13T03:45:24Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    3,
                    45,
                    24,
                    6,
                    287,
                    0
                ],
                "title": "Agentic Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Information Retrieval"
                },
                "summary": "Since the 1970s, information retrieval (IR) has long been defined as the\nprocess of acquiring relevant information items from a pre-defined corpus to\nsatisfy user information needs. Traditional IR systems, while effective in\ndomains like web search, are constrained by their reliance on static,\npre-defined information items. To this end, this paper introduces agentic\ninformation retrieval (Agentic IR), a transformative next-generation paradigm\nfor IR driven by large language models (LLMs) and AI agents. The central shift\nin agentic IR is the evolving definition of ``information'' from static,\npre-defined information items to dynamic, context-dependent information states.\nInformation state refers to a particular information context that the user is\nright in within a dynamic environment, encompassing not only the acquired\ninformation items but also real-time user preferences, contextual factors, and\ndecision-making processes. In such a way, traditional information retrieval,\nfocused on acquiring relevant information items based on user queries, can be\nnaturally extended to achieving the target information state given the user\ninstruction, which thereby defines the agentic information retrieval. We\nsystematically discuss agentic IR from various aspects, i.e., task formulation,\narchitecture, evaluation, case studies, as well as challenges and future\nprospects. We believe that the concept of agentic IR introduced in this paper\nnot only broadens the scope of information retrieval research but also lays the\nfoundation for a more adaptive, interactive, and intelligent next-generation IR\nparadigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the 1970s, information retrieval (IR) has long been defined as the\nprocess of acquiring relevant information items from a pre-defined corpus to\nsatisfy user information needs. Traditional IR systems, while effective in\ndomains like web search, are constrained by their reliance on static,\npre-defined information items. To this end, this paper introduces agentic\ninformation retrieval (Agentic IR), a transformative next-generation paradigm\nfor IR driven by large language models (LLMs) and AI agents. The central shift\nin agentic IR is the evolving definition of ``information'' from static,\npre-defined information items to dynamic, context-dependent information states.\nInformation state refers to a particular information context that the user is\nright in within a dynamic environment, encompassing not only the acquired\ninformation items but also real-time user preferences, contextual factors, and\ndecision-making processes. In such a way, traditional information retrieval,\nfocused on acquiring relevant information items based on user queries, can be\nnaturally extended to achieving the target information state given the user\ninstruction, which thereby defines the agentic information retrieval. We\nsystematically discuss agentic IR from various aspects, i.e., task formulation,\narchitecture, evaluation, case studies, as well as challenges and future\nprospects. We believe that the concept of agentic IR introduced in this paper\nnot only broadens the scope of information retrieval research but also lays the\nfoundation for a more adaptive, interactive, and intelligent next-generation IR\nparadigm."
                },
                "authors": [
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Junwei Liao"
                    },
                    {
                        "name": "Ning Li"
                    },
                    {
                        "name": "Kounianhua Du"
                    },
                    {
                        "name": "Jianghao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jianghao Lin"
                },
                "author": "Jianghao Lin",
                "arxiv_comment": "11 pages, perspective paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09713v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09713v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13845v1",
                "updated": "2025-02-19T16:08:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    8,
                    17,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:08:17Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    8,
                    17,
                    2,
                    50,
                    0
                ],
                "title": "Enhancing LLM-Based Recommendations Through Personalized Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-Based Recommendations Through Personalized Reasoning"
                },
                "summary": "Current recommendation systems powered by large language models (LLMs) often\nunderutilize their reasoning capabilities due to a lack of explicit logical\nstructuring. To address this limitation, we introduce CoT-Rec, a framework that\nintegrates Chain-of-Thought (CoT) reasoning into LLM-driven recommendations by\nincorporating two crucial processes: user preference analysis and item\nperception evaluation. CoT-Rec operates in two key phases: (1) personalized\ndata extraction, where user preferences and item perceptions are identified,\nand (2) personalized data application, where this information is leveraged to\nrefine recommendations. Our experimental analysis demonstrates that CoT-Rec\nimproves recommendation accuracy by making better use of LLMs' reasoning\npotential. The implementation is publicly available at\nhttps://anonymous.4open.science/r/CoT-Rec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current recommendation systems powered by large language models (LLMs) often\nunderutilize their reasoning capabilities due to a lack of explicit logical\nstructuring. To address this limitation, we introduce CoT-Rec, a framework that\nintegrates Chain-of-Thought (CoT) reasoning into LLM-driven recommendations by\nincorporating two crucial processes: user preference analysis and item\nperception evaluation. CoT-Rec operates in two key phases: (1) personalized\ndata extraction, where user preferences and item perceptions are identified,\nand (2) personalized data application, where this information is leveraged to\nrefine recommendations. Our experimental analysis demonstrates that CoT-Rec\nimproves recommendation accuracy by making better use of LLMs' reasoning\npotential. The implementation is publicly available at\nhttps://anonymous.4open.science/r/CoT-Rec."
                },
                "authors": [
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Xueshuo Yan"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Guangping Zhang"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Li Shang"
                    },
                    {
                        "name": "Ning Gu"
                    }
                ],
                "author_detail": {
                    "name": "Ning Gu"
                },
                "author": "Ning Gu",
                "arxiv_comment": "7 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13843v1",
                "updated": "2025-02-19T16:02:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    2,
                    59,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:02:59Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    2,
                    59,
                    2,
                    50,
                    0
                ],
                "title": "Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based\n  User Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based\n  User Agents"
                },
                "summary": "Large Language Model (LLM)-based user agents have emerged as a powerful tool\nfor improving recommender systems by simulating user interactions. However,\nexisting methods struggle with cross-domain scenarios due to inefficient memory\nstructures, leading to irrelevant information retention and failure to account\nfor social influence factors such as popularity. To address these limitations,\nwe introduce AgentCF++, a novel framework featuring a dual-layer memory\narchitecture and a two-step fusion mechanism to filter domain-specific\npreferences effectively. Additionally, we propose interest groups with shared\nmemory, allowing the model to capture the impact of popularity trends on users\nwith similar interests. Through extensive experiments on multiple cross-domain\ndatasets, AgentCF++ demonstrates superior performance over baseline models,\nhighlighting its effectiveness in refining user behavior simulation for\nrecommender systems. Our code is available at\nhttps://anonymous.4open.science/r/AgentCF-plus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based user agents have emerged as a powerful tool\nfor improving recommender systems by simulating user interactions. However,\nexisting methods struggle with cross-domain scenarios due to inefficient memory\nstructures, leading to irrelevant information retention and failure to account\nfor social influence factors such as popularity. To address these limitations,\nwe introduce AgentCF++, a novel framework featuring a dual-layer memory\narchitecture and a two-step fusion mechanism to filter domain-specific\npreferences effectively. Additionally, we propose interest groups with shared\nmemory, allowing the model to capture the impact of popularity trends on users\nwith similar interests. Through extensive experiments on multiple cross-domain\ndatasets, AgentCF++ demonstrates superior performance over baseline models,\nhighlighting its effectiveness in refining user behavior simulation for\nrecommender systems. Our code is available at\nhttps://anonymous.4open.science/r/AgentCF-plus."
                },
                "authors": [
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Shengkang Gu"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Guangping Zhang"
                    },
                    {
                        "name": "Mingzhe Han"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Li Shang"
                    },
                    {
                        "name": "Ning Gu"
                    }
                ],
                "author_detail": {
                    "name": "Ning Gu"
                },
                "author": "Ning Gu",
                "arxiv_comment": "6 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13842v1",
                "updated": "2025-02-19T16:02:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    2,
                    23,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:02:23Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    2,
                    23,
                    2,
                    50,
                    0
                ],
                "title": "Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster\n  Adaptive Internal Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster\n  Adaptive Internal Thinking"
                },
                "summary": "Large language models (LLMs) face inherent performance bottlenecks under\nparameter constraints, particularly in processing critical tokens that demand\ncomplex reasoning. Empirical analysis reveals challenging tokens induce abrupt\ngradient spikes across layers, exposing architectural stress points in standard\nTransformers. Building on this insight, we propose Inner Thinking Transformer\n(ITT), which reimagines layer computations as implicit thinking steps. ITT\ndynamically allocates computation through Adaptive Token Routing, iteratively\nrefines representations via Residual Thinking Connections, and distinguishes\nreasoning phases using Thinking Step Encoding. ITT enables deeper processing of\ncritical tokens without parameter expansion. Evaluations across 162M-466M\nparameter models show ITT achieves 96.5\\% performance of a 466M Transformer\nusing only 162M parameters, reduces training data by 43.2\\%, and outperforms\nTransformer/Loop variants in 11 benchmarks. By enabling elastic computation\nallocation during inference, ITT balances performance and efficiency through\narchitecture-aware optimization of implicit thinking pathways.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face inherent performance bottlenecks under\nparameter constraints, particularly in processing critical tokens that demand\ncomplex reasoning. Empirical analysis reveals challenging tokens induce abrupt\ngradient spikes across layers, exposing architectural stress points in standard\nTransformers. Building on this insight, we propose Inner Thinking Transformer\n(ITT), which reimagines layer computations as implicit thinking steps. ITT\ndynamically allocates computation through Adaptive Token Routing, iteratively\nrefines representations via Residual Thinking Connections, and distinguishes\nreasoning phases using Thinking Step Encoding. ITT enables deeper processing of\ncritical tokens without parameter expansion. Evaluations across 162M-466M\nparameter models show ITT achieves 96.5\\% performance of a 466M Transformer\nusing only 162M parameters, reduces training data by 43.2\\%, and outperforms\nTransformer/Loop variants in 11 benchmarks. By enabling elastic computation\nallocation during inference, ITT balances performance and efficiency through\narchitecture-aware optimization of implicit thinking pathways."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Yanxi Xie"
                    },
                    {
                        "name": "Jiawei Sheng"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Hua Wu"
                    },
                    {
                        "name": "Haifeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Wang"
                },
                "author": "Haifeng Wang",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13836v1",
                "updated": "2025-02-19T15:58:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    58,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T15:58:09Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    58,
                    9,
                    2,
                    50,
                    0
                ],
                "title": "Quantifying Memorization and Retriever Performance in\n  Retrieval-Augmented Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Memorization and Retriever Performance in\n  Retrieval-Augmented Vision-Language Models"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in question\nanswering (QA), but metrics for assessing their reliance on memorization versus\nretrieval remain underdeveloped. Moreover, while finetuned models are\nstate-of-the-art on closed-domain tasks, general-purpose models like GPT-4o\nexhibit strong zero-shot performance. This raises questions about the\ntrade-offs between memorization, generalization, and retrieval. In this work,\nwe analyze the extent to which multimodal retrieval-augmented VLMs memorize\ntraining data compared to baseline VLMs. Using the WebQA benchmark, we contrast\nfinetuned models with baseline VLMs on multihop retrieval and question\nanswering, examining the impact of finetuning on data memorization. To quantify\nmemorization in end-to-end retrieval and QA systems, we propose several proxy\nmetrics by investigating instances where QA succeeds despite retrieval failing.\nOur results reveal the extent to which finetuned models rely on memorization.\nIn contrast, retrieval-augmented VLMs have lower memorization scores, at the\ncost of accuracy (72% vs 52% on WebQA test set). As such, our measures pose a\nchallenge for future work to reconcile memorization and generalization in both\nOpen-Domain QA and joint Retrieval-QA tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable capabilities in question\nanswering (QA), but metrics for assessing their reliance on memorization versus\nretrieval remain underdeveloped. Moreover, while finetuned models are\nstate-of-the-art on closed-domain tasks, general-purpose models like GPT-4o\nexhibit strong zero-shot performance. This raises questions about the\ntrade-offs between memorization, generalization, and retrieval. In this work,\nwe analyze the extent to which multimodal retrieval-augmented VLMs memorize\ntraining data compared to baseline VLMs. Using the WebQA benchmark, we contrast\nfinetuned models with baseline VLMs on multihop retrieval and question\nanswering, examining the impact of finetuning on data memorization. To quantify\nmemorization in end-to-end retrieval and QA systems, we propose several proxy\nmetrics by investigating instances where QA succeeds despite retrieval failing.\nOur results reveal the extent to which finetuned models rely on memorization.\nIn contrast, retrieval-augmented VLMs have lower memorization scores, at the\ncost of accuracy (72% vs 52% on WebQA test set). As such, our measures pose a\nchallenge for future work to reconcile memorization and generalization in both\nOpen-Domain QA and joint Retrieval-QA tasks."
                },
                "authors": [
                    {
                        "name": "Peter Carragher"
                    },
                    {
                        "name": "Abhinand Jha"
                    },
                    {
                        "name": "R Raghav"
                    },
                    {
                        "name": "Kathleen M. Carley"
                    }
                ],
                "author_detail": {
                    "name": "Kathleen M. Carley"
                },
                "author": "Kathleen M. Carley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13834v1",
                "updated": "2025-02-19T15:54:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    54,
                    21,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T15:54:21Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    54,
                    21,
                    2,
                    50,
                    0
                ],
                "title": "Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning"
                },
                "summary": "Large language models (LLMs) can prove mathematical theorems formally by\ngenerating proof steps (\\textit{a.k.a.} tactics) within a proof system.\nHowever, the space of possible tactics is vast and complex, while the available\ntraining data for formal proofs is limited, posing a significant challenge to\nLLM-based tactic generation. To address this, we introduce a neuro-symbolic\ntactic generator that synergizes the mathematical intuition learned by LLMs\nwith domain-specific insights encoded by symbolic methods. The key aspect of\nthis integration is identifying which parts of mathematical reasoning are best\nsuited to LLMs and which to symbolic methods. While the high-level idea of\nneuro-symbolic integration is broadly applicable to various mathematical\nproblems, in this paper, we focus specifically on Olympiad inequalities\n(Figure~1). We analyze how humans solve these problems and distill the\ntechniques into two types of tactics: (1) scaling, handled by symbolic methods,\nand (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with\nLLMs to prune and rank the proof goals for efficient proof search. We evaluate\nour framework on 161 challenging inequalities from multiple mathematics\ncompetitions, achieving state-of-the-art performance and significantly\noutperforming existing LLM and symbolic approaches without requiring additional\ntraining data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can prove mathematical theorems formally by\ngenerating proof steps (\\textit{a.k.a.} tactics) within a proof system.\nHowever, the space of possible tactics is vast and complex, while the available\ntraining data for formal proofs is limited, posing a significant challenge to\nLLM-based tactic generation. To address this, we introduce a neuro-symbolic\ntactic generator that synergizes the mathematical intuition learned by LLMs\nwith domain-specific insights encoded by symbolic methods. The key aspect of\nthis integration is identifying which parts of mathematical reasoning are best\nsuited to LLMs and which to symbolic methods. While the high-level idea of\nneuro-symbolic integration is broadly applicable to various mathematical\nproblems, in this paper, we focus specifically on Olympiad inequalities\n(Figure~1). We analyze how humans solve these problems and distill the\ntechniques into two types of tactics: (1) scaling, handled by symbolic methods,\nand (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with\nLLMs to prune and rank the proof goals for efficient proof search. We evaluate\nour framework on 161 challenging inequalities from multiple mathematics\ncompetitions, achieving state-of-the-art performance and significantly\noutperforming existing LLM and symbolic approaches without requiring additional\ntraining data."
                },
                "authors": [
                    {
                        "name": "Zenan Li"
                    },
                    {
                        "name": "Zhaoyu Li"
                    },
                    {
                        "name": "Wen Tang"
                    },
                    {
                        "name": "Xian Zhang"
                    },
                    {
                        "name": "Yuan Yao"
                    },
                    {
                        "name": "Xujie Si"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Kaiyu Yang"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxing Ma"
                },
                "author": "Xiaoxing Ma",
                "arxiv_comment": "Published as a conference paper at ICLR 2025. Code is available at\n  https://github.com/Lizn-zn/NeqLIPS/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11054v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11054v3",
                "updated": "2025-02-19T15:36:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    36,
                    47,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-16T09:27:44Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    27,
                    44,
                    6,
                    47,
                    0
                ],
                "title": "Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on\n  Large Language Models"
                },
                "summary": "Multi-turn jailbreak attacks simulate real-world human interactions by\nengaging large language models (LLMs) in iterative dialogues, exposing critical\nsafety vulnerabilities. However, existing methods often struggle to balance\nsemantic coherence with attack effectiveness, resulting in either benign\nsemantic drift or ineffective detection evasion. To address this challenge, we\npropose Reasoning-Augmented Conversation, a novel multi-turn jailbreak\nframework that reformulates harmful queries into benign reasoning tasks and\nleverages LLMs' strong reasoning capabilities to compromise safety alignment.\nSpecifically, we introduce an attack state machine framework to systematically\nmodel problem translation and iterative reasoning, ensuring coherent query\ngeneration across multiple turns. Building on this framework, we design\ngain-guided exploration, self-play, and rejection feedback modules to preserve\nattack semantics, enhance effectiveness, and sustain reasoning-driven attack\nprogression. Extensive experiments on multiple LLMs demonstrate that RACE\nachieves state-of-the-art attack effectiveness in complex conversational\nscenarios, with attack success rates (ASRs) increasing by up to 96%. Notably,\nour approach achieves ASRs of 82% and 92% against leading commercial models,\nOpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at\nhttps://github.com/NY1024/RACE to facilitate further research in this critical\ndomain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn jailbreak attacks simulate real-world human interactions by\nengaging large language models (LLMs) in iterative dialogues, exposing critical\nsafety vulnerabilities. However, existing methods often struggle to balance\nsemantic coherence with attack effectiveness, resulting in either benign\nsemantic drift or ineffective detection evasion. To address this challenge, we\npropose Reasoning-Augmented Conversation, a novel multi-turn jailbreak\nframework that reformulates harmful queries into benign reasoning tasks and\nleverages LLMs' strong reasoning capabilities to compromise safety alignment.\nSpecifically, we introduce an attack state machine framework to systematically\nmodel problem translation and iterative reasoning, ensuring coherent query\ngeneration across multiple turns. Building on this framework, we design\ngain-guided exploration, self-play, and rejection feedback modules to preserve\nattack semantics, enhance effectiveness, and sustain reasoning-driven attack\nprogression. Extensive experiments on multiple LLMs demonstrate that RACE\nachieves state-of-the-art attack effectiveness in complex conversational\nscenarios, with attack success rates (ASRs) increasing by up to 96%. Notably,\nour approach achieves ASRs of 82% and 92% against leading commercial models,\nOpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at\nhttps://github.com/NY1024/RACE to facilitate further research in this critical\ndomain."
                },
                "authors": [
                    {
                        "name": "Zonghao Ying"
                    },
                    {
                        "name": "Deyue Zhang"
                    },
                    {
                        "name": "Zonglei Jing"
                    },
                    {
                        "name": "Yisong Xiao"
                    },
                    {
                        "name": "Quanchen Zou"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Xiangzheng Zhang"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11054v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11054v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12010v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12010v2",
                "updated": "2025-02-19T15:36:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    36,
                    26,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-15T19:21:14Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    19,
                    21,
                    14,
                    1,
                    289,
                    0
                ],
                "title": "Bias Similarity Across Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias Similarity Across Large Language Models"
                },
                "summary": "Bias in machine learning models, particularly in Large Language Models, is a\ncritical issue as these systems shape important societal decisions. While\nprevious studies have examined bias in individual LLMs, comparisons of bias\nacross models remain underexplored. To address this gap, we analyze 13 LLMs\nfrom five families, evaluating bias through output distribution across multiple\ndimensions using two datasets (4K and 1M questions). Our results show that\nfine-tuning has minimal impact on output distributions, and proprietary models\ntend to overly response as unknowns to minimize bias, compromising accuracy and\nutility. In addition, open-source models like Llama3-Chat and Gemma2-it\ndemonstrate fairness comparable to proprietary models like GPT-4, challenging\nthe assumption that larger, closed-source models are inherently less biased. We\nalso find that bias scores for disambiguated questions are more extreme,\nraising concerns about reverse discrimination. These findings highlight the\nneed for improved bias mitigation strategies and more comprehensive evaluation\nmetrics for fairness in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in machine learning models, particularly in Large Language Models, is a\ncritical issue as these systems shape important societal decisions. While\nprevious studies have examined bias in individual LLMs, comparisons of bias\nacross models remain underexplored. To address this gap, we analyze 13 LLMs\nfrom five families, evaluating bias through output distribution across multiple\ndimensions using two datasets (4K and 1M questions). Our results show that\nfine-tuning has minimal impact on output distributions, and proprietary models\ntend to overly response as unknowns to minimize bias, compromising accuracy and\nutility. In addition, open-source models like Llama3-Chat and Gemma2-it\ndemonstrate fairness comparable to proprietary models like GPT-4, challenging\nthe assumption that larger, closed-source models are inherently less biased. We\nalso find that bias scores for disambiguated questions are more extreme,\nraising concerns about reverse discrimination. These findings highlight the\nneed for improved bias mitigation strategies and more comprehensive evaluation\nmetrics for fairness in LLMs."
                },
                "authors": [
                    {
                        "name": "Hyejun Jeong"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Amir Houmansadr"
                    }
                ],
                "author_detail": {
                    "name": "Amir Houmansadr"
                },
                "author": "Amir Houmansadr",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12010v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12010v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13822v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13822v1",
                "updated": "2025-02-19T15:33:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    33,
                    55,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T15:33:55Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    33,
                    55,
                    2,
                    50,
                    0
                ],
                "title": "Uncertainty quantification for Markov chains with application to\n  temporal difference learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty quantification for Markov chains with application to\n  temporal difference learning"
                },
                "summary": "Markov chains are fundamental to statistical machine learning, underpinning\nkey methodologies such as Markov Chain Monte Carlo (MCMC) sampling and temporal\ndifference (TD) learning in reinforcement learning (RL). Given their widespread\nuse, it is crucial to establish rigorous probabilistic guarantees on their\nconvergence, uncertainty, and stability. In this work, we develop novel,\nhigh-dimensional concentration inequalities and Berry-Esseen bounds for vector-\nand matrix-valued functions of Markov chains, addressing key limitations in\nexisting theoretical tools for handling dependent data. We leverage these\nresults to analyze the TD learning algorithm, a widely used method for policy\nevaluation in RL. Our analysis yields a sharp high-probability consistency\nguarantee that matches the asymptotic variance up to logarithmic factors.\nFurthermore, we establish a $O(T^{-\\frac{1}{4}}\\log T)$ distributional\nconvergence rate for the Gaussian approximation of the TD estimator, measured\nin convex distance. These findings provide new insights into statistical\ninference for RL algorithms, bridging the gaps between classical stochastic\napproximation theory and modern reinforcement learning applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Markov chains are fundamental to statistical machine learning, underpinning\nkey methodologies such as Markov Chain Monte Carlo (MCMC) sampling and temporal\ndifference (TD) learning in reinforcement learning (RL). Given their widespread\nuse, it is crucial to establish rigorous probabilistic guarantees on their\nconvergence, uncertainty, and stability. In this work, we develop novel,\nhigh-dimensional concentration inequalities and Berry-Esseen bounds for vector-\nand matrix-valued functions of Markov chains, addressing key limitations in\nexisting theoretical tools for handling dependent data. We leverage these\nresults to analyze the TD learning algorithm, a widely used method for policy\nevaluation in RL. Our analysis yields a sharp high-probability consistency\nguarantee that matches the asymptotic variance up to logarithmic factors.\nFurthermore, we establish a $O(T^{-\\frac{1}{4}}\\log T)$ distributional\nconvergence rate for the Gaussian approximation of the TD estimator, measured\nin convex distance. These findings provide new insights into statistical\ninference for RL algorithms, bridging the gaps between classical stochastic\napproximation theory and modern reinforcement learning applications."
                },
                "authors": [
                    {
                        "name": "Weichen Wu"
                    },
                    {
                        "name": "Yuting Wei"
                    },
                    {
                        "name": "Alessandro Rinaldo"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Rinaldo"
                },
                "author": "Alessandro Rinaldo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13822v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14745v2",
                "updated": "2025-02-19T15:32:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    32,
                    29,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-17T16:59:46Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    16,
                    59,
                    46,
                    3,
                    291,
                    0
                ],
                "title": "Semi-supervised Fine-tuning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-supervised Fine-tuning for Large Language Models"
                },
                "summary": "Supervised fine-tuning (SFT) is crucial in adapting large language model\n(LLMs) to a specific domain or task. However, only a limited amount of labeled\ndata is available in practical applications, which poses a severe challenge for\nSFT in yielding satisfactory results. Therefore, a data-efficient framework\nthat can fully exploit labeled and unlabeled data for LLM fine-tuning is highly\nanticipated.Towards this end, we introduce a semi-supervised\nfine-tuning(SemiFT) task and a framework named SemiEvol for LLM alignment from\na propagate-and-select manner. For knowledge propagation, SemiEvol adopts a\nbi-level approach, propagating knowledge from labeled data to unlabeled data\nthrough both in-weight and in-context methods. For knowledge selection,\nSemiEvol incorporates a collaborative learning mechanism, selecting\nhigher-quality pseudo-response samples. We conducted experiments using\nGPT-4o-mini and Llama-3.1 on seven general or domain-specific datasets,\ndemonstrating significant improvements in model performance on target data.\nFurthermore, we compared SemiEvol with SFT and self-evolution methods,\nhighlighting its practicality in hybrid data scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) is crucial in adapting large language model\n(LLMs) to a specific domain or task. However, only a limited amount of labeled\ndata is available in practical applications, which poses a severe challenge for\nSFT in yielding satisfactory results. Therefore, a data-efficient framework\nthat can fully exploit labeled and unlabeled data for LLM fine-tuning is highly\nanticipated.Towards this end, we introduce a semi-supervised\nfine-tuning(SemiFT) task and a framework named SemiEvol for LLM alignment from\na propagate-and-select manner. For knowledge propagation, SemiEvol adopts a\nbi-level approach, propagating knowledge from labeled data to unlabeled data\nthrough both in-weight and in-context methods. For knowledge selection,\nSemiEvol incorporates a collaborative learning mechanism, selecting\nhigher-quality pseudo-response samples. We conducted experiments using\nGPT-4o-mini and Llama-3.1 on seven general or domain-specific datasets,\ndemonstrating significant improvements in model performance on target data.\nFurthermore, we compared SemiEvol with SFT and self-evolution methods,\nhighlighting its practicality in hybrid data scenarios."
                },
                "authors": [
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Zhiping Xiao"
                    },
                    {
                        "name": "Wei Ju"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "arxiv_comment": "Github Repo: https://github.com/luo-junyu/SemiEvol",
                "arxiv_journal_ref": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13820v1",
                "updated": "2025-02-19T15:32:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    32,
                    11,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T15:32:11Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    32,
                    11,
                    2,
                    50,
                    0
                ],
                "title": "Scoring Verifiers: Evaluating Synthetic Verification in Code and\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scoring Verifiers: Evaluating Synthetic Verification in Code and\n  Reasoning"
                },
                "summary": "Code verification has recently found great success as a critical component in\ntraining large scale reasoning models for coding. Synthetic techniques such as\nself-generated test cases and reward models provide a way to enhance code\ncapabilities beyond predefined tests. Building on these advancements, we\npropose new benchmarks designed to systematically evaluate the impact of\nsynthetic verification methods on assessing solution correctness. We introduce\nHE-R, HE-R+, MBPP-R, and MBPP-R+, which transform existing coding benchmarks\ninto scoring and ranking datasets to evaluate the effectiveness of synthetic\nverifiers. Using these benchmarks, we analyze synthetic verification methods in\nstandard, reasoning-based, and reward-based LLMs. Our results show that recent\nreasoning models significantly improve test case generation and that scaling\ntest cases enhances verification accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code verification has recently found great success as a critical component in\ntraining large scale reasoning models for coding. Synthetic techniques such as\nself-generated test cases and reward models provide a way to enhance code\ncapabilities beyond predefined tests. Building on these advancements, we\npropose new benchmarks designed to systematically evaluate the impact of\nsynthetic verification methods on assessing solution correctness. We introduce\nHE-R, HE-R+, MBPP-R, and MBPP-R+, which transform existing coding benchmarks\ninto scoring and ranking datasets to evaluate the effectiveness of synthetic\nverifiers. Using these benchmarks, we analyze synthetic verification methods in\nstandard, reasoning-based, and reward-based LLMs. Our results show that recent\nreasoning models significantly improve test case generation and that scaling\ntest cases enhances verification accuracy."
                },
                "authors": [
                    {
                        "name": "Aleksander Ficek"
                    },
                    {
                        "name": "Somshubra Majumdar"
                    },
                    {
                        "name": "Vahid Noroozi"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13818v1",
                "updated": "2025-02-19T15:31:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    31,
                    13,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T15:31:13Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    31,
                    13,
                    2,
                    50,
                    0
                ],
                "title": "Building Age Estimation: A New Multi-Modal Benchmark Dataset and\n  Community Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Age Estimation: A New Multi-Modal Benchmark Dataset and\n  Community Challenge"
                },
                "summary": "Estimating the construction year of buildings is of great importance for\nsustainability. Sustainable buildings minimize energy consumption and are a key\npart of responsible and sustainable urban planning and development to\neffectively combat climate change. By using Artificial Intelligence (AI) and\nrecently proposed Transformer models, we are able to estimate the construction\nepoch of buildings from a multi-modal dataset. In this paper, we introduce a\nnew benchmark multi-modal dataset, i.e. the Map your City Dataset (MyCD),\ncontaining top-view Very High Resolution (VHR) images, Earth Observation (EO)\nmulti-spectral data from the Copernicus Sentinel-2 satellite constellation, and\nstreet-view images in many different cities in Europe, co-localized with\nrespect to the building under study and labelled with the construction epoch.\nWe assess EO generalization performance on new/ previously unseen cities that\nhave been held-out from training and appear only during inference. In this\nwork, we present the community-based data challenge we organized based on MyCD.\nThe ESA AI4EO Challenge MapYourCity was opened in 2024 for 4 months. Here, we\npresent the Top-4 performing models, and the main evaluation results. During\ninference, the performance of the models using both all three input modalities\nand only the two top-view modalities, i.e. without the street-view images, is\nexamined. The evaluation results show that the models are effective and can\nachieve good performance on this difficult real-world task of estimating the\nage of buildings, even on previously unseen cities, as well as even using only\nthe two top-view modalities (i.e. VHR and Sentinel-2) during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the construction year of buildings is of great importance for\nsustainability. Sustainable buildings minimize energy consumption and are a key\npart of responsible and sustainable urban planning and development to\neffectively combat climate change. By using Artificial Intelligence (AI) and\nrecently proposed Transformer models, we are able to estimate the construction\nepoch of buildings from a multi-modal dataset. In this paper, we introduce a\nnew benchmark multi-modal dataset, i.e. the Map your City Dataset (MyCD),\ncontaining top-view Very High Resolution (VHR) images, Earth Observation (EO)\nmulti-spectral data from the Copernicus Sentinel-2 satellite constellation, and\nstreet-view images in many different cities in Europe, co-localized with\nrespect to the building under study and labelled with the construction epoch.\nWe assess EO generalization performance on new/ previously unseen cities that\nhave been held-out from training and appear only during inference. In this\nwork, we present the community-based data challenge we organized based on MyCD.\nThe ESA AI4EO Challenge MapYourCity was opened in 2024 for 4 months. Here, we\npresent the Top-4 performing models, and the main evaluation results. During\ninference, the performance of the models using both all three input modalities\nand only the two top-view modalities, i.e. without the street-view images, is\nexamined. The evaluation results show that the models are effective and can\nachieve good performance on this difficult real-world task of estimating the\nage of buildings, even on previously unseen cities, as well as even using only\nthe two top-view modalities (i.e. VHR and Sentinel-2) during inference."
                },
                "authors": [
                    {
                        "name": "Nikolaos Dionelis"
                    },
                    {
                        "name": "Nicolas Longp"
                    },
                    {
                        "name": "Alessandra Feliciotti"
                    },
                    {
                        "name": "Mattia Marconcini"
                    },
                    {
                        "name": "Devis Peressutti"
                    },
                    {
                        "name": "Nika Oman Kadunc"
                    },
                    {
                        "name": "JaeWan Park"
                    },
                    {
                        "name": "Hagai Raja Sinulingga"
                    },
                    {
                        "name": "Steve Andreas Immanuel"
                    },
                    {
                        "name": "Ba Tran"
                    },
                    {
                        "name": "Caroline Arnold"
                    }
                ],
                "author_detail": {
                    "name": "Caroline Arnold"
                },
                "author": "Caroline Arnold",
                "arxiv_comment": "6 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01548v2",
                "updated": "2025-02-19T15:09:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    9,
                    47,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-03T17:30:06Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    17,
                    30,
                    6,
                    0,
                    34,
                    0
                ],
                "title": "Comment on \"Sequential validation of treatment heterogeneity\" and\n  \"Comment on generic machine learning inference on heterogeneous treatment\n  effects in randomized experiments\"",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comment on \"Sequential validation of treatment heterogeneity\" and\n  \"Comment on generic machine learning inference on heterogeneous treatment\n  effects in randomized experiments\""
                },
                "summary": "We warmly thank Kosuke Imai, Michael Lingzhi Li, and Stefan Wager for their\ngracious and insightful comments. We are particularly encouraged that both\npieces recognize the importance of the research agenda the lecture laid out,\nwhich we see as critical for applied researchers. It is also great to see that\nboth underscore the potential of the basic approach we propose - targeting\nsummary features of the CATE after proxy estimation with sample splitting. We\nare also happy that both papers push us (and the reader) to continue thinking\nabout the inference problem associated with sample splitting. We recognize that\nour current paper is only scratching the surface of this interesting agenda.\nOur proposal is certainly not the only option, and it is exciting that both\npapers provide and assess alternatives. Hopefully, this will generate even more\nwork in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We warmly thank Kosuke Imai, Michael Lingzhi Li, and Stefan Wager for their\ngracious and insightful comments. We are particularly encouraged that both\npieces recognize the importance of the research agenda the lecture laid out,\nwhich we see as critical for applied researchers. It is also great to see that\nboth underscore the potential of the basic approach we propose - targeting\nsummary features of the CATE after proxy estimation with sample splitting. We\nare also happy that both papers push us (and the reader) to continue thinking\nabout the inference problem associated with sample splitting. We recognize that\nour current paper is only scratching the surface of this interesting agenda.\nOur proposal is certainly not the only option, and it is exciting that both\npapers provide and assess alternatives. Hopefully, this will generate even more\nwork in this area."
                },
                "authors": [
                    {
                        "name": "Victor Chernozhukov"
                    },
                    {
                        "name": "Mert Demirer"
                    },
                    {
                        "name": "Esther Duflo"
                    },
                    {
                        "name": "Ivn Fernndez-Val"
                    }
                ],
                "author_detail": {
                    "name": "Ivn Fernndez-Val"
                },
                "author": "Ivn Fernndez-Val",
                "arxiv_comment": "5 pages, 2 tables, comment on arXiv:2405.05534 and arXiv:2502.06758",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62P20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07068v2",
                "updated": "2025-02-19T15:05:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    5,
                    39,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-10T21:59:27Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    21,
                    59,
                    27,
                    0,
                    41,
                    0
                ],
                "title": "Specializing Large Language Models to Simulate Survey Response\n  Distributions for Global Populations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specializing Large Language Models to Simulate Survey Response\n  Distributions for Global Populations"
                },
                "summary": "Large-scale surveys are essential tools for informing social science research\nand policy, but running surveys is costly and time-intensive. If we could\naccurately simulate group-level survey results, this would therefore be very\nvaluable to social science research. Prior work has explored the use of large\nlanguage models (LLMs) for simulating human behaviors, mostly through\nprompting. In this paper, we are the first to specialize LLMs for the task of\nsimulating survey response distributions. As a testbed, we use country-level\nresults from two global cultural surveys. We devise a fine-tuning method based\non first-token probabilities to minimize divergence between predicted and\nactual response distributions for a given question. Then, we show that this\nmethod substantially outperforms other methods and zero-shot classifiers, even\non unseen questions, countries, and a completely unseen survey. While even our\nbest models struggle with the task, especially on unseen questions, our results\ndemonstrate the benefits of specialization for simulation, which may accelerate\nprogress towards sufficiently accurate simulation in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale surveys are essential tools for informing social science research\nand policy, but running surveys is costly and time-intensive. If we could\naccurately simulate group-level survey results, this would therefore be very\nvaluable to social science research. Prior work has explored the use of large\nlanguage models (LLMs) for simulating human behaviors, mostly through\nprompting. In this paper, we are the first to specialize LLMs for the task of\nsimulating survey response distributions. As a testbed, we use country-level\nresults from two global cultural surveys. We devise a fine-tuning method based\non first-token probabilities to minimize divergence between predicted and\nactual response distributions for a given question. Then, we show that this\nmethod substantially outperforms other methods and zero-shot classifiers, even\non unseen questions, countries, and a completely unseen survey. While even our\nbest models struggle with the task, especially on unseen questions, our results\ndemonstrate the benefits of specialization for simulation, which may accelerate\nprogress towards sufficiently accurate simulation in the future."
                },
                "authors": [
                    {
                        "name": "Yong Cao"
                    },
                    {
                        "name": "Haijiang Liu"
                    },
                    {
                        "name": "Arnav Arora"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    },
                    {
                        "name": "Paul Rttger"
                    },
                    {
                        "name": "Daniel Hershcovich"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Hershcovich"
                },
                "author": "Daniel Hershcovich",
                "arxiv_comment": "15 pages, 9 figures, accepted to NAACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13794v1",
                "updated": "2025-02-19T14:58:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    58,
                    48,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:58:48Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    58,
                    48,
                    2,
                    50,
                    0
                ],
                "title": "LESA: Learnable LLM Layer Scaling-Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LESA: Learnable LLM Layer Scaling-Up"
                },
                "summary": "Training Large Language Models (LLMs) from scratch requires immense\ncomputational resources, making it prohibitively expensive. Model scaling-up\noffers a promising solution by leveraging the parameters of smaller models to\ncreate larger ones. However, existing depth scaling-up methods rely on\nempirical heuristic rules for layer duplication, which result in poorer\ninitialization and slower convergence during continual pre-training. We propose\n\\textbf{LESA}, a novel learnable method for depth scaling-up. By concatenating\nparameters from each layer and applying Singular Value Decomposition, we\nuncover latent patterns between layers, suggesting that inter-layer parameters\ncan be learned. LESA uses a neural network to predict the parameters inserted\nbetween adjacent layers, enabling better initialization and faster training.\nExperiments show that LESA outperforms existing baselines, achieving superior\nperformance with less than half the computational cost during continual\npre-training. Extensive analyses demonstrate its effectiveness across different\nmodel sizes and tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models (LLMs) from scratch requires immense\ncomputational resources, making it prohibitively expensive. Model scaling-up\noffers a promising solution by leveraging the parameters of smaller models to\ncreate larger ones. However, existing depth scaling-up methods rely on\nempirical heuristic rules for layer duplication, which result in poorer\ninitialization and slower convergence during continual pre-training. We propose\n\\textbf{LESA}, a novel learnable method for depth scaling-up. By concatenating\nparameters from each layer and applying Singular Value Decomposition, we\nuncover latent patterns between layers, suggesting that inter-layer parameters\ncan be learned. LESA uses a neural network to predict the parameters inserted\nbetween adjacent layers, enabling better initialization and faster training.\nExperiments show that LESA outperforms existing baselines, achieving superior\nperformance with less than half the computational cost during continual\npre-training. Extensive analyses demonstrate its effectiveness across different\nmodel sizes and tasks."
                },
                "authors": [
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13791v1",
                "updated": "2025-02-19T14:58:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    58,
                    4,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    58,
                    4,
                    2,
                    50,
                    0
                ],
                "title": "From Tools to Teammates: Evaluating LLMs in Multi-Session Coding\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Tools to Teammates: Evaluating LLMs in Multi-Session Coding\n  Interactions"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in working environments\nfor a wide range of tasks, excelling at solving individual problems in\nisolation. However, are they also able to effectively collaborate over\nlong-term interactions? To investigate this, we introduce MemoryCode, a\nsynthetic multi-session dataset designed to test LLMs' ability to track and\nexecute simple coding instructions amid irrelevant information, simulating a\nrealistic setting. While all the models we tested handle isolated instructions\nwell, even the performance of state-of-the-art models like GPT-4o deteriorates\nwhen instructions are spread across sessions. Our analysis suggests this is due\nto their failure to retrieve and integrate information over long instruction\nchains. Our results highlight a fundamental limitation of current LLMs,\nrestricting their ability to collaborate effectively in long interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in working environments\nfor a wide range of tasks, excelling at solving individual problems in\nisolation. However, are they also able to effectively collaborate over\nlong-term interactions? To investigate this, we introduce MemoryCode, a\nsynthetic multi-session dataset designed to test LLMs' ability to track and\nexecute simple coding instructions amid irrelevant information, simulating a\nrealistic setting. While all the models we tested handle isolated instructions\nwell, even the performance of state-of-the-art models like GPT-4o deteriorates\nwhen instructions are spread across sessions. Our analysis suggests this is due\nto their failure to retrieve and integrate information over long instruction\nchains. Our results highlight a fundamental limitation of current LLMs,\nrestricting their ability to collaborate effectively in long interactions."
                },
                "authors": [
                    {
                        "name": "Nathanal Carraz Rakotonirina"
                    },
                    {
                        "name": "Mohammed Hamdy"
                    },
                    {
                        "name": "Jon Ander Campos"
                    },
                    {
                        "name": "Lucas Weber"
                    },
                    {
                        "name": "Alberto Testoni"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    },
                    {
                        "name": "Sandro Pezzelle"
                    },
                    {
                        "name": "Marco Del Tredici"
                    }
                ],
                "author_detail": {
                    "name": "Marco Del Tredici"
                },
                "author": "Marco Del Tredici",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13790v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13790v1",
                "updated": "2025-02-19T14:57:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    57,
                    55,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:57:55Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    57,
                    55,
                    2,
                    50,
                    0
                ],
                "title": "A Zero-Inflated Poisson Latent Position Cluster Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Zero-Inflated Poisson Latent Position Cluster Model"
                },
                "summary": "The latent position network model (LPM) is a popular approach for the\nstatistical analysis of network data. A central aspect of this model is that it\nassigns nodes to random positions in a latent space, such that the probability\nof an interaction between each pair of individuals or nodes is determined by\ntheir distance in this latent space. A key feature of this model is that it\nallows one to visualize nuanced structures via the latent space representation.\nThe LPM can be further extended to the Latent Position Cluster Model (LPCM), to\naccommodate the clustering of nodes by assuming that the latent positions are\ndistributed following a finite mixture distribution. In this paper, we extend\nthe LPCM to accommodate missing network data and apply this to non-negative\ndiscrete weighted social networks. By treating missing data as ``unusual'' zero\ninteractions, we propose a combination of the LPCM with the zero-inflated\nPoisson distribution. Statistical inference is based on a novel partially\ncollapsed Markov chain Monte Carlo algorithm, where a\nMixture-of-Finite-Mixtures (MFM) model is adopted to automatically determine\nthe number of clusters and optimal group partitioning. Our algorithm features a\ntruncated absorb-eject move, which is a novel adaptation of an idea commonly\nused in collapsed samplers, within the context of MFMs. Another aspect of our\nwork is that we illustrate our results on 3-dimensional latent spaces,\nmaintaining clear visualizations while achieving more flexibility than\n2-dimensional models. The performance of this approach is illustrated via two\ncarefully designed simulation studies, as well as four different publicly\navailable real networks, where some interesting new perspectives are uncovered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The latent position network model (LPM) is a popular approach for the\nstatistical analysis of network data. A central aspect of this model is that it\nassigns nodes to random positions in a latent space, such that the probability\nof an interaction between each pair of individuals or nodes is determined by\ntheir distance in this latent space. A key feature of this model is that it\nallows one to visualize nuanced structures via the latent space representation.\nThe LPM can be further extended to the Latent Position Cluster Model (LPCM), to\naccommodate the clustering of nodes by assuming that the latent positions are\ndistributed following a finite mixture distribution. In this paper, we extend\nthe LPCM to accommodate missing network data and apply this to non-negative\ndiscrete weighted social networks. By treating missing data as ``unusual'' zero\ninteractions, we propose a combination of the LPCM with the zero-inflated\nPoisson distribution. Statistical inference is based on a novel partially\ncollapsed Markov chain Monte Carlo algorithm, where a\nMixture-of-Finite-Mixtures (MFM) model is adopted to automatically determine\nthe number of clusters and optimal group partitioning. Our algorithm features a\ntruncated absorb-eject move, which is a novel adaptation of an idea commonly\nused in collapsed samplers, within the context of MFMs. Another aspect of our\nwork is that we illustrate our results on 3-dimensional latent spaces,\nmaintaining clear visualizations while achieving more flexibility than\n2-dimensional models. The performance of this approach is illustrated via two\ncarefully designed simulation studies, as well as four different publicly\navailable real networks, where some interesting new perspectives are uncovered."
                },
                "authors": [
                    {
                        "name": "Chaoyi Lu"
                    },
                    {
                        "name": "Riccardo Rastelli"
                    },
                    {
                        "name": "Nial Friel"
                    }
                ],
                "author_detail": {
                    "name": "Nial Friel"
                },
                "author": "Nial Friel",
                "arxiv_comment": "37 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13790v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13790v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13789v1",
                "updated": "2025-02-19T14:57:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    57,
                    51,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:57:51Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    57,
                    51,
                    2,
                    50,
                    0
                ],
                "title": "From Correctness to Comprehension: AI Agents for Personalized Error\n  Diagnosis in Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Correctness to Comprehension: AI Agents for Personalized Error\n  Diagnosis in Education"
                },
                "summary": "Large Language Models (LLMs), such as GPT-4, have demonstrated impressive\nmathematical reasoning capabilities, achieving near-perfect performance on\nbenchmarks like GSM8K. However, their application in personalized education\nremains limited due to an overemphasis on correctness over error diagnosis and\nfeedback generation. Current models fail to provide meaningful insights into\nthe causes of student mistakes, limiting their utility in educational contexts.\nTo address these challenges, we present three key contributions. First, we\nintroduce \\textbf{MathCCS} (Mathematical Classification and Constructive\nSuggestions), a multi-modal benchmark designed for systematic error analysis\nand tailored feedback. MathCCS includes real-world problems, expert-annotated\nerror categories, and longitudinal student data. Evaluations of\nstate-of-the-art models, including \\textit{Qwen2-VL}, \\textit{LLaVA-OV},\n\\textit{Claude-3.5-Sonnet} and \\textit{GPT-4o}, reveal that none achieved\nclassification accuracy above 30\\% or generated high-quality suggestions\n(average scores below 4/10), highlighting a significant gap from human-level\nperformance. Second, we develop a sequential error analysis framework that\nleverages historical data to track trends and improve diagnostic precision.\nFinally, we propose a multi-agent collaborative framework that combines a Time\nSeries Agent for historical analysis and an MLLM Agent for real-time\nrefinement, enhancing error classification and feedback generation. Together,\nthese contributions provide a robust platform for advancing personalized\neducation, bridging the gap between current AI capabilities and the demands of\nreal-world teaching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT-4, have demonstrated impressive\nmathematical reasoning capabilities, achieving near-perfect performance on\nbenchmarks like GSM8K. However, their application in personalized education\nremains limited due to an overemphasis on correctness over error diagnosis and\nfeedback generation. Current models fail to provide meaningful insights into\nthe causes of student mistakes, limiting their utility in educational contexts.\nTo address these challenges, we present three key contributions. First, we\nintroduce \\textbf{MathCCS} (Mathematical Classification and Constructive\nSuggestions), a multi-modal benchmark designed for systematic error analysis\nand tailored feedback. MathCCS includes real-world problems, expert-annotated\nerror categories, and longitudinal student data. Evaluations of\nstate-of-the-art models, including \\textit{Qwen2-VL}, \\textit{LLaVA-OV},\n\\textit{Claude-3.5-Sonnet} and \\textit{GPT-4o}, reveal that none achieved\nclassification accuracy above 30\\% or generated high-quality suggestions\n(average scores below 4/10), highlighting a significant gap from human-level\nperformance. Second, we develop a sequential error analysis framework that\nleverages historical data to track trends and improve diagnostic precision.\nFinally, we propose a multi-agent collaborative framework that combines a Time\nSeries Agent for historical analysis and an MLLM Agent for real-time\nrefinement, enhancing error classification and feedback generation. Together,\nthese contributions provide a robust platform for advancing personalized\neducation, bridging the gap between current AI capabilities and the demands of\nreal-world teaching."
                },
                "authors": [
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Dingjie Song"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Tianlong Xu"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13783v1",
                "updated": "2025-02-19T14:48:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    48,
                    25,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:48:25Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    48,
                    25,
                    2,
                    50,
                    0
                ],
                "title": "Generative Large Recommendation Models: Emerging Trends in LLMs for\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Large Recommendation Models: Emerging Trends in LLMs for\n  Recommendation"
                },
                "summary": "In the era of information overload, recommendation systems play a pivotal\nrole in filtering data and delivering personalized content. Recent advancements\nin feature interaction and user behavior modeling have significantly enhanced\nthe recall and ranking processes of these systems. With the rise of large\nlanguage models (LLMs), new opportunities have emerged to further improve\nrecommendation systems. This tutorial explores two primary approaches for\nintegrating LLMs: LLMs-enhanced recommendations, which leverage the reasoning\ncapabilities of general LLMs, and generative large recommendation models, which\nfocus on scaling and sophistication. While the former has been extensively\ncovered in existing literature, the latter remains underexplored. This tutorial\naims to fill this gap by providing a comprehensive overview of generative large\nrecommendation models, including their recent advancements, challenges, and\npotential research directions. Key topics include data quality, scaling laws,\nuser behavior mining, and efficiency in training and inference. By engaging\nwith this tutorial, participants will gain insights into the latest\ndevelopments and future opportunities in the field, aiding both academic\nresearch and practical applications. The timely nature of this exploration\nsupports the rapid evolution of recommendation systems, offering valuable\nguidance for researchers and practitioners alike.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of information overload, recommendation systems play a pivotal\nrole in filtering data and delivering personalized content. Recent advancements\nin feature interaction and user behavior modeling have significantly enhanced\nthe recall and ranking processes of these systems. With the rise of large\nlanguage models (LLMs), new opportunities have emerged to further improve\nrecommendation systems. This tutorial explores two primary approaches for\nintegrating LLMs: LLMs-enhanced recommendations, which leverage the reasoning\ncapabilities of general LLMs, and generative large recommendation models, which\nfocus on scaling and sophistication. While the former has been extensively\ncovered in existing literature, the latter remains underexplored. This tutorial\naims to fill this gap by providing a comprehensive overview of generative large\nrecommendation models, including their recent advancements, challenges, and\npotential research directions. Key topics include data quality, scaling laws,\nuser behavior mining, and efficiency in training and inference. By engaging\nwith this tutorial, participants will gain insights into the latest\ndevelopments and future opportunities in the field, aiding both academic\nresearch and practical applications. The timely nature of this exploration\nsupports the rapid evolution of recommendation systems, offering valuable\nguidance for researchers and practitioners alike."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Luankang Zhang"
                    },
                    {
                        "name": "Jin Yao Chin"
                    },
                    {
                        "name": "Yufei Ye"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "This paper has been accepted for the tutorial track at WWW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13780v1",
                "updated": "2025-02-19T14:45:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    45,
                    17,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:45:17Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    45,
                    17,
                    2,
                    50,
                    0
                ],
                "title": "Translation in the Hands of Many:Centering Lay Users in Machine\n  Translation Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translation in the Hands of Many:Centering Lay Users in Machine\n  Translation Interactions"
                },
                "summary": "Converging societal and technical factors have transformed language\ntechnologies into user-facing applications employed across languages. Machine\nTranslation (MT) has become a global tool, with cross-lingual services now also\nsupported by dialogue systems powered by multilingual Large Language Models\n(LLMs). This accessibility has expanded MT's reach to a vast base of lay users,\noften with little to no expertise in the languages or the technology itself.\nDespite this, the understanding of MT consumed by this diverse group of users\n-- their needs, experiences, and interactions with these systems -- remains\nlimited. This paper traces the shift in MT user profiles, focusing on\nnon-expert users and how their engagement with these systems may change with\nLLMs. We identify three key factors -- usability, trust, and literacy -- that\nshape these interactions and must be addressed to align MT with user needs. By\nexploring these dimensions, we offer insights to guide future MT with a\nuser-centered approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Converging societal and technical factors have transformed language\ntechnologies into user-facing applications employed across languages. Machine\nTranslation (MT) has become a global tool, with cross-lingual services now also\nsupported by dialogue systems powered by multilingual Large Language Models\n(LLMs). This accessibility has expanded MT's reach to a vast base of lay users,\noften with little to no expertise in the languages or the technology itself.\nDespite this, the understanding of MT consumed by this diverse group of users\n-- their needs, experiences, and interactions with these systems -- remains\nlimited. This paper traces the shift in MT user profiles, focusing on\nnon-expert users and how their engagement with these systems may change with\nLLMs. We identify three key factors -- usability, trust, and literacy -- that\nshape these interactions and must be addressed to align MT with user needs. By\nexploring these dimensions, we offer insights to guide future MT with a\nuser-centered approach."
                },
                "authors": [
                    {
                        "name": "Beatrice Savoldi"
                    },
                    {
                        "name": "Alan Ramponi"
                    },
                    {
                        "name": "Matteo Negri"
                    },
                    {
                        "name": "Luisa Bentivogli"
                    }
                ],
                "author_detail": {
                    "name": "Luisa Bentivogli"
                },
                "author": "Luisa Bentivogli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13776v1",
                "updated": "2025-02-19T14:39:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    39,
                    59,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:39:59Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    39,
                    59,
                    2,
                    50,
                    0
                ],
                "title": "EHOP: A Dataset of Everyday NP-Hard Optimization Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EHOP: A Dataset of Everyday NP-Hard Optimization Problems"
                },
                "summary": "We introduce the dataset of Everyday Hard Optimization Problems (EHOP), a\ncollection of NP-hard optimization problems expressed in natural language. EHOP\nincludes problem formulations that could be found in computer science\ntextbooks, versions that are dressed up as problems that could arise in real\nlife, and variants of well-known problems with inverted rules. We find that\nstate-of-the-art LLMs, across multiple prompting strategies, systematically\nsolve textbook problems more accurately than their real-life and inverted\ncounterparts. We argue that this constitutes evidence that LLMs adapt solutions\nseen during training, rather than leveraging reasoning abilities that would\nenable them to generalize to novel problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the dataset of Everyday Hard Optimization Problems (EHOP), a\ncollection of NP-hard optimization problems expressed in natural language. EHOP\nincludes problem formulations that could be found in computer science\ntextbooks, versions that are dressed up as problems that could arise in real\nlife, and variants of well-known problems with inverted rules. We find that\nstate-of-the-art LLMs, across multiple prompting strategies, systematically\nsolve textbook problems more accurately than their real-life and inverted\ncounterparts. We argue that this constitutes evidence that LLMs adapt solutions\nseen during training, rather than leveraging reasoning abilities that would\nenable them to generalize to novel problems."
                },
                "authors": [
                    {
                        "name": "Alex Duchnowski"
                    },
                    {
                        "name": "Ellie Pavlick"
                    },
                    {
                        "name": "Alexander Koller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Koller"
                },
                "author": "Alexander Koller",
                "arxiv_comment": "18 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13775v1",
                "updated": "2025-02-19T14:38:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    38,
                    57,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:38:57Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    38,
                    57,
                    2,
                    50,
                    0
                ],
                "title": "VITAL: A New Dataset for Benchmarking Pluralistic Alignment in\n  Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VITAL: A New Dataset for Benchmarking Pluralistic Alignment in\n  Healthcare"
                },
                "summary": "Alignment techniques have become central to ensuring that Large Language\nModels (LLMs) generate outputs consistent with human values. However, existing\nalignment paradigms often model an averaged or monolithic preference, failing\nto account for the diversity of perspectives across cultures, demographics, and\ncommunities. This limitation is particularly critical in health-related\nscenarios, where plurality is essential due to the influence of culture,\nreligion, personal values, and conflicting opinions. Despite progress in\npluralistic alignment, no prior work has focused on health, likely due to the\nunavailability of publicly available datasets. To address this gap, we\nintroduce VITAL, a new benchmark dataset comprising 13.1K value-laden\nsituations and 5.4K multiple-choice questions focused on health, designed to\nassess and benchmark pluralistic alignment methodologies. Through extensive\nevaluation of eight LLMs of varying sizes, we demonstrate that existing\npluralistic alignment techniques fall short in effectively accommodating\ndiverse healthcare beliefs, underscoring the need for tailored AI alignment in\nspecific domains. This work highlights the limitations of current approaches\nand lays the groundwork for developing health-specific alignment solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment techniques have become central to ensuring that Large Language\nModels (LLMs) generate outputs consistent with human values. However, existing\nalignment paradigms often model an averaged or monolithic preference, failing\nto account for the diversity of perspectives across cultures, demographics, and\ncommunities. This limitation is particularly critical in health-related\nscenarios, where plurality is essential due to the influence of culture,\nreligion, personal values, and conflicting opinions. Despite progress in\npluralistic alignment, no prior work has focused on health, likely due to the\nunavailability of publicly available datasets. To address this gap, we\nintroduce VITAL, a new benchmark dataset comprising 13.1K value-laden\nsituations and 5.4K multiple-choice questions focused on health, designed to\nassess and benchmark pluralistic alignment methodologies. Through extensive\nevaluation of eight LLMs of varying sizes, we demonstrate that existing\npluralistic alignment techniques fall short in effectively accommodating\ndiverse healthcare beliefs, underscoring the need for tailored AI alignment in\nspecific domains. This work highlights the limitations of current approaches\nand lays the groundwork for developing health-specific alignment solutions."
                },
                "authors": [
                    {
                        "name": "Anudeex Shetty"
                    },
                    {
                        "name": "Amin Beheshti"
                    },
                    {
                        "name": "Mark Dras"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13767v1",
                "updated": "2025-02-19T14:28:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    28,
                    42,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:28:42Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    28,
                    42,
                    2,
                    50,
                    0
                ],
                "title": "AI Software Engineer: Programming with Trust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Software Engineer: Programming with Trust"
                },
                "summary": "Large Language Models (LLMs) have shown surprising proficiency in generating\ncode snippets, promising to automate large parts of software engineering via\nartificial intelligence (AI). We argue that successfully deploying AI software\nengineers requires a level of trust equal to or even greater than the trust\nestablished by human-driven software engineering practices. The recent trend\ntoward LLM agents offers a path toward integrating the power of LLMs to create\nnew code with the power of analysis tools to increase trust in the code. This\nopinion piece comments on whether LLM agents could dominate software\nengineering workflows in the future and whether the focus of programming will\nshift from programming at scale to programming with trust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown surprising proficiency in generating\ncode snippets, promising to automate large parts of software engineering via\nartificial intelligence (AI). We argue that successfully deploying AI software\nengineers requires a level of trust equal to or even greater than the trust\nestablished by human-driven software engineering practices. The recent trend\ntoward LLM agents offers a path toward integrating the power of LLMs to create\nnew code with the power of analysis tools to increase trust in the code. This\nopinion piece comments on whether LLM agents could dominate software\nengineering workflows in the future and whether the focus of programming will\nshift from programming at scale to programming with trust."
                },
                "authors": [
                    {
                        "name": "Abhik Roychoudhury"
                    },
                    {
                        "name": "Corina Pasareanu"
                    },
                    {
                        "name": "Michael Pradel"
                    },
                    {
                        "name": "Baishakhi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Baishakhi Ray"
                },
                "author": "Baishakhi Ray",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13766v1",
                "updated": "2025-02-19T14:27:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    27,
                    40,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:27:40Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    27,
                    40,
                    2,
                    50,
                    0
                ],
                "title": "GIMMICK -- Globally Inclusive Multimodal Multitask Cultural Knowledge\n  Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GIMMICK -- Globally Inclusive Multimodal Multitask Cultural Knowledge\n  Benchmarking"
                },
                "summary": "Large Vision-Language Models (LVLMs) have recently gained attention due to\ntheir distinctive performance and broad applicability. While it has been\npreviously shown that their efficacy in usage scenarios involving non-Western\ncontexts falls short, existing studies are limited in scope, covering just a\nnarrow range of cultures, focusing exclusively on a small number of cultural\naspects, or evaluating a limited selection of models on a single task only.\nTowards globally inclusive LVLM research, we introduce GIMMICK, an extensive\nmultimodal benchmark designed to assess a broad spectrum of cultural knowledge\nacross 144 countries representing six global macro-regions. GIMMICK comprises\nsix tasks built upon three new datasets that span 728 unique cultural events or\nfacets on which we evaluated 20 LVLMs and 11 LLMs, including five proprietary\nand 26 open-weight models of all sizes. We systematically examine (1) regional\ncultural biases, (2) the influence of model size, (3) input modalities, and (4)\nexternal cues. Our analyses reveal strong biases toward Western cultures across\nmodels and tasks and highlight strong correlations between model size and\nperformance, as well as the effectiveness of multimodal input and external\ngeographic cues. We further find that models have more knowledge of tangible\nthan intangible aspects (e.g., food vs. rituals) and that they excel in\nrecognizing broad cultural origins but struggle with a more nuanced\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have recently gained attention due to\ntheir distinctive performance and broad applicability. While it has been\npreviously shown that their efficacy in usage scenarios involving non-Western\ncontexts falls short, existing studies are limited in scope, covering just a\nnarrow range of cultures, focusing exclusively on a small number of cultural\naspects, or evaluating a limited selection of models on a single task only.\nTowards globally inclusive LVLM research, we introduce GIMMICK, an extensive\nmultimodal benchmark designed to assess a broad spectrum of cultural knowledge\nacross 144 countries representing six global macro-regions. GIMMICK comprises\nsix tasks built upon three new datasets that span 728 unique cultural events or\nfacets on which we evaluated 20 LVLMs and 11 LLMs, including five proprietary\nand 26 open-weight models of all sizes. We systematically examine (1) regional\ncultural biases, (2) the influence of model size, (3) input modalities, and (4)\nexternal cues. Our analyses reveal strong biases toward Western cultures across\nmodels and tasks and highlight strong correlations between model size and\nperformance, as well as the effectiveness of multimodal input and external\ngeographic cues. We further find that models have more knowledge of tangible\nthan intangible aspects (e.g., food vs. rituals) and that they excel in\nrecognizing broad cultural origins but struggle with a more nuanced\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Florian Schneider"
                    },
                    {
                        "name": "Carolin Holtermann"
                    },
                    {
                        "name": "Chris Biemann"
                    },
                    {
                        "name": "Anne Lauscher"
                    }
                ],
                "author_detail": {
                    "name": "Anne Lauscher"
                },
                "author": "Anne Lauscher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13759v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13759v1",
                "updated": "2025-02-19T14:21:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    21,
                    25,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:21:25Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    21,
                    25,
                    2,
                    50,
                    0
                ],
                "title": "Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and\n  Human-Like Reasoning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geolocation with Real Human Gameplay Data: A Large-Scale Dataset and\n  Human-Like Reasoning Framework"
                },
                "summary": "Geolocation, the task of identifying an image's location, requires complex\nreasoning and is crucial for navigation, monitoring, and cultural preservation.\nHowever, current methods often produce coarse, imprecise, and non-interpretable\nlocalization. A major challenge lies in the quality and scale of existing\ngeolocation datasets. These datasets are typically small-scale and\nautomatically constructed, leading to noisy data and inconsistent task\ndifficulty, with images that either reveal answers too easily or lack\nsufficient clues for reliable inference. To address these challenges, we\nintroduce a comprehensive geolocation framework with three key components:\nGeoComp, a large-scale dataset; GeoCoT, a novel reasoning method; and GeoEval,\nan evaluation metric, collectively designed to address critical challenges and\ndrive advancements in geolocation research. At the core of this framework is\nGeoComp (Geolocation Competition Dataset), a large-scale dataset collected from\na geolocation game platform involving 740K users over two years. It comprises\n25 million entries of metadata and 3 million geo-tagged locations spanning much\nof the globe, with each location annotated thousands to tens of thousands of\ntimes by human users. The dataset offers diverse difficulty levels for detailed\nanalysis and highlights key gaps in current models. Building on this dataset,\nwe propose Geographical Chain-of-Thought (GeoCoT), a novel multi-step reasoning\nframework designed to enhance the reasoning capabilities of Large Vision Models\n(LVMs) in geolocation tasks. GeoCoT improves performance by integrating\ncontextual and spatial cues through a multi-step process that mimics human\ngeolocation reasoning. Finally, using the GeoEval metric, we demonstrate that\nGeoCoT significantly boosts geolocation accuracy by up to 25% while enhancing\ninterpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geolocation, the task of identifying an image's location, requires complex\nreasoning and is crucial for navigation, monitoring, and cultural preservation.\nHowever, current methods often produce coarse, imprecise, and non-interpretable\nlocalization. A major challenge lies in the quality and scale of existing\ngeolocation datasets. These datasets are typically small-scale and\nautomatically constructed, leading to noisy data and inconsistent task\ndifficulty, with images that either reveal answers too easily or lack\nsufficient clues for reliable inference. To address these challenges, we\nintroduce a comprehensive geolocation framework with three key components:\nGeoComp, a large-scale dataset; GeoCoT, a novel reasoning method; and GeoEval,\nan evaluation metric, collectively designed to address critical challenges and\ndrive advancements in geolocation research. At the core of this framework is\nGeoComp (Geolocation Competition Dataset), a large-scale dataset collected from\na geolocation game platform involving 740K users over two years. It comprises\n25 million entries of metadata and 3 million geo-tagged locations spanning much\nof the globe, with each location annotated thousands to tens of thousands of\ntimes by human users. The dataset offers diverse difficulty levels for detailed\nanalysis and highlights key gaps in current models. Building on this dataset,\nwe propose Geographical Chain-of-Thought (GeoCoT), a novel multi-step reasoning\nframework designed to enhance the reasoning capabilities of Large Vision Models\n(LVMs) in geolocation tasks. GeoCoT improves performance by integrating\ncontextual and spatial cues through a multi-step process that mimics human\ngeolocation reasoning. Finally, using the GeoEval metric, we demonstrate that\nGeoCoT significantly boosts geolocation accuracy by up to 25% while enhancing\ninterpretability."
                },
                "authors": [
                    {
                        "name": "Zirui Song"
                    },
                    {
                        "name": "Jingpu Yang"
                    },
                    {
                        "name": "Yuan Huang"
                    },
                    {
                        "name": "Jonathan Tonglet"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Tao Cheng"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Iryna Gurevych"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "arxiv_comment": "Access dataset: https://huggingface.co/datasets/ShirohAO/tuxun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13759v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13759v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13754v1",
                "updated": "2025-02-19T14:16:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    16,
                    47,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:16:47Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    16,
                    47,
                    2,
                    50,
                    0
                ],
                "title": "Capturing Rich Behavior Representations: A Dynamic Action Semantic-Aware\n  Graph Transformer for Video Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capturing Rich Behavior Representations: A Dynamic Action Semantic-Aware\n  Graph Transformer for Video Captioning"
                },
                "summary": "Existing video captioning methods merely provide shallow or simplistic\nrepresentations of object behaviors, resulting in superficial and ambiguous\ndescriptions. However, object behavior is dynamic and complex. To\ncomprehensively capture the essence of object behavior, we propose a dynamic\naction semantic-aware graph transformer. Firstly, a multi-scale temporal\nmodeling module is designed to flexibly learn long and short-term latent action\nfeatures. It not only acquires latent action features across time scales, but\nalso considers local latent action details, enhancing the coherence and\nsensitiveness of latent action representations. Secondly, a visual-action\nsemantic aware module is proposed to adaptively capture semantic\nrepresentations related to object behavior, enhancing the richness and\naccurateness of action representations. By harnessing the collaborative efforts\nof these two modules,we can acquire rich behavior representations to generate\nhuman-like natural descriptions. Finally, this rich behavior representations\nand object representations are used to construct a temporal objects-action\ngraph, which is fed into the graph transformer to model the complex temporal\ndependencies between objects and actions. To avoid adding complexity in the\ninference phase, the behavioral knowledge of the objects will be distilled into\na simple network through knowledge distillation. The experimental results on\nMSVD and MSR-VTT datasets demonstrate that the proposed method achieves\nsignificant performance improvements across multiple metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing video captioning methods merely provide shallow or simplistic\nrepresentations of object behaviors, resulting in superficial and ambiguous\ndescriptions. However, object behavior is dynamic and complex. To\ncomprehensively capture the essence of object behavior, we propose a dynamic\naction semantic-aware graph transformer. Firstly, a multi-scale temporal\nmodeling module is designed to flexibly learn long and short-term latent action\nfeatures. It not only acquires latent action features across time scales, but\nalso considers local latent action details, enhancing the coherence and\nsensitiveness of latent action representations. Secondly, a visual-action\nsemantic aware module is proposed to adaptively capture semantic\nrepresentations related to object behavior, enhancing the richness and\naccurateness of action representations. By harnessing the collaborative efforts\nof these two modules,we can acquire rich behavior representations to generate\nhuman-like natural descriptions. Finally, this rich behavior representations\nand object representations are used to construct a temporal objects-action\ngraph, which is fed into the graph transformer to model the complex temporal\ndependencies between objects and actions. To avoid adding complexity in the\ninference phase, the behavioral knowledge of the objects will be distilled into\na simple network through knowledge distillation. The experimental results on\nMSVD and MSR-VTT datasets demonstrate that the proposed method achieves\nsignificant performance improvements across multiple metrics."
                },
                "authors": [
                    {
                        "name": "Caihua Liu"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Wenjing Xue"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Xia Feng"
                    }
                ],
                "author_detail": {
                    "name": "Xia Feng"
                },
                "author": "Xia Feng",
                "arxiv_comment": "5 pages, 3 figures, published ICASSP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13753v1",
                "updated": "2025-02-19T14:15:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    15,
                    49,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:15:49Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    15,
                    49,
                    2,
                    50,
                    0
                ],
                "title": "SCALAR: Scientific Citation-based Live Assessment of Long-context\n  Academic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCALAR: Scientific Citation-based Live Assessment of Long-context\n  Academic Reasoning"
                },
                "summary": "Evaluating large language models' (LLMs) long-context understanding\ncapabilities remains challenging. We present SCALAR (Scientific Citation-based\nLive Assessment of Long-context Academic Reasoning), a novel benchmark that\nleverages academic papers and their citation networks. SCALAR features\nautomatic generation of high-quality ground truth labels without human\nannotation, controllable difficulty levels, and a dynamic updating mechanism\nthat prevents data contamination. Using ICLR 2025 papers, we evaluate 8\nstate-of-the-art LLMs, revealing key insights about their capabilities and\nlimitations in processing long scientific documents across different context\nlengths and reasoning types. Our benchmark provides a reliable and sustainable\nway to track progress in long-context understanding as LLM capabilities evolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models' (LLMs) long-context understanding\ncapabilities remains challenging. We present SCALAR (Scientific Citation-based\nLive Assessment of Long-context Academic Reasoning), a novel benchmark that\nleverages academic papers and their citation networks. SCALAR features\nautomatic generation of high-quality ground truth labels without human\nannotation, controllable difficulty levels, and a dynamic updating mechanism\nthat prevents data contamination. Using ICLR 2025 papers, we evaluate 8\nstate-of-the-art LLMs, revealing key insights about their capabilities and\nlimitations in processing long scientific documents across different context\nlengths and reasoning types. Our benchmark provides a reliable and sustainable\nway to track progress in long-context understanding as LLM capabilities evolve."
                },
                "authors": [
                    {
                        "name": "Renxi Wang"
                    },
                    {
                        "name": "Honglin Mu"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Lizhi Lin"
                    },
                    {
                        "name": "Yunlong Feng"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Haonan Li"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Li"
                },
                "author": "Haonan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14766v2",
                "updated": "2025-02-19T14:11:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    11,
                    10,
                    2,
                    50,
                    0
                ],
                "published": "2024-05-23T16:33:18Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    16,
                    33,
                    18,
                    3,
                    144,
                    0
                ],
                "title": "Evaluating Large Language Models for Public Health Classification and\n  Extraction Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models for Public Health Classification and\n  Extraction Tasks"
                },
                "summary": "Advances in Large Language Models (LLMs) have led to significant interest in\ntheir potential to support human experts across a range of domains, including\npublic health. In this work we present automated evaluations of LLMs for public\nhealth tasks involving the classification and extraction of free text. We\ncombine six externally annotated datasets with seven new internally annotated\ndatasets to evaluate LLMs for processing text related to: health burden,\nepidemiological risk factors, and public health interventions. We evaluate\neleven open-weight LLMs (7-123 billion parameters) across all tasks using\nzero-shot in-context learning. We find that Llama-3.3-70B-Instruct is the\nhighest performing model, achieving the best results on 8/16 tasks (using\nmicro-F1 scores). We see significant variation across tasks with all\nopen-weight LLMs scoring below 60% micro-F1 on some challenging tasks, such as\nContact Classification, while all LLMs achieve greater than 80% micro-F1 on\nothers, such as GI Illness Classification. For a subset of 11 tasks, we also\nevaluate three GPT-4 and GPT-4o series models and find comparable results to\nLlama-3.3-70B-Instruct. Overall, based on these initial results we find\npromising signs that LLMs may be useful tools for public health experts to\nextract information from a wide variety of free text sources, and support\npublic health surveillance, research, and interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in Large Language Models (LLMs) have led to significant interest in\ntheir potential to support human experts across a range of domains, including\npublic health. In this work we present automated evaluations of LLMs for public\nhealth tasks involving the classification and extraction of free text. We\ncombine six externally annotated datasets with seven new internally annotated\ndatasets to evaluate LLMs for processing text related to: health burden,\nepidemiological risk factors, and public health interventions. We evaluate\neleven open-weight LLMs (7-123 billion parameters) across all tasks using\nzero-shot in-context learning. We find that Llama-3.3-70B-Instruct is the\nhighest performing model, achieving the best results on 8/16 tasks (using\nmicro-F1 scores). We see significant variation across tasks with all\nopen-weight LLMs scoring below 60% micro-F1 on some challenging tasks, such as\nContact Classification, while all LLMs achieve greater than 80% micro-F1 on\nothers, such as GI Illness Classification. For a subset of 11 tasks, we also\nevaluate three GPT-4 and GPT-4o series models and find comparable results to\nLlama-3.3-70B-Instruct. Overall, based on these initial results we find\npromising signs that LLMs may be useful tools for public health experts to\nextract information from a wide variety of free text sources, and support\npublic health surveillance, research, and interventions."
                },
                "authors": [
                    {
                        "name": "Joshua Harris"
                    },
                    {
                        "name": "Timothy Laurence"
                    },
                    {
                        "name": "Leo Loman"
                    },
                    {
                        "name": "Fan Grayson"
                    },
                    {
                        "name": "Toby Nonnenmacher"
                    },
                    {
                        "name": "Harry Long"
                    },
                    {
                        "name": "Loes WalsGriffith"
                    },
                    {
                        "name": "Amy Douglas"
                    },
                    {
                        "name": "Holly Fountain"
                    },
                    {
                        "name": "Stelios Georgiou"
                    },
                    {
                        "name": "Jo Hardstaff"
                    },
                    {
                        "name": "Kathryn Hopkins"
                    },
                    {
                        "name": "Y-Ling Chi"
                    },
                    {
                        "name": "Galena Kuyumdzhieva"
                    },
                    {
                        "name": "Lesley Larkin"
                    },
                    {
                        "name": "Samuel Collins"
                    },
                    {
                        "name": "Hamish Mohammed"
                    },
                    {
                        "name": "Thomas Finnie"
                    },
                    {
                        "name": "Luke Hounsome"
                    },
                    {
                        "name": "Michael Borowitz"
                    },
                    {
                        "name": "Steven Riley"
                    }
                ],
                "author_detail": {
                    "name": "Steven Riley"
                },
                "author": "Steven Riley",
                "arxiv_comment": "36 pages. Feedback and comments are highly appreciated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13747v1",
                "updated": "2025-02-19T14:10:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    10,
                    15,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:10:15Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    10,
                    15,
                    2,
                    50,
                    0
                ],
                "title": "Reverse Markov Learning: Multi-Step Generative Models for Complex\n  Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reverse Markov Learning: Multi-Step Generative Models for Complex\n  Distributions"
                },
                "summary": "Learning complex distributions is a fundamental challenge in contemporary\napplications. Generative models, such as diffusion models, have demonstrated\nremarkable success in overcoming many limitations of traditional statistical\nmethods. Shen and Meinshausen (2024) introduced engression, a generative\napproach based on scoring rules that maps noise (and covariates, if available)\ndirectly to data. While effective, engression struggles with highly complex\ndistributions, such as those encountered in image data. In this work, we extend\nengression to improve its capability in learning complex distributions. We\npropose a framework that defines a general forward process transitioning from\nthe target distribution to a known distribution (e.g., Gaussian) and then\nlearns a reverse Markov process using multiple engression models. This reverse\nprocess reconstructs the target distribution step by step. Our approach\nsupports general forward processes, allows for dimension reduction, and\nnaturally discretizes the generative process. As a special case, when using a\ndiffusion-based forward process, our framework offers a method to discretize\nthe training and inference of diffusion models efficiently. Empirical\nevaluations on simulated and climate data validate our theoretical insights,\ndemonstrating the effectiveness of our approach in capturing complex\ndistributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning complex distributions is a fundamental challenge in contemporary\napplications. Generative models, such as diffusion models, have demonstrated\nremarkable success in overcoming many limitations of traditional statistical\nmethods. Shen and Meinshausen (2024) introduced engression, a generative\napproach based on scoring rules that maps noise (and covariates, if available)\ndirectly to data. While effective, engression struggles with highly complex\ndistributions, such as those encountered in image data. In this work, we extend\nengression to improve its capability in learning complex distributions. We\npropose a framework that defines a general forward process transitioning from\nthe target distribution to a known distribution (e.g., Gaussian) and then\nlearns a reverse Markov process using multiple engression models. This reverse\nprocess reconstructs the target distribution step by step. Our approach\nsupports general forward processes, allows for dimension reduction, and\nnaturally discretizes the generative process. As a special case, when using a\ndiffusion-based forward process, our framework offers a method to discretize\nthe training and inference of diffusion models efficiently. Empirical\nevaluations on simulated and climate data validate our theoretical insights,\ndemonstrating the effectiveness of our approach in capturing complex\ndistributions."
                },
                "authors": [
                    {
                        "name": "Xinwei Shen"
                    },
                    {
                        "name": "Nicolai Meinshausen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13745v1",
                "updated": "2025-02-19T14:08:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    8,
                    43,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:08:43Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    8,
                    43,
                    2,
                    50,
                    0
                ],
                "title": "Hunting pre-stellar cores with APEX: Corona Australis 151, the densest\n  pre-stellar core or the youngest protostar?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hunting pre-stellar cores with APEX: Corona Australis 151, the densest\n  pre-stellar core or the youngest protostar?"
                },
                "summary": "Context. Pre-stellar cores are the birthplaces of Sun-like stars and\nrepresent the initial conditions for the assembly of protoplanetary systems.\nDue to their short lifespans, they are rare. In recent efforts to increase the\nnumber of such sources identified in the Solar neighbourhood, we have selected\na sample of 40 starless cores from the publicly available core catalogs of the\nHerschel Gould Belt survey. In this work, we focus on one of the sources that\nstands out for its high central density: Corona Australis 151. Aims. We use\nmolecular lines that trace dense gas (n>=10^6 cm-3) to confirm the\nexceptionally high density of this object, to study its physical structure, and\nto understand its evolutionary stage. Methods. We detected the N2H+ 3-2 and 5-4\ntransitions, and the N2D+ 3-2, 4-3, and 6-5 lines with the APEX telescope. We\nuse the Herschel continuum data to infer a spherically symmetric model of the\ncore's density and temperature. This is used as input to perform\nnon-local-thermodynamic-equilibrium radiative transfer to fit the observed five\nlines. Results. Our analysis confirms that this core is characterised by very\nhigh densities (a few x 10^7 cm-3 at the centre) and cold temperatures. We\ninfer a high deuteration level of N2D+/N2H+=0.50, indicative of an advanced\nevolutionary stage. In the large bandwidth covered by the APEX data, we detect\nseveral other deuterated species, including CHD2OH, D2CO, and ND3. We also\ndetect multiple sulphurated species that present broader lines with signs of\nhigh-velocity wings. Conclusions. The observation of high-velocity wings and\nthe fact that the linewidths of N2H+ and N2D+ become larger with increasing\nfrequency can be interpreted either as an indication of supersonic infall\nmotions developing in the central parts of a very evolved pre-stellar core or\nas the signature of outflows from a very low luminosity object (VeLLO).\n*SHORTENED*",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. Pre-stellar cores are the birthplaces of Sun-like stars and\nrepresent the initial conditions for the assembly of protoplanetary systems.\nDue to their short lifespans, they are rare. In recent efforts to increase the\nnumber of such sources identified in the Solar neighbourhood, we have selected\na sample of 40 starless cores from the publicly available core catalogs of the\nHerschel Gould Belt survey. In this work, we focus on one of the sources that\nstands out for its high central density: Corona Australis 151. Aims. We use\nmolecular lines that trace dense gas (n>=10^6 cm-3) to confirm the\nexceptionally high density of this object, to study its physical structure, and\nto understand its evolutionary stage. Methods. We detected the N2H+ 3-2 and 5-4\ntransitions, and the N2D+ 3-2, 4-3, and 6-5 lines with the APEX telescope. We\nuse the Herschel continuum data to infer a spherically symmetric model of the\ncore's density and temperature. This is used as input to perform\nnon-local-thermodynamic-equilibrium radiative transfer to fit the observed five\nlines. Results. Our analysis confirms that this core is characterised by very\nhigh densities (a few x 10^7 cm-3 at the centre) and cold temperatures. We\ninfer a high deuteration level of N2D+/N2H+=0.50, indicative of an advanced\nevolutionary stage. In the large bandwidth covered by the APEX data, we detect\nseveral other deuterated species, including CHD2OH, D2CO, and ND3. We also\ndetect multiple sulphurated species that present broader lines with signs of\nhigh-velocity wings. Conclusions. The observation of high-velocity wings and\nthe fact that the linewidths of N2H+ and N2D+ become larger with increasing\nfrequency can be interpreted either as an indication of supersonic infall\nmotions developing in the central parts of a very evolved pre-stellar core or\nas the signature of outflows from a very low luminosity object (VeLLO).\n*SHORTENED*"
                },
                "authors": [
                    {
                        "name": "E. Redaelli"
                    },
                    {
                        "name": "S. Spezzano"
                    },
                    {
                        "name": "P. Caselli"
                    },
                    {
                        "name": "J. Harju"
                    },
                    {
                        "name": "D. Arzoumanian"
                    },
                    {
                        "name": "O. Sipil"
                    },
                    {
                        "name": "A. Belloche"
                    },
                    {
                        "name": "F. Wyrowski"
                    },
                    {
                        "name": "J. E. Pineda"
                    }
                ],
                "author_detail": {
                    "name": "J. E. Pineda"
                },
                "author": "J. E. Pineda",
                "arxiv_comment": "Accepted for publication in A&A (19 Feb. 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13744v1",
                "updated": "2025-02-19T14:07:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    7,
                    37,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:07:37Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    7,
                    37,
                    2,
                    50,
                    0
                ],
                "title": "The Risk-Neutral Equivalent Pricing of Model-Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Risk-Neutral Equivalent Pricing of Model-Uncertainty"
                },
                "summary": "Existing approaches to asset-pricing under model-uncertainty adapt classical\nutility-maximisation frameworks and seek theoretical comprehensiveness. We move\ntoward practice by considering binary model-uncertainties and by switching\nattention from 'preference' to 'constraints'. Economic asset-pricing in this\nsetting is found to decompose naturally into the viable pricing of model-risk\nand of non-model risk separately such that the former has a unique and\nintuitive risk-neutral equivalent formulation with convenient properties. Its\nparameter, a dynamically conserved constant of model-risk inference, allows an\nintegrated representation of ex-ante risk-pricing and bias, such that their\nex-post price-effects can be disentangled, through well-known price anomalies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing approaches to asset-pricing under model-uncertainty adapt classical\nutility-maximisation frameworks and seek theoretical comprehensiveness. We move\ntoward practice by considering binary model-uncertainties and by switching\nattention from 'preference' to 'constraints'. Economic asset-pricing in this\nsetting is found to decompose naturally into the viable pricing of model-risk\nand of non-model risk separately such that the former has a unique and\nintuitive risk-neutral equivalent formulation with convenient properties. Its\nparameter, a dynamically conserved constant of model-risk inference, allows an\nintegrated representation of ex-ante risk-pricing and bias, such that their\nex-post price-effects can be disentangled, through well-known price anomalies."
                },
                "authors": [
                    {
                        "name": "Ken Kangda Wren"
                    }
                ],
                "author_detail": {
                    "name": "Ken Kangda Wren"
                },
                "author": "Ken Kangda Wren",
                "arxiv_comment": "25 pages of main text, 13 pages of Appendix and Bibliography",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.MF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.MF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13743v1",
                "updated": "2025-02-19T14:07:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    7,
                    34,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:07:34Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    7,
                    34,
                    2,
                    50,
                    0
                ],
                "title": "Inference of Abstraction for Grounded Predicate Logic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference of Abstraction for Grounded Predicate Logic"
                },
                "summary": "An important open question in AI is what simple and natural principle enables\na machine to reason logically for meaningful abstraction with grounded symbols.\nThis paper explores a conceptually new approach to combining probabilistic\nreasoning and predicative symbolic reasoning over data. We return to the era of\nreasoning with a full joint distribution before the advent of Bayesian\nnetworks. We then discuss that a full joint distribution over models of\nexponential size in propositional logic and of infinite size in predicate logic\nshould be simply derived from a full joint distribution over data of linear\nsize. We show that the same process is not only enough to generalise the\nlogical consequence relation of predicate logic but also to provide a new\nperspective to rethink well-known limitations such as the undecidability of\npredicate logic, the symbol grounding problem and the principle of explosion.\nThe reproducibility of this theoretical work is fully demonstrated by the\nincluded proofs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An important open question in AI is what simple and natural principle enables\na machine to reason logically for meaningful abstraction with grounded symbols.\nThis paper explores a conceptually new approach to combining probabilistic\nreasoning and predicative symbolic reasoning over data. We return to the era of\nreasoning with a full joint distribution before the advent of Bayesian\nnetworks. We then discuss that a full joint distribution over models of\nexponential size in propositional logic and of infinite size in predicate logic\nshould be simply derived from a full joint distribution over data of linear\nsize. We show that the same process is not only enough to generalise the\nlogical consequence relation of predicate logic but also to provide a new\nperspective to rethink well-known limitations such as the undecidability of\npredicate logic, the symbol grounding problem and the principle of explosion.\nThe reproducibility of this theoretical work is fully demonstrated by the\nincluded proofs."
                },
                "authors": [
                    {
                        "name": "Hiroyuki Kido"
                    }
                ],
                "author_detail": {
                    "name": "Hiroyuki Kido"
                },
                "author": "Hiroyuki Kido",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13740v1",
                "updated": "2025-02-19T14:05:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    5,
                    50,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:05:50Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    5,
                    50,
                    2,
                    50,
                    0
                ],
                "title": "Benchmarking of Different YOLO Models for CAPTCHAs Detection and\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking of Different YOLO Models for CAPTCHAs Detection and\n  Classification"
                },
                "summary": "This paper provides an analysis and comparison of the YOLOv5, YOLOv8 and\nYOLOv10 models for webpage CAPTCHAs detection using the datasets collected from\nthe web and darknet as well as synthetized data of webpages. The study examines\nthe nano (n), small (s), and medium (m) variants of YOLO architectures and use\nmetrics such as Precision, Recall, F1 score, mAP@50 and inference speed to\ndetermine the real-life utility. Additionally, the possibility of tuning the\ntrained model to detect new CAPTCHA patterns efficiently was examined as it is\na crucial part of real-life applications. The image slicing method was proposed\nas a way to improve the metrics of detection on oversized input images which\ncan be a common scenario in webpages analysis. Models in version nano achieved\nthe best results in terms of speed, while more complexed architectures scored\nbetter in terms of other metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an analysis and comparison of the YOLOv5, YOLOv8 and\nYOLOv10 models for webpage CAPTCHAs detection using the datasets collected from\nthe web and darknet as well as synthetized data of webpages. The study examines\nthe nano (n), small (s), and medium (m) variants of YOLO architectures and use\nmetrics such as Precision, Recall, F1 score, mAP@50 and inference speed to\ndetermine the real-life utility. Additionally, the possibility of tuning the\ntrained model to detect new CAPTCHA patterns efficiently was examined as it is\na crucial part of real-life applications. The image slicing method was proposed\nas a way to improve the metrics of detection on oversized input images which\ncan be a common scenario in webpages analysis. Models in version nano achieved\nthe best results in terms of speed, while more complexed architectures scored\nbetter in terms of other metrics."
                },
                "authors": [
                    {
                        "name": "Mikoaj Wysocki"
                    },
                    {
                        "name": "Henryk Gierszal"
                    },
                    {
                        "name": "Piotr Tyczka"
                    },
                    {
                        "name": "Sophia Karagiorgou"
                    },
                    {
                        "name": "George Pantelis"
                    }
                ],
                "author_detail": {
                    "name": "George Pantelis"
                },
                "author": "George Pantelis",
                "arxiv_journal_ref": "IEEE Big Data 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13738v1",
                "updated": "2025-02-19T14:04:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    4,
                    46,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:04:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    4,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "Enhancing Input-Label Mapping in In-Context Learning with Contrastive\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Input-Label Mapping in In-Context Learning with Contrastive\n  Decoding"
                },
                "summary": "Large language models (LLMs) excel at a range of tasks through in-context\nlearning (ICL), where only a few task examples guide their predictions.\nHowever, prior research highlights that LLMs often overlook input-label mapping\ninformation in ICL, relying more on their pre-trained knowledge. To address\nthis issue, we introduce In-Context Contrastive Decoding (ICCD), a novel method\nthat emphasizes input-label mapping by contrasting the output distributions\nbetween positive and negative in-context examples. Experiments on 7 natural\nlanguage understanding (NLU) tasks show that our ICCD method brings consistent\nand significant improvement (up to +2.1 improvement on average) upon 6\ndifferent scales of LLMs without requiring additional training. Our approach is\nversatile, enhancing performance with various demonstration selection methods,\ndemonstrating its broad applicability and effectiveness. The code and scripts\nwill be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at a range of tasks through in-context\nlearning (ICL), where only a few task examples guide their predictions.\nHowever, prior research highlights that LLMs often overlook input-label mapping\ninformation in ICL, relying more on their pre-trained knowledge. To address\nthis issue, we introduce In-Context Contrastive Decoding (ICCD), a novel method\nthat emphasizes input-label mapping by contrasting the output distributions\nbetween positive and negative in-context examples. Experiments on 7 natural\nlanguage understanding (NLU) tasks show that our ICCD method brings consistent\nand significant improvement (up to +2.1 improvement on average) upon 6\ndifferent scales of LLMs without requiring additional training. Our approach is\nversatile, enhancing performance with various demonstration selection methods,\ndemonstrating its broad applicability and effectiveness. The code and scripts\nwill be publicly released."
                },
                "authors": [
                    {
                        "name": "Keqin Peng"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Yuanxin Ouyang"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Yancheng Yuan"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13731v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13731v1",
                "updated": "2025-02-19T13:56:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    56,
                    20,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T13:56:20Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    56,
                    20,
                    2,
                    50,
                    0
                ],
                "title": "Robust Counterfactual Inference in Markov Decision Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Counterfactual Inference in Markov Decision Processes"
                },
                "summary": "This paper addresses a key limitation in existing counterfactual inference\nmethods for Markov Decision Processes (MDPs). Current approaches assume a\nspecific causal model to make counterfactuals identifiable. However, there are\nusually many causal models that align with the observational and interventional\ndistributions of an MDP, each yielding different counterfactual distributions,\nso fixing a particular causal model limits the validity (and usefulness) of\ncounterfactual inference. We propose a novel non-parametric approach that\ncomputes tight bounds on counterfactual transition probabilities across all\ncompatible causal models. Unlike previous methods that require solving\nprohibitively large optimisation problems (with variables that grow\nexponentially in the size of the MDP), our approach provides closed-form\nexpressions for these bounds, making computation highly efficient and scalable\nfor non-trivial MDPs. Once such an interval counterfactual MDP is constructed,\nour method identifies robust counterfactual policies that optimise the\nworst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluate\nour method on various case studies, demonstrating improved robustness over\nexisting methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses a key limitation in existing counterfactual inference\nmethods for Markov Decision Processes (MDPs). Current approaches assume a\nspecific causal model to make counterfactuals identifiable. However, there are\nusually many causal models that align with the observational and interventional\ndistributions of an MDP, each yielding different counterfactual distributions,\nso fixing a particular causal model limits the validity (and usefulness) of\ncounterfactual inference. We propose a novel non-parametric approach that\ncomputes tight bounds on counterfactual transition probabilities across all\ncompatible causal models. Unlike previous methods that require solving\nprohibitively large optimisation problems (with variables that grow\nexponentially in the size of the MDP), our approach provides closed-form\nexpressions for these bounds, making computation highly efficient and scalable\nfor non-trivial MDPs. Once such an interval counterfactual MDP is constructed,\nour method identifies robust counterfactual policies that optimise the\nworst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluate\nour method on various case studies, demonstrating improved robustness over\nexisting methods."
                },
                "authors": [
                    {
                        "name": "Jessica Lally"
                    },
                    {
                        "name": "Milad Kazemi"
                    },
                    {
                        "name": "Nicola Paoletti"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Paoletti"
                },
                "author": "Nicola Paoletti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13731v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13731v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13728v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13728v1",
                "updated": "2025-02-19T13:54:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    54,
                    44,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T13:54:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    54,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Secure Federated Data Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secure Federated Data Distillation"
                },
                "summary": "Dataset Distillation (DD) is a powerful technique for reducing large datasets\ninto compact, representative synthetic datasets, accelerating Machine Learning\ntraining. However, traditional DD methods operate in a centralized manner,\nwhich poses significant privacy threats and reduces its applicability. To\nmitigate these risks, we propose a Secure Federated Data Distillation framework\n(SFDD) to decentralize the distillation process while preserving privacy.Unlike\nexisting Federated Distillation techniques that focus on training global models\nwith distilled knowledge, our approach aims to produce a distilled dataset\nwithout exposing local contributions. We leverage the gradient-matching-based\ndistillation method, adapting it for a distributed setting where clients\ncontribute to the distillation process without sharing raw data. The central\naggregator iteratively refines a synthetic dataset by integrating client-side\nupdates while ensuring data confidentiality. To make our approach resilient to\ninference attacks perpetrated by the server that could exploit gradient updates\nto reconstruct private data, we create an optimized Local Differential Privacy\napproach, called LDPO-RLD (Label Differential Privacy Obfuscation via\nRandomized Linear Dispersion). Furthermore, we assess the framework's\nresilience against malicious clients executing backdoor attacks and demonstrate\nrobustness under the assumption of a sufficient number of participating\nclients. Our experimental results demonstrate the effectiveness of SFDD and\nthat the proposed defense concretely mitigates the identified vulnerabilities,\nwith minimal impact on the performance of the distilled dataset. By addressing\nthe interplay between privacy and federation in dataset distillation, this work\nadvances the field of privacy-preserving Machine Learning making our SFDD\nframework a viable solution for sensitive data-sharing applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dataset Distillation (DD) is a powerful technique for reducing large datasets\ninto compact, representative synthetic datasets, accelerating Machine Learning\ntraining. However, traditional DD methods operate in a centralized manner,\nwhich poses significant privacy threats and reduces its applicability. To\nmitigate these risks, we propose a Secure Federated Data Distillation framework\n(SFDD) to decentralize the distillation process while preserving privacy.Unlike\nexisting Federated Distillation techniques that focus on training global models\nwith distilled knowledge, our approach aims to produce a distilled dataset\nwithout exposing local contributions. We leverage the gradient-matching-based\ndistillation method, adapting it for a distributed setting where clients\ncontribute to the distillation process without sharing raw data. The central\naggregator iteratively refines a synthetic dataset by integrating client-side\nupdates while ensuring data confidentiality. To make our approach resilient to\ninference attacks perpetrated by the server that could exploit gradient updates\nto reconstruct private data, we create an optimized Local Differential Privacy\napproach, called LDPO-RLD (Label Differential Privacy Obfuscation via\nRandomized Linear Dispersion). Furthermore, we assess the framework's\nresilience against malicious clients executing backdoor attacks and demonstrate\nrobustness under the assumption of a sufficient number of participating\nclients. Our experimental results demonstrate the effectiveness of SFDD and\nthat the proposed defense concretely mitigates the identified vulnerabilities,\nwith minimal impact on the performance of the distilled dataset. By addressing\nthe interplay between privacy and federation in dataset distillation, this work\nadvances the field of privacy-preserving Machine Learning making our SFDD\nframework a viable solution for sensitive data-sharing applications."
                },
                "authors": [
                    {
                        "name": "Marco Arazzi"
                    },
                    {
                        "name": "Mert Cihangiroglu"
                    },
                    {
                        "name": "Serena Nicolazzo"
                    },
                    {
                        "name": "Antonino Nocera"
                    }
                ],
                "author_detail": {
                    "name": "Antonino Nocera"
                },
                "author": "Antonino Nocera",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13728v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13728v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13725v1",
                "updated": "2025-02-19T13:52:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    52,
                    26,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T13:52:26Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    52,
                    26,
                    2,
                    50,
                    0
                ],
                "title": "Adapting Large Language Models for Time Series Modeling via a Novel\n  Parameter-efficient Adaptation Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Large Language Models for Time Series Modeling via a Novel\n  Parameter-efficient Adaptation Method"
                },
                "summary": "Time series modeling holds significant importance in many real-world\napplications and has been extensively studied. While pre-trained foundation\nmodels have made impressive strides in the fields of natural language\nprocessing (NLP) and computer vision (CV), their development in time series\ndomains has been constrained by data sparsity. A series of recent studies have\ndemonstrated that large language models (LLMs) possess robust pattern\nrecognition and reasoning abilities over complex sequences of tokens. However,\nthe current literature have yet striked a high-quality balance between (a)\neffectively aligning the time series and natural language modalities, and (b)\nkeeping the inference efficiency. To address the above issues, we now propose\nthe Time-LlaMA framework. Time-LlaMA first converts the time series input into\ntoken embeddings through a linear tokenization mechanism. Second, the time\nseries token embeddings are aligned with the text prompts. Third, to further\nadapt the LLM backbone for time series modeling, we have developed a dynamic\nlow-rank adaptation technique (D-LoRA). D-LoRA dynamically chooses the most\nsuitable LoRA modules at each layer of the Transformer backbone for each time\nseries input, enhancing the model's predictive capabilities. Our experimental\nresults on an extensive collection of challenging real-world time series tasks\nconfirm that our proposed method achieves the state-of-the-art (SOTA)\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series modeling holds significant importance in many real-world\napplications and has been extensively studied. While pre-trained foundation\nmodels have made impressive strides in the fields of natural language\nprocessing (NLP) and computer vision (CV), their development in time series\ndomains has been constrained by data sparsity. A series of recent studies have\ndemonstrated that large language models (LLMs) possess robust pattern\nrecognition and reasoning abilities over complex sequences of tokens. However,\nthe current literature have yet striked a high-quality balance between (a)\neffectively aligning the time series and natural language modalities, and (b)\nkeeping the inference efficiency. To address the above issues, we now propose\nthe Time-LlaMA framework. Time-LlaMA first converts the time series input into\ntoken embeddings through a linear tokenization mechanism. Second, the time\nseries token embeddings are aligned with the text prompts. Third, to further\nadapt the LLM backbone for time series modeling, we have developed a dynamic\nlow-rank adaptation technique (D-LoRA). D-LoRA dynamically chooses the most\nsuitable LoRA modules at each layer of the Transformer backbone for each time\nseries input, enhancing the model's predictive capabilities. Our experimental\nresults on an extensive collection of challenging real-world time series tasks\nconfirm that our proposed method achieves the state-of-the-art (SOTA)\nperformance."
                },
                "authors": [
                    {
                        "name": "Juyuan Zhang"
                    },
                    {
                        "name": "Wei Zhu"
                    },
                    {
                        "name": "Jiechao Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jiechao Gao"
                },
                "author": "Jiechao Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13723v1",
                "updated": "2025-02-19T13:51:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    51,
                    5,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T13:51:05Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    51,
                    5,
                    2,
                    50,
                    0
                ],
                "title": "Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs\n  with Refined Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs\n  with Refined Values"
                },
                "summary": "We introduce Direct Value Optimization (DVO), an innovative reinforcement\nlearning framework for enhancing large language models in complex reasoning\ntasks. Unlike traditional methods relying on preference labels, DVO utilizes\nvalue signals at individual reasoning steps, optimizing models via a mean\nsquared error loss. The key benefit of DVO lies in its fine-grained\nsupervision, circumventing the need for labor-intensive human annotations.\nTarget values within the DVO are estimated using either Monte Carlo Tree Search\nor an outcome value model. Our empirical analysis on both mathematical and\ncommonsense reasoning tasks shows that DVO consistently outperforms existing\noffline preference optimization techniques, even with fewer training steps.\nThese findings underscore the importance of value signals in advancing\nreasoning capabilities and highlight DVO as a superior methodology under\nscenarios lacking explicit human preference information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Direct Value Optimization (DVO), an innovative reinforcement\nlearning framework for enhancing large language models in complex reasoning\ntasks. Unlike traditional methods relying on preference labels, DVO utilizes\nvalue signals at individual reasoning steps, optimizing models via a mean\nsquared error loss. The key benefit of DVO lies in its fine-grained\nsupervision, circumventing the need for labor-intensive human annotations.\nTarget values within the DVO are estimated using either Monte Carlo Tree Search\nor an outcome value model. Our empirical analysis on both mathematical and\ncommonsense reasoning tasks shows that DVO consistently outperforms existing\noffline preference optimization techniques, even with fewer training steps.\nThese findings underscore the importance of value signals in advancing\nreasoning capabilities and highlight DVO as a superior methodology under\nscenarios lacking explicit human preference information."
                },
                "authors": [
                    {
                        "name": "Hongbo Zhang"
                    },
                    {
                        "name": "Han Cui"
                    },
                    {
                        "name": "Guangsheng Bao"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13719v1",
                "updated": "2025-02-19T13:45:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    45,
                    27,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T13:45:27Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    45,
                    27,
                    2,
                    50,
                    0
                ],
                "title": "TrustRAG: An Information Assistant with Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrustRAG: An Information Assistant with Retrieval Augmented Generation"
                },
                "summary": "\\Ac{RAG} has emerged as a crucial technique for enhancing large models with\nreal-time and domain-specific knowledge. While numerous improvements and\nopen-source tools have been proposed to refine the \\ac{RAG} framework for\naccuracy, relatively little attention has been given to improving the\ntrustworthiness of generated results. To address this gap, we introduce\nTrustRAG, a novel framework that enhances \\ac{RAG} from three perspectives:\nindexing, retrieval, and generation. Specifically, in the indexing stage, we\npropose a semantic-enhanced chunking strategy that incorporates hierarchical\nindexing to supplement each chunk with contextual information, ensuring\nsemantic completeness. In the retrieval stage, we introduce a utility-based\nfiltering mechanism to identify high-quality information, supporting answer\ngeneration while reducing input length. In the generation stage, we propose\nfine-grained citation enhancement, which detects opinion-bearing sentences in\nresponses and infers citation relationships at the sentence-level, thereby\nimproving citation accuracy. We open-source the TrustRAG framework and provide\na demonstration studio designed for excerpt-based question answering tasks\n\\footnote{https://huggingface.co/spaces/golaxy/TrustRAG}. Based on these, we\naim to help researchers: 1) systematically enhancing the trustworthiness of\n\\ac{RAG} systems and (2) developing their own \\ac{RAG} systems with more\nreliable outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\\Ac{RAG} has emerged as a crucial technique for enhancing large models with\nreal-time and domain-specific knowledge. While numerous improvements and\nopen-source tools have been proposed to refine the \\ac{RAG} framework for\naccuracy, relatively little attention has been given to improving the\ntrustworthiness of generated results. To address this gap, we introduce\nTrustRAG, a novel framework that enhances \\ac{RAG} from three perspectives:\nindexing, retrieval, and generation. Specifically, in the indexing stage, we\npropose a semantic-enhanced chunking strategy that incorporates hierarchical\nindexing to supplement each chunk with contextual information, ensuring\nsemantic completeness. In the retrieval stage, we introduce a utility-based\nfiltering mechanism to identify high-quality information, supporting answer\ngeneration while reducing input length. In the generation stage, we propose\nfine-grained citation enhancement, which detects opinion-bearing sentences in\nresponses and infers citation relationships at the sentence-level, thereby\nimproving citation accuracy. We open-source the TrustRAG framework and provide\na demonstration studio designed for excerpt-based question answering tasks\n\\footnote{https://huggingface.co/spaces/golaxy/TrustRAG}. Based on these, we\naim to help researchers: 1) systematically enhancing the trustworthiness of\n\\ac{RAG} systems and (2) developing their own \\ac{RAG} systems with more\nreliable outputs."
                },
                "authors": [
                    {
                        "name": "Yixing Fan"
                    },
                    {
                        "name": "Qiang Yan"
                    },
                    {
                        "name": "Wenshan Wang"
                    },
                    {
                        "name": "Jiafeng Guo"
                    },
                    {
                        "name": "Ruqing Zhang"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13007v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13007v2",
                "updated": "2025-02-19T13:35:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    35,
                    48,
                    2,
                    50,
                    0
                ],
                "published": "2025-01-22T16:49:37Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    49,
                    37,
                    2,
                    22,
                    0
                ],
                "title": "PairJudge RM: Perform Best-of-N Sampling with Knockout Tournament",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PairJudge RM: Perform Best-of-N Sampling with Knockout Tournament"
                },
                "summary": "Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large\nLanguage Models (LLMs), relies on reward models to select the best candidate\nsolution from multiple generations. However, traditional reward models often\nassign arbitrary and inconsistent scores, limiting their effectiveness. To\naddress this, we propose a Pairwise Judge Reward Model (PariJudge RM) combined\nwith a knockout tournament for BoN sampling. Instead of assigning absolute\nscores, given one math problem, PariJudge RM judges two candidate solutions'\ncorrectness with chain-of-thought reasoning simultaneously. This approach\neliminates the need for scoring and enables cross-validation of solutions\nthrough parallel judgment. In the knockout tournament, PariJudge RM conducts\npairwise Judgment between candidate solutions and eliminates the incorrect ones\niteratively. We construct PairJudge-432K, a large-scale dataset of 432K\npairwise judgments derived from NumiaMath and annotated using\n\\texttt{gemini-1.5-flash}, and train the PariJudge RM via supervised\nfine-tuning. Experiments on MATH-500 and the Olympiad Bench demonstrate\nsignificant improvements over baseline reward models. And a 40\\% to 60\\%\nrelative improvement is achieved on the top 50\\% challenging problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large\nLanguage Models (LLMs), relies on reward models to select the best candidate\nsolution from multiple generations. However, traditional reward models often\nassign arbitrary and inconsistent scores, limiting their effectiveness. To\naddress this, we propose a Pairwise Judge Reward Model (PariJudge RM) combined\nwith a knockout tournament for BoN sampling. Instead of assigning absolute\nscores, given one math problem, PariJudge RM judges two candidate solutions'\ncorrectness with chain-of-thought reasoning simultaneously. This approach\neliminates the need for scoring and enables cross-validation of solutions\nthrough parallel judgment. In the knockout tournament, PariJudge RM conducts\npairwise Judgment between candidate solutions and eliminates the incorrect ones\niteratively. We construct PairJudge-432K, a large-scale dataset of 432K\npairwise judgments derived from NumiaMath and annotated using\n\\texttt{gemini-1.5-flash}, and train the PariJudge RM via supervised\nfine-tuning. Experiments on MATH-500 and the Olympiad Bench demonstrate\nsignificant improvements over baseline reward models. And a 40\\% to 60\\%\nrelative improvement is achieved on the top 50\\% challenging problems."
                },
                "authors": [
                    {
                        "name": "Yantao Liu"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Rui Min"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "in progress work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13007v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03035v3",
                "updated": "2025-02-19T13:11:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    11,
                    14,
                    2,
                    50,
                    0
                ],
                "published": "2025-01-06T14:23:02Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    23,
                    2,
                    0,
                    6,
                    0
                ],
                "title": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning"
                },
                "summary": "Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. Our results demonstrate that aggressive\nquantization methods like AWQ and GPTQ introduce up to 32.39% accuracy\ndegradation (average 11.31%) on Llama-3 models, particularly in numerical\ncomputation and reasoning planning. To address this, we introduce a\nmultidimensional evaluation framework combining qualitative capability analysis\nand quantitative error assessment. We further develop targeted recovery\nstrategies, showing that fine-tuning quantized models on only 545 task-specific\nexamples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to\nnear full-precision levels. Additionally, our error assessment pipeline\nachieves 98.9% accuracy in diagnosing and localizing errors across 3,366\nfailure cases, providing actionable insights for mitigating\nquantization-induced degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. Our results demonstrate that aggressive\nquantization methods like AWQ and GPTQ introduce up to 32.39% accuracy\ndegradation (average 11.31%) on Llama-3 models, particularly in numerical\ncomputation and reasoning planning. To address this, we introduce a\nmultidimensional evaluation framework combining qualitative capability analysis\nand quantitative error assessment. We further develop targeted recovery\nstrategies, showing that fine-tuning quantized models on only 545 task-specific\nexamples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to\nnear full-precision levels. Additionally, our error assessment pipeline\nachieves 98.9% accuracy in diagnosing and localizing errors across 3,366\nfailure cases, providing actionable insights for mitigating\nquantization-induced degradation."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Yupeng Su"
                    },
                    {
                        "name": "Runming Yang"
                    },
                    {
                        "name": "Congkai Xie"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Zhongwei Xie"
                    },
                    {
                        "name": "Ngai Wong"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06846v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06846v3",
                "updated": "2025-02-19T13:08:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    8,
                    42,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-09T13:06:43Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    13,
                    6,
                    43,
                    2,
                    283,
                    0
                ],
                "title": "Joint Fine-tuning and Conversion of Pretrained Speech and Language\n  Models towards Linear Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Fine-tuning and Conversion of Pretrained Speech and Language\n  Models towards Linear Complexity"
                },
                "summary": "Architectures such as Linformer and Mamba have recently emerged as\ncompetitive linear time replacements for transformers. However, corresponding\nlarge pretrained models are often unavailable, especially in non-text domains.\nTo remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)\napproach that jointly converts a transformer model to a linear time substitute\nand fine-tunes it to a target task. We also compare several means to guide the\nfine-tuning to optimally retain the desired inference capability from the\noriginal model. The methods differ in their use of the target model and the\ntrajectory of the parameters. In a series of empirical studies on language\nprocessing, language modeling, and speech processing, we show that CALD can\neffectively recover the result of the original model, and that the guiding\nstrategy contributes to the result. Some reasons for the variation are\nsuggested.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectures such as Linformer and Mamba have recently emerged as\ncompetitive linear time replacements for transformers. However, corresponding\nlarge pretrained models are often unavailable, especially in non-text domains.\nTo remedy this, we present a Cross-Architecture Layerwise Distillation (CALD)\napproach that jointly converts a transformer model to a linear time substitute\nand fine-tunes it to a target task. We also compare several means to guide the\nfine-tuning to optimally retain the desired inference capability from the\noriginal model. The methods differ in their use of the target model and the\ntrajectory of the parameters. In a series of empirical studies on language\nprocessing, language modeling, and speech processing, we show that CALD can\neffectively recover the result of the original model, and that the guiding\nstrategy contributes to the result. Some reasons for the variation are\nsuggested."
                },
                "authors": [
                    {
                        "name": "Mutian He"
                    },
                    {
                        "name": "Philip N. Garner"
                    }
                ],
                "author_detail": {
                    "name": "Philip N. Garner"
                },
                "author": "Philip N. Garner",
                "arxiv_comment": "17 pages, 5 figures; ICLR2025 camera ready. Code:\n  https://github.com/idiap/linearize-distill-pretrained-transformers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06846v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06846v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20127v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20127v2",
                "updated": "2025-02-19T13:08:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    8,
                    34,
                    2,
                    50,
                    0
                ],
                "published": "2024-12-28T12:11:28Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    12,
                    11,
                    28,
                    5,
                    363,
                    0
                ],
                "title": "M-MAD: Multidimensional Multi-Agent Debate for Advanced Machine\n  Translation Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-MAD: Multidimensional Multi-Agent Debate for Advanced Machine\n  Translation Evaluation"
                },
                "summary": "Recent advancements in large language models (LLMs) have given rise to the\nLLM-as-a-judge paradigm, showcasing their potential to deliver human-like\njudgments. However, in the field of machine translation (MT) evaluation,\ncurrent LLM-as-a-judge methods fall short of learned automatic metrics. In this\npaper, we propose Multidimensional Multi-Agent Debate (M-MAD), a systematic\nLLM-based multi-agent framework for advanced LLM-as-a-judge MT evaluation. Our\nfindings demonstrate that M-MAD achieves significant advancements by (1)\ndecoupling heuristic MQM criteria into distinct evaluation dimensions for\nfine-grained assessments; (2) employing multi-agent debates to harness the\ncollaborative reasoning capabilities of LLMs; (3) synthesizing\ndimension-specific results into a final evaluation judgment to ensure robust\nand reliable outcomes. Comprehensive experiments show that M-MAD not only\noutperforms all existing LLM-as-a-judge methods but also competes with\nstate-of-the-art reference-based automatic metrics, even when powered by a\nsuboptimal model like GPT-4o mini. Detailed ablations and analysis highlight\nthe superiority of our framework design, offering a fresh perspective for\nLLM-as-a-judge paradigm. Our code and data are publicly available at\nhttps://github.com/SU-JIAYUAN/M-MAD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have given rise to the\nLLM-as-a-judge paradigm, showcasing their potential to deliver human-like\njudgments. However, in the field of machine translation (MT) evaluation,\ncurrent LLM-as-a-judge methods fall short of learned automatic metrics. In this\npaper, we propose Multidimensional Multi-Agent Debate (M-MAD), a systematic\nLLM-based multi-agent framework for advanced LLM-as-a-judge MT evaluation. Our\nfindings demonstrate that M-MAD achieves significant advancements by (1)\ndecoupling heuristic MQM criteria into distinct evaluation dimensions for\nfine-grained assessments; (2) employing multi-agent debates to harness the\ncollaborative reasoning capabilities of LLMs; (3) synthesizing\ndimension-specific results into a final evaluation judgment to ensure robust\nand reliable outcomes. Comprehensive experiments show that M-MAD not only\noutperforms all existing LLM-as-a-judge methods but also competes with\nstate-of-the-art reference-based automatic metrics, even when powered by a\nsuboptimal model like GPT-4o mini. Detailed ablations and analysis highlight\nthe superiority of our framework design, offering a fresh perspective for\nLLM-as-a-judge paradigm. Our code and data are publicly available at\nhttps://github.com/SU-JIAYUAN/M-MAD."
                },
                "authors": [
                    {
                        "name": "Jiayuan Su"
                    },
                    {
                        "name": "Zhaopeng Feng"
                    },
                    {
                        "name": "Jiamei Zheng"
                    },
                    {
                        "name": "Jiahan Ren"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Hongwei Wang"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "Work in progress. Code and data are available at\n  https://github.com/SU-JIAYUAN/M-MAD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20127v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20127v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13691v1",
                "updated": "2025-02-19T13:03:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    3,
                    6,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T13:03:06Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    3,
                    6,
                    2,
                    50,
                    0
                ],
                "title": "Is This Collection Worth My LLM's Time? Automatically Measuring\n  Information Potential in Text Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is This Collection Worth My LLM's Time? Automatically Measuring\n  Information Potential in Text Corpora"
                },
                "summary": "As large language models (LLMs) converge towards similar capabilities, the\nkey to advancing their performance lies in identifying and incorporating\nvaluable new information sources. However, evaluating which text collections\nare worth the substantial investment required for digitization, preprocessing,\nand integration into LLM systems remains a significant challenge. We present a\nnovel approach to this challenge: an automated pipeline that evaluates the\npotential information gain from text collections without requiring model\ntraining or fine-tuning. Our method generates multiple choice questions (MCQs)\nfrom texts and measures an LLM's performance both with and without access to\nthe source material. The performance gap between these conditions serves as a\nproxy for the collection's information potential. We validate our approach\nusing three strategically selected datasets: EPFL PhD manuscripts (likely\ncontaining novel specialized knowledge), Wikipedia articles (presumably part of\ntraining data), and a synthetic baseline dataset. Our results demonstrate that\nthis method effectively identifies collections containing valuable novel\ninformation, providing a practical tool for prioritizing data acquisition and\nintegration efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) converge towards similar capabilities, the\nkey to advancing their performance lies in identifying and incorporating\nvaluable new information sources. However, evaluating which text collections\nare worth the substantial investment required for digitization, preprocessing,\nand integration into LLM systems remains a significant challenge. We present a\nnovel approach to this challenge: an automated pipeline that evaluates the\npotential information gain from text collections without requiring model\ntraining or fine-tuning. Our method generates multiple choice questions (MCQs)\nfrom texts and measures an LLM's performance both with and without access to\nthe source material. The performance gap between these conditions serves as a\nproxy for the collection's information potential. We validate our approach\nusing three strategically selected datasets: EPFL PhD manuscripts (likely\ncontaining novel specialized knowledge), Wikipedia articles (presumably part of\ntraining data), and a synthetic baseline dataset. Our results demonstrate that\nthis method effectively identifies collections containing valuable novel\ninformation, providing a practical tool for prioritizing data acquisition and\nintegration efforts."
                },
                "authors": [
                    {
                        "name": "Tristan Karch"
                    },
                    {
                        "name": "Luca Engel"
                    },
                    {
                        "name": "Philippe Schwaller"
                    },
                    {
                        "name": "Frdric Kaplan"
                    }
                ],
                "author_detail": {
                    "name": "Frdric Kaplan"
                },
                "author": "Frdric Kaplan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13686v1",
                "updated": "2025-02-19T12:54:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    54,
                    39,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T12:54:39Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    54,
                    39,
                    2,
                    50,
                    0
                ],
                "title": "Graph Signal Inference by Learning Narrowband Spectral Kernels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Signal Inference by Learning Narrowband Spectral Kernels"
                },
                "summary": "While a common assumption in graph signal analysis is the smoothness of the\nsignals or the band-limitedness of their spectrum, in many instances the\nspectrum of real graph data may be concentrated at multiple regions of the\nspectrum, possibly including mid-to-high-frequency components. In this work, we\npropose a novel graph signal model where the signal spectrum is represented\nthrough the combination of narrowband kernels in the graph frequency domain. We\nthen present an algorithm that jointly learns the model by optimizing the\nkernel parameters and the signal representation coefficients from a collection\nof graph signals. Our problem formulation has the flexibility of permitting the\nincorporation of signals possibly acquired on different graphs into the\nlearning algorithm. We then theoretically study the signal reconstruction\nperformance of the proposed method, by also elaborating on when joint learning\non multiple graphs is preferable to learning an individual model on each graph.\nExperimental results on several graph data sets shows that the proposed method\noffers quite satisfactory signal interpolation accuracy in comparison with a\nvariety of reference approaches in the literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While a common assumption in graph signal analysis is the smoothness of the\nsignals or the band-limitedness of their spectrum, in many instances the\nspectrum of real graph data may be concentrated at multiple regions of the\nspectrum, possibly including mid-to-high-frequency components. In this work, we\npropose a novel graph signal model where the signal spectrum is represented\nthrough the combination of narrowband kernels in the graph frequency domain. We\nthen present an algorithm that jointly learns the model by optimizing the\nkernel parameters and the signal representation coefficients from a collection\nof graph signals. Our problem formulation has the flexibility of permitting the\nincorporation of signals possibly acquired on different graphs into the\nlearning algorithm. We then theoretically study the signal reconstruction\nperformance of the proposed method, by also elaborating on when joint learning\non multiple graphs is preferable to learning an individual model on each graph.\nExperimental results on several graph data sets shows that the proposed method\noffers quite satisfactory signal interpolation accuracy in comparison with a\nvariety of reference approaches in the literature."
                },
                "authors": [
                    {
                        "name": "Osman Furkan Kar"
                    },
                    {
                        "name": "Glce Turhan"
                    },
                    {
                        "name": "Elif Vural"
                    }
                ],
                "author_detail": {
                    "name": "Elif Vural"
                },
                "author": "Elif Vural",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13685v1",
                "updated": "2025-02-19T12:53:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    53,
                    55,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T12:53:55Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    53,
                    55,
                    2,
                    50,
                    0
                ],
                "title": "MoM: Linear Sequence Modeling with Mixture-of-Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoM: Linear Sequence Modeling with Mixture-of-Memories"
                },
                "summary": "Linear sequence modeling methods, such as linear attention, state space\nmodeling, and linear RNNs, offer significant efficiency improvements by\nreducing the complexity of training and inference. However, these methods\ntypically compress the entire input sequence into a single fixed-size memory\nstate, which leads to suboptimal performance on recall-intensive downstream\ntasks. Drawing inspiration from neuroscience, particularly the brain's ability\nto maintain robust long-term memory while mitigating \"memory interference\", we\nintroduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes\nmultiple independent memory states, with a router network directing input\ntokens to specific memory states. This approach greatly enhances the overall\nmemory capacity while minimizing memory interference. As a result, MoM performs\nexceptionally well on recall-intensive tasks, surpassing existing linear\nsequence modeling techniques. Despite incorporating multiple memory states, the\ncomputation of each memory state remains linear in complexity, allowing MoM to\nretain the linear-complexity advantage during training, while\nconstant-complexity during inference. Our experimental results show that MoM\nsignificantly outperforms current linear sequence models on downstream language\ntasks, particularly recall-intensive tasks, and even achieves performance\ncomparable to Transformer models. The code is released at\nhttps://github.com/OpenSparseLLMs/MoM and is also released as a part of\nhttps://github.com/OpenSparseLLMs/Linear-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear sequence modeling methods, such as linear attention, state space\nmodeling, and linear RNNs, offer significant efficiency improvements by\nreducing the complexity of training and inference. However, these methods\ntypically compress the entire input sequence into a single fixed-size memory\nstate, which leads to suboptimal performance on recall-intensive downstream\ntasks. Drawing inspiration from neuroscience, particularly the brain's ability\nto maintain robust long-term memory while mitigating \"memory interference\", we\nintroduce a novel architecture called Mixture-of-Memories (MoM). MoM utilizes\nmultiple independent memory states, with a router network directing input\ntokens to specific memory states. This approach greatly enhances the overall\nmemory capacity while minimizing memory interference. As a result, MoM performs\nexceptionally well on recall-intensive tasks, surpassing existing linear\nsequence modeling techniques. Despite incorporating multiple memory states, the\ncomputation of each memory state remains linear in complexity, allowing MoM to\nretain the linear-complexity advantage during training, while\nconstant-complexity during inference. Our experimental results show that MoM\nsignificantly outperforms current linear sequence models on downstream language\ntasks, particularly recall-intensive tasks, and even achieves performance\ncomparable to Transformer models. The code is released at\nhttps://github.com/OpenSparseLLMs/MoM and is also released as a part of\nhttps://github.com/OpenSparseLLMs/Linear-MoE."
                },
                "authors": [
                    {
                        "name": "Jusen Du"
                    },
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Disen Lan"
                    },
                    {
                        "name": "Jiaxi Hu"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "Technical report, 14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13682v1",
                "updated": "2025-02-19T12:51:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    51,
                    46,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T12:51:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    51,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "Cross-Comparison of Sampling Algorithms for Pulse Profile Modeling of\n  PSR J0740+6620",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Comparison of Sampling Algorithms for Pulse Profile Modeling of\n  PSR J0740+6620"
                },
                "summary": "In the last few years, NICER data has enabled mass and radius inferences for\nvarious pulsars, and thus shed light on the equation of state for dense nuclear\nmatter. This is achieved through a technique called pulse profile modeling. The\nimportance of the results necessitates careful validation and testing of the\nrobustness of the inference procedure. In this paper, we investigate the effect\nof sampler choice for X-PSI (X-ray Pulse Simulation and Inference), an\nopen-source package for pulse profile modeling and Bayesian statistical\ninference that has been used extensively for analysis of NICER data. We focus\non the specific case of the high-mass pulsar PSR J0740+6620. Using synthetic\ndata that mimics the most recently analyzed NICER and XMM-Newton data sets of\nPSR J0740+6620, we evaluate the parameter recovery performance, convergence,\nand computational cost for MultiNest's multimodal nested sampling algorithm and\nUltraNest's slice nested sampling algorithm. We find that both samplers perform\nreliably, producing accurate and unbiased parameter estimation results when\nanalyzing simulated data. We also investigate the consequences for inference\nusing the real data for PSR J0740+6620, finding that both samplers produce\nconsistent credible intervals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the last few years, NICER data has enabled mass and radius inferences for\nvarious pulsars, and thus shed light on the equation of state for dense nuclear\nmatter. This is achieved through a technique called pulse profile modeling. The\nimportance of the results necessitates careful validation and testing of the\nrobustness of the inference procedure. In this paper, we investigate the effect\nof sampler choice for X-PSI (X-ray Pulse Simulation and Inference), an\nopen-source package for pulse profile modeling and Bayesian statistical\ninference that has been used extensively for analysis of NICER data. We focus\non the specific case of the high-mass pulsar PSR J0740+6620. Using synthetic\ndata that mimics the most recently analyzed NICER and XMM-Newton data sets of\nPSR J0740+6620, we evaluate the parameter recovery performance, convergence,\nand computational cost for MultiNest's multimodal nested sampling algorithm and\nUltraNest's slice nested sampling algorithm. We find that both samplers perform\nreliably, producing accurate and unbiased parameter estimation results when\nanalyzing simulated data. We also investigate the consequences for inference\nusing the real data for PSR J0740+6620, finding that both samplers produce\nconsistent credible intervals."
                },
                "authors": [
                    {
                        "name": "Mariska Hoogkamer"
                    },
                    {
                        "name": "Yves Kini"
                    },
                    {
                        "name": "Tuomo Salmi"
                    },
                    {
                        "name": "Anna L. Watts"
                    },
                    {
                        "name": "Johannes Buchner"
                    }
                ],
                "author_detail": {
                    "name": "Johannes Buchner"
                },
                "author": "Johannes Buchner",
                "arxiv_comment": "Submitted to PRD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13681v1",
                "updated": "2025-02-19T12:51:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    51,
                    35,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T12:51:35Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    51,
                    35,
                    2,
                    50,
                    0
                ],
                "title": "An LLM-based Agent for Reliable Docker Environment Configuration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-based Agent for Reliable Docker Environment Configuration"
                },
                "summary": "Environment configuration is a critical yet time-consuming step in software\ndevelopment, especially when dealing with unfamiliar code repositories. While\nLarge Language Models (LLMs) demonstrate the potential to accomplish software\nengineering tasks, existing methods for environment configuration often rely on\nmanual efforts or fragile scripts, leading to inefficiencies and unreliable\noutcomes. We introduce Repo2Run, the first LLM-based agent designed to fully\nautomate environment configuration and generate executable Dockerfiles for\narbitrary Python repositories. We address two major challenges: (1) enabling\nthe LLM agent to configure environments within isolated Docker containers, and\n(2) ensuring the successful configuration process is recorded and accurately\ntransferred to a Dockerfile without error. To achieve this, we propose atomic\nconfiguration synthesis, featuring a dual-environment architecture (internal\nand external environment) with a rollback mechanism to prevent environment\n\"pollution\" from failed commands, guaranteeing atomic execution (execute fully\nor not at all) and a Dockerfile generator to transfer successful configuration\nsteps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark\nof 420 recent Python repositories with unit tests, where it achieves an 86.0%\nsuccess rate, outperforming the best baseline by 63.9%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Environment configuration is a critical yet time-consuming step in software\ndevelopment, especially when dealing with unfamiliar code repositories. While\nLarge Language Models (LLMs) demonstrate the potential to accomplish software\nengineering tasks, existing methods for environment configuration often rely on\nmanual efforts or fragile scripts, leading to inefficiencies and unreliable\noutcomes. We introduce Repo2Run, the first LLM-based agent designed to fully\nautomate environment configuration and generate executable Dockerfiles for\narbitrary Python repositories. We address two major challenges: (1) enabling\nthe LLM agent to configure environments within isolated Docker containers, and\n(2) ensuring the successful configuration process is recorded and accurately\ntransferred to a Dockerfile without error. To achieve this, we propose atomic\nconfiguration synthesis, featuring a dual-environment architecture (internal\nand external environment) with a rollback mechanism to prevent environment\n\"pollution\" from failed commands, guaranteeing atomic execution (execute fully\nor not at all) and a Dockerfile generator to transfer successful configuration\nsteps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark\nof 420 recent Python repositories with unit tests, where it achieves an 86.0%\nsuccess rate, outperforming the best baseline by 63.9%."
                },
                "authors": [
                    {
                        "name": "Ruida Hu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Xinchen Wang"
                    },
                    {
                        "name": "Cuiyun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Cuiyun Gao"
                },
                "author": "Cuiyun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17003v4",
                "updated": "2025-02-19T12:35:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    35,
                    31,
                    2,
                    50,
                    0
                ],
                "published": "2024-08-30T04:35:59Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    4,
                    35,
                    59,
                    4,
                    243,
                    0
                ],
                "title": "Safety Layers in Aligned Large Language Models: The Key to LLM Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety Layers in Aligned Large Language Models: The Key to LLM Security"
                },
                "summary": "Aligned LLMs are secure, capable of recognizing and refusing to answer\nmalicious questions. However, the role of internal parameters in maintaining\nsuch security is not well understood yet, further these models can be\nvulnerable to security degradation when subjected to fine-tuning attacks. To\naddress these challenges, our work uncovers the mechanism behind security in\naligned LLMs at the parameter level, identifying a small set of contiguous\nlayers in the middle of the model that are crucial for distinguishing malicious\nqueries from normal ones, referred to as ``safety layers\". We first confirm the\nexistence of these safety layers by analyzing variations in input vectors\nwithin the model's internal layers. Additionally, we leverage the\nover-rejection phenomenon and parameters scaling analysis to precisely locate\nthe safety layers. Building on these findings, we propose a novel fine-tuning\napproach, Safely Partial-Parameter Fine-Tuning (SPPFT), that fixes the gradient\nof the safety layers during fine-tuning to address the security degradation.\nOur experiments demonstrate that the proposed approach can significantly\npreserve LLM security while maintaining performance and reducing computational\nresources compared to full fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligned LLMs are secure, capable of recognizing and refusing to answer\nmalicious questions. However, the role of internal parameters in maintaining\nsuch security is not well understood yet, further these models can be\nvulnerable to security degradation when subjected to fine-tuning attacks. To\naddress these challenges, our work uncovers the mechanism behind security in\naligned LLMs at the parameter level, identifying a small set of contiguous\nlayers in the middle of the model that are crucial for distinguishing malicious\nqueries from normal ones, referred to as ``safety layers\". We first confirm the\nexistence of these safety layers by analyzing variations in input vectors\nwithin the model's internal layers. Additionally, we leverage the\nover-rejection phenomenon and parameters scaling analysis to precisely locate\nthe safety layers. Building on these findings, we propose a novel fine-tuning\napproach, Safely Partial-Parameter Fine-Tuning (SPPFT), that fixes the gradient\nof the safety layers during fine-tuning to address the security degradation.\nOur experiments demonstrate that the proposed approach can significantly\npreserve LLM security while maintaining performance and reducing computational\nresources compared to full fine-tuning."
                },
                "authors": [
                    {
                        "name": "Shen Li"
                    },
                    {
                        "name": "Liuyi Yao"
                    },
                    {
                        "name": "Lan Zhang"
                    },
                    {
                        "name": "Yaliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yaliang Li"
                },
                "author": "Yaliang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13674v1",
                "updated": "2025-02-19T12:31:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    31,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T12:31:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    31,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "SCOPE: A Self-supervised Framework for Improving Faithfulness in\n  Conditional Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: A Self-supervised Framework for Improving Faithfulness in\n  Conditional Text Generation"
                },
                "summary": "Large Language Models (LLMs), when used for conditional text generation,\noften produce hallucinations, i.e., information that is unfaithful or not\ngrounded in the input context. This issue arises in typical conditional text\ngeneration tasks, such as text summarization and data-to-text generation, where\nthe goal is to produce fluent text based on contextual input. When fine-tuned\non specific domains, LLMs struggle to provide faithful answers to a given\ncontext, often adding information or generating errors. One underlying cause of\nthis issue is that LLMs rely on statistical patterns learned from their\ntraining data. This reliance can interfere with the model's ability to stay\nfaithful to a provided context, leading to the generation of ungrounded\ninformation. We build upon this observation and introduce a novel\nself-supervised method for generating a training set of unfaithful samples. We\nthen refine the model using a training process that encourages the generation\nof grounded outputs over unfaithful ones, drawing on preference-based training.\nOur approach leads to significantly more grounded text generation,\noutperforming existing self-supervised techniques in faithfulness, as evaluated\nthrough automatic metrics, LLM-based assessments, and human evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), when used for conditional text generation,\noften produce hallucinations, i.e., information that is unfaithful or not\ngrounded in the input context. This issue arises in typical conditional text\ngeneration tasks, such as text summarization and data-to-text generation, where\nthe goal is to produce fluent text based on contextual input. When fine-tuned\non specific domains, LLMs struggle to provide faithful answers to a given\ncontext, often adding information or generating errors. One underlying cause of\nthis issue is that LLMs rely on statistical patterns learned from their\ntraining data. This reliance can interfere with the model's ability to stay\nfaithful to a provided context, leading to the generation of ungrounded\ninformation. We build upon this observation and introduce a novel\nself-supervised method for generating a training set of unfaithful samples. We\nthen refine the model using a training process that encourages the generation\nof grounded outputs over unfaithful ones, drawing on preference-based training.\nOur approach leads to significantly more grounded text generation,\noutperforming existing self-supervised techniques in faithfulness, as evaluated\nthrough automatic metrics, LLM-based assessments, and human evaluations."
                },
                "authors": [
                    {
                        "name": "Song Duong"
                    },
                    {
                        "name": "Florian Le Bronnec"
                    },
                    {
                        "name": "Alexandre Allauzen"
                    },
                    {
                        "name": "Vincent Guigue"
                    },
                    {
                        "name": "Alberto Lumbreras"
                    },
                    {
                        "name": "Laure Soulier"
                    },
                    {
                        "name": "Patrick Gallinari"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Gallinari"
                },
                "author": "Patrick Gallinari",
                "arxiv_comment": "10 pages, ICLR 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14097v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14097v2",
                "updated": "2025-02-19T12:14:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    14,
                    17,
                    2,
                    50,
                    0
                ],
                "published": "2024-07-19T08:08:17Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    8,
                    8,
                    17,
                    4,
                    201,
                    0
                ],
                "title": "Forward-Forward Learning achieves Highly Selective Latent\n  Representations for Out-of-Distribution Detection in Fully Spiking Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forward-Forward Learning achieves Highly Selective Latent\n  Representations for Out-of-Distribution Detection in Fully Spiking Neural\n  Networks"
                },
                "summary": "In recent years, Artificial Intelligence (AI) models have achieved remarkable\nsuccess across various domains, yet challenges persist in two critical areas:\nensuring robustness against uncertain inputs and drastically increasing model\nefficiency during training and inference. Spiking Neural Networks (SNNs),\ninspired by biological systems, offer a promising avenue for overcoming these\nlimitations. By operating in an event-driven manner, SNNs achieve low energy\nconsumption and can naturally implement biological methods known for their high\nnoise tolerance. In this work, we explore the potential of the spiking\nForward-Forward Algorithm (FFA) to address these challenges, leveraging its\nrepresentational properties for both Out-of-Distribution (OoD) detection and\ninterpretability. To achieve this, we exploit the sparse and highly specialized\nneural latent space of FF networks to estimate the likelihood of a sample\nbelonging to the training distribution. Additionally, we propose a novel,\ngradient-free attribution method to detect features that drive a sample away\nfrom class distributions, addressing the challenges posed by the lack of\ngradients in most visual interpretability methods for spiking models. We\nevaluate our OoD detection algorithm on well-known image datasets (e.g.,\nOmniglot, Not-MNIST, CIFAR10), outperforming previous methods proposed in the\nrecent literature for OoD detection in spiking networks. Furthermore, our\nattribution method precisely identifies salient OoD features, such as artifacts\nor missing regions, hence providing a visual explanatory interface for the user\nto understand why unknown inputs are identified as such by the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Artificial Intelligence (AI) models have achieved remarkable\nsuccess across various domains, yet challenges persist in two critical areas:\nensuring robustness against uncertain inputs and drastically increasing model\nefficiency during training and inference. Spiking Neural Networks (SNNs),\ninspired by biological systems, offer a promising avenue for overcoming these\nlimitations. By operating in an event-driven manner, SNNs achieve low energy\nconsumption and can naturally implement biological methods known for their high\nnoise tolerance. In this work, we explore the potential of the spiking\nForward-Forward Algorithm (FFA) to address these challenges, leveraging its\nrepresentational properties for both Out-of-Distribution (OoD) detection and\ninterpretability. To achieve this, we exploit the sparse and highly specialized\nneural latent space of FF networks to estimate the likelihood of a sample\nbelonging to the training distribution. Additionally, we propose a novel,\ngradient-free attribution method to detect features that drive a sample away\nfrom class distributions, addressing the challenges posed by the lack of\ngradients in most visual interpretability methods for spiking models. We\nevaluate our OoD detection algorithm on well-known image datasets (e.g.,\nOmniglot, Not-MNIST, CIFAR10), outperforming previous methods proposed in the\nrecent literature for OoD detection in spiking networks. Furthermore, our\nattribution method precisely identifies salient OoD features, such as artifacts\nor missing regions, hence providing a visual explanatory interface for the user\nto understand why unknown inputs are identified as such by the proposed method."
                },
                "authors": [
                    {
                        "name": "Erik B. Terres-Escudero"
                    },
                    {
                        "name": "Javier Del Ser"
                    },
                    {
                        "name": "Aitor Martnez-Seras"
                    },
                    {
                        "name": "Pablo Garcia-Bringas"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Garcia-Bringas"
                },
                "author": "Pablo Garcia-Bringas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14097v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14097v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13656v1",
                "updated": "2025-02-19T12:07:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    7,
                    53,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T12:07:53Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    7,
                    53,
                    2,
                    50,
                    0
                ],
                "title": "Refining Sentence Embedding Model through Ranking Sentences Generation\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Sentence Embedding Model through Ranking Sentences Generation\n  with Large Language Models"
                },
                "summary": "Sentence embedding is essential for many NLP tasks, with contrastive learning\nmethods achieving strong performance using annotated datasets like NLI. Yet,\nthe reliance on manual labels limits scalability. Recent studies leverage large\nlanguage models (LLMs) to generate sentence pairs, reducing annotation\ndependency. However, they overlook ranking information crucial for fine-grained\nsemantic distinctions. To tackle this challenge, we propose a method for\ncontrolling the generation direction of LLMs in the latent space. Unlike\nunconstrained generation, the controlled approach ensures meaningful semantic\ndivergence. Then, we refine exist sentence embedding model by integrating\nranking information and semantic information. Experiments on multiple\nbenchmarks demonstrate that our method achieves new SOTA performance with a\nmodest cost in ranking sentence synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentence embedding is essential for many NLP tasks, with contrastive learning\nmethods achieving strong performance using annotated datasets like NLI. Yet,\nthe reliance on manual labels limits scalability. Recent studies leverage large\nlanguage models (LLMs) to generate sentence pairs, reducing annotation\ndependency. However, they overlook ranking information crucial for fine-grained\nsemantic distinctions. To tackle this challenge, we propose a method for\ncontrolling the generation direction of LLMs in the latent space. Unlike\nunconstrained generation, the controlled approach ensures meaningful semantic\ndivergence. Then, we refine exist sentence embedding model by integrating\nranking information and semantic information. Experiments on multiple\nbenchmarks demonstrate that our method achieves new SOTA performance with a\nmodest cost in ranking sentence synthesis."
                },
                "authors": [
                    {
                        "name": "Liyang He"
                    },
                    {
                        "name": "Chenglong Liu"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Zhenya Huang"
                    },
                    {
                        "name": "Shulan Ruan"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12825v2",
                "updated": "2025-02-19T11:57:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    57,
                    19,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-18T12:46:18Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    12,
                    46,
                    18,
                    1,
                    49,
                    0
                ],
                "title": "Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment\n  Revealing Hidden Fault Lines in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment\n  Revealing Hidden Fault Lines in Large Language Models"
                },
                "summary": "When encountering increasingly frequent performance improvements or cost\nreductions from a new large language model (LLM), developers of applications\nleveraging LLMs must decide whether to take advantage of these improvements or\nstay with older tried-and-tested models. Low perceived switching frictions can\nlead to choices that do not consider more subtle behavior changes that the\ntransition may induce. Our experiments use a popular game-theoretic behavioral\neconomics model of trust to show stark differences in the trusting behavior of\nOpenAI's and DeepSeek's models. We highlight a collapse in the economic trust\nbehavior of the o1-mini and o3-mini models as they reconcile profit-maximizing\nand risk-seeking with future returns from trust, and contrast it with\nDeepSeek's more sophisticated and profitable trusting behavior that stems from\nan ability to incorporate deeper concepts like forward planning and\ntheory-of-mind. As LLMs form the basis for high-stakes commercial systems, our\nresults highlight the perils of relying on LLM performance benchmarks that are\ntoo narrowly defined and suggest that careful analysis of their hidden fault\nlines should be part of any organization's AI strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When encountering increasingly frequent performance improvements or cost\nreductions from a new large language model (LLM), developers of applications\nleveraging LLMs must decide whether to take advantage of these improvements or\nstay with older tried-and-tested models. Low perceived switching frictions can\nlead to choices that do not consider more subtle behavior changes that the\ntransition may induce. Our experiments use a popular game-theoretic behavioral\neconomics model of trust to show stark differences in the trusting behavior of\nOpenAI's and DeepSeek's models. We highlight a collapse in the economic trust\nbehavior of the o1-mini and o3-mini models as they reconcile profit-maximizing\nand risk-seeking with future returns from trust, and contrast it with\nDeepSeek's more sophisticated and profitable trusting behavior that stems from\nan ability to incorporate deeper concepts like forward planning and\ntheory-of-mind. As LLMs form the basis for high-stakes commercial systems, our\nresults highlight the perils of relying on LLM performance benchmarks that are\ntoo narrowly defined and suggest that careful analysis of their hidden fault\nlines should be part of any organization's AI strategy."
                },
                "authors": [
                    {
                        "name": "Rubing Li"
                    },
                    {
                        "name": "Joo Sedoc"
                    },
                    {
                        "name": "Arun Sundararajan"
                    }
                ],
                "author_detail": {
                    "name": "Arun Sundararajan"
                },
                "author": "Arun Sundararajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13652v1",
                "updated": "2025-02-19T11:57:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    57,
                    2,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T11:57:02Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    57,
                    2,
                    2,
                    50,
                    0
                ],
                "title": "C2T: A Classifier-Based Tree Construction Method in Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C2T: A Classifier-Based Tree Construction Method in Speculative Decoding"
                },
                "summary": "The growing scale of Large Language Models (LLMs) has exacerbated inference\nlatency and computational costs. Speculative decoding methods, which aim to\nmitigate these issues, often face inefficiencies in the construction of token\ntrees and the verification of candidate tokens. Existing strategies, including\nchain mode, static tree, and dynamic tree approaches, have limitations in\naccurately preparing candidate token trees for verification. We propose a novel\nmethod named C2T that adopts a lightweight classifier to generate and prune\ntoken trees dynamically. Our classifier considers additional feature variables\nbeyond the commonly used joint probability to predict the confidence score for\neach draft token to determine whether it is the candidate token for\nverification. This method outperforms state-of-the-art (SOTA) methods such as\nEAGLE-2 on multiple benchmarks, by reducing the total number of candidate\ntokens by 25% while maintaining or even improving the acceptance length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing scale of Large Language Models (LLMs) has exacerbated inference\nlatency and computational costs. Speculative decoding methods, which aim to\nmitigate these issues, often face inefficiencies in the construction of token\ntrees and the verification of candidate tokens. Existing strategies, including\nchain mode, static tree, and dynamic tree approaches, have limitations in\naccurately preparing candidate token trees for verification. We propose a novel\nmethod named C2T that adopts a lightweight classifier to generate and prune\ntoken trees dynamically. Our classifier considers additional feature variables\nbeyond the commonly used joint probability to predict the confidence score for\neach draft token to determine whether it is the candidate token for\nverification. This method outperforms state-of-the-art (SOTA) methods such as\nEAGLE-2 on multiple benchmarks, by reducing the total number of candidate\ntokens by 25% while maintaining or even improving the acceptance length."
                },
                "authors": [
                    {
                        "name": "Feiye Huo"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Shengli Sun"
                    }
                ],
                "author_detail": {
                    "name": "Shengli Sun"
                },
                "author": "Shengli Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13648v1",
                "updated": "2025-02-19T11:49:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    49,
                    23,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T11:49:23Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    49,
                    23,
                    2,
                    50,
                    0
                ],
                "title": "Reliability Across Parametric and External Knowledge: Understanding\n  Knowledge Handling in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliability Across Parametric and External Knowledge: Understanding\n  Knowledge Handling in LLMs"
                },
                "summary": "Large Language Models (LLMs) enhance their problem-solving capability by\nleveraging both parametric and external knowledge. Beyond leveraging external\nknowledge to improve response accuracy, they require key capabilities for\nreliable knowledge-handling: resolving conflicts between knowledge sources,\navoiding distraction from uninformative external knowledge, and abstaining when\nsufficient knowledge is unavailable. Prior studies have examined these\nscenarios in isolation or with limited scope. To systematically evaluate these\ncapabilities, we introduce a comprehensive framework for analyzing\nknowledge-handling based on two key dimensions: the presence of parametric\nknowledge and the informativeness of external knowledge. Through analysis, we\nidentify biases in knowledge utilization and examine how the ability to handle\none scenario impacts performance in others. Furthermore, we demonstrate that\ntraining on data constructed based on the knowledge-handling scenarios improves\nLLMs' reliability in integrating and utilizing knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) enhance their problem-solving capability by\nleveraging both parametric and external knowledge. Beyond leveraging external\nknowledge to improve response accuracy, they require key capabilities for\nreliable knowledge-handling: resolving conflicts between knowledge sources,\navoiding distraction from uninformative external knowledge, and abstaining when\nsufficient knowledge is unavailable. Prior studies have examined these\nscenarios in isolation or with limited scope. To systematically evaluate these\ncapabilities, we introduce a comprehensive framework for analyzing\nknowledge-handling based on two key dimensions: the presence of parametric\nknowledge and the informativeness of external knowledge. Through analysis, we\nidentify biases in knowledge utilization and examine how the ability to handle\none scenario impacts performance in others. Furthermore, we demonstrate that\ntraining on data constructed based on the knowledge-handling scenarios improves\nLLMs' reliability in integrating and utilizing knowledge."
                },
                "authors": [
                    {
                        "name": "Youna Kim"
                    },
                    {
                        "name": "Minjoon Choi"
                    },
                    {
                        "name": "Sungmin Cho"
                    },
                    {
                        "name": "Hyuhng Joon Kim"
                    },
                    {
                        "name": "Sang-goo Lee"
                    },
                    {
                        "name": "Taeuk Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taeuk Kim"
                },
                "author": "Taeuk Kim",
                "arxiv_comment": "under-review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15226v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15226v3",
                "updated": "2025-02-19T11:45:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    45,
                    6,
                    2,
                    50,
                    0
                ],
                "published": "2024-07-21T17:24:25Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    17,
                    24,
                    25,
                    6,
                    203,
                    0
                ],
                "title": "Variation Bayesian Interference for Multiple Extended Targets or\n  Unresolved Group Targets Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variation Bayesian Interference for Multiple Extended Targets or\n  Unresolved Group Targets Tracking"
                },
                "summary": "In this work, we propose a tracking method for multiple extended targets or\nunresolvable group targets based on the Variational Bayesian Inference (VBI).\nFirstly, based on the most commonly used Random Matrix Model (RMM), the joint\nstates of a single target are modeled as a Gamma Gaussian Inverse Wishart\n(GGIW) distribution, and the multi-target joint association variables are\ninvolved in the estimation together as unknown information with a prior\ndistribution. A shape evolution model and VBI are employed to address the\nshortcomings of the RMM. Through the VBI, we can derive the approximate\nvariational posterior for the exact multi-target posterior. Furthermore, to\ndemonstrate the applicability of the method in real-world tracking scenarios,\nwe present two potential lightweight schemes. The first is based on clustering,\nwhich effectively prunes the joint association events. The second is a\nsimplification of the variational posterior through marginal association\nprobabilities. We demonstrate the effectiveness of the proposed method using\nsimulation experiments, and the proposed method outperforms current\nstate-of-the-art methods in terms of accuracy and adaptability. This manuscript\nis only a preprint version, a completer and more official version will be\nuploaded as soon as possible",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we propose a tracking method for multiple extended targets or\nunresolvable group targets based on the Variational Bayesian Inference (VBI).\nFirstly, based on the most commonly used Random Matrix Model (RMM), the joint\nstates of a single target are modeled as a Gamma Gaussian Inverse Wishart\n(GGIW) distribution, and the multi-target joint association variables are\ninvolved in the estimation together as unknown information with a prior\ndistribution. A shape evolution model and VBI are employed to address the\nshortcomings of the RMM. Through the VBI, we can derive the approximate\nvariational posterior for the exact multi-target posterior. Furthermore, to\ndemonstrate the applicability of the method in real-world tracking scenarios,\nwe present two potential lightweight schemes. The first is based on clustering,\nwhich effectively prunes the joint association events. The second is a\nsimplification of the variational posterior through marginal association\nprobabilities. We demonstrate the effectiveness of the proposed method using\nsimulation experiments, and the proposed method outperforms current\nstate-of-the-art methods in terms of accuracy and adaptability. This manuscript\nis only a preprint version, a completer and more official version will be\nuploaded as soon as possible"
                },
                "authors": [
                    {
                        "name": "Yuanhao Cheng"
                    },
                    {
                        "name": "Yunhe Cao"
                    },
                    {
                        "name": "Tat-Soon Yeo"
                    },
                    {
                        "name": "Yulin Zhang"
                    },
                    {
                        "name": "Fu Jie"
                    }
                ],
                "author_detail": {
                    "name": "Fu Jie"
                },
                "author": "Fu Jie",
                "arxiv_comment": "21 pages, 15 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15226v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15226v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13647v1",
                "updated": "2025-02-19T11:44:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    44,
                    27,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T11:44:27Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    44,
                    27,
                    2,
                    50,
                    0
                ],
                "title": "Instruction Tuning on Public Government and Cultural Data for\n  Low-Resource Language: a Case Study in Kazakh",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Tuning on Public Government and Cultural Data for\n  Low-Resource Language: a Case Study in Kazakh"
                },
                "summary": "Instruction tuning in low-resource languages remains underexplored due to\nlimited text data, particularly in government and cultural domains. To address\nthis, we introduce and open-source a large-scale (10,600 samples)\ninstruction-following (IFT) dataset, covering key institutional and cultural\nknowledge relevant to Kazakhstan. Our dataset enhances LLMs' understanding of\nprocedural, legal, and structural governance topics. We employ LLM-assisted\ndata generation, comparing open-weight and closed-weight models for dataset\nconstruction, and select GPT-4o as the backbone. Each entity of our dataset\nundergoes full manual verification to ensure high quality. We also show that\nfine-tuning Qwen, Falcon, and Gemma on our dataset leads to consistent\nperformance improvements in both multiple-choice and generative tasks,\ndemonstrating the potential of LLM-assisted instruction tuning for low-resource\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning in low-resource languages remains underexplored due to\nlimited text data, particularly in government and cultural domains. To address\nthis, we introduce and open-source a large-scale (10,600 samples)\ninstruction-following (IFT) dataset, covering key institutional and cultural\nknowledge relevant to Kazakhstan. Our dataset enhances LLMs' understanding of\nprocedural, legal, and structural governance topics. We employ LLM-assisted\ndata generation, comparing open-weight and closed-weight models for dataset\nconstruction, and select GPT-4o as the backbone. Each entity of our dataset\nundergoes full manual verification to ensure high quality. We also show that\nfine-tuning Qwen, Falcon, and Gemma on our dataset leads to consistent\nperformance improvements in both multiple-choice and generative tasks,\ndemonstrating the potential of LLM-assisted instruction tuning for low-resource\nlanguages."
                },
                "authors": [
                    {
                        "name": "Nurkhan Laiyk"
                    },
                    {
                        "name": "Daniil Orel"
                    },
                    {
                        "name": "Rituraj Joshi"
                    },
                    {
                        "name": "Maiya Goloburda"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13646v1",
                "updated": "2025-02-19T11:41:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    41,
                    40,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T11:41:40Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    41,
                    40,
                    2,
                    50,
                    0
                ],
                "title": "D.Va: Validate Your Demonstration First Before You Use It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D.Va: Validate Your Demonstration First Before You Use It"
                },
                "summary": "In-context learning (ICL) has demonstrated significant potential in enhancing\nthe capabilities of large language models (LLMs) during inference. It's\nwell-established that ICL heavily relies on selecting effective demonstrations\nto generate outputs that better align with the expected results. As for\ndemonstration selection, previous approaches have typically relied on intuitive\nmetrics to evaluate the effectiveness of demonstrations, which often results in\nlimited robustness and poor cross-model generalization capabilities. To tackle\nthese challenges, we propose a novel method, \\textbf{D}emonstration\n\\textbf{VA}lidation (\\textbf{D.Va}), which integrates a demonstration\nvalidation perspective into this field. By introducing the demonstration\nvalidation mechanism, our method effectively identifies demonstrations that are\nboth effective and highly generalizable. \\textbf{D.Va} surpasses all existing\ndemonstration selection techniques across both natural language understanding\n(NLU) and natural language generation (NLG) tasks. Additionally, we demonstrate\nthe robustness and generalizability of our approach across various language\nmodels with different retrieval models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has demonstrated significant potential in enhancing\nthe capabilities of large language models (LLMs) during inference. It's\nwell-established that ICL heavily relies on selecting effective demonstrations\nto generate outputs that better align with the expected results. As for\ndemonstration selection, previous approaches have typically relied on intuitive\nmetrics to evaluate the effectiveness of demonstrations, which often results in\nlimited robustness and poor cross-model generalization capabilities. To tackle\nthese challenges, we propose a novel method, \\textbf{D}emonstration\n\\textbf{VA}lidation (\\textbf{D.Va}), which integrates a demonstration\nvalidation perspective into this field. By introducing the demonstration\nvalidation mechanism, our method effectively identifies demonstrations that are\nboth effective and highly generalizable. \\textbf{D.Va} surpasses all existing\ndemonstration selection techniques across both natural language understanding\n(NLU) and natural language generation (NLG) tasks. Additionally, we demonstrate\nthe robustness and generalizability of our approach across various language\nmodels with different retrieval models."
                },
                "authors": [
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Zhiqing Xiao"
                    },
                    {
                        "name": "Ruixuan Xiao"
                    },
                    {
                        "name": "Lirong Gao"
                    },
                    {
                        "name": "Junbo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junbo Zhao"
                },
                "author": "Junbo Zhao",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17560v2",
                "updated": "2025-02-19T11:35:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    35,
                    43,
                    2,
                    50,
                    0
                ],
                "published": "2024-12-23T13:28:15Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    28,
                    15,
                    0,
                    358,
                    0
                ],
                "title": "GQSA: Group Quantization and Sparsity for Accelerating Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GQSA: Group Quantization and Sparsity for Accelerating Large Language\n  Model Inference"
                },
                "summary": "Model compression has emerged as a mainstream solution to reduce memory usage\nand computational overhead. This paper presents Group Quantization and Sparse\nAcceleration (GQSA), a novel compression technique tailored for LLMs.\nTraditional methods typically focus exclusively on either quantization or\nsparsification, but relying on a single strategy often results in significant\nperformance loss at high compression rates. In contrast, GQSA integrates\nquantization and sparsification in a tightly coupled manner, leveraging\nGPU-friendly structured group sparsity and quantization for efficient\nacceleration. Building upon system-algorithm co-design principles, we propose a\ntwo-stage sparse optimization strategy that ensures the performance superiority\nof the compressed model. On the engine side, we introduce a \"task-centric\"\nparallel strategy, which, to the best of our knowledge, is the first\napplication in the domain of sparse computing. Compared to the traditional 2:4\nsparse method, the GQSA offers a more flexible and adjustable sparsity rate, as\nwell as a higher weight compression rate, and is efficiently compatible with\nweight-only quantization methods. Experimental results demonstrate that, under\nthe GQSA W4S50% compression setting, the model's accuracy surpasses that of\nboth 2:4 pruning and W2 quantization. Furthermore, at the inference level, GQSA\noutperforms W2 by 1.26$\\times$ and 2:4 pruning by 2.35$\\times$ in terms of\nspeed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model compression has emerged as a mainstream solution to reduce memory usage\nand computational overhead. This paper presents Group Quantization and Sparse\nAcceleration (GQSA), a novel compression technique tailored for LLMs.\nTraditional methods typically focus exclusively on either quantization or\nsparsification, but relying on a single strategy often results in significant\nperformance loss at high compression rates. In contrast, GQSA integrates\nquantization and sparsification in a tightly coupled manner, leveraging\nGPU-friendly structured group sparsity and quantization for efficient\nacceleration. Building upon system-algorithm co-design principles, we propose a\ntwo-stage sparse optimization strategy that ensures the performance superiority\nof the compressed model. On the engine side, we introduce a \"task-centric\"\nparallel strategy, which, to the best of our knowledge, is the first\napplication in the domain of sparse computing. Compared to the traditional 2:4\nsparse method, the GQSA offers a more flexible and adjustable sparsity rate, as\nwell as a higher weight compression rate, and is efficiently compatible with\nweight-only quantization methods. Experimental results demonstrate that, under\nthe GQSA W4S50% compression setting, the model's accuracy surpasses that of\nboth 2:4 pruning and W2 quantization. Furthermore, at the inference level, GQSA\noutperforms W2 by 1.26$\\times$ and 2:4 pruning by 2.35$\\times$ in terms of\nspeed."
                },
                "authors": [
                    {
                        "name": "Chao Zeng"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Mei"
                    },
                    {
                        "name": "Lean Fu"
                    }
                ],
                "author_detail": {
                    "name": "Lean Fu"
                },
                "author": "Lean Fu",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13640v1",
                "updated": "2025-02-19T11:33:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    33,
                    22,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T11:33:22Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    33,
                    22,
                    2,
                    50,
                    0
                ],
                "title": "Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts"
                },
                "summary": "Large language models (LLMs) are known to have the potential to generate\nharmful content, posing risks to users. While significant progress has been\nmade in developing taxonomies for LLM risks and safety evaluation prompts, most\nstudies have focused on monolingual contexts, primarily in English. However,\nlanguage- and region-specific risks in bilingual contexts are often overlooked,\nand core findings can diverge from those in monolingual settings. In this\npaper, we introduce Qorgau, a novel dataset specifically designed for safety\nevaluation in Kazakh and Russian, reflecting the unique bilingual context in\nKazakhstan, where both Kazakh (a low-resource language) and Russian (a\nhigh-resource language) are spoken. Experiments with both multilingual and\nlanguage-specific LLMs reveal notable differences in safety performance,\nemphasizing the need for tailored, region-specific datasets to ensure the\nresponsible and safe deployment of LLMs in countries like Kazakhstan. Warning:\nthis paper contains example data that may be offensive, harmful, or biased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are known to have the potential to generate\nharmful content, posing risks to users. While significant progress has been\nmade in developing taxonomies for LLM risks and safety evaluation prompts, most\nstudies have focused on monolingual contexts, primarily in English. However,\nlanguage- and region-specific risks in bilingual contexts are often overlooked,\nand core findings can diverge from those in monolingual settings. In this\npaper, we introduce Qorgau, a novel dataset specifically designed for safety\nevaluation in Kazakh and Russian, reflecting the unique bilingual context in\nKazakhstan, where both Kazakh (a low-resource language) and Russian (a\nhigh-resource language) are spoken. Experiments with both multilingual and\nlanguage-specific LLMs reveal notable differences in safety performance,\nemphasizing the need for tailored, region-specific datasets to ensure the\nresponsible and safe deployment of LLMs in countries like Kazakhstan. Warning:\nthis paper contains example data that may be offensive, harmful, or biased."
                },
                "authors": [
                    {
                        "name": "Maiya Goloburda"
                    },
                    {
                        "name": "Nurkhan Laiyk"
                    },
                    {
                        "name": "Diana Turmakhan"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Mukhammed Togmanov"
                    },
                    {
                        "name": "Jonibek Mansurov"
                    },
                    {
                        "name": "Askhat Sametov"
                    },
                    {
                        "name": "Nurdaulet Mukhituly"
                    },
                    {
                        "name": "Minghan Wang"
                    },
                    {
                        "name": "Daniil Orel"
                    },
                    {
                        "name": "Zain Muhammad Mujahid"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Preslav Nakov"
                    }
                ],
                "author_detail": {
                    "name": "Preslav Nakov"
                },
                "author": "Preslav Nakov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13638v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13638v1",
                "updated": "2025-02-19T11:24:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    24,
                    51,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T11:24:51Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    24,
                    51,
                    2,
                    50,
                    0
                ],
                "title": "Integrating Inverse and Forward Modeling for Sparse Temporal Data from\n  Sensor Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Inverse and Forward Modeling for Sparse Temporal Data from\n  Sensor Networks"
                },
                "summary": "We present CavePerception, a framework for the analysis of sparse data from\nsensor networks that incorporates elements of inverse modeling and forward\nmodeling. By integrating machine learning with physical modeling in a\nhypotheses space, we aim to improve the interpretability of sparse, noisy, and\npotentially incomplete sensor data. The framework assumes data from a\ntwo-dimensional sensor network laid out in a graph structure that detects\ncertain objects, with certain motion patterns. Examples of such sensors are\nmagnetometers. Given knowledge about the objects and the way they act on the\nsensors, one can develop a data generator that produces data from simulated\nmotions of the objects across the sensor field. The framework uses the\nsimulated data to infer object behaviors across the sensor network. The\napproach is experimentally tested on real-world data, where magnetometers are\nused on an airport to detect and identify aircraft motions. Experiments\ndemonstrate the value of integrating inverse and forward modeling, enabling\nintelligent systems to better understand and predict complex, sensor-driven\nevents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present CavePerception, a framework for the analysis of sparse data from\nsensor networks that incorporates elements of inverse modeling and forward\nmodeling. By integrating machine learning with physical modeling in a\nhypotheses space, we aim to improve the interpretability of sparse, noisy, and\npotentially incomplete sensor data. The framework assumes data from a\ntwo-dimensional sensor network laid out in a graph structure that detects\ncertain objects, with certain motion patterns. Examples of such sensors are\nmagnetometers. Given knowledge about the objects and the way they act on the\nsensors, one can develop a data generator that produces data from simulated\nmotions of the objects across the sensor field. The framework uses the\nsimulated data to infer object behaviors across the sensor network. The\napproach is experimentally tested on real-world data, where magnetometers are\nused on an airport to detect and identify aircraft motions. Experiments\ndemonstrate the value of integrating inverse and forward modeling, enabling\nintelligent systems to better understand and predict complex, sensor-driven\nevents."
                },
                "authors": [
                    {
                        "name": "Julian Vexler"
                    },
                    {
                        "name": "Bjrn Vieten"
                    },
                    {
                        "name": "Martin Nelke"
                    },
                    {
                        "name": "Stefan Kramer"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Kramer"
                },
                "author": "Stefan Kramer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13638v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13638v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01280v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01280v2",
                "updated": "2025-02-19T11:13:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    13,
                    18,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-03T11:51:24Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    11,
                    51,
                    24,
                    0,
                    34,
                    0
                ],
                "title": "Trajectory Map-Matching in Urban Road Networks Based on RSS Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory Map-Matching in Urban Road Networks Based on RSS Measurements"
                },
                "summary": "This paper proposes an RSS-based approach to reconstruct vehicle trajectories\nwithin a road network, enforcing signal propagation rules and vehicle mobility\nconstraints to mitigate the impact of RSS noise and sparsity. The key challenge\nlies in leveraging latent spatiotemporal correlations within RSS data while\nnavigating complex road networks. To address this, we develop a Hidden Markov\nModel (HMM)-based RSS embedding (HRE) technique that employs alternating\noptimization to infer vehicle trajectories from RSS measurements. This model\ncaptures spatiotemporal dependencies while a road graph ensures network\ncompliance. Additionally, we introduce a maximum speed-constrained rough\ntrajectory estimation (MSR) method to guide the optimization process, enabling\nrapid convergence to a favorable local solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes an RSS-based approach to reconstruct vehicle trajectories\nwithin a road network, enforcing signal propagation rules and vehicle mobility\nconstraints to mitigate the impact of RSS noise and sparsity. The key challenge\nlies in leveraging latent spatiotemporal correlations within RSS data while\nnavigating complex road networks. To address this, we develop a Hidden Markov\nModel (HMM)-based RSS embedding (HRE) technique that employs alternating\noptimization to infer vehicle trajectories from RSS measurements. This model\ncaptures spatiotemporal dependencies while a road graph ensures network\ncompliance. Additionally, we introduce a maximum speed-constrained rough\ntrajectory estimation (MSR) method to guide the optimization process, enabling\nrapid convergence to a favorable local solution."
                },
                "authors": [
                    {
                        "name": "Zheng Xing"
                    },
                    {
                        "name": "Weibing Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Weibing Zhao"
                },
                "author": "Weibing Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01280v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01280v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13632v1",
                "updated": "2025-02-19T11:10:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    10,
                    19,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T11:10:19Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    10,
                    19,
                    2,
                    50,
                    0
                ],
                "title": "Concept Layers: Enhancing Interpretability and Intervenability via LLM\n  Conceptualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept Layers: Enhancing Interpretability and Intervenability via LLM\n  Conceptualization"
                },
                "summary": "The opaque nature of Large Language Models (LLMs) has led to significant\nresearch efforts aimed at enhancing their interpretability, primarily through\npost-hoc methods. More recent in-hoc approaches, such as Concept Bottleneck\nModels (CBMs), offer both interpretability and intervenability by incorporating\nexplicit concept representations. However, these methods suffer from key\nlimitations, including reliance on labeled concept datasets and significant\narchitectural modifications that challenges re-integration into existing system\npipelines. In this work, we introduce a new methodology for incorporating\ninterpretability and intervenability into an existing model by integrating\nConcept Layers (CLs) into its architecture. Our approach projects the model's\ninternal vector representations into a conceptual, explainable vector space\nbefore reconstructing and feeding them back into the model. Furthermore, we\neliminate the need for a human-selected concept set by algorithmically\nsearching an ontology for a set of concepts that can be either task-specific or\ntask-agnostic. We evaluate CLs across multiple tasks, demonstrating that they\nmaintain the original model's performance and agreement while enabling\nmeaningful interventions. Additionally, we present a proof of concept\nshowcasing an intervenability interface, allowing users to adjust model\nbehavior dynamically, such as mitigating biases during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The opaque nature of Large Language Models (LLMs) has led to significant\nresearch efforts aimed at enhancing their interpretability, primarily through\npost-hoc methods. More recent in-hoc approaches, such as Concept Bottleneck\nModels (CBMs), offer both interpretability and intervenability by incorporating\nexplicit concept representations. However, these methods suffer from key\nlimitations, including reliance on labeled concept datasets and significant\narchitectural modifications that challenges re-integration into existing system\npipelines. In this work, we introduce a new methodology for incorporating\ninterpretability and intervenability into an existing model by integrating\nConcept Layers (CLs) into its architecture. Our approach projects the model's\ninternal vector representations into a conceptual, explainable vector space\nbefore reconstructing and feeding them back into the model. Furthermore, we\neliminate the need for a human-selected concept set by algorithmically\nsearching an ontology for a set of concepts that can be either task-specific or\ntask-agnostic. We evaluate CLs across multiple tasks, demonstrating that they\nmaintain the original model's performance and agreement while enabling\nmeaningful interventions. Additionally, we present a proof of concept\nshowcasing an intervenability interface, allowing users to adjust model\nbehavior dynamically, such as mitigating biases during inference."
                },
                "authors": [
                    {
                        "name": "Or Raphael Bidusa"
                    },
                    {
                        "name": "Shaul Markovitch"
                    }
                ],
                "author_detail": {
                    "name": "Shaul Markovitch"
                },
                "author": "Shaul Markovitch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v2",
                "updated": "2025-02-19T11:10:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    10,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12145v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12145v3",
                "updated": "2025-02-19T11:09:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    9,
                    15,
                    2,
                    50,
                    0
                ],
                "published": "2024-12-10T10:14:03Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    14,
                    3,
                    1,
                    345,
                    0
                ],
                "title": "Na'vi or Knave: Jailbreaking Language Models via Metaphorical Avatars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Na'vi or Knave: Jailbreaking Language Models via Metaphorical Avatars"
                },
                "summary": "Metaphor serves as an implicit approach to convey information, while enabling\nthe generalized comprehension of complex subjects. However, metaphor can\npotentially be exploited to bypass the safety alignment mechanisms of Large\nLanguage Models (LLMs), leading to the theft of harmful knowledge. In our\nstudy, we introduce a novel attack framework that exploits the imaginative\ncapacity of LLMs to achieve jailbreaking, the J\\underline{\\textbf{A}}ilbreak\n\\underline{\\textbf{V}}ia \\underline{\\textbf{A}}dversarial\nMe\\underline{\\textbf{TA}} -pho\\underline{\\textbf{R}} (\\textit{AVATAR}).\nSpecifically, to elicit the harmful response, AVATAR extracts harmful entities\nfrom a given harmful target and maps them to innocuous adversarial entities\nbased on LLM's imagination. Then, according to these metaphors, the harmful\ntarget is nested within human-like interaction for jailbreaking adaptively.\nExperimental results demonstrate that AVATAR can effectively and transferablly\njailbreak LLMs and achieve a state-of-the-art attack success rate across\nmultiple advanced LLMs. Our study exposes a security risk in LLMs from their\nendogenous imaginative capabilities. Furthermore, the analytical study reveals\nthe vulnerability of LLM to adversarial metaphors and the necessity of\ndeveloping defense methods against jailbreaking caused by the adversarial\nmetaphor. \\textcolor{orange}{ \\textbf{Warning: This paper contains potentially\nharmful content from LLMs.}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaphor serves as an implicit approach to convey information, while enabling\nthe generalized comprehension of complex subjects. However, metaphor can\npotentially be exploited to bypass the safety alignment mechanisms of Large\nLanguage Models (LLMs), leading to the theft of harmful knowledge. In our\nstudy, we introduce a novel attack framework that exploits the imaginative\ncapacity of LLMs to achieve jailbreaking, the J\\underline{\\textbf{A}}ilbreak\n\\underline{\\textbf{V}}ia \\underline{\\textbf{A}}dversarial\nMe\\underline{\\textbf{TA}} -pho\\underline{\\textbf{R}} (\\textit{AVATAR}).\nSpecifically, to elicit the harmful response, AVATAR extracts harmful entities\nfrom a given harmful target and maps them to innocuous adversarial entities\nbased on LLM's imagination. Then, according to these metaphors, the harmful\ntarget is nested within human-like interaction for jailbreaking adaptively.\nExperimental results demonstrate that AVATAR can effectively and transferablly\njailbreak LLMs and achieve a state-of-the-art attack success rate across\nmultiple advanced LLMs. Our study exposes a security risk in LLMs from their\nendogenous imaginative capabilities. Furthermore, the analytical study reveals\nthe vulnerability of LLM to adversarial metaphors and the necessity of\ndeveloping defense methods against jailbreaking caused by the adversarial\nmetaphor. \\textcolor{orange}{ \\textbf{Warning: This paper contains potentially\nharmful content from LLMs.}}"
                },
                "authors": [
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Junqi Tong"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Qi Li"
                    }
                ],
                "author_detail": {
                    "name": "Qi Li"
                },
                "author": "Qi Li",
                "arxiv_comment": "We still need to polish our paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12145v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12145v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.07156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.07156v2",
                "updated": "2025-02-19T11:07:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    7,
                    8,
                    2,
                    50,
                    0
                ],
                "published": "2023-11-13T08:50:48Z",
                "published_parsed": [
                    2023,
                    11,
                    13,
                    8,
                    50,
                    48,
                    0,
                    317,
                    0
                ],
                "title": "Deep mixture of linear mixed models for complex longitudinal data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep mixture of linear mixed models for complex longitudinal data"
                },
                "summary": "Mixtures of linear mixed models are widely used for modelling longitudinal\ndata for which observation times differ between subjects. In typical\napplications, temporal trends are described using a basis expansion, with basis\ncoefficients treated as random effects varying by subject. Additional random\neffects can describe variation between mixture components, or other known\nsources of variation in complex experimental designs. A key advantage of these\nmodels is that they provide a natural mechanism for clustering, which can be\nhelpful for interpretation in many applications. Current versions of mixtures\nof linear mixed models are not specifically designed for the case where there\nare many observations per subject and a complex temporal trend, which requires\na large number of basis functions to capture. In this case, the\nsubject-specific basis coefficients are a high-dimensional random effects\nvector, for which the covariance matrix is hard to specify and estimate,\nespecially if it varies between mixture components. To address this issue, we\nconsider the use of recently-developed deep mixture of factor analyzers models\nas the prior for the random effects. The resulting deep mixture of linear mixed\nmodels is well-suited to high-dimensional settings, and we describe an\nefficient variational inference approach to posterior computation. The efficacy\nof the method is demonstrated on both real and simulated data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixtures of linear mixed models are widely used for modelling longitudinal\ndata for which observation times differ between subjects. In typical\napplications, temporal trends are described using a basis expansion, with basis\ncoefficients treated as random effects varying by subject. Additional random\neffects can describe variation between mixture components, or other known\nsources of variation in complex experimental designs. A key advantage of these\nmodels is that they provide a natural mechanism for clustering, which can be\nhelpful for interpretation in many applications. Current versions of mixtures\nof linear mixed models are not specifically designed for the case where there\nare many observations per subject and a complex temporal trend, which requires\na large number of basis functions to capture. In this case, the\nsubject-specific basis coefficients are a high-dimensional random effects\nvector, for which the covariance matrix is hard to specify and estimate,\nespecially if it varies between mixture components. To address this issue, we\nconsider the use of recently-developed deep mixture of factor analyzers models\nas the prior for the random effects. The resulting deep mixture of linear mixed\nmodels is well-suited to high-dimensional settings, and we describe an\nefficient variational inference approach to posterior computation. The efficacy\nof the method is demonstrated on both real and simulated data."
                },
                "authors": [
                    {
                        "name": "Lucas Kock"
                    },
                    {
                        "name": "Nadja Klein"
                    },
                    {
                        "name": "David J. Nott"
                    }
                ],
                "author_detail": {
                    "name": "David J. Nott"
                },
                "author": "David J. Nott",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.07156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.07156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13626v1",
                "updated": "2025-02-19T11:03:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    3,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T11:03:09Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    3,
                    9,
                    2,
                    50,
                    0
                ],
                "title": "AI-Empowered Catalyst Discovery: A Survey from Classical Machine\n  Learning Approaches to Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Empowered Catalyst Discovery: A Survey from Classical Machine\n  Learning Approaches to Large Language Models"
                },
                "summary": "Catalysts are essential for accelerating chemical reactions and enhancing\nselectivity, which is crucial for the sustainable production of energy,\nmaterials, and bioactive compounds. Catalyst discovery is fundamental yet\nchallenging in computational chemistry and has garnered significant attention\ndue to the promising performance of advanced Artificial Intelligence (AI)\ntechniques. The development of Large Language Models (LLMs) notably accelerates\nprogress in the discovery of both homogeneous and heterogeneous catalysts,\nwhere their chemical reactions differ significantly in material phases,\ntemperature, dynamics, etc. However, there is currently no comprehensive survey\nthat discusses the progress and latest developments in both areas, particularly\nwith the application of LLM techniques. To address this gap, this paper\npresents a thorough and systematic survey of AI-empowered catalyst discovery,\nemploying a unified and general categorization for homogeneous and\nheterogeneous catalysts. We examine the progress of AI-empowered catalyst\ndiscovery, highlighting their individual advantages and disadvantages, and\ndiscuss the challenges faced in this field. Furthermore, we suggest potential\ndirections for future research from the perspective of computer science. Our\ngoal is to assist researchers in computational chemistry, computer science, and\nrelated fields in easily tracking the latest advancements, providing a clear\noverview and roadmap of this area. We also organize and make accessible\nrelevant resources, including article lists and datasets, in an open repository\nat\nhttps://github.com/LuckyGirl-XU/Awesome-Artificial-Intelligence-Empowered-Catalyst-Discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catalysts are essential for accelerating chemical reactions and enhancing\nselectivity, which is crucial for the sustainable production of energy,\nmaterials, and bioactive compounds. Catalyst discovery is fundamental yet\nchallenging in computational chemistry and has garnered significant attention\ndue to the promising performance of advanced Artificial Intelligence (AI)\ntechniques. The development of Large Language Models (LLMs) notably accelerates\nprogress in the discovery of both homogeneous and heterogeneous catalysts,\nwhere their chemical reactions differ significantly in material phases,\ntemperature, dynamics, etc. However, there is currently no comprehensive survey\nthat discusses the progress and latest developments in both areas, particularly\nwith the application of LLM techniques. To address this gap, this paper\npresents a thorough and systematic survey of AI-empowered catalyst discovery,\nemploying a unified and general categorization for homogeneous and\nheterogeneous catalysts. We examine the progress of AI-empowered catalyst\ndiscovery, highlighting their individual advantages and disadvantages, and\ndiscuss the challenges faced in this field. Furthermore, we suggest potential\ndirections for future research from the perspective of computer science. Our\ngoal is to assist researchers in computational chemistry, computer science, and\nrelated fields in easily tracking the latest advancements, providing a clear\noverview and roadmap of this area. We also organize and make accessible\nrelevant resources, including article lists and datasets, in an open repository\nat\nhttps://github.com/LuckyGirl-XU/Awesome-Artificial-Intelligence-Empowered-Catalyst-Discovery."
                },
                "authors": [
                    {
                        "name": "Yuanyuan Xu"
                    },
                    {
                        "name": "Hanchen Wang"
                    },
                    {
                        "name": "Wenjie Zhang"
                    },
                    {
                        "name": "Lexing Xie"
                    },
                    {
                        "name": "Yin Chen"
                    },
                    {
                        "name": "Flora Salim"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Justin Gooding"
                    },
                    {
                        "name": "Toby Walsh"
                    }
                ],
                "author_detail": {
                    "name": "Toby Walsh"
                },
                "author": "Toby Walsh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2502.13966v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13966v1",
                "updated": "2025-02-19T18:59:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    59,
                    32,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:59:32Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    59,
                    32,
                    2,
                    50,
                    0
                ],
                "title": "Where's the Bug? Attention Probing for Scalable Fault Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Where's the Bug? Attention Probing for Scalable Fault Localization"
                },
                "summary": "Ensuring code correctness remains a challenging problem even as large\nlanguage models (LLMs) become increasingly capable at code-related tasks. While\nLLM-based program repair systems can propose bug fixes using only a user's bug\nreport, their effectiveness is fundamentally limited by their ability to\nperform fault localization (FL), a challenging problem for both humans and\nLLMs. Existing FL approaches rely on executable test cases, require training on\ncostly and often noisy line-level annotations, or demand resource-intensive\nLLMs. In this paper, we present Bug Attention Probe (BAP), a method which\nlearns state-of-the-art fault localization without any direct localization\nlabels, outperforming traditional FL baselines and prompting of large-scale\nLLMs. We evaluate our approach across a variety of code settings, including\nreal-world Java bugs from the standard Defects4J dataset as well as seven other\ndatasets which span a diverse set of bug types and languages. Averaged across\nall eight datasets, BAP improves by 34.6% top-1 accuracy compared to the\nstrongest baseline and 93.4% over zero-shot prompting GPT-4o. BAP is also\nsignificantly more efficient than prompting, outperforming large open-weight\nmodels at a small fraction of the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring code correctness remains a challenging problem even as large\nlanguage models (LLMs) become increasingly capable at code-related tasks. While\nLLM-based program repair systems can propose bug fixes using only a user's bug\nreport, their effectiveness is fundamentally limited by their ability to\nperform fault localization (FL), a challenging problem for both humans and\nLLMs. Existing FL approaches rely on executable test cases, require training on\ncostly and often noisy line-level annotations, or demand resource-intensive\nLLMs. In this paper, we present Bug Attention Probe (BAP), a method which\nlearns state-of-the-art fault localization without any direct localization\nlabels, outperforming traditional FL baselines and prompting of large-scale\nLLMs. We evaluate our approach across a variety of code settings, including\nreal-world Java bugs from the standard Defects4J dataset as well as seven other\ndatasets which span a diverse set of bug types and languages. Averaged across\nall eight datasets, BAP improves by 34.6% top-1 accuracy compared to the\nstrongest baseline and 93.4% over zero-shot prompting GPT-4o. BAP is also\nsignificantly more efficient than prompting, outperforming large open-weight\nmodels at a small fraction of the computational cost."
                },
                "authors": [
                    {
                        "name": "Adam Stein"
                    },
                    {
                        "name": "Arthur Wayne"
                    },
                    {
                        "name": "Aaditya Naik"
                    },
                    {
                        "name": "Mayur Naik"
                    },
                    {
                        "name": "Eric Wong"
                    }
                ],
                "author_detail": {
                    "name": "Eric Wong"
                },
                "author": "Eric Wong",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13966v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13966v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13965v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13965v1",
                "updated": "2025-02-19T18:59:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    59,
                    30,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:59:30Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    59,
                    30,
                    2,
                    50,
                    0
                ],
                "title": "Autellix: An Efficient Serving Engine for LLM Agents as General Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autellix: An Efficient Serving Engine for LLM Agents as General Programs"
                },
                "summary": "Large language model (LLM) applications are evolving beyond simple chatbots\ninto dynamic, general-purpose agentic programs, which scale LLM calls and\noutput tokens to help AI agents reason, explore, and solve complex tasks.\nHowever, existing LLM serving systems ignore dependencies between programs and\ncalls, missing significant opportunities for optimization. Our analysis reveals\nthat programs submitted to LLM serving engines experience long cumulative wait\ntimes, primarily due to head-of-line blocking at both the individual LLM\nrequest and the program. To address this, we introduce Autellix, an LLM serving\nsystem that treats programs as first-class citizens to minimize their\nend-to-end latencies. Autellix intercepts LLM calls submitted by programs,\nenriching schedulers with program-level context. We propose two scheduling\nalgorithms-for single-threaded and distributed programs-that preempt and\nprioritize LLM calls based on their programs' previously completed calls. Our\nevaluation demonstrates that across diverse LLMs and agentic workloads,\nAutellix improves throughput of programs by 4-15x at the same latency compared\nto state-of-the-art systems, such as vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) applications are evolving beyond simple chatbots\ninto dynamic, general-purpose agentic programs, which scale LLM calls and\noutput tokens to help AI agents reason, explore, and solve complex tasks.\nHowever, existing LLM serving systems ignore dependencies between programs and\ncalls, missing significant opportunities for optimization. Our analysis reveals\nthat programs submitted to LLM serving engines experience long cumulative wait\ntimes, primarily due to head-of-line blocking at both the individual LLM\nrequest and the program. To address this, we introduce Autellix, an LLM serving\nsystem that treats programs as first-class citizens to minimize their\nend-to-end latencies. Autellix intercepts LLM calls submitted by programs,\nenriching schedulers with program-level context. We propose two scheduling\nalgorithms-for single-threaded and distributed programs-that preempt and\nprioritize LLM calls based on their programs' previously completed calls. Our\nevaluation demonstrates that across diverse LLMs and agentic workloads,\nAutellix improves throughput of programs by 4-15x at the same latency compared\nto state-of-the-art systems, such as vLLM."
                },
                "authors": [
                    {
                        "name": "Michael Luo"
                    },
                    {
                        "name": "Xiaoxiang Shi"
                    },
                    {
                        "name": "Colin Cai"
                    },
                    {
                        "name": "Tianjun Zhang"
                    },
                    {
                        "name": "Justin Wong"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Chi Wang"
                    },
                    {
                        "name": "Yanping Huang"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13965v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13965v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13963v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13963v1",
                "updated": "2025-02-19T18:59:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    59,
                    15,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:59:15Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    59,
                    15,
                    2,
                    50,
                    0
                ],
                "title": "MuDAF: Long-Context Multi-Document Attention Focusing through\n  Contrastive Learning on Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MuDAF: Long-Context Multi-Document Attention Focusing through\n  Contrastive Learning on Attention Heads"
                },
                "summary": "Large Language Models (LLMs) frequently show distracted attention due to\nirrelevant information in the input, which severely impairs their long-context\ncapabilities. Inspired by recent studies on the effectiveness of retrieval\nheads in long-context factutality, we aim at addressing this distraction issue\nthrough improving such retrieval heads directly. We propose Multi-Document\nAttention Focusing (MuDAF), a novel method that explicitly optimizes the\nattention distribution at the head level through contrastive learning.\nAccording to the experimental results, MuDAF can significantly improve the\nlong-context question answering performance of LLMs, especially in\nmulti-document question answering. Extensive evaluations on retrieval scores\nand attention visualizations show that MuDAF possesses great potential in\nmaking attention heads more focused on relevant information and reducing\nattention distractions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) frequently show distracted attention due to\nirrelevant information in the input, which severely impairs their long-context\ncapabilities. Inspired by recent studies on the effectiveness of retrieval\nheads in long-context factutality, we aim at addressing this distraction issue\nthrough improving such retrieval heads directly. We propose Multi-Document\nAttention Focusing (MuDAF), a novel method that explicitly optimizes the\nattention distribution at the head level through contrastive learning.\nAccording to the experimental results, MuDAF can significantly improve the\nlong-context question answering performance of LLMs, especially in\nmulti-document question answering. Extensive evaluations on retrieval scores\nand attention visualizations show that MuDAF possesses great potential in\nmaking attention heads more focused on relevant information and reducing\nattention distractions."
                },
                "authors": [
                    {
                        "name": "Weihao Liu"
                    },
                    {
                        "name": "Ning Wu"
                    },
                    {
                        "name": "Shiping Yang"
                    },
                    {
                        "name": "Wenbiao Ding"
                    },
                    {
                        "name": "Shining Liang"
                    },
                    {
                        "name": "Ming Gong"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13963v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13957v1",
                "updated": "2025-02-19T18:56:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    56,
                    3,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:56:03Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    56,
                    3,
                    2,
                    50,
                    0
                ],
                "title": "RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-Gym: Optimizing Reasoning and Search Agents with Process Supervision"
                },
                "summary": "Retrieval-augmented generation (RAG) has shown great potential for\nknowledge-intensive tasks, but its traditional architectures rely on static\nretrieval, limiting their effectiveness for complex questions that require\nsequential information-seeking. While agentic reasoning and search offer a more\nadaptive approach, most existing methods depend heavily on prompt engineering.\nIn this work, we introduce RAG-Gym, a unified optimization framework that\nenhances information-seeking agents through fine-grained process supervision at\neach search step. We also propose ReSearch, a novel agent architecture that\nsynergizes answer reasoning and search query generation within the RAG-Gym\nframework. Experiments on four challenging datasets show that RAG-Gym improves\nperformance by up to 25.6\\% across various agent architectures, with ReSearch\nconsistently outperforming existing baselines. Further analysis highlights the\neffectiveness of advanced LLMs as process reward judges and the transferability\nof trained reward models as verifiers for different LLMs. Additionally, we\nexamine the scaling properties of training and inference in agentic RAG. The\nproject homepage is available at https://rag-gym.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has shown great potential for\nknowledge-intensive tasks, but its traditional architectures rely on static\nretrieval, limiting their effectiveness for complex questions that require\nsequential information-seeking. While agentic reasoning and search offer a more\nadaptive approach, most existing methods depend heavily on prompt engineering.\nIn this work, we introduce RAG-Gym, a unified optimization framework that\nenhances information-seeking agents through fine-grained process supervision at\neach search step. We also propose ReSearch, a novel agent architecture that\nsynergizes answer reasoning and search query generation within the RAG-Gym\nframework. Experiments on four challenging datasets show that RAG-Gym improves\nperformance by up to 25.6\\% across various agent architectures, with ReSearch\nconsistently outperforming existing baselines. Further analysis highlights the\neffectiveness of advanced LLMs as process reward judges and the transferability\nof trained reward models as verifiers for different LLMs. Additionally, we\nexamine the scaling properties of training and inference in agentic RAG. The\nproject homepage is available at https://rag-gym.github.io/."
                },
                "authors": [
                    {
                        "name": "Guangzhi Xiong"
                    },
                    {
                        "name": "Qiao Jin"
                    },
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Yin Fang"
                    },
                    {
                        "name": "Haolin Liu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Fangyuan Chen"
                    },
                    {
                        "name": "Zhixing Song"
                    },
                    {
                        "name": "Dengyu Wang"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Zhiyong Lu"
                    },
                    {
                        "name": "Aidong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Aidong Zhang"
                },
                "author": "Aidong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13953v1",
                "updated": "2025-02-19T18:53:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    53,
                    16,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:53:16Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    53,
                    16,
                    2,
                    50,
                    0
                ],
                "title": "Neurosymbolic artificial intelligence via large language models and\n  coherence-driven inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neurosymbolic artificial intelligence via large language models and\n  coherence-driven inference"
                },
                "summary": "We devise an algorithm to generate sets of propositions that objectively\ninstantiate graphs that support coherence-driven inference. We then benchmark\nthe ability of large language models (LLMs) to reconstruct coherence graphs\nfrom (a straightforward transformation of) propositions expressed in natural\nlanguage, with promising results from a single prompt to models optimized for\nreasoning. Combining coherence-driven inference with consistency evaluations by\nneural models may advance the state of the art in machine cognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We devise an algorithm to generate sets of propositions that objectively\ninstantiate graphs that support coherence-driven inference. We then benchmark\nthe ability of large language models (LLMs) to reconstruct coherence graphs\nfrom (a straightforward transformation of) propositions expressed in natural\nlanguage, with promising results from a single prompt to models optimized for\nreasoning. Combining coherence-driven inference with consistency evaluations by\nneural models may advance the state of the art in machine cognition."
                },
                "authors": [
                    {
                        "name": "Steve Huntsman"
                    },
                    {
                        "name": "Jewell Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Jewell Thomas"
                },
                "author": "Jewell Thomas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.03315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.03315v2",
                "updated": "2025-02-19T18:52:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    52,
                    54,
                    2,
                    50,
                    0
                ],
                "published": "2023-09-06T18:56:20Z",
                "published_parsed": [
                    2023,
                    9,
                    6,
                    18,
                    56,
                    20,
                    2,
                    249,
                    0
                ],
                "title": "Robotic Table Tennis: A Case Study into a High Speed Learning System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic Table Tennis: A Case Study into a High Speed Learning System"
                },
                "summary": "We present a deep-dive into a real-world robotic learning system that, in\nprevious work, was shown to be capable of hundreds of table tennis rallies with\na human and has the ability to precisely return the ball to desired targets.\nThis system puts together a highly optimized perception subsystem, a high-speed\nlow-latency robot controller, a simulation paradigm that can prevent damage in\nthe real world and also train policies for zero-shot transfer, and automated\nreal world environment resets that enable autonomous training and evaluation on\nphysical robots. We complement a complete system description, including\nnumerous design decisions that are typically not widely disseminated, with a\ncollection of studies that clarify the importance of mitigating various sources\nof latency, accounting for training and deployment distribution shifts,\nrobustness of the perception system, sensitivity to policy hyper-parameters,\nand choice of action space. A video demonstrating the components of the system\nand details of experimental results can be found at\nhttps://youtu.be/uFcnWjB42I0.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a deep-dive into a real-world robotic learning system that, in\nprevious work, was shown to be capable of hundreds of table tennis rallies with\na human and has the ability to precisely return the ball to desired targets.\nThis system puts together a highly optimized perception subsystem, a high-speed\nlow-latency robot controller, a simulation paradigm that can prevent damage in\nthe real world and also train policies for zero-shot transfer, and automated\nreal world environment resets that enable autonomous training and evaluation on\nphysical robots. We complement a complete system description, including\nnumerous design decisions that are typically not widely disseminated, with a\ncollection of studies that clarify the importance of mitigating various sources\nof latency, accounting for training and deployment distribution shifts,\nrobustness of the perception system, sensitivity to policy hyper-parameters,\nand choice of action space. A video demonstrating the components of the system\nand details of experimental results can be found at\nhttps://youtu.be/uFcnWjB42I0."
                },
                "authors": [
                    {
                        "name": "David B. D'Ambrosio"
                    },
                    {
                        "name": "Jonathan Abelian"
                    },
                    {
                        "name": "Saminda Abeyruwan"
                    },
                    {
                        "name": "Michael Ahn"
                    },
                    {
                        "name": "Alex Bewley"
                    },
                    {
                        "name": "Justin Boyd"
                    },
                    {
                        "name": "Krzysztof Choromanski"
                    },
                    {
                        "name": "Omar Cortes"
                    },
                    {
                        "name": "Erwin Coumans"
                    },
                    {
                        "name": "Tianli Ding"
                    },
                    {
                        "name": "Wenbo Gao"
                    },
                    {
                        "name": "Laura Graesser"
                    },
                    {
                        "name": "Atil Iscen"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    },
                    {
                        "name": "Deepali Jain"
                    },
                    {
                        "name": "Juhana Kangaspunta"
                    },
                    {
                        "name": "Satoshi Kataoka"
                    },
                    {
                        "name": "Gus Kouretas"
                    },
                    {
                        "name": "Yuheng Kuang"
                    },
                    {
                        "name": "Nevena Lazic"
                    },
                    {
                        "name": "Corey Lynch"
                    },
                    {
                        "name": "Reza Mahjourian"
                    },
                    {
                        "name": "Sherry Q. Moore"
                    },
                    {
                        "name": "Thinh Nguyen"
                    },
                    {
                        "name": "Ken Oslund"
                    },
                    {
                        "name": "Barney J Reed"
                    },
                    {
                        "name": "Krista Reymann"
                    },
                    {
                        "name": "Pannag R. Sanketi"
                    },
                    {
                        "name": "Anish Shankar"
                    },
                    {
                        "name": "Pierre Sermanet"
                    },
                    {
                        "name": "Vikas Sindhwani"
                    },
                    {
                        "name": "Avi Singh"
                    },
                    {
                        "name": "Vincent Vanhoucke"
                    },
                    {
                        "name": "Grace Vesom"
                    },
                    {
                        "name": "Peng Xu"
                    }
                ],
                "author_detail": {
                    "name": "Peng Xu"
                },
                "author": "Peng Xu",
                "arxiv_doi": "10.15607/RSS.2023.XIX.006",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.15607/RSS.2023.XIX.006",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.03315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.03315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published and presented at Robotics: Science and Systems (RSS2023)",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13946v1",
                "updated": "2025-02-19T18:42:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    42,
                    45,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:42:45Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    42,
                    45,
                    2,
                    50,
                    0
                ],
                "title": "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety\n  Mechanisms Tend to Be Anchored in The Template Region",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety\n  Mechanisms Tend to Be Anchored in The Template Region"
                },
                "summary": "The safety alignment of large language models (LLMs) remains vulnerable, as\ntheir initial behavior can be easily jailbroken by even relatively simple\nattacks. Since infilling a fixed template between the input instruction and\ninitial model output is a common practice for existing LLMs, we hypothesize\nthat this template is a key factor behind their vulnerabilities: LLMs'\nsafety-related decision-making overly relies on the aggregated information from\nthe template region, which largely influences these models' safety behavior. We\nrefer to this issue as template-anchored safety alignment. In this paper, we\nconduct extensive experiments and verify that template-anchored safety\nalignment is widespread across various aligned LLMs. Our mechanistic analyses\ndemonstrate how it leads to models' susceptibility when encountering\ninference-time jailbreak attacks. Furthermore, we show that detaching safety\nmechanisms from the template region is promising in mitigating vulnerabilities\nto jailbreak attacks. We encourage future research to develop more robust\nsafety alignment techniques that reduce reliance on the template region.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The safety alignment of large language models (LLMs) remains vulnerable, as\ntheir initial behavior can be easily jailbroken by even relatively simple\nattacks. Since infilling a fixed template between the input instruction and\ninitial model output is a common practice for existing LLMs, we hypothesize\nthat this template is a key factor behind their vulnerabilities: LLMs'\nsafety-related decision-making overly relies on the aggregated information from\nthe template region, which largely influences these models' safety behavior. We\nrefer to this issue as template-anchored safety alignment. In this paper, we\nconduct extensive experiments and verify that template-anchored safety\nalignment is widespread across various aligned LLMs. Our mechanistic analyses\ndemonstrate how it leads to models' susceptibility when encountering\ninference-time jailbreak attacks. Furthermore, we show that detaching safety\nmechanisms from the template region is promising in mitigating vulnerabilities\nto jailbreak attacks. We encourage future research to develop more robust\nsafety alignment techniques that reduce reliance on the template region."
                },
                "authors": [
                    {
                        "name": "Chak Tou Leong"
                    },
                    {
                        "name": "Qingyu Yin"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Wenjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Li"
                },
                "author": "Wenjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03663v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03663v3",
                "updated": "2025-02-19T18:34:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    34,
                    19,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-04T17:59:41Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    17,
                    59,
                    41,
                    4,
                    278,
                    0
                ],
                "title": "Learning from Committee: Reasoning Distillation from a Mixture of\n  Teachers with Peer-Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Committee: Reasoning Distillation from a Mixture of\n  Teachers with Peer-Review"
                },
                "summary": "While reasoning capabilities typically emerge in large language models (LLMs)\nwith tens of billions of parameters, recent research focuses on improving\nsmaller open-source models through knowledge distillation (KD) from commercial\nLLMs. However, many of these studies rely solely on responses from a single LLM\nas the gold rationale, unlike the natural human learning process, which\ninvolves understanding both the correct answers and the reasons behind\nmistakes. In this paper, we introduce a novel Fault-Aware DistIllation via\nPeer-Review (FAIR) approach: 1) Instead of merely obtaining rationales from\nteachers, our method asks teachers to identify and explain the student's\nmistakes, providing customized instruction learning data. 2) We design a\nsimulated peer-review process between teacher LLMs, which selects only the\ngenerated rationales above the acceptance threshold. This reduces the chance of\nteachers guessing correctly with flawed rationale, improving instructional data\nquality. Comprehensive experiments and analysis on mathematical, commonsense,\nand logical reasoning tasks demonstrate the effectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reasoning capabilities typically emerge in large language models (LLMs)\nwith tens of billions of parameters, recent research focuses on improving\nsmaller open-source models through knowledge distillation (KD) from commercial\nLLMs. However, many of these studies rely solely on responses from a single LLM\nas the gold rationale, unlike the natural human learning process, which\ninvolves understanding both the correct answers and the reasons behind\nmistakes. In this paper, we introduce a novel Fault-Aware DistIllation via\nPeer-Review (FAIR) approach: 1) Instead of merely obtaining rationales from\nteachers, our method asks teachers to identify and explain the student's\nmistakes, providing customized instruction learning data. 2) We design a\nsimulated peer-review process between teacher LLMs, which selects only the\ngenerated rationales above the acceptance threshold. This reduces the chance of\nteachers guessing correctly with flawed rationale, improving instructional data\nquality. Comprehensive experiments and analysis on mathematical, commonsense,\nand logical reasoning tasks demonstrate the effectiveness of our method."
                },
                "authors": [
                    {
                        "name": "Zhuochun Li"
                    },
                    {
                        "name": "Yuelyu Ji"
                    },
                    {
                        "name": "Rui Meng"
                    },
                    {
                        "name": "Daqing He"
                    }
                ],
                "author_detail": {
                    "name": "Daqing He"
                },
                "author": "Daqing He",
                "arxiv_comment": "16 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03663v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03663v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02890v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02890v4",
                "updated": "2025-02-19T18:18:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    18,
                    11,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-03T18:28:10Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    18,
                    28,
                    10,
                    3,
                    277,
                    0
                ],
                "title": "Theoretically Grounded Framework for LLM Watermarking: A\n  Distribution-Adaptive Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretically Grounded Framework for LLM Watermarking: A\n  Distribution-Adaptive Approach"
                },
                "summary": "Watermarking has emerged as a crucial method to distinguish AI-generated text\nfrom human-created text. In this paper, we present a novel theoretical\nframework for watermarking Large Language Models (LLMs) that jointly optimizes\nboth the watermarking scheme and the detection process. Our approach focuses on\nmaximizing detection performance while maintaining control over the worst-case\nType-I error and text distortion. We characterize \\emph{the universally minimum\nType-II error}, showing a fundamental trade-off between watermark detectability\nand text distortion. Importantly, we identify that the optimal watermarking\nschemes are adaptive to the LLM generative distribution. Building on our\ntheoretical insights, we propose an efficient, model-agnostic,\ndistribution-adaptive watermarking algorithm, utilizing a surrogate model\nalongside the Gumbel-max trick. Experiments conducted on Llama2-13B and\nMistral-8$\\times$7B models confirm the effectiveness of our approach.\nAdditionally, we examine incorporating robustness into our framework, paving a\nway to future watermarking systems that withstand adversarial attacks more\neffectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking has emerged as a crucial method to distinguish AI-generated text\nfrom human-created text. In this paper, we present a novel theoretical\nframework for watermarking Large Language Models (LLMs) that jointly optimizes\nboth the watermarking scheme and the detection process. Our approach focuses on\nmaximizing detection performance while maintaining control over the worst-case\nType-I error and text distortion. We characterize \\emph{the universally minimum\nType-II error}, showing a fundamental trade-off between watermark detectability\nand text distortion. Importantly, we identify that the optimal watermarking\nschemes are adaptive to the LLM generative distribution. Building on our\ntheoretical insights, we propose an efficient, model-agnostic,\ndistribution-adaptive watermarking algorithm, utilizing a surrogate model\nalongside the Gumbel-max trick. Experiments conducted on Llama2-13B and\nMistral-8$\\times$7B models confirm the effectiveness of our approach.\nAdditionally, we examine incorporating robustness into our framework, paving a\nway to future watermarking systems that withstand adversarial attacks more\neffectively."
                },
                "authors": [
                    {
                        "name": "Haiyun He"
                    },
                    {
                        "name": "Yepeng Liu"
                    },
                    {
                        "name": "Ziqiao Wang"
                    },
                    {
                        "name": "Yongyi Mao"
                    },
                    {
                        "name": "Yuheng Bu"
                    }
                ],
                "author_detail": {
                    "name": "Yuheng Bu"
                },
                "author": "Yuheng Bu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02890v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02890v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13925v1",
                "updated": "2025-02-19T18:04:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    4,
                    44,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:04:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    4,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual\n  Narratives in Image Sequences?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single Frames: Can LMMs Comprehend Temporal and Contextual\n  Narratives in Image Sequences?"
                },
                "summary": "Large Multimodal Models (LMMs) have achieved remarkable success across\nvarious visual-language tasks. However, existing benchmarks predominantly focus\non single-image understanding, leaving the analysis of image sequences largely\nunexplored. To address this limitation, we introduce StripCipher, a\ncomprehensive benchmark designed to evaluate capabilities of LMMs to comprehend\nand reason over sequential images. StripCipher comprises a human-annotated\ndataset and three challenging subtasks: visual narrative comprehension,\ncontextual frame prediction, and temporal narrative reordering. Our evaluation\nof $16$ state-of-the-art LMMs, including GPT-4o and Qwen2.5VL, reveals a\nsignificant performance gap compared to human capabilities, particularly in\ntasks that require reordering shuffled sequential images. For instance, GPT-4o\nachieves only 23.93% accuracy in the reordering subtask, which is 56.07% lower\nthan human performance. Further quantitative analysis discuss several factors,\nsuch as input format of images, affecting the performance of LLMs in sequential\nunderstanding, underscoring the fundamental challenges that remain in the\ndevelopment of LMMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) have achieved remarkable success across\nvarious visual-language tasks. However, existing benchmarks predominantly focus\non single-image understanding, leaving the analysis of image sequences largely\nunexplored. To address this limitation, we introduce StripCipher, a\ncomprehensive benchmark designed to evaluate capabilities of LMMs to comprehend\nand reason over sequential images. StripCipher comprises a human-annotated\ndataset and three challenging subtasks: visual narrative comprehension,\ncontextual frame prediction, and temporal narrative reordering. Our evaluation\nof $16$ state-of-the-art LMMs, including GPT-4o and Qwen2.5VL, reveals a\nsignificant performance gap compared to human capabilities, particularly in\ntasks that require reordering shuffled sequential images. For instance, GPT-4o\nachieves only 23.93% accuracy in the reordering subtask, which is 56.07% lower\nthan human performance. Further quantitative analysis discuss several factors,\nsuch as input format of images, affecting the performance of LLMs in sequential\nunderstanding, underscoring the fundamental challenges that remain in the\ndevelopment of LMMs."
                },
                "authors": [
                    {
                        "name": "Xiaochen Wang"
                    },
                    {
                        "name": "Heming Xia"
                    },
                    {
                        "name": "Jialin Song"
                    },
                    {
                        "name": "Longyu Guan"
                    },
                    {
                        "name": "Yixin Yang"
                    },
                    {
                        "name": "Qingxiu Dong"
                    },
                    {
                        "name": "Weiyao Luo"
                    },
                    {
                        "name": "Yifan Pu"
                    },
                    {
                        "name": "Yiru Wang"
                    },
                    {
                        "name": "Xiangdi Meng"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Zhifang Sui"
                    }
                ],
                "author_detail": {
                    "name": "Zhifang Sui"
                },
                "author": "Zhifang Sui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13923v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13923v1",
                "updated": "2025-02-19T18:00:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    0,
                    14,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T18:00:14Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    18,
                    0,
                    14,
                    2,
                    50,
                    0
                ],
                "title": "Qwen2.5-VL Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qwen2.5-VL Technical Report"
                },
                "summary": "We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language\nseries, which demonstrates significant advancements in both foundational\ncapabilities and innovative functionalities. Qwen2.5-VL achieves a major leap\nforward in understanding and interacting with the world through enhanced visual\nrecognition, precise object localization, robust document parsing, and\nlong-video comprehension. A standout feature of Qwen2.5-VL is its ability to\nlocalize objects using bounding boxes or points accurately. It provides robust\nstructured data extraction from invoices, forms, and tables, as well as\ndetailed analysis of charts, diagrams, and layouts. To handle complex inputs,\nQwen2.5-VL introduces dynamic resolution processing and absolute time encoding,\nenabling it to process images of varying sizes and videos of extended durations\n(up to hours) with second-level event localization. This allows the model to\nnatively perceive spatial scales and temporal dynamics without relying on\ntraditional normalization techniques. By training a native dynamic-resolution\nVision Transformer (ViT) from scratch and incorporating Window Attention, we\nreduce computational overhead while maintaining native resolution. As a result,\nQwen2.5-VL excels not only in static image and document understanding but also\nas an interactive visual agent capable of reasoning, tool usage, and task\nexecution in real-world scenarios such as operating computers and mobile\ndevices. Qwen2.5-VL is available in three sizes, addressing diverse use cases\nfrom edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model\nmatches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly\nexcelling in document and diagram understanding. Additionally, Qwen2.5-VL\nmaintains robust linguistic performance, preserving the core language\ncompetencies of the Qwen2.5 LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Qwen2.5-VL, the latest flagship model of Qwen vision-language\nseries, which demonstrates significant advancements in both foundational\ncapabilities and innovative functionalities. Qwen2.5-VL achieves a major leap\nforward in understanding and interacting with the world through enhanced visual\nrecognition, precise object localization, robust document parsing, and\nlong-video comprehension. A standout feature of Qwen2.5-VL is its ability to\nlocalize objects using bounding boxes or points accurately. It provides robust\nstructured data extraction from invoices, forms, and tables, as well as\ndetailed analysis of charts, diagrams, and layouts. To handle complex inputs,\nQwen2.5-VL introduces dynamic resolution processing and absolute time encoding,\nenabling it to process images of varying sizes and videos of extended durations\n(up to hours) with second-level event localization. This allows the model to\nnatively perceive spatial scales and temporal dynamics without relying on\ntraditional normalization techniques. By training a native dynamic-resolution\nVision Transformer (ViT) from scratch and incorporating Window Attention, we\nreduce computational overhead while maintaining native resolution. As a result,\nQwen2.5-VL excels not only in static image and document understanding but also\nas an interactive visual agent capable of reasoning, tool usage, and task\nexecution in real-world scenarios such as operating computers and mobile\ndevices. Qwen2.5-VL is available in three sizes, addressing diverse use cases\nfrom edge AI to high-performance computing. The flagship Qwen2.5-VL-72B model\nmatches state-of-the-art models like GPT-4o and Claude 3.5 Sonnet, particularly\nexcelling in document and diagram understanding. Additionally, Qwen2.5-VL\nmaintains robust linguistic performance, preserving the core language\ncompetencies of the Qwen2.5 LLM."
                },
                "authors": [
                    {
                        "name": "Shuai Bai"
                    },
                    {
                        "name": "Keqin Chen"
                    },
                    {
                        "name": "Xuejing Liu"
                    },
                    {
                        "name": "Jialin Wang"
                    },
                    {
                        "name": "Wenbin Ge"
                    },
                    {
                        "name": "Sibo Song"
                    },
                    {
                        "name": "Kai Dang"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Shijie Wang"
                    },
                    {
                        "name": "Jun Tang"
                    },
                    {
                        "name": "Humen Zhong"
                    },
                    {
                        "name": "Yuanzhi Zhu"
                    },
                    {
                        "name": "Mingkun Yang"
                    },
                    {
                        "name": "Zhaohai Li"
                    },
                    {
                        "name": "Jianqiang Wan"
                    },
                    {
                        "name": "Pengfei Wang"
                    },
                    {
                        "name": "Wei Ding"
                    },
                    {
                        "name": "Zheren Fu"
                    },
                    {
                        "name": "Yiheng Xu"
                    },
                    {
                        "name": "Jiabo Ye"
                    },
                    {
                        "name": "Xi Zhang"
                    },
                    {
                        "name": "Tianbao Xie"
                    },
                    {
                        "name": "Zesen Cheng"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Zhibo Yang"
                    },
                    {
                        "name": "Haiyang Xu"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13923v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13923v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13922v1",
                "updated": "2025-02-19T17:59:03Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    59,
                    3,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T17:59:03Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    59,
                    3,
                    2,
                    50,
                    0
                ],
                "title": "LongPO: Long Context Self-Evolution of Large Language Models through\n  Short-to-Long Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongPO: Long Context Self-Evolution of Large Language Models through\n  Short-to-Long Preference Optimization"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities\nthrough pretraining and alignment. However, superior short-context LLMs may\nunderperform in long-context scenarios due to insufficient long-context\nalignment. This alignment process remains challenging due to the impracticality\nof human annotation for extended contexts and the difficulty in balancing\nshort- and long-context performance. To address these challenges, we introduce\nLongPO, that enables short-context LLMs to self-evolve to excel on long-context\ntasks by internally transferring short-context capabilities. LongPO harnesses\nLLMs to learn from self-generated short-to-long preference data, comprising\npaired responses generated for identical instructions with long-context inputs\nand their compressed short-context counterparts, respectively. This preference\nreveals capabilities and potentials of LLMs cultivated during short-context\nalignment that may be diminished in under-aligned long-context scenarios.\nAdditionally, LongPO incorporates a short-to-long KL constraint to mitigate\nshort-context performance decline during long-context alignment. When applied\nto Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully\nretains short-context performance and largely outperforms naive SFT and DPO in\nboth long- and short-context tasks. Specifically, \\ourMethod-trained models can\nachieve results on long-context benchmarks comparable to, or even surpassing,\nthose of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context\nannotation and larger parameter scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities\nthrough pretraining and alignment. However, superior short-context LLMs may\nunderperform in long-context scenarios due to insufficient long-context\nalignment. This alignment process remains challenging due to the impracticality\nof human annotation for extended contexts and the difficulty in balancing\nshort- and long-context performance. To address these challenges, we introduce\nLongPO, that enables short-context LLMs to self-evolve to excel on long-context\ntasks by internally transferring short-context capabilities. LongPO harnesses\nLLMs to learn from self-generated short-to-long preference data, comprising\npaired responses generated for identical instructions with long-context inputs\nand their compressed short-context counterparts, respectively. This preference\nreveals capabilities and potentials of LLMs cultivated during short-context\nalignment that may be diminished in under-aligned long-context scenarios.\nAdditionally, LongPO incorporates a short-to-long KL constraint to mitigate\nshort-context performance decline during long-context alignment. When applied\nto Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully\nretains short-context performance and largely outperforms naive SFT and DPO in\nboth long- and short-context tasks. Specifically, \\ourMethod-trained models can\nachieve results on long-context benchmarks comparable to, or even surpassing,\nthose of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context\nannotation and larger parameter scales."
                },
                "authors": [
                    {
                        "name": "Guanzheng Chen"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Michael Qizhe Shieh"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13921v1",
                "updated": "2025-02-19T17:53:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    59,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T17:53:59Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    59,
                    2,
                    50,
                    0
                ],
                "title": "Exploring Code Language Models for Automated HLS-based Hardware\n  Generation: Benchmark, Infrastructure and Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Code Language Models for Automated HLS-based Hardware\n  Generation: Benchmark, Infrastructure and Analysis"
                },
                "summary": "Recent advances in code generation have illuminated the potential of\nemploying large language models (LLMs) for general-purpose programming\nlanguages such as Python and C++, opening new opportunities for automating\nsoftware development and enhancing programmer productivity. The potential of\nLLMs in software programming has sparked significant interest in exploring\nautomated hardware generation and automation. Although preliminary endeavors\nhave been made to adopt LLMs in generating hardware description languages\n(HDLs), several challenges persist in this direction. First, the volume of\navailable HDL training data is substantially smaller compared to that for\nsoftware programming languages. Second, the pre-trained LLMs, mainly tailored\nfor software code, tend to produce HDL designs that are more error-prone.\nThird, the generation of HDL requires a significantly higher number of tokens\ncompared to software programming, leading to inefficiencies in cost and energy\nconsumption. To tackle these challenges, this paper explores leveraging LLMs to\ngenerate High-Level Synthesis (HLS)-based hardware design. Although code\ngeneration for domain-specific programming languages is not new in the\nliterature, we aim to provide experimental results, insights, benchmarks, and\nevaluation infrastructure to investigate the suitability of HLS over low-level\nHDLs for LLM-assisted hardware design generation. To achieve this, we first\nfinetune pre-trained models for HLS-based hardware generation, using a\ncollected dataset with text prompts and corresponding reference HLS designs. An\nLLM-assisted framework is then proposed to automate end-to-end hardware code\ngeneration, which also investigates the impact of chain-of-thought and feedback\nloops promoting techniques on HLS-design generation. Limited by the timeframe\nof this research, we plan to evaluate more advanced reasoning models in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in code generation have illuminated the potential of\nemploying large language models (LLMs) for general-purpose programming\nlanguages such as Python and C++, opening new opportunities for automating\nsoftware development and enhancing programmer productivity. The potential of\nLLMs in software programming has sparked significant interest in exploring\nautomated hardware generation and automation. Although preliminary endeavors\nhave been made to adopt LLMs in generating hardware description languages\n(HDLs), several challenges persist in this direction. First, the volume of\navailable HDL training data is substantially smaller compared to that for\nsoftware programming languages. Second, the pre-trained LLMs, mainly tailored\nfor software code, tend to produce HDL designs that are more error-prone.\nThird, the generation of HDL requires a significantly higher number of tokens\ncompared to software programming, leading to inefficiencies in cost and energy\nconsumption. To tackle these challenges, this paper explores leveraging LLMs to\ngenerate High-Level Synthesis (HLS)-based hardware design. Although code\ngeneration for domain-specific programming languages is not new in the\nliterature, we aim to provide experimental results, insights, benchmarks, and\nevaluation infrastructure to investigate the suitability of HLS over low-level\nHDLs for LLM-assisted hardware design generation. To achieve this, we first\nfinetune pre-trained models for HLS-based hardware generation, using a\ncollected dataset with text prompts and corresponding reference HLS designs. An\nLLM-assisted framework is then proposed to automate end-to-end hardware code\ngeneration, which also investigates the impact of chain-of-thought and feedback\nloops promoting techniques on HLS-design generation. Limited by the timeframe\nof this research, we plan to evaluate more advanced reasoning models in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiahao Gai"
                    },
                    {
                        "name": "Hao"
                    },
                    {
                        "name": "Chen"
                    },
                    {
                        "name": "Zhican Wang"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Wanru Zhao"
                    },
                    {
                        "name": "Nicholas Lane"
                    },
                    {
                        "name": "Hongxiang Fan"
                    }
                ],
                "author_detail": {
                    "name": "Hongxiang Fan"
                },
                "arxiv_affiliation": "Mark",
                "author": "Hongxiang Fan",
                "arxiv_comment": "Paper accepted by ASP-DAC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13920v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13920v1",
                "updated": "2025-02-19T17:53:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    43,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T17:53:43Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    53,
                    43,
                    2,
                    50,
                    0
                ],
                "title": "Exploring Personalized Health Support through Data-Driven, Theory-Guided\n  LLMs: A Case Study in Sleep Health",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Personalized Health Support through Data-Driven, Theory-Guided\n  LLMs: A Case Study in Sleep Health"
                },
                "summary": "Despite the prevalence of sleep-tracking devices, many individuals struggle\nto translate data into actionable improvements in sleep health. Current methods\noften provide data-driven suggestions but may not be feasible and adaptive to\nreal-life constraints and individual contexts. We present HealthGuru, a novel\nlarge language model-powered chatbot to enhance sleep health through\ndata-driven, theory-guided, and adaptive recommendations with conversational\nbehavior change support. HealthGuru's multi-agent framework integrates wearable\ndevice data, contextual information, and a contextual multi-armed bandit model\nto suggest tailored sleep-enhancing activities. The system facilitates natural\nconversations while incorporating data-driven insights and theoretical behavior\nchange techniques. Our eight-week in-the-wild deployment study with 16\nparticipants compared HealthGuru to a baseline chatbot. Results show improved\nmetrics like sleep duration and activity scores, higher quality responses, and\nincreased user motivation for behavior change with HealthGuru. We also identify\nchallenges and design considerations for personalization and user engagement in\nhealth chatbots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the prevalence of sleep-tracking devices, many individuals struggle\nto translate data into actionable improvements in sleep health. Current methods\noften provide data-driven suggestions but may not be feasible and adaptive to\nreal-life constraints and individual contexts. We present HealthGuru, a novel\nlarge language model-powered chatbot to enhance sleep health through\ndata-driven, theory-guided, and adaptive recommendations with conversational\nbehavior change support. HealthGuru's multi-agent framework integrates wearable\ndevice data, contextual information, and a contextual multi-armed bandit model\nto suggest tailored sleep-enhancing activities. The system facilitates natural\nconversations while incorporating data-driven insights and theoretical behavior\nchange techniques. Our eight-week in-the-wild deployment study with 16\nparticipants compared HealthGuru to a baseline chatbot. Results show improved\nmetrics like sleep duration and activity scores, higher quality responses, and\nincreased user motivation for behavior change with HealthGuru. We also identify\nchallenges and design considerations for personalization and user engagement in\nhealth chatbots."
                },
                "authors": [
                    {
                        "name": "Xingbo Wang"
                    },
                    {
                        "name": "Janessa Griffith"
                    },
                    {
                        "name": "Daniel A. Adler"
                    },
                    {
                        "name": "Joey Castillo"
                    },
                    {
                        "name": "Tanzeem Choudhury"
                    },
                    {
                        "name": "Fei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Wang"
                },
                "author": "Fei Wang",
                "arxiv_doi": "10.1145/3706598.3713852",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706598.3713852",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.13920v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13920v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to CHI Conference on Human Factors in Computing Systems (CHI\n  2025)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13913v1",
                "updated": "2025-02-19T17:46:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    46,
                    30,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T17:46:30Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    46,
                    30,
                    2,
                    50,
                    0
                ],
                "title": "How Do LLMs Perform Two-Hop Reasoning in Context?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Do LLMs Perform Two-Hop Reasoning in Context?"
                },
                "summary": "\"Socrates is human. All humans are mortal. Therefore, Socrates is mortal.\"\nThis classical example demonstrates two-hop reasoning, where a conclusion\nlogically follows from two connected premises. While transformer-based Large\nLanguage Models (LLMs) can make two-hop reasoning, they tend to collapse to\nrandom guessing when faced with distracting premises. To understand the\nunderlying mechanism, we train a three-layer transformer on synthetic two-hop\nreasoning tasks. The training dynamics show two stages: a slow learning phase,\nwhere the 3-layer transformer performs random guessing like LLMs, followed by\nan abrupt phase transitions, where the 3-layer transformer suddenly reaches\n$100%$ accuracy. Through reverse engineering, we explain the inner mechanisms\nfor how models learn to randomly guess between distractions initially, and how\nthey learn to ignore distractions eventually. We further propose a\nthree-parameter model that supports the causal claims for the mechanisms to the\ntraining dynamics of the transformer. Finally, experiments on LLMs suggest that\nthe discovered mechanisms generalize across scales. Our methodologies provide\nnew perspectives for scientific understandings of LLMs and our findings provide\nnew insights into how reasoning emerges during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Socrates is human. All humans are mortal. Therefore, Socrates is mortal.\"\nThis classical example demonstrates two-hop reasoning, where a conclusion\nlogically follows from two connected premises. While transformer-based Large\nLanguage Models (LLMs) can make two-hop reasoning, they tend to collapse to\nrandom guessing when faced with distracting premises. To understand the\nunderlying mechanism, we train a three-layer transformer on synthetic two-hop\nreasoning tasks. The training dynamics show two stages: a slow learning phase,\nwhere the 3-layer transformer performs random guessing like LLMs, followed by\nan abrupt phase transitions, where the 3-layer transformer suddenly reaches\n$100%$ accuracy. Through reverse engineering, we explain the inner mechanisms\nfor how models learn to randomly guess between distractions initially, and how\nthey learn to ignore distractions eventually. We further propose a\nthree-parameter model that supports the causal claims for the mechanisms to the\ntraining dynamics of the transformer. Finally, experiments on LLMs suggest that\nthe discovered mechanisms generalize across scales. Our methodologies provide\nnew perspectives for scientific understandings of LLMs and our findings provide\nnew insights into how reasoning emerges during training."
                },
                "authors": [
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Hanlin Zhu"
                    },
                    {
                        "name": "Ruiqi Zhang"
                    },
                    {
                        "name": "Jiantao Jiao"
                    },
                    {
                        "name": "Song Mei"
                    },
                    {
                        "name": "Michael I. Jordan"
                    },
                    {
                        "name": "Stuart Russell"
                    }
                ],
                "author_detail": {
                    "name": "Stuart Russell"
                },
                "author": "Stuart Russell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13909v1",
                "updated": "2025-02-19T17:41:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    41,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T17:41:09Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    41,
                    9,
                    2,
                    50,
                    0
                ],
                "title": "Lost in Sequence: Do Large Language Models Understand Sequential\n  Recommendation?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lost in Sequence: Do Large Language Models Understand Sequential\n  Recommendation?"
                },
                "summary": "Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently emerged as promising tools for\nrecommendation thanks to their advanced textual understanding ability and\ncontext-awareness. Despite the current practice of training and evaluating\nLLM-based recommendation (LLM4Rec) models under a sequential recommendation\nscenario, we found that whether these models understand the sequential\ninformation inherent in users' item interaction sequences has been largely\noverlooked. In this paper, we first demonstrate through a series of experiments\nthat existing LLM4Rec models do not fully capture sequential information both\nduring training and inference. Then, we propose a simple yet effective\nLLM-based sequential recommender, called LLM-SRec, a method that enhances the\nintegration of sequential information into LLMs by distilling the user\nrepresentations extracted from a pre-trained CF-SRec model into LLMs. Our\nextensive experiments show that LLM-SRec enhances LLMs' ability to understand\nusers' item interaction sequences, ultimately leading to improved\nrecommendation performance. Furthermore, unlike existing LLM4Rec models that\nrequire fine-tuning of LLMs, LLM-SRec achieves state-of-the-art performance by\ntraining only a few lightweight MLPs, highlighting its practicality in\nreal-world applications. Our code is available at\nhttps://github.com/Sein-Kim/LLM-SRec."
                },
                "authors": [
                    {
                        "name": "Sein Kim"
                    },
                    {
                        "name": "Hongseok Kang"
                    },
                    {
                        "name": "Kibum Kim"
                    },
                    {
                        "name": "Jiwan Kim"
                    },
                    {
                        "name": "Donghyun Kim"
                    },
                    {
                        "name": "Minchul Yang"
                    },
                    {
                        "name": "Kwangjin Oh"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Chanyoung Park"
                    }
                ],
                "author_detail": {
                    "name": "Chanyoung Park"
                },
                "author": "Chanyoung Park",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13908v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13908v1",
                "updated": "2025-02-19T17:40:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    40,
                    32,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T17:40:32Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    40,
                    32,
                    2,
                    50,
                    0
                ],
                "title": "Judging the Judges: A Collection of LLM-Generated Relevance Judgements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judging the Judges: A Collection of LLM-Generated Relevance Judgements"
                },
                "summary": "Using Large Language Models (LLMs) for relevance assessments offers promising\nopportunities to improve Information Retrieval (IR), Natural Language\nProcessing (NLP), and related fields. Indeed, LLMs hold the promise of allowing\nIR experimenters to build evaluation collections with a fraction of the manual\nhuman labor currently required. This could help with fresh topics on which\nthere is still limited knowledge and could mitigate the challenges of\nevaluating ranking systems in low-resource scenarios, where it is challenging\nto find human annotators. Given the fast-paced recent developments in the\ndomain, many questions concerning LLMs as assessors are yet to be answered.\nAmong the aspects that require further investigation, we can list the impact of\nvarious components in a relevance judgment generation pipeline, such as the\nprompt used or the LLM chosen.\n  This paper benchmarks and reports on the results of a large-scale automatic\nrelevance judgment evaluation, the LLMJudge challenge at SIGIR 2024, where\ndifferent relevance assessment approaches were proposed. In detail, we release\nand benchmark 42 LLM-generated labels of the TREC 2023 Deep Learning track\nrelevance judgments produced by eight international teams who participated in\nthe challenge. Given their diverse nature, these automatically generated\nrelevance judgments can help the community not only investigate systematic\nbiases caused by LLMs but also explore the effectiveness of ensemble models,\nanalyze the trade-offs between different models and human assessors, and\nadvance methodologies for improving automated evaluation techniques. The\nreleased resource is available at the following link:\nhttps://llm4eval.github.io/LLMJudge-benchmark/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models (LLMs) for relevance assessments offers promising\nopportunities to improve Information Retrieval (IR), Natural Language\nProcessing (NLP), and related fields. Indeed, LLMs hold the promise of allowing\nIR experimenters to build evaluation collections with a fraction of the manual\nhuman labor currently required. This could help with fresh topics on which\nthere is still limited knowledge and could mitigate the challenges of\nevaluating ranking systems in low-resource scenarios, where it is challenging\nto find human annotators. Given the fast-paced recent developments in the\ndomain, many questions concerning LLMs as assessors are yet to be answered.\nAmong the aspects that require further investigation, we can list the impact of\nvarious components in a relevance judgment generation pipeline, such as the\nprompt used or the LLM chosen.\n  This paper benchmarks and reports on the results of a large-scale automatic\nrelevance judgment evaluation, the LLMJudge challenge at SIGIR 2024, where\ndifferent relevance assessment approaches were proposed. In detail, we release\nand benchmark 42 LLM-generated labels of the TREC 2023 Deep Learning track\nrelevance judgments produced by eight international teams who participated in\nthe challenge. Given their diverse nature, these automatically generated\nrelevance judgments can help the community not only investigate systematic\nbiases caused by LLMs but also explore the effectiveness of ensemble models,\nanalyze the trade-offs between different models and human assessors, and\nadvance methodologies for improving automated evaluation techniques. The\nreleased resource is available at the following link:\nhttps://llm4eval.github.io/LLMJudge-benchmark/"
                },
                "authors": [
                    {
                        "name": "Hossein A. Rahmani"
                    },
                    {
                        "name": "Clemencia Siro"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Nick Craswell"
                    },
                    {
                        "name": "Charles L. A. Clarke"
                    },
                    {
                        "name": "Guglielmo Faggioli"
                    },
                    {
                        "name": "Bhaskar Mitra"
                    },
                    {
                        "name": "Paul Thomas"
                    },
                    {
                        "name": "Emine Yilmaz"
                    }
                ],
                "author_detail": {
                    "name": "Emine Yilmaz"
                },
                "author": "Emine Yilmaz",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13908v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13908v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15867v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15867v4",
                "updated": "2025-02-19T17:35:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    35,
                    2,
                    2,
                    50,
                    0
                ],
                "published": "2024-08-28T15:35:05Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    35,
                    5,
                    2,
                    241,
                    0
                ],
                "title": "Practical Challenges for Reliable RIS Deployment in Heterogeneous\n  Multi-Operator Multi-Band Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Challenges for Reliable RIS Deployment in Heterogeneous\n  Multi-Operator Multi-Band Networks"
                },
                "summary": "Reconfigurable intelligent surfaces (RISs) have been introduced as arrays of\nnearly passive elements with software-tunable electromagnetic properties to\ndynamically manipulate the reflection/transmission of radio signals. Research\nworks in this area are focused on two applications, namely {\\it user-assist}\nRIS aiming at tuning the RIS to enhance the quality-of-service (QoS) of target\nusers, and the {\\it malicious} RIS aiming for an attacker to degrade the QoS at\nvictim receivers through generating {\\it intended} destructive interference.\nWhile both user-assist and malicious RIS applications have been explored\nextensively, the impact of RIS deployments on imposing {\\it unintended}\ninterference on various wireless user-equipments (EUs) remains underexplored.\nThis paper investigates the challenges of integrating RISs into multi-carrier,\nmulti-user, and multi-operator networks. We discuss how RIS deployments\nintended to benefit specific users can negatively impact other users served at\nvarious carrier frequencies through different network operators. While not an\nideal solution, we discuss how ultra-narrowband metasurfaces can be\nincorporated into the manufacturing of RISs to mitigate some challenges of RIS\ndeployment in wireless networks. We also present a simulation scenario to\nilluminate some practical challenges associated with the deployment of RISs in\nshared public environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surfaces (RISs) have been introduced as arrays of\nnearly passive elements with software-tunable electromagnetic properties to\ndynamically manipulate the reflection/transmission of radio signals. Research\nworks in this area are focused on two applications, namely {\\it user-assist}\nRIS aiming at tuning the RIS to enhance the quality-of-service (QoS) of target\nusers, and the {\\it malicious} RIS aiming for an attacker to degrade the QoS at\nvictim receivers through generating {\\it intended} destructive interference.\nWhile both user-assist and malicious RIS applications have been explored\nextensively, the impact of RIS deployments on imposing {\\it unintended}\ninterference on various wireless user-equipments (EUs) remains underexplored.\nThis paper investigates the challenges of integrating RISs into multi-carrier,\nmulti-user, and multi-operator networks. We discuss how RIS deployments\nintended to benefit specific users can negatively impact other users served at\nvarious carrier frequencies through different network operators. While not an\nideal solution, we discuss how ultra-narrowband metasurfaces can be\nincorporated into the manufacturing of RISs to mitigate some challenges of RIS\ndeployment in wireless networks. We also present a simulation scenario to\nilluminate some practical challenges associated with the deployment of RISs in\nshared public environments."
                },
                "authors": [
                    {
                        "name": "Mehdi Monemi"
                    },
                    {
                        "name": "Mehdi Rasti"
                    },
                    {
                        "name": "Arthur S. de Sena"
                    },
                    {
                        "name": "Mohammad Amir Fallah"
                    },
                    {
                        "name": "Matti Latva-Aho"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15867v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15867v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13897v1",
                "updated": "2025-02-19T17:31:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    31,
                    51,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T17:31:51Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    31,
                    51,
                    2,
                    50,
                    0
                ],
                "title": "DataSciBench: An LLM Agent Benchmark for Data Science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DataSciBench: An LLM Agent Benchmark for Data Science"
                },
                "summary": "This paper presents DataSciBench, a comprehensive benchmark for evaluating\nLarge Language Model (LLM) capabilities in data science. Recent related\nbenchmarks have primarily focused on single tasks, easily obtainable ground\ntruth, and straightforward evaluation metrics, which limits the scope of tasks\nthat can be evaluated. In contrast, DataSciBench is constructed based on a more\ncomprehensive and curated collection of natural and challenging prompts for\nuncertain ground truth and evaluation metrics. We develop a semi-automated\npipeline for generating ground truth (GT) and validating evaluation metrics.\nThis pipeline utilizes and implements an LLM-based self-consistency and human\nverification strategy to produce accurate GT by leveraging collected prompts,\npredefined task types, and aggregate functions (metrics). Furthermore, we\npropose an innovative Task - Function - Code (TFC) framework to assess each\ncode execution outcome based on precisely defined metrics and programmatic\nrules. Our experimental framework involves testing 6 API-based models, 8\nopen-source general models, and 9 open-source code generation models using the\ndiverse set of prompts we have gathered. This approach aims to provide a more\ncomprehensive and rigorous evaluation of LLMs in data science, revealing their\nstrengths and weaknesses. Experimental results demonstrate that API-based\nmodels outperform open-sourced models on all metrics and\nDeepseek-Coder-33B-Instruct achieves the highest score among open-sourced\nmodels. We release all code and data at https://github.com/THUDM/DataSciBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents DataSciBench, a comprehensive benchmark for evaluating\nLarge Language Model (LLM) capabilities in data science. Recent related\nbenchmarks have primarily focused on single tasks, easily obtainable ground\ntruth, and straightforward evaluation metrics, which limits the scope of tasks\nthat can be evaluated. In contrast, DataSciBench is constructed based on a more\ncomprehensive and curated collection of natural and challenging prompts for\nuncertain ground truth and evaluation metrics. We develop a semi-automated\npipeline for generating ground truth (GT) and validating evaluation metrics.\nThis pipeline utilizes and implements an LLM-based self-consistency and human\nverification strategy to produce accurate GT by leveraging collected prompts,\npredefined task types, and aggregate functions (metrics). Furthermore, we\npropose an innovative Task - Function - Code (TFC) framework to assess each\ncode execution outcome based on precisely defined metrics and programmatic\nrules. Our experimental framework involves testing 6 API-based models, 8\nopen-source general models, and 9 open-source code generation models using the\ndiverse set of prompts we have gathered. This approach aims to provide a more\ncomprehensive and rigorous evaluation of LLMs in data science, revealing their\nstrengths and weaknesses. Experimental results demonstrate that API-based\nmodels outperform open-sourced models on all metrics and\nDeepseek-Coder-33B-Instruct achieves the highest score among open-sourced\nmodels. We release all code and data at https://github.com/THUDM/DataSciBench."
                },
                "authors": [
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Sining Zhoubian"
                    },
                    {
                        "name": "Min Cai"
                    },
                    {
                        "name": "Fengzu Li"
                    },
                    {
                        "name": "Lekang Yang"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Tianjiao Dong"
                    },
                    {
                        "name": "Ziniu Hu"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Yisong Yue"
                    }
                ],
                "author_detail": {
                    "name": "Yisong Yue"
                },
                "author": "Yisong Yue",
                "arxiv_comment": "40 pages, 7 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03672v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03672v2",
                "updated": "2025-02-19T17:21:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    21,
                    53,
                    2,
                    50,
                    0
                ],
                "published": "2024-11-06T05:11:25Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    5,
                    11,
                    25,
                    2,
                    311,
                    0
                ],
                "title": "MetaSSC: Enhancing 3D Semantic Scene Completion for Autonomous Driving\n  through Meta-Learning and Long-sequence Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaSSC: Enhancing 3D Semantic Scene Completion for Autonomous Driving\n  through Meta-Learning and Long-sequence Modeling"
                },
                "summary": "Semantic scene completion (SSC) is essential for achieving comprehensive\nperception in autonomous driving systems. However, existing SSC methods often\noverlook the high deployment costs in real-world applications. Traditional\narchitectures, such as 3D Convolutional Neural Networks (3D CNNs) and\nself-attention mechanisms, face challenges in efficiently capturing long-range\ndependencies within 3D voxel grids, limiting their effectiveness. To address\nthese issues, we introduce MetaSSC, a novel meta-learning-based framework for\nSSC that leverages deformable convolution, large-kernel attention, and the\nMamba (D-LKA-M) model. Our approach begins with a voxel-based semantic\nsegmentation (SS) pretraining task, aimed at exploring the semantics and\ngeometry of incomplete regions while acquiring transferable meta-knowledge.\nUsing simulated cooperative perception datasets, we supervise the perception\ntraining of a single vehicle using aggregated sensor data from multiple nearby\nconnected autonomous vehicles (CAVs), generating richer and more comprehensive\nlabels. This meta-knowledge is then adapted to the target domain through a\ndual-phase training strategy that does not add extra model parameters, enabling\nefficient deployment. To further enhance the model's capability in capturing\nlong-sequence relationships within 3D voxel grids, we integrate Mamba blocks\nwith deformable convolution and large-kernel attention into the backbone\nnetwork. Extensive experiments demonstrate that MetaSSC achieves\nstate-of-the-art performance, significantly outperforming competing models\nwhile also reducing deployment costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic scene completion (SSC) is essential for achieving comprehensive\nperception in autonomous driving systems. However, existing SSC methods often\noverlook the high deployment costs in real-world applications. Traditional\narchitectures, such as 3D Convolutional Neural Networks (3D CNNs) and\nself-attention mechanisms, face challenges in efficiently capturing long-range\ndependencies within 3D voxel grids, limiting their effectiveness. To address\nthese issues, we introduce MetaSSC, a novel meta-learning-based framework for\nSSC that leverages deformable convolution, large-kernel attention, and the\nMamba (D-LKA-M) model. Our approach begins with a voxel-based semantic\nsegmentation (SS) pretraining task, aimed at exploring the semantics and\ngeometry of incomplete regions while acquiring transferable meta-knowledge.\nUsing simulated cooperative perception datasets, we supervise the perception\ntraining of a single vehicle using aggregated sensor data from multiple nearby\nconnected autonomous vehicles (CAVs), generating richer and more comprehensive\nlabels. This meta-knowledge is then adapted to the target domain through a\ndual-phase training strategy that does not add extra model parameters, enabling\nefficient deployment. To further enhance the model's capability in capturing\nlong-sequence relationships within 3D voxel grids, we integrate Mamba blocks\nwith deformable convolution and large-kernel attention into the backbone\nnetwork. Extensive experiments demonstrate that MetaSSC achieves\nstate-of-the-art performance, significantly outperforming competing models\nwhile also reducing deployment costs."
                },
                "authors": [
                    {
                        "name": "Yansong Qu"
                    },
                    {
                        "name": "Zixuan Xu"
                    },
                    {
                        "name": "Zilin Huang"
                    },
                    {
                        "name": "Zihao Sheng"
                    },
                    {
                        "name": "Tiantian Chen"
                    },
                    {
                        "name": "Sikai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Sikai Chen"
                },
                "author": "Sikai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03672v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11589v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11589v3",
                "updated": "2025-02-19T17:12:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    12,
                    45,
                    2,
                    50,
                    0
                ],
                "published": "2024-06-17T14:34:14Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    14,
                    34,
                    14,
                    0,
                    169,
                    0
                ],
                "title": "CoSQA+: Pioneering the Multi-Choice Code Search Benchmark with\n  Test-Driven Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSQA+: Pioneering the Multi-Choice Code Search Benchmark with\n  Test-Driven Agents"
                },
                "summary": "Semantic code search, retrieving code that matches a given natural language\nquery, is an important task to improve productivity in software engineering.\nExisting code search datasets face limitations: they rely on human annotators\nwho assess code primarily through semantic understanding rather than functional\nverification, leading to potential inaccuracies and scalability issues.\nAdditionally, current evaluation metrics often overlook the multi-choice nature\nof code search. This paper introduces CoSQA+, pairing high-quality queries from\nCoSQA with multiple suitable codes. We develop an automated pipeline featuring\nmultiple model-based candidate selections and the novel test-driven agent\nannotation system. Among a single Large Language Model (LLM) annotator and\nPython expert annotators (without test-based verification), agents leverage\ntest-based verification and achieve the highest accuracy of 96.4%. Through\nextensive experiments, CoSQA+ has demonstrated superior quality over CoSQA.\nModels trained on CoSQA+ exhibit improved performance. We provide the code and\ndata at https://github.com/DeepSoftwareAnalytics/CoSQA_Plus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic code search, retrieving code that matches a given natural language\nquery, is an important task to improve productivity in software engineering.\nExisting code search datasets face limitations: they rely on human annotators\nwho assess code primarily through semantic understanding rather than functional\nverification, leading to potential inaccuracies and scalability issues.\nAdditionally, current evaluation metrics often overlook the multi-choice nature\nof code search. This paper introduces CoSQA+, pairing high-quality queries from\nCoSQA with multiple suitable codes. We develop an automated pipeline featuring\nmultiple model-based candidate selections and the novel test-driven agent\nannotation system. Among a single Large Language Model (LLM) annotator and\nPython expert annotators (without test-based verification), agents leverage\ntest-based verification and achieve the highest accuracy of 96.4%. Through\nextensive experiments, CoSQA+ has demonstrated superior quality over CoSQA.\nModels trained on CoSQA+ exhibit improved performance. We provide the code and\ndata at https://github.com/DeepSoftwareAnalytics/CoSQA_Plus."
                },
                "authors": [
                    {
                        "name": "Jing Gong"
                    },
                    {
                        "name": "Yanghui Wu"
                    },
                    {
                        "name": "Linxi Liang"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "15 pages, 4 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11589v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11589v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13879v1",
                "updated": "2025-02-19T17:03:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    3,
                    50,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T17:03:50Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    17,
                    3,
                    50,
                    2,
                    50,
                    0
                ],
                "title": "A measurement-based approach to analyze the power consumption of the\n  softwarized 5G core",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A measurement-based approach to analyze the power consumption of the\n  softwarized 5G core"
                },
                "summary": "In light of the ever growing energy needs of the ICT sector, a value that is\nbecoming increasingly important for a mobile network is its power consumption.\nHowever, the transition away from legacy network deployments tightly coupled\nwith the underlying hardware and the adoption of the Network Function\nVirtualization (NFV) paradigm has made more difficult to accurately evaluate\ntheir energy and carbon footprint. In this paper, we propose and validate a\nmeasurement-based approach to analyze the power consumption of a virtualized 5G\ncore network (5GC) deployment. We design an experimental testbed using\ncommercial off-the-shelf (COTS) hardware and open-source software as a sample\narchitecture simulating an edge computing node and supporting three different\nvirtualization options. We make use of both hardware-based and software-based\npower meters to investigate the power consumption trends associated with\nincreasing levels of traffic and multiple 5GC deployment types. The results\nshow the feasibility of a real-time power monitoring system and highlight how\ndeployment choices, such as virtualization framework and 5GC software, can\nsignificantly impact on the power consumption of the network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In light of the ever growing energy needs of the ICT sector, a value that is\nbecoming increasingly important for a mobile network is its power consumption.\nHowever, the transition away from legacy network deployments tightly coupled\nwith the underlying hardware and the adoption of the Network Function\nVirtualization (NFV) paradigm has made more difficult to accurately evaluate\ntheir energy and carbon footprint. In this paper, we propose and validate a\nmeasurement-based approach to analyze the power consumption of a virtualized 5G\ncore network (5GC) deployment. We design an experimental testbed using\ncommercial off-the-shelf (COTS) hardware and open-source software as a sample\narchitecture simulating an edge computing node and supporting three different\nvirtualization options. We make use of both hardware-based and software-based\npower meters to investigate the power consumption trends associated with\nincreasing levels of traffic and multiple 5GC deployment types. The results\nshow the feasibility of a real-time power monitoring system and highlight how\ndeployment choices, such as virtualization framework and 5GC software, can\nsignificantly impact on the power consumption of the network."
                },
                "authors": [
                    {
                        "name": "Arturo Bellin"
                    },
                    {
                        "name": "Fabrizio Granelli"
                    },
                    {
                        "name": "Daniele Munaretto"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Munaretto"
                },
                "author": "Daniele Munaretto",
                "arxiv_doi": "10.1016/j.comnet.2024.110312",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.comnet.2024.110312",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.13879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Computer Networks, Volume 244, 2024, 110312",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13870v1",
                "updated": "2025-02-19T16:49:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    49,
                    55,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:49:55Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    49,
                    55,
                    2,
                    50,
                    0
                ],
                "title": "SPEX: Scaling Feature Interaction Explanations for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPEX: Scaling Feature Interaction Explanations for LLMs"
                },
                "summary": "Large language models (LLMs) have revolutionized machine learning due to\ntheir ability to capture complex interactions between input features. Popular\npost-hoc explanation methods like SHAP provide marginal feature attributions,\nwhile their extensions to interaction importances only scale to small input\nlengths ($\\approx 20$). We propose Spectral Explainer (SPEX), a model-agnostic\ninteraction attribution algorithm that efficiently scales to large input\nlengths ($\\approx 1000)$. SPEX exploits underlying natural sparsity among\ninteractions -- common in real-world data -- and applies a sparse Fourier\ntransform using a channel decoding algorithm to efficiently identify important\ninteractions. We perform experiments across three difficult long-context\ndatasets that require LLMs to utilize interactions between inputs to complete\nthe task. For large inputs, SPEX outperforms marginal attribution methods by up\nto 20% in terms of faithfully reconstructing LLM outputs. Further, SPEX\nsuccessfully identifies key features and interactions that strongly influence\nmodel output. For one of our datasets, HotpotQA, SPEX provides interactions\nthat align with human annotations. Finally, we use our model-agnostic approach\nto generate explanations to demonstrate abstract reasoning in closed-source\nLLMs (GPT-4o mini) and compositional reasoning in vision-language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized machine learning due to\ntheir ability to capture complex interactions between input features. Popular\npost-hoc explanation methods like SHAP provide marginal feature attributions,\nwhile their extensions to interaction importances only scale to small input\nlengths ($\\approx 20$). We propose Spectral Explainer (SPEX), a model-agnostic\ninteraction attribution algorithm that efficiently scales to large input\nlengths ($\\approx 1000)$. SPEX exploits underlying natural sparsity among\ninteractions -- common in real-world data -- and applies a sparse Fourier\ntransform using a channel decoding algorithm to efficiently identify important\ninteractions. We perform experiments across three difficult long-context\ndatasets that require LLMs to utilize interactions between inputs to complete\nthe task. For large inputs, SPEX outperforms marginal attribution methods by up\nto 20% in terms of faithfully reconstructing LLM outputs. Further, SPEX\nsuccessfully identifies key features and interactions that strongly influence\nmodel output. For one of our datasets, HotpotQA, SPEX provides interactions\nthat align with human annotations. Finally, we use our model-agnostic approach\nto generate explanations to demonstrate abstract reasoning in closed-source\nLLMs (GPT-4o mini) and compositional reasoning in vision-language models."
                },
                "authors": [
                    {
                        "name": "Justin Singh Kang"
                    },
                    {
                        "name": "Landon Butler"
                    },
                    {
                        "name": "Abhineet Agarwal"
                    },
                    {
                        "name": "Yigit Efe Erginbas"
                    },
                    {
                        "name": "Ramtin Pedarsani"
                    },
                    {
                        "name": "Kannan Ramchandran"
                    },
                    {
                        "name": "Bin Yu"
                    }
                ],
                "author_detail": {
                    "name": "Bin Yu"
                },
                "author": "Bin Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12633v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12633v2",
                "updated": "2025-02-19T16:45:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    45,
                    48,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-18T08:24:52Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    8,
                    24,
                    52,
                    1,
                    49,
                    0
                ],
                "title": "One Size doesn't Fit All: A Personalized Conversational Tutoring Agent\n  for Mathematics Instruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Size doesn't Fit All: A Personalized Conversational Tutoring Agent\n  for Mathematics Instruction"
                },
                "summary": "Large language models (LLMs) have been increasingly employed in various\nintelligent educational systems, simulating human tutors to facilitate\neffective human-machine interaction. However, previous studies often overlook\nthe significance of recognizing and adapting to individual learner\ncharacteristics. Such adaptation is crucial for enhancing student engagement\nand learning efficiency, particularly in mathematics instruction, where diverse\nlearning styles require personalized strategies to promote comprehension and\nenthusiasm. In this paper, we propose a \\textbf{P}erson\\textbf{A}lized\n\\textbf{C}onversational tutoring ag\\textbf{E}nt (PACE) for mathematics\ninstruction. PACE simulates students' learning styles based on the Felder and\nSilverman learning style model, aligning with each student's persona. In this\nway, our PACE can effectively assess the personality of students, allowing to\ndevelop individualized teaching strategies that resonate with their unique\nlearning styles. To further enhance students' comprehension, PACE employs the\nSocratic teaching method to provide instant feedback and encourage deep\nthinking. By constructing personalized teaching data and training models, PACE\ndemonstrates the ability to identify and adapt to the unique needs of each\nstudent, significantly improving the overall learning experience and outcomes.\nMoreover, we establish multi-aspect evaluation criteria and conduct extensive\nanalysis to assess the performance of personalized teaching. Experimental\nresults demonstrate the superiority of our model in personalizing the\neducational experience and motivating students compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been increasingly employed in various\nintelligent educational systems, simulating human tutors to facilitate\neffective human-machine interaction. However, previous studies often overlook\nthe significance of recognizing and adapting to individual learner\ncharacteristics. Such adaptation is crucial for enhancing student engagement\nand learning efficiency, particularly in mathematics instruction, where diverse\nlearning styles require personalized strategies to promote comprehension and\nenthusiasm. In this paper, we propose a \\textbf{P}erson\\textbf{A}lized\n\\textbf{C}onversational tutoring ag\\textbf{E}nt (PACE) for mathematics\ninstruction. PACE simulates students' learning styles based on the Felder and\nSilverman learning style model, aligning with each student's persona. In this\nway, our PACE can effectively assess the personality of students, allowing to\ndevelop individualized teaching strategies that resonate with their unique\nlearning styles. To further enhance students' comprehension, PACE employs the\nSocratic teaching method to provide instant feedback and encourage deep\nthinking. By constructing personalized teaching data and training models, PACE\ndemonstrates the ability to identify and adapt to the unique needs of each\nstudent, significantly improving the overall learning experience and outcomes.\nMoreover, we establish multi-aspect evaluation criteria and conduct extensive\nanalysis to assess the performance of personalized teaching. Experimental\nresults demonstrate the superiority of our model in personalizing the\neducational experience and motivating students compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Ben Liu"
                    },
                    {
                        "name": "Jihan Zhang"
                    },
                    {
                        "name": "Fangquan Lin"
                    },
                    {
                        "name": "Xu Jia"
                    },
                    {
                        "name": "Min Peng"
                    }
                ],
                "author_detail": {
                    "name": "Min Peng"
                },
                "author": "Min Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12633v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12633v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09575v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09575v2",
                "updated": "2025-02-19T16:32:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    32,
                    42,
                    2,
                    50,
                    0
                ],
                "published": "2024-09-15T01:32:57Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    1,
                    32,
                    57,
                    6,
                    259,
                    0
                ],
                "title": "Traffic Scene Generation from Natural Language Description for\n  Autonomous Vehicles with Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic Scene Generation from Natural Language Description for\n  Autonomous Vehicles with Large Language Model"
                },
                "summary": "Text-to-scene generation typically limits environmental diversity by\ngenerating key scenarios along predetermined paths. To address these\nconstraints, we propose a novel text-to-traffic scene framework that leverages\na large language model (LLM) to autonomously generate diverse traffic scenarios\nfor the CARLA simulator based on natural language descriptions. Our pipeline\ncomprises several key stages: (1) Prompt Analysis, where natural language\ninputs are decomposed; (2) Road Retrieval, selecting optimal roads from a\ndatabase; (3) Agent Planning, detailing agent types and behaviors; (4) Road\nRanking, scoring roads to match scenario requirements; and (5) Scene\nGeneration, rendering the planned scenarios in the simulator. This framework\nsupports both routine and critical traffic scenarios, enhancing its\napplicability. We demonstrate that our approach not only diversifies agent\nplanning and road selection but also significantly reduces the average\ncollision rate from 8% to 3.5% in SafeBench. Additionally, our framework\nimproves narration and reasoning for driving captioning tasks. Our\ncontributions and resources are publicly available at\nhttps://basiclab.github.io/TTSG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-scene generation typically limits environmental diversity by\ngenerating key scenarios along predetermined paths. To address these\nconstraints, we propose a novel text-to-traffic scene framework that leverages\na large language model (LLM) to autonomously generate diverse traffic scenarios\nfor the CARLA simulator based on natural language descriptions. Our pipeline\ncomprises several key stages: (1) Prompt Analysis, where natural language\ninputs are decomposed; (2) Road Retrieval, selecting optimal roads from a\ndatabase; (3) Agent Planning, detailing agent types and behaviors; (4) Road\nRanking, scoring roads to match scenario requirements; and (5) Scene\nGeneration, rendering the planned scenarios in the simulator. This framework\nsupports both routine and critical traffic scenarios, enhancing its\napplicability. We demonstrate that our approach not only diversifies agent\nplanning and road selection but also significantly reduces the average\ncollision rate from 8% to 3.5% in SafeBench. Additionally, our framework\nimproves narration and reasoning for driving captioning tasks. Our\ncontributions and resources are publicly available at\nhttps://basiclab.github.io/TTSG."
                },
                "authors": [
                    {
                        "name": "Bo-Kai Ruan"
                    },
                    {
                        "name": "Hao-Tang Tsui"
                    },
                    {
                        "name": "Yung-Hui Li"
                    },
                    {
                        "name": "Hong-Han Shuai"
                    }
                ],
                "author_detail": {
                    "name": "Hong-Han Shuai"
                },
                "author": "Hong-Han Shuai",
                "arxiv_comment": "update to the newest version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09575v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09575v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20727v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20727v2",
                "updated": "2025-02-19T16:26:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    26,
                    44,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-28T04:47:39Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    4,
                    47,
                    39,
                    0,
                    302,
                    0
                ],
                "title": "Faster WIND: Accelerating Iterative Best-of-$N$ Distillation for LLM\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster WIND: Accelerating Iterative Best-of-$N$ Distillation for LLM\n  Alignment"
                },
                "summary": "Recent advances in aligning large language models with human preferences have\ncorroborated the growing importance of best-of-N distillation (BOND). However,\nthe iterative BOND algorithm is prohibitively expensive in practice due to the\nsample and computation inefficiency. This paper addresses the problem by\nrevealing a unified game-theoretic connection between iterative BOND and\nself-play alignment, which unifies seemingly disparate algorithmic paradigms.\nBased on the connection, we establish a novel framework, WIN rate Dominance\n(WIND), with a series of efficient algorithms for regularized win rate\ndominance optimization that approximates iterative BOND in the parameter space.\nWe provides provable sample efficiency guarantee for one of the WIND variant\nwith the square loss objective. The experimental results confirm that our\nalgorithm not only accelerates the computation, but also achieves superior\nsample efficiency compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in aligning large language models with human preferences have\ncorroborated the growing importance of best-of-N distillation (BOND). However,\nthe iterative BOND algorithm is prohibitively expensive in practice due to the\nsample and computation inefficiency. This paper addresses the problem by\nrevealing a unified game-theoretic connection between iterative BOND and\nself-play alignment, which unifies seemingly disparate algorithmic paradigms.\nBased on the connection, we establish a novel framework, WIN rate Dominance\n(WIND), with a series of efficient algorithms for regularized win rate\ndominance optimization that approximates iterative BOND in the parameter space.\nWe provides provable sample efficiency guarantee for one of the WIND variant\nwith the square loss objective. The experimental results confirm that our\nalgorithm not only accelerates the computation, but also achieves superior\nsample efficiency compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "Jincheng Mei"
                    },
                    {
                        "name": "Hanjun Dai"
                    },
                    {
                        "name": "Zixin Wen"
                    },
                    {
                        "name": "Shicong Cen"
                    },
                    {
                        "name": "Dale Schuurmans"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Bo Dai"
                    }
                ],
                "author_detail": {
                    "name": "Bo Dai"
                },
                "author": "Bo Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20727v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20727v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09713v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09713v3",
                "updated": "2025-02-19T16:24:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    24,
                    30,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-13T03:45:24Z",
                "published_parsed": [
                    2024,
                    10,
                    13,
                    3,
                    45,
                    24,
                    6,
                    287,
                    0
                ],
                "title": "Agentic Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Information Retrieval"
                },
                "summary": "Since the 1970s, information retrieval (IR) has long been defined as the\nprocess of acquiring relevant information items from a pre-defined corpus to\nsatisfy user information needs. Traditional IR systems, while effective in\ndomains like web search, are constrained by their reliance on static,\npre-defined information items. To this end, this paper introduces agentic\ninformation retrieval (Agentic IR), a transformative next-generation paradigm\nfor IR driven by large language models (LLMs) and AI agents. The central shift\nin agentic IR is the evolving definition of ``information'' from static,\npre-defined information items to dynamic, context-dependent information states.\nInformation state refers to a particular information context that the user is\nright in within a dynamic environment, encompassing not only the acquired\ninformation items but also real-time user preferences, contextual factors, and\ndecision-making processes. In such a way, traditional information retrieval,\nfocused on acquiring relevant information items based on user queries, can be\nnaturally extended to achieving the target information state given the user\ninstruction, which thereby defines the agentic information retrieval. We\nsystematically discuss agentic IR from various aspects, i.e., task formulation,\narchitecture, evaluation, case studies, as well as challenges and future\nprospects. We believe that the concept of agentic IR introduced in this paper\nnot only broadens the scope of information retrieval research but also lays the\nfoundation for a more adaptive, interactive, and intelligent next-generation IR\nparadigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the 1970s, information retrieval (IR) has long been defined as the\nprocess of acquiring relevant information items from a pre-defined corpus to\nsatisfy user information needs. Traditional IR systems, while effective in\ndomains like web search, are constrained by their reliance on static,\npre-defined information items. To this end, this paper introduces agentic\ninformation retrieval (Agentic IR), a transformative next-generation paradigm\nfor IR driven by large language models (LLMs) and AI agents. The central shift\nin agentic IR is the evolving definition of ``information'' from static,\npre-defined information items to dynamic, context-dependent information states.\nInformation state refers to a particular information context that the user is\nright in within a dynamic environment, encompassing not only the acquired\ninformation items but also real-time user preferences, contextual factors, and\ndecision-making processes. In such a way, traditional information retrieval,\nfocused on acquiring relevant information items based on user queries, can be\nnaturally extended to achieving the target information state given the user\ninstruction, which thereby defines the agentic information retrieval. We\nsystematically discuss agentic IR from various aspects, i.e., task formulation,\narchitecture, evaluation, case studies, as well as challenges and future\nprospects. We believe that the concept of agentic IR introduced in this paper\nnot only broadens the scope of information retrieval research but also lays the\nfoundation for a more adaptive, interactive, and intelligent next-generation IR\nparadigm."
                },
                "authors": [
                    {
                        "name": "Weinan Zhang"
                    },
                    {
                        "name": "Junwei Liao"
                    },
                    {
                        "name": "Ning Li"
                    },
                    {
                        "name": "Kounianhua Du"
                    },
                    {
                        "name": "Jianghao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jianghao Lin"
                },
                "author": "Jianghao Lin",
                "arxiv_comment": "11 pages, perspective paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09713v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09713v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13845v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13845v1",
                "updated": "2025-02-19T16:08:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    8,
                    17,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:08:17Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    8,
                    17,
                    2,
                    50,
                    0
                ],
                "title": "Enhancing LLM-Based Recommendations Through Personalized Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM-Based Recommendations Through Personalized Reasoning"
                },
                "summary": "Current recommendation systems powered by large language models (LLMs) often\nunderutilize their reasoning capabilities due to a lack of explicit logical\nstructuring. To address this limitation, we introduce CoT-Rec, a framework that\nintegrates Chain-of-Thought (CoT) reasoning into LLM-driven recommendations by\nincorporating two crucial processes: user preference analysis and item\nperception evaluation. CoT-Rec operates in two key phases: (1) personalized\ndata extraction, where user preferences and item perceptions are identified,\nand (2) personalized data application, where this information is leveraged to\nrefine recommendations. Our experimental analysis demonstrates that CoT-Rec\nimproves recommendation accuracy by making better use of LLMs' reasoning\npotential. The implementation is publicly available at\nhttps://anonymous.4open.science/r/CoT-Rec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current recommendation systems powered by large language models (LLMs) often\nunderutilize their reasoning capabilities due to a lack of explicit logical\nstructuring. To address this limitation, we introduce CoT-Rec, a framework that\nintegrates Chain-of-Thought (CoT) reasoning into LLM-driven recommendations by\nincorporating two crucial processes: user preference analysis and item\nperception evaluation. CoT-Rec operates in two key phases: (1) personalized\ndata extraction, where user preferences and item perceptions are identified,\nand (2) personalized data application, where this information is leveraged to\nrefine recommendations. Our experimental analysis demonstrates that CoT-Rec\nimproves recommendation accuracy by making better use of LLMs' reasoning\npotential. The implementation is publicly available at\nhttps://anonymous.4open.science/r/CoT-Rec."
                },
                "authors": [
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Xueshuo Yan"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Guangping Zhang"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Li Shang"
                    },
                    {
                        "name": "Ning Gu"
                    }
                ],
                "author_detail": {
                    "name": "Ning Gu"
                },
                "author": "Ning Gu",
                "arxiv_comment": "7 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13845v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13845v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.06656v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.06656v3",
                "updated": "2025-02-19T16:05:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    5,
                    47,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-10T16:47:00Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    16,
                    47,
                    0,
                    0,
                    41,
                    0
                ],
                "title": "A Frontier AI Risk Management Framework: Bridging the Gap Between\n  Current AI Practices and Established Risk Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Frontier AI Risk Management Framework: Bridging the Gap Between\n  Current AI Practices and Established Risk Management"
                },
                "summary": "The recent development of powerful AI systems has highlighted the need for\nrobust risk management frameworks in the AI industry. Although companies have\nbegun to implement safety frameworks, current approaches often lack the\nsystematic rigor found in other high-risk industries. This paper presents a\ncomprehensive risk management framework for the development of frontier AI that\nbridges this gap by integrating established risk management principles with\nemerging AI-specific practices. The framework consists of four key components:\n(1) risk identification (through literature review, open-ended red-teaming, and\nrisk modeling), (2) risk analysis and evaluation using quantitative metrics and\nclearly defined thresholds, (3) risk treatment through mitigation measures such\nas containment, deployment controls, and assurance processes, and (4) risk\ngovernance establishing clear organizational structures and accountability.\nDrawing from best practices in mature industries such as aviation or nuclear\npower, while accounting for AI's unique challenges, this framework provides AI\ndevelopers with actionable guidelines for implementing robust risk management.\nThe paper details how each component should be implemented throughout the\nlife-cycle of the AI system - from planning through deployment - and emphasizes\nthe importance and feasibility of conducting risk management work prior to the\nfinal training run to minimize the burden associated with it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent development of powerful AI systems has highlighted the need for\nrobust risk management frameworks in the AI industry. Although companies have\nbegun to implement safety frameworks, current approaches often lack the\nsystematic rigor found in other high-risk industries. This paper presents a\ncomprehensive risk management framework for the development of frontier AI that\nbridges this gap by integrating established risk management principles with\nemerging AI-specific practices. The framework consists of four key components:\n(1) risk identification (through literature review, open-ended red-teaming, and\nrisk modeling), (2) risk analysis and evaluation using quantitative metrics and\nclearly defined thresholds, (3) risk treatment through mitigation measures such\nas containment, deployment controls, and assurance processes, and (4) risk\ngovernance establishing clear organizational structures and accountability.\nDrawing from best practices in mature industries such as aviation or nuclear\npower, while accounting for AI's unique challenges, this framework provides AI\ndevelopers with actionable guidelines for implementing robust risk management.\nThe paper details how each component should be implemented throughout the\nlife-cycle of the AI system - from planning through deployment - and emphasizes\nthe importance and feasibility of conducting risk management work prior to the\nfinal training run to minimize the burden associated with it."
                },
                "authors": [
                    {
                        "name": "Simeon Campos"
                    },
                    {
                        "name": "Henry Papadatos"
                    },
                    {
                        "name": "Fabien Roger"
                    },
                    {
                        "name": "Chlo Touzet"
                    },
                    {
                        "name": "Otter Quarks"
                    },
                    {
                        "name": "Malcolm Murray"
                    }
                ],
                "author_detail": {
                    "name": "Malcolm Murray"
                },
                "author": "Malcolm Murray",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.06656v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.06656v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13843v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13843v1",
                "updated": "2025-02-19T16:02:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    2,
                    59,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:02:59Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    2,
                    59,
                    2,
                    50,
                    0
                ],
                "title": "Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based\n  User Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Cross-Domain Recommendations with Memory-Optimized LLM-Based\n  User Agents"
                },
                "summary": "Large Language Model (LLM)-based user agents have emerged as a powerful tool\nfor improving recommender systems by simulating user interactions. However,\nexisting methods struggle with cross-domain scenarios due to inefficient memory\nstructures, leading to irrelevant information retention and failure to account\nfor social influence factors such as popularity. To address these limitations,\nwe introduce AgentCF++, a novel framework featuring a dual-layer memory\narchitecture and a two-step fusion mechanism to filter domain-specific\npreferences effectively. Additionally, we propose interest groups with shared\nmemory, allowing the model to capture the impact of popularity trends on users\nwith similar interests. Through extensive experiments on multiple cross-domain\ndatasets, AgentCF++ demonstrates superior performance over baseline models,\nhighlighting its effectiveness in refining user behavior simulation for\nrecommender systems. Our code is available at\nhttps://anonymous.4open.science/r/AgentCF-plus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based user agents have emerged as a powerful tool\nfor improving recommender systems by simulating user interactions. However,\nexisting methods struggle with cross-domain scenarios due to inefficient memory\nstructures, leading to irrelevant information retention and failure to account\nfor social influence factors such as popularity. To address these limitations,\nwe introduce AgentCF++, a novel framework featuring a dual-layer memory\narchitecture and a two-step fusion mechanism to filter domain-specific\npreferences effectively. Additionally, we propose interest groups with shared\nmemory, allowing the model to capture the impact of popularity trends on users\nwith similar interests. Through extensive experiments on multiple cross-domain\ndatasets, AgentCF++ demonstrates superior performance over baseline models,\nhighlighting its effectiveness in refining user behavior simulation for\nrecommender systems. Our code is available at\nhttps://anonymous.4open.science/r/AgentCF-plus."
                },
                "authors": [
                    {
                        "name": "Jiahao Liu"
                    },
                    {
                        "name": "Shengkang Gu"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Guangping Zhang"
                    },
                    {
                        "name": "Mingzhe Han"
                    },
                    {
                        "name": "Hansu Gu"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Li Shang"
                    },
                    {
                        "name": "Ning Gu"
                    }
                ],
                "author_detail": {
                    "name": "Ning Gu"
                },
                "author": "Ning Gu",
                "arxiv_comment": "6 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13843v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13842v1",
                "updated": "2025-02-19T16:02:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    2,
                    23,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T16:02:23Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    16,
                    2,
                    23,
                    2,
                    50,
                    0
                ],
                "title": "Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster\n  Adaptive Internal Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster\n  Adaptive Internal Thinking"
                },
                "summary": "Large language models (LLMs) face inherent performance bottlenecks under\nparameter constraints, particularly in processing critical tokens that demand\ncomplex reasoning. Empirical analysis reveals challenging tokens induce abrupt\ngradient spikes across layers, exposing architectural stress points in standard\nTransformers. Building on this insight, we propose Inner Thinking Transformer\n(ITT), which reimagines layer computations as implicit thinking steps. ITT\ndynamically allocates computation through Adaptive Token Routing, iteratively\nrefines representations via Residual Thinking Connections, and distinguishes\nreasoning phases using Thinking Step Encoding. ITT enables deeper processing of\ncritical tokens without parameter expansion. Evaluations across 162M-466M\nparameter models show ITT achieves 96.5\\% performance of a 466M Transformer\nusing only 162M parameters, reduces training data by 43.2\\%, and outperforms\nTransformer/Loop variants in 11 benchmarks. By enabling elastic computation\nallocation during inference, ITT balances performance and efficiency through\narchitecture-aware optimization of implicit thinking pathways.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face inherent performance bottlenecks under\nparameter constraints, particularly in processing critical tokens that demand\ncomplex reasoning. Empirical analysis reveals challenging tokens induce abrupt\ngradient spikes across layers, exposing architectural stress points in standard\nTransformers. Building on this insight, we propose Inner Thinking Transformer\n(ITT), which reimagines layer computations as implicit thinking steps. ITT\ndynamically allocates computation through Adaptive Token Routing, iteratively\nrefines representations via Residual Thinking Connections, and distinguishes\nreasoning phases using Thinking Step Encoding. ITT enables deeper processing of\ncritical tokens without parameter expansion. Evaluations across 162M-466M\nparameter models show ITT achieves 96.5\\% performance of a 466M Transformer\nusing only 162M parameters, reduces training data by 43.2\\%, and outperforms\nTransformer/Loop variants in 11 benchmarks. By enabling elastic computation\nallocation during inference, ITT balances performance and efficiency through\narchitecture-aware optimization of implicit thinking pathways."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Yanxi Xie"
                    },
                    {
                        "name": "Jiawei Sheng"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Hua Wu"
                    },
                    {
                        "name": "Haifeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Wang"
                },
                "author": "Haifeng Wang",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13836v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13836v1",
                "updated": "2025-02-19T15:58:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    58,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T15:58:09Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    58,
                    9,
                    2,
                    50,
                    0
                ],
                "title": "Quantifying Memorization and Retriever Performance in\n  Retrieval-Augmented Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Memorization and Retriever Performance in\n  Retrieval-Augmented Vision-Language Models"
                },
                "summary": "Large Language Models (LLMs) demonstrate remarkable capabilities in question\nanswering (QA), but metrics for assessing their reliance on memorization versus\nretrieval remain underdeveloped. Moreover, while finetuned models are\nstate-of-the-art on closed-domain tasks, general-purpose models like GPT-4o\nexhibit strong zero-shot performance. This raises questions about the\ntrade-offs between memorization, generalization, and retrieval. In this work,\nwe analyze the extent to which multimodal retrieval-augmented VLMs memorize\ntraining data compared to baseline VLMs. Using the WebQA benchmark, we contrast\nfinetuned models with baseline VLMs on multihop retrieval and question\nanswering, examining the impact of finetuning on data memorization. To quantify\nmemorization in end-to-end retrieval and QA systems, we propose several proxy\nmetrics by investigating instances where QA succeeds despite retrieval failing.\nOur results reveal the extent to which finetuned models rely on memorization.\nIn contrast, retrieval-augmented VLMs have lower memorization scores, at the\ncost of accuracy (72% vs 52% on WebQA test set). As such, our measures pose a\nchallenge for future work to reconcile memorization and generalization in both\nOpen-Domain QA and joint Retrieval-QA tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate remarkable capabilities in question\nanswering (QA), but metrics for assessing their reliance on memorization versus\nretrieval remain underdeveloped. Moreover, while finetuned models are\nstate-of-the-art on closed-domain tasks, general-purpose models like GPT-4o\nexhibit strong zero-shot performance. This raises questions about the\ntrade-offs between memorization, generalization, and retrieval. In this work,\nwe analyze the extent to which multimodal retrieval-augmented VLMs memorize\ntraining data compared to baseline VLMs. Using the WebQA benchmark, we contrast\nfinetuned models with baseline VLMs on multihop retrieval and question\nanswering, examining the impact of finetuning on data memorization. To quantify\nmemorization in end-to-end retrieval and QA systems, we propose several proxy\nmetrics by investigating instances where QA succeeds despite retrieval failing.\nOur results reveal the extent to which finetuned models rely on memorization.\nIn contrast, retrieval-augmented VLMs have lower memorization scores, at the\ncost of accuracy (72% vs 52% on WebQA test set). As such, our measures pose a\nchallenge for future work to reconcile memorization and generalization in both\nOpen-Domain QA and joint Retrieval-QA tasks."
                },
                "authors": [
                    {
                        "name": "Peter Carragher"
                    },
                    {
                        "name": "Abhinand Jha"
                    },
                    {
                        "name": "R Raghav"
                    },
                    {
                        "name": "Kathleen M. Carley"
                    }
                ],
                "author_detail": {
                    "name": "Kathleen M. Carley"
                },
                "author": "Kathleen M. Carley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13836v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13836v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13834v1",
                "updated": "2025-02-19T15:54:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    54,
                    21,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T15:54:21Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    54,
                    21,
                    2,
                    50,
                    0
                ],
                "title": "Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning"
                },
                "summary": "Large language models (LLMs) can prove mathematical theorems formally by\ngenerating proof steps (\\textit{a.k.a.} tactics) within a proof system.\nHowever, the space of possible tactics is vast and complex, while the available\ntraining data for formal proofs is limited, posing a significant challenge to\nLLM-based tactic generation. To address this, we introduce a neuro-symbolic\ntactic generator that synergizes the mathematical intuition learned by LLMs\nwith domain-specific insights encoded by symbolic methods. The key aspect of\nthis integration is identifying which parts of mathematical reasoning are best\nsuited to LLMs and which to symbolic methods. While the high-level idea of\nneuro-symbolic integration is broadly applicable to various mathematical\nproblems, in this paper, we focus specifically on Olympiad inequalities\n(Figure~1). We analyze how humans solve these problems and distill the\ntechniques into two types of tactics: (1) scaling, handled by symbolic methods,\nand (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with\nLLMs to prune and rank the proof goals for efficient proof search. We evaluate\nour framework on 161 challenging inequalities from multiple mathematics\ncompetitions, achieving state-of-the-art performance and significantly\noutperforming existing LLM and symbolic approaches without requiring additional\ntraining data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) can prove mathematical theorems formally by\ngenerating proof steps (\\textit{a.k.a.} tactics) within a proof system.\nHowever, the space of possible tactics is vast and complex, while the available\ntraining data for formal proofs is limited, posing a significant challenge to\nLLM-based tactic generation. To address this, we introduce a neuro-symbolic\ntactic generator that synergizes the mathematical intuition learned by LLMs\nwith domain-specific insights encoded by symbolic methods. The key aspect of\nthis integration is identifying which parts of mathematical reasoning are best\nsuited to LLMs and which to symbolic methods. While the high-level idea of\nneuro-symbolic integration is broadly applicable to various mathematical\nproblems, in this paper, we focus specifically on Olympiad inequalities\n(Figure~1). We analyze how humans solve these problems and distill the\ntechniques into two types of tactics: (1) scaling, handled by symbolic methods,\nand (2) rewriting, handled by LLMs. In addition, we combine symbolic tools with\nLLMs to prune and rank the proof goals for efficient proof search. We evaluate\nour framework on 161 challenging inequalities from multiple mathematics\ncompetitions, achieving state-of-the-art performance and significantly\noutperforming existing LLM and symbolic approaches without requiring additional\ntraining data."
                },
                "authors": [
                    {
                        "name": "Zenan Li"
                    },
                    {
                        "name": "Zhaoyu Li"
                    },
                    {
                        "name": "Wen Tang"
                    },
                    {
                        "name": "Xian Zhang"
                    },
                    {
                        "name": "Yuan Yao"
                    },
                    {
                        "name": "Xujie Si"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Kaiyu Yang"
                    },
                    {
                        "name": "Xiaoxing Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxing Ma"
                },
                "author": "Xiaoxing Ma",
                "arxiv_comment": "Published as a conference paper at ICLR 2025. Code is available at\n  https://github.com/Lizn-zn/NeqLIPS/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11054v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11054v3",
                "updated": "2025-02-19T15:36:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    36,
                    47,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-16T09:27:44Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    27,
                    44,
                    6,
                    47,
                    0
                ],
                "title": "Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on\n  Large Language Models"
                },
                "summary": "Multi-turn jailbreak attacks simulate real-world human interactions by\nengaging large language models (LLMs) in iterative dialogues, exposing critical\nsafety vulnerabilities. However, existing methods often struggle to balance\nsemantic coherence with attack effectiveness, resulting in either benign\nsemantic drift or ineffective detection evasion. To address this challenge, we\npropose Reasoning-Augmented Conversation, a novel multi-turn jailbreak\nframework that reformulates harmful queries into benign reasoning tasks and\nleverages LLMs' strong reasoning capabilities to compromise safety alignment.\nSpecifically, we introduce an attack state machine framework to systematically\nmodel problem translation and iterative reasoning, ensuring coherent query\ngeneration across multiple turns. Building on this framework, we design\ngain-guided exploration, self-play, and rejection feedback modules to preserve\nattack semantics, enhance effectiveness, and sustain reasoning-driven attack\nprogression. Extensive experiments on multiple LLMs demonstrate that RACE\nachieves state-of-the-art attack effectiveness in complex conversational\nscenarios, with attack success rates (ASRs) increasing by up to 96%. Notably,\nour approach achieves ASRs of 82% and 92% against leading commercial models,\nOpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at\nhttps://github.com/NY1024/RACE to facilitate further research in this critical\ndomain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn jailbreak attacks simulate real-world human interactions by\nengaging large language models (LLMs) in iterative dialogues, exposing critical\nsafety vulnerabilities. However, existing methods often struggle to balance\nsemantic coherence with attack effectiveness, resulting in either benign\nsemantic drift or ineffective detection evasion. To address this challenge, we\npropose Reasoning-Augmented Conversation, a novel multi-turn jailbreak\nframework that reformulates harmful queries into benign reasoning tasks and\nleverages LLMs' strong reasoning capabilities to compromise safety alignment.\nSpecifically, we introduce an attack state machine framework to systematically\nmodel problem translation and iterative reasoning, ensuring coherent query\ngeneration across multiple turns. Building on this framework, we design\ngain-guided exploration, self-play, and rejection feedback modules to preserve\nattack semantics, enhance effectiveness, and sustain reasoning-driven attack\nprogression. Extensive experiments on multiple LLMs demonstrate that RACE\nachieves state-of-the-art attack effectiveness in complex conversational\nscenarios, with attack success rates (ASRs) increasing by up to 96%. Notably,\nour approach achieves ASRs of 82% and 92% against leading commercial models,\nOpenAI o1 and DeepSeek R1, underscoring its potency. We release our code at\nhttps://github.com/NY1024/RACE to facilitate further research in this critical\ndomain."
                },
                "authors": [
                    {
                        "name": "Zonghao Ying"
                    },
                    {
                        "name": "Deyue Zhang"
                    },
                    {
                        "name": "Zonglei Jing"
                    },
                    {
                        "name": "Yisong Xiao"
                    },
                    {
                        "name": "Quanchen Zou"
                    },
                    {
                        "name": "Aishan Liu"
                    },
                    {
                        "name": "Siyuan Liang"
                    },
                    {
                        "name": "Xiangzheng Zhang"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11054v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11054v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12010v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12010v2",
                "updated": "2025-02-19T15:36:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    36,
                    26,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-15T19:21:14Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    19,
                    21,
                    14,
                    1,
                    289,
                    0
                ],
                "title": "Bias Similarity Across Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias Similarity Across Large Language Models"
                },
                "summary": "Bias in machine learning models, particularly in Large Language Models, is a\ncritical issue as these systems shape important societal decisions. While\nprevious studies have examined bias in individual LLMs, comparisons of bias\nacross models remain underexplored. To address this gap, we analyze 13 LLMs\nfrom five families, evaluating bias through output distribution across multiple\ndimensions using two datasets (4K and 1M questions). Our results show that\nfine-tuning has minimal impact on output distributions, and proprietary models\ntend to overly response as unknowns to minimize bias, compromising accuracy and\nutility. In addition, open-source models like Llama3-Chat and Gemma2-it\ndemonstrate fairness comparable to proprietary models like GPT-4, challenging\nthe assumption that larger, closed-source models are inherently less biased. We\nalso find that bias scores for disambiguated questions are more extreme,\nraising concerns about reverse discrimination. These findings highlight the\nneed for improved bias mitigation strategies and more comprehensive evaluation\nmetrics for fairness in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bias in machine learning models, particularly in Large Language Models, is a\ncritical issue as these systems shape important societal decisions. While\nprevious studies have examined bias in individual LLMs, comparisons of bias\nacross models remain underexplored. To address this gap, we analyze 13 LLMs\nfrom five families, evaluating bias through output distribution across multiple\ndimensions using two datasets (4K and 1M questions). Our results show that\nfine-tuning has minimal impact on output distributions, and proprietary models\ntend to overly response as unknowns to minimize bias, compromising accuracy and\nutility. In addition, open-source models like Llama3-Chat and Gemma2-it\ndemonstrate fairness comparable to proprietary models like GPT-4, challenging\nthe assumption that larger, closed-source models are inherently less biased. We\nalso find that bias scores for disambiguated questions are more extreme,\nraising concerns about reverse discrimination. These findings highlight the\nneed for improved bias mitigation strategies and more comprehensive evaluation\nmetrics for fairness in LLMs."
                },
                "authors": [
                    {
                        "name": "Hyejun Jeong"
                    },
                    {
                        "name": "Shiqing Ma"
                    },
                    {
                        "name": "Amir Houmansadr"
                    }
                ],
                "author_detail": {
                    "name": "Amir Houmansadr"
                },
                "author": "Amir Houmansadr",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12010v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12010v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14745v2",
                "updated": "2025-02-19T15:32:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    32,
                    29,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-17T16:59:46Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    16,
                    59,
                    46,
                    3,
                    291,
                    0
                ],
                "title": "Semi-supervised Fine-tuning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-supervised Fine-tuning for Large Language Models"
                },
                "summary": "Supervised fine-tuning (SFT) is crucial in adapting large language model\n(LLMs) to a specific domain or task. However, only a limited amount of labeled\ndata is available in practical applications, which poses a severe challenge for\nSFT in yielding satisfactory results. Therefore, a data-efficient framework\nthat can fully exploit labeled and unlabeled data for LLM fine-tuning is highly\nanticipated.Towards this end, we introduce a semi-supervised\nfine-tuning(SemiFT) task and a framework named SemiEvol for LLM alignment from\na propagate-and-select manner. For knowledge propagation, SemiEvol adopts a\nbi-level approach, propagating knowledge from labeled data to unlabeled data\nthrough both in-weight and in-context methods. For knowledge selection,\nSemiEvol incorporates a collaborative learning mechanism, selecting\nhigher-quality pseudo-response samples. We conducted experiments using\nGPT-4o-mini and Llama-3.1 on seven general or domain-specific datasets,\ndemonstrating significant improvements in model performance on target data.\nFurthermore, we compared SemiEvol with SFT and self-evolution methods,\nhighlighting its practicality in hybrid data scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) is crucial in adapting large language model\n(LLMs) to a specific domain or task. However, only a limited amount of labeled\ndata is available in practical applications, which poses a severe challenge for\nSFT in yielding satisfactory results. Therefore, a data-efficient framework\nthat can fully exploit labeled and unlabeled data for LLM fine-tuning is highly\nanticipated.Towards this end, we introduce a semi-supervised\nfine-tuning(SemiFT) task and a framework named SemiEvol for LLM alignment from\na propagate-and-select manner. For knowledge propagation, SemiEvol adopts a\nbi-level approach, propagating knowledge from labeled data to unlabeled data\nthrough both in-weight and in-context methods. For knowledge selection,\nSemiEvol incorporates a collaborative learning mechanism, selecting\nhigher-quality pseudo-response samples. We conducted experiments using\nGPT-4o-mini and Llama-3.1 on seven general or domain-specific datasets,\ndemonstrating significant improvements in model performance on target data.\nFurthermore, we compared SemiEvol with SFT and self-evolution methods,\nhighlighting its practicality in hybrid data scenarios."
                },
                "authors": [
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Xiusi Chen"
                    },
                    {
                        "name": "Zhiping Xiao"
                    },
                    {
                        "name": "Wei Ju"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "arxiv_comment": "Github Repo: https://github.com/luo-junyu/SemiEvol",
                "arxiv_journal_ref": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13820v1",
                "updated": "2025-02-19T15:32:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    32,
                    11,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T15:32:11Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    32,
                    11,
                    2,
                    50,
                    0
                ],
                "title": "Scoring Verifiers: Evaluating Synthetic Verification in Code and\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scoring Verifiers: Evaluating Synthetic Verification in Code and\n  Reasoning"
                },
                "summary": "Code verification has recently found great success as a critical component in\ntraining large scale reasoning models for coding. Synthetic techniques such as\nself-generated test cases and reward models provide a way to enhance code\ncapabilities beyond predefined tests. Building on these advancements, we\npropose new benchmarks designed to systematically evaluate the impact of\nsynthetic verification methods on assessing solution correctness. We introduce\nHE-R, HE-R+, MBPP-R, and MBPP-R+, which transform existing coding benchmarks\ninto scoring and ranking datasets to evaluate the effectiveness of synthetic\nverifiers. Using these benchmarks, we analyze synthetic verification methods in\nstandard, reasoning-based, and reward-based LLMs. Our results show that recent\nreasoning models significantly improve test case generation and that scaling\ntest cases enhances verification accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code verification has recently found great success as a critical component in\ntraining large scale reasoning models for coding. Synthetic techniques such as\nself-generated test cases and reward models provide a way to enhance code\ncapabilities beyond predefined tests. Building on these advancements, we\npropose new benchmarks designed to systematically evaluate the impact of\nsynthetic verification methods on assessing solution correctness. We introduce\nHE-R, HE-R+, MBPP-R, and MBPP-R+, which transform existing coding benchmarks\ninto scoring and ranking datasets to evaluate the effectiveness of synthetic\nverifiers. Using these benchmarks, we analyze synthetic verification methods in\nstandard, reasoning-based, and reward-based LLMs. Our results show that recent\nreasoning models significantly improve test case generation and that scaling\ntest cases enhances verification accuracy."
                },
                "authors": [
                    {
                        "name": "Aleksander Ficek"
                    },
                    {
                        "name": "Somshubra Majumdar"
                    },
                    {
                        "name": "Vahid Noroozi"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07068v2",
                "updated": "2025-02-19T15:05:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    15,
                    5,
                    39,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-10T21:59:27Z",
                "published_parsed": [
                    2025,
                    2,
                    10,
                    21,
                    59,
                    27,
                    0,
                    41,
                    0
                ],
                "title": "Specializing Large Language Models to Simulate Survey Response\n  Distributions for Global Populations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Specializing Large Language Models to Simulate Survey Response\n  Distributions for Global Populations"
                },
                "summary": "Large-scale surveys are essential tools for informing social science research\nand policy, but running surveys is costly and time-intensive. If we could\naccurately simulate group-level survey results, this would therefore be very\nvaluable to social science research. Prior work has explored the use of large\nlanguage models (LLMs) for simulating human behaviors, mostly through\nprompting. In this paper, we are the first to specialize LLMs for the task of\nsimulating survey response distributions. As a testbed, we use country-level\nresults from two global cultural surveys. We devise a fine-tuning method based\non first-token probabilities to minimize divergence between predicted and\nactual response distributions for a given question. Then, we show that this\nmethod substantially outperforms other methods and zero-shot classifiers, even\non unseen questions, countries, and a completely unseen survey. While even our\nbest models struggle with the task, especially on unseen questions, our results\ndemonstrate the benefits of specialization for simulation, which may accelerate\nprogress towards sufficiently accurate simulation in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale surveys are essential tools for informing social science research\nand policy, but running surveys is costly and time-intensive. If we could\naccurately simulate group-level survey results, this would therefore be very\nvaluable to social science research. Prior work has explored the use of large\nlanguage models (LLMs) for simulating human behaviors, mostly through\nprompting. In this paper, we are the first to specialize LLMs for the task of\nsimulating survey response distributions. As a testbed, we use country-level\nresults from two global cultural surveys. We devise a fine-tuning method based\non first-token probabilities to minimize divergence between predicted and\nactual response distributions for a given question. Then, we show that this\nmethod substantially outperforms other methods and zero-shot classifiers, even\non unseen questions, countries, and a completely unseen survey. While even our\nbest models struggle with the task, especially on unseen questions, our results\ndemonstrate the benefits of specialization for simulation, which may accelerate\nprogress towards sufficiently accurate simulation in the future."
                },
                "authors": [
                    {
                        "name": "Yong Cao"
                    },
                    {
                        "name": "Haijiang Liu"
                    },
                    {
                        "name": "Arnav Arora"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    },
                    {
                        "name": "Paul Rttger"
                    },
                    {
                        "name": "Daniel Hershcovich"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Hershcovich"
                },
                "author": "Daniel Hershcovich",
                "arxiv_comment": "15 pages, 9 figures, accepted to NAACL 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13794v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13794v1",
                "updated": "2025-02-19T14:58:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    58,
                    48,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:58:48Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    58,
                    48,
                    2,
                    50,
                    0
                ],
                "title": "LESA: Learnable LLM Layer Scaling-Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LESA: Learnable LLM Layer Scaling-Up"
                },
                "summary": "Training Large Language Models (LLMs) from scratch requires immense\ncomputational resources, making it prohibitively expensive. Model scaling-up\noffers a promising solution by leveraging the parameters of smaller models to\ncreate larger ones. However, existing depth scaling-up methods rely on\nempirical heuristic rules for layer duplication, which result in poorer\ninitialization and slower convergence during continual pre-training. We propose\n\\textbf{LESA}, a novel learnable method for depth scaling-up. By concatenating\nparameters from each layer and applying Singular Value Decomposition, we\nuncover latent patterns between layers, suggesting that inter-layer parameters\ncan be learned. LESA uses a neural network to predict the parameters inserted\nbetween adjacent layers, enabling better initialization and faster training.\nExperiments show that LESA outperforms existing baselines, achieving superior\nperformance with less than half the computational cost during continual\npre-training. Extensive analyses demonstrate its effectiveness across different\nmodel sizes and tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models (LLMs) from scratch requires immense\ncomputational resources, making it prohibitively expensive. Model scaling-up\noffers a promising solution by leveraging the parameters of smaller models to\ncreate larger ones. However, existing depth scaling-up methods rely on\nempirical heuristic rules for layer duplication, which result in poorer\ninitialization and slower convergence during continual pre-training. We propose\n\\textbf{LESA}, a novel learnable method for depth scaling-up. By concatenating\nparameters from each layer and applying Singular Value Decomposition, we\nuncover latent patterns between layers, suggesting that inter-layer parameters\ncan be learned. LESA uses a neural network to predict the parameters inserted\nbetween adjacent layers, enabling better initialization and faster training.\nExperiments show that LESA outperforms existing baselines, achieving superior\nperformance with less than half the computational cost during continual\npre-training. Extensive analyses demonstrate its effectiveness across different\nmodel sizes and tasks."
                },
                "authors": [
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Zouying Cao"
                    },
                    {
                        "name": "Xinbei Ma"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13794v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13794v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13791v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13791v1",
                "updated": "2025-02-19T14:58:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    58,
                    4,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:58:04Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    58,
                    4,
                    2,
                    50,
                    0
                ],
                "title": "From Tools to Teammates: Evaluating LLMs in Multi-Session Coding\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Tools to Teammates: Evaluating LLMs in Multi-Session Coding\n  Interactions"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in working environments\nfor a wide range of tasks, excelling at solving individual problems in\nisolation. However, are they also able to effectively collaborate over\nlong-term interactions? To investigate this, we introduce MemoryCode, a\nsynthetic multi-session dataset designed to test LLMs' ability to track and\nexecute simple coding instructions amid irrelevant information, simulating a\nrealistic setting. While all the models we tested handle isolated instructions\nwell, even the performance of state-of-the-art models like GPT-4o deteriorates\nwhen instructions are spread across sessions. Our analysis suggests this is due\nto their failure to retrieve and integrate information over long instruction\nchains. Our results highlight a fundamental limitation of current LLMs,\nrestricting their ability to collaborate effectively in long interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in working environments\nfor a wide range of tasks, excelling at solving individual problems in\nisolation. However, are they also able to effectively collaborate over\nlong-term interactions? To investigate this, we introduce MemoryCode, a\nsynthetic multi-session dataset designed to test LLMs' ability to track and\nexecute simple coding instructions amid irrelevant information, simulating a\nrealistic setting. While all the models we tested handle isolated instructions\nwell, even the performance of state-of-the-art models like GPT-4o deteriorates\nwhen instructions are spread across sessions. Our analysis suggests this is due\nto their failure to retrieve and integrate information over long instruction\nchains. Our results highlight a fundamental limitation of current LLMs,\nrestricting their ability to collaborate effectively in long interactions."
                },
                "authors": [
                    {
                        "name": "Nathanal Carraz Rakotonirina"
                    },
                    {
                        "name": "Mohammed Hamdy"
                    },
                    {
                        "name": "Jon Ander Campos"
                    },
                    {
                        "name": "Lucas Weber"
                    },
                    {
                        "name": "Alberto Testoni"
                    },
                    {
                        "name": "Marzieh Fadaee"
                    },
                    {
                        "name": "Sandro Pezzelle"
                    },
                    {
                        "name": "Marco Del Tredici"
                    }
                ],
                "author_detail": {
                    "name": "Marco Del Tredici"
                },
                "author": "Marco Del Tredici",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13791v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13791v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13789v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13789v1",
                "updated": "2025-02-19T14:57:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    57,
                    51,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:57:51Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    57,
                    51,
                    2,
                    50,
                    0
                ],
                "title": "From Correctness to Comprehension: AI Agents for Personalized Error\n  Diagnosis in Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Correctness to Comprehension: AI Agents for Personalized Error\n  Diagnosis in Education"
                },
                "summary": "Large Language Models (LLMs), such as GPT-4, have demonstrated impressive\nmathematical reasoning capabilities, achieving near-perfect performance on\nbenchmarks like GSM8K. However, their application in personalized education\nremains limited due to an overemphasis on correctness over error diagnosis and\nfeedback generation. Current models fail to provide meaningful insights into\nthe causes of student mistakes, limiting their utility in educational contexts.\nTo address these challenges, we present three key contributions. First, we\nintroduce \\textbf{MathCCS} (Mathematical Classification and Constructive\nSuggestions), a multi-modal benchmark designed for systematic error analysis\nand tailored feedback. MathCCS includes real-world problems, expert-annotated\nerror categories, and longitudinal student data. Evaluations of\nstate-of-the-art models, including \\textit{Qwen2-VL}, \\textit{LLaVA-OV},\n\\textit{Claude-3.5-Sonnet} and \\textit{GPT-4o}, reveal that none achieved\nclassification accuracy above 30\\% or generated high-quality suggestions\n(average scores below 4/10), highlighting a significant gap from human-level\nperformance. Second, we develop a sequential error analysis framework that\nleverages historical data to track trends and improve diagnostic precision.\nFinally, we propose a multi-agent collaborative framework that combines a Time\nSeries Agent for historical analysis and an MLLM Agent for real-time\nrefinement, enhancing error classification and feedback generation. Together,\nthese contributions provide a robust platform for advancing personalized\neducation, bridging the gap between current AI capabilities and the demands of\nreal-world teaching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT-4, have demonstrated impressive\nmathematical reasoning capabilities, achieving near-perfect performance on\nbenchmarks like GSM8K. However, their application in personalized education\nremains limited due to an overemphasis on correctness over error diagnosis and\nfeedback generation. Current models fail to provide meaningful insights into\nthe causes of student mistakes, limiting their utility in educational contexts.\nTo address these challenges, we present three key contributions. First, we\nintroduce \\textbf{MathCCS} (Mathematical Classification and Constructive\nSuggestions), a multi-modal benchmark designed for systematic error analysis\nand tailored feedback. MathCCS includes real-world problems, expert-annotated\nerror categories, and longitudinal student data. Evaluations of\nstate-of-the-art models, including \\textit{Qwen2-VL}, \\textit{LLaVA-OV},\n\\textit{Claude-3.5-Sonnet} and \\textit{GPT-4o}, reveal that none achieved\nclassification accuracy above 30\\% or generated high-quality suggestions\n(average scores below 4/10), highlighting a significant gap from human-level\nperformance. Second, we develop a sequential error analysis framework that\nleverages historical data to track trends and improve diagnostic precision.\nFinally, we propose a multi-agent collaborative framework that combines a Time\nSeries Agent for historical analysis and an MLLM Agent for real-time\nrefinement, enhancing error classification and feedback generation. Together,\nthese contributions provide a robust platform for advancing personalized\neducation, bridging the gap between current AI capabilities and the demands of\nreal-world teaching."
                },
                "authors": [
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Dingjie Song"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Tianlong Xu"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13789v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13789v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13783v1",
                "updated": "2025-02-19T14:48:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    48,
                    25,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:48:25Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    48,
                    25,
                    2,
                    50,
                    0
                ],
                "title": "Generative Large Recommendation Models: Emerging Trends in LLMs for\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Large Recommendation Models: Emerging Trends in LLMs for\n  Recommendation"
                },
                "summary": "In the era of information overload, recommendation systems play a pivotal\nrole in filtering data and delivering personalized content. Recent advancements\nin feature interaction and user behavior modeling have significantly enhanced\nthe recall and ranking processes of these systems. With the rise of large\nlanguage models (LLMs), new opportunities have emerged to further improve\nrecommendation systems. This tutorial explores two primary approaches for\nintegrating LLMs: LLMs-enhanced recommendations, which leverage the reasoning\ncapabilities of general LLMs, and generative large recommendation models, which\nfocus on scaling and sophistication. While the former has been extensively\ncovered in existing literature, the latter remains underexplored. This tutorial\naims to fill this gap by providing a comprehensive overview of generative large\nrecommendation models, including their recent advancements, challenges, and\npotential research directions. Key topics include data quality, scaling laws,\nuser behavior mining, and efficiency in training and inference. By engaging\nwith this tutorial, participants will gain insights into the latest\ndevelopments and future opportunities in the field, aiding both academic\nresearch and practical applications. The timely nature of this exploration\nsupports the rapid evolution of recommendation systems, offering valuable\nguidance for researchers and practitioners alike.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of information overload, recommendation systems play a pivotal\nrole in filtering data and delivering personalized content. Recent advancements\nin feature interaction and user behavior modeling have significantly enhanced\nthe recall and ranking processes of these systems. With the rise of large\nlanguage models (LLMs), new opportunities have emerged to further improve\nrecommendation systems. This tutorial explores two primary approaches for\nintegrating LLMs: LLMs-enhanced recommendations, which leverage the reasoning\ncapabilities of general LLMs, and generative large recommendation models, which\nfocus on scaling and sophistication. While the former has been extensively\ncovered in existing literature, the latter remains underexplored. This tutorial\naims to fill this gap by providing a comprehensive overview of generative large\nrecommendation models, including their recent advancements, challenges, and\npotential research directions. Key topics include data quality, scaling laws,\nuser behavior mining, and efficiency in training and inference. By engaging\nwith this tutorial, participants will gain insights into the latest\ndevelopments and future opportunities in the field, aiding both academic\nresearch and practical applications. The timely nature of this exploration\nsupports the rapid evolution of recommendation systems, offering valuable\nguidance for researchers and practitioners alike."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Luankang Zhang"
                    },
                    {
                        "name": "Jin Yao Chin"
                    },
                    {
                        "name": "Yufei Ye"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "arxiv_comment": "This paper has been accepted for the tutorial track at WWW 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13780v1",
                "updated": "2025-02-19T14:45:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    45,
                    17,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:45:17Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    45,
                    17,
                    2,
                    50,
                    0
                ],
                "title": "Translation in the Hands of Many:Centering Lay Users in Machine\n  Translation Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translation in the Hands of Many:Centering Lay Users in Machine\n  Translation Interactions"
                },
                "summary": "Converging societal and technical factors have transformed language\ntechnologies into user-facing applications employed across languages. Machine\nTranslation (MT) has become a global tool, with cross-lingual services now also\nsupported by dialogue systems powered by multilingual Large Language Models\n(LLMs). This accessibility has expanded MT's reach to a vast base of lay users,\noften with little to no expertise in the languages or the technology itself.\nDespite this, the understanding of MT consumed by this diverse group of users\n-- their needs, experiences, and interactions with these systems -- remains\nlimited. This paper traces the shift in MT user profiles, focusing on\nnon-expert users and how their engagement with these systems may change with\nLLMs. We identify three key factors -- usability, trust, and literacy -- that\nshape these interactions and must be addressed to align MT with user needs. By\nexploring these dimensions, we offer insights to guide future MT with a\nuser-centered approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Converging societal and technical factors have transformed language\ntechnologies into user-facing applications employed across languages. Machine\nTranslation (MT) has become a global tool, with cross-lingual services now also\nsupported by dialogue systems powered by multilingual Large Language Models\n(LLMs). This accessibility has expanded MT's reach to a vast base of lay users,\noften with little to no expertise in the languages or the technology itself.\nDespite this, the understanding of MT consumed by this diverse group of users\n-- their needs, experiences, and interactions with these systems -- remains\nlimited. This paper traces the shift in MT user profiles, focusing on\nnon-expert users and how their engagement with these systems may change with\nLLMs. We identify three key factors -- usability, trust, and literacy -- that\nshape these interactions and must be addressed to align MT with user needs. By\nexploring these dimensions, we offer insights to guide future MT with a\nuser-centered approach."
                },
                "authors": [
                    {
                        "name": "Beatrice Savoldi"
                    },
                    {
                        "name": "Alan Ramponi"
                    },
                    {
                        "name": "Matteo Negri"
                    },
                    {
                        "name": "Luisa Bentivogli"
                    }
                ],
                "author_detail": {
                    "name": "Luisa Bentivogli"
                },
                "author": "Luisa Bentivogli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00819v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00819v2",
                "updated": "2025-02-19T14:45:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    45,
                    6,
                    2,
                    50,
                    0
                ],
                "published": "2025-01-01T12:23:36Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    12,
                    23,
                    36,
                    2,
                    1,
                    0
                ],
                "title": "Public Access Defibrillator Deployment for Cardiac Arrests: A\n  Learn-Then-Optimize Approach with SHAP-based Interpretable Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Public Access Defibrillator Deployment for Cardiac Arrests: A\n  Learn-Then-Optimize Approach with SHAP-based Interpretable Analytics"
                },
                "summary": "Out-of-hospital cardiac arrest (OHCA) survival rates remain extremely low due\nto challenges in the timely accessibility of medical devices. Therefore,\neffective deployment of automated external defibrillators (AED) can\nsignificantly increase survival rates. Precise and interpretable predictions of\nOHCA occurrences provide a solid foundation for efficient and robust AED\ndeployment optimization. This study develops a novel learn-then-optimize\napproach, integrating three key components: a machine learning prediction\nmodel, SHAP-based interpretable analytics, and a SHAP-guided integer\nprogramming (SIP) model. The machine learning model is trained utilizing only\ngeographic data as inputs to overcome data availability obstacles, and its\nstrong predictive performance validates the feasibility of interpretation.\nFurthermore, the SHAP model elaborates on the contribution of each geographic\nfeature to the OHCA occurrences. Finally, an integer programming model is\nformulated for optimizing AED deployment, incorporating SHAP-weighted OHCA\ndensities. Various numerical experiments are conducted across different\nsettings. Based on comparative and sensitive analysis, the optimization effect\nof our approach is verified and valuable insights are derived to provide\nsubstantial support for theoretical extension and practical implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-hospital cardiac arrest (OHCA) survival rates remain extremely low due\nto challenges in the timely accessibility of medical devices. Therefore,\neffective deployment of automated external defibrillators (AED) can\nsignificantly increase survival rates. Precise and interpretable predictions of\nOHCA occurrences provide a solid foundation for efficient and robust AED\ndeployment optimization. This study develops a novel learn-then-optimize\napproach, integrating three key components: a machine learning prediction\nmodel, SHAP-based interpretable analytics, and a SHAP-guided integer\nprogramming (SIP) model. The machine learning model is trained utilizing only\ngeographic data as inputs to overcome data availability obstacles, and its\nstrong predictive performance validates the feasibility of interpretation.\nFurthermore, the SHAP model elaborates on the contribution of each geographic\nfeature to the OHCA occurrences. Finally, an integer programming model is\nformulated for optimizing AED deployment, incorporating SHAP-weighted OHCA\ndensities. Various numerical experiments are conducted across different\nsettings. Based on comparative and sensitive analysis, the optimization effect\nof our approach is verified and valuable insights are derived to provide\nsubstantial support for theoretical extension and practical implementation."
                },
                "authors": [
                    {
                        "name": "Chih-Yuan Yang"
                    },
                    {
                        "name": "Keng-Hou Leong"
                    },
                    {
                        "name": "Kexin Cao"
                    },
                    {
                        "name": "Mingchuan Yang"
                    },
                    {
                        "name": "Wai Kin Victor Chan"
                    }
                ],
                "author_detail": {
                    "name": "Wai Kin Victor Chan"
                },
                "author": "Wai Kin Victor Chan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00819v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00819v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13776v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13776v1",
                "updated": "2025-02-19T14:39:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    39,
                    59,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:39:59Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    39,
                    59,
                    2,
                    50,
                    0
                ],
                "title": "EHOP: A Dataset of Everyday NP-Hard Optimization Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EHOP: A Dataset of Everyday NP-Hard Optimization Problems"
                },
                "summary": "We introduce the dataset of Everyday Hard Optimization Problems (EHOP), a\ncollection of NP-hard optimization problems expressed in natural language. EHOP\nincludes problem formulations that could be found in computer science\ntextbooks, versions that are dressed up as problems that could arise in real\nlife, and variants of well-known problems with inverted rules. We find that\nstate-of-the-art LLMs, across multiple prompting strategies, systematically\nsolve textbook problems more accurately than their real-life and inverted\ncounterparts. We argue that this constitutes evidence that LLMs adapt solutions\nseen during training, rather than leveraging reasoning abilities that would\nenable them to generalize to novel problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the dataset of Everyday Hard Optimization Problems (EHOP), a\ncollection of NP-hard optimization problems expressed in natural language. EHOP\nincludes problem formulations that could be found in computer science\ntextbooks, versions that are dressed up as problems that could arise in real\nlife, and variants of well-known problems with inverted rules. We find that\nstate-of-the-art LLMs, across multiple prompting strategies, systematically\nsolve textbook problems more accurately than their real-life and inverted\ncounterparts. We argue that this constitutes evidence that LLMs adapt solutions\nseen during training, rather than leveraging reasoning abilities that would\nenable them to generalize to novel problems."
                },
                "authors": [
                    {
                        "name": "Alex Duchnowski"
                    },
                    {
                        "name": "Ellie Pavlick"
                    },
                    {
                        "name": "Alexander Koller"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Koller"
                },
                "author": "Alexander Koller",
                "arxiv_comment": "18 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13776v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13776v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13775v1",
                "updated": "2025-02-19T14:38:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    38,
                    57,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:38:57Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    38,
                    57,
                    2,
                    50,
                    0
                ],
                "title": "VITAL: A New Dataset for Benchmarking Pluralistic Alignment in\n  Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VITAL: A New Dataset for Benchmarking Pluralistic Alignment in\n  Healthcare"
                },
                "summary": "Alignment techniques have become central to ensuring that Large Language\nModels (LLMs) generate outputs consistent with human values. However, existing\nalignment paradigms often model an averaged or monolithic preference, failing\nto account for the diversity of perspectives across cultures, demographics, and\ncommunities. This limitation is particularly critical in health-related\nscenarios, where plurality is essential due to the influence of culture,\nreligion, personal values, and conflicting opinions. Despite progress in\npluralistic alignment, no prior work has focused on health, likely due to the\nunavailability of publicly available datasets. To address this gap, we\nintroduce VITAL, a new benchmark dataset comprising 13.1K value-laden\nsituations and 5.4K multiple-choice questions focused on health, designed to\nassess and benchmark pluralistic alignment methodologies. Through extensive\nevaluation of eight LLMs of varying sizes, we demonstrate that existing\npluralistic alignment techniques fall short in effectively accommodating\ndiverse healthcare beliefs, underscoring the need for tailored AI alignment in\nspecific domains. This work highlights the limitations of current approaches\nand lays the groundwork for developing health-specific alignment solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alignment techniques have become central to ensuring that Large Language\nModels (LLMs) generate outputs consistent with human values. However, existing\nalignment paradigms often model an averaged or monolithic preference, failing\nto account for the diversity of perspectives across cultures, demographics, and\ncommunities. This limitation is particularly critical in health-related\nscenarios, where plurality is essential due to the influence of culture,\nreligion, personal values, and conflicting opinions. Despite progress in\npluralistic alignment, no prior work has focused on health, likely due to the\nunavailability of publicly available datasets. To address this gap, we\nintroduce VITAL, a new benchmark dataset comprising 13.1K value-laden\nsituations and 5.4K multiple-choice questions focused on health, designed to\nassess and benchmark pluralistic alignment methodologies. Through extensive\nevaluation of eight LLMs of varying sizes, we demonstrate that existing\npluralistic alignment techniques fall short in effectively accommodating\ndiverse healthcare beliefs, underscoring the need for tailored AI alignment in\nspecific domains. This work highlights the limitations of current approaches\nand lays the groundwork for developing health-specific alignment solutions."
                },
                "authors": [
                    {
                        "name": "Anudeex Shetty"
                    },
                    {
                        "name": "Amin Beheshti"
                    },
                    {
                        "name": "Mark Dras"
                    },
                    {
                        "name": "Usman Naseem"
                    }
                ],
                "author_detail": {
                    "name": "Usman Naseem"
                },
                "author": "Usman Naseem",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13767v1",
                "updated": "2025-02-19T14:28:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    28,
                    42,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:28:42Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    28,
                    42,
                    2,
                    50,
                    0
                ],
                "title": "AI Software Engineer: Programming with Trust",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Software Engineer: Programming with Trust"
                },
                "summary": "Large Language Models (LLMs) have shown surprising proficiency in generating\ncode snippets, promising to automate large parts of software engineering via\nartificial intelligence (AI). We argue that successfully deploying AI software\nengineers requires a level of trust equal to or even greater than the trust\nestablished by human-driven software engineering practices. The recent trend\ntoward LLM agents offers a path toward integrating the power of LLMs to create\nnew code with the power of analysis tools to increase trust in the code. This\nopinion piece comments on whether LLM agents could dominate software\nengineering workflows in the future and whether the focus of programming will\nshift from programming at scale to programming with trust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown surprising proficiency in generating\ncode snippets, promising to automate large parts of software engineering via\nartificial intelligence (AI). We argue that successfully deploying AI software\nengineers requires a level of trust equal to or even greater than the trust\nestablished by human-driven software engineering practices. The recent trend\ntoward LLM agents offers a path toward integrating the power of LLMs to create\nnew code with the power of analysis tools to increase trust in the code. This\nopinion piece comments on whether LLM agents could dominate software\nengineering workflows in the future and whether the focus of programming will\nshift from programming at scale to programming with trust."
                },
                "authors": [
                    {
                        "name": "Abhik Roychoudhury"
                    },
                    {
                        "name": "Corina Pasareanu"
                    },
                    {
                        "name": "Michael Pradel"
                    },
                    {
                        "name": "Baishakhi Ray"
                    }
                ],
                "author_detail": {
                    "name": "Baishakhi Ray"
                },
                "author": "Baishakhi Ray",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13766v1",
                "updated": "2025-02-19T14:27:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    27,
                    40,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:27:40Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    27,
                    40,
                    2,
                    50,
                    0
                ],
                "title": "GIMMICK -- Globally Inclusive Multimodal Multitask Cultural Knowledge\n  Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GIMMICK -- Globally Inclusive Multimodal Multitask Cultural Knowledge\n  Benchmarking"
                },
                "summary": "Large Vision-Language Models (LVLMs) have recently gained attention due to\ntheir distinctive performance and broad applicability. While it has been\npreviously shown that their efficacy in usage scenarios involving non-Western\ncontexts falls short, existing studies are limited in scope, covering just a\nnarrow range of cultures, focusing exclusively on a small number of cultural\naspects, or evaluating a limited selection of models on a single task only.\nTowards globally inclusive LVLM research, we introduce GIMMICK, an extensive\nmultimodal benchmark designed to assess a broad spectrum of cultural knowledge\nacross 144 countries representing six global macro-regions. GIMMICK comprises\nsix tasks built upon three new datasets that span 728 unique cultural events or\nfacets on which we evaluated 20 LVLMs and 11 LLMs, including five proprietary\nand 26 open-weight models of all sizes. We systematically examine (1) regional\ncultural biases, (2) the influence of model size, (3) input modalities, and (4)\nexternal cues. Our analyses reveal strong biases toward Western cultures across\nmodels and tasks and highlight strong correlations between model size and\nperformance, as well as the effectiveness of multimodal input and external\ngeographic cues. We further find that models have more knowledge of tangible\nthan intangible aspects (e.g., food vs. rituals) and that they excel in\nrecognizing broad cultural origins but struggle with a more nuanced\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (LVLMs) have recently gained attention due to\ntheir distinctive performance and broad applicability. While it has been\npreviously shown that their efficacy in usage scenarios involving non-Western\ncontexts falls short, existing studies are limited in scope, covering just a\nnarrow range of cultures, focusing exclusively on a small number of cultural\naspects, or evaluating a limited selection of models on a single task only.\nTowards globally inclusive LVLM research, we introduce GIMMICK, an extensive\nmultimodal benchmark designed to assess a broad spectrum of cultural knowledge\nacross 144 countries representing six global macro-regions. GIMMICK comprises\nsix tasks built upon three new datasets that span 728 unique cultural events or\nfacets on which we evaluated 20 LVLMs and 11 LLMs, including five proprietary\nand 26 open-weight models of all sizes. We systematically examine (1) regional\ncultural biases, (2) the influence of model size, (3) input modalities, and (4)\nexternal cues. Our analyses reveal strong biases toward Western cultures across\nmodels and tasks and highlight strong correlations between model size and\nperformance, as well as the effectiveness of multimodal input and external\ngeographic cues. We further find that models have more knowledge of tangible\nthan intangible aspects (e.g., food vs. rituals) and that they excel in\nrecognizing broad cultural origins but struggle with a more nuanced\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Florian Schneider"
                    },
                    {
                        "name": "Carolin Holtermann"
                    },
                    {
                        "name": "Chris Biemann"
                    },
                    {
                        "name": "Anne Lauscher"
                    }
                ],
                "author_detail": {
                    "name": "Anne Lauscher"
                },
                "author": "Anne Lauscher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13753v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13753v1",
                "updated": "2025-02-19T14:15:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    15,
                    49,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:15:49Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    15,
                    49,
                    2,
                    50,
                    0
                ],
                "title": "SCALAR: Scientific Citation-based Live Assessment of Long-context\n  Academic Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCALAR: Scientific Citation-based Live Assessment of Long-context\n  Academic Reasoning"
                },
                "summary": "Evaluating large language models' (LLMs) long-context understanding\ncapabilities remains challenging. We present SCALAR (Scientific Citation-based\nLive Assessment of Long-context Academic Reasoning), a novel benchmark that\nleverages academic papers and their citation networks. SCALAR features\nautomatic generation of high-quality ground truth labels without human\nannotation, controllable difficulty levels, and a dynamic updating mechanism\nthat prevents data contamination. Using ICLR 2025 papers, we evaluate 8\nstate-of-the-art LLMs, revealing key insights about their capabilities and\nlimitations in processing long scientific documents across different context\nlengths and reasoning types. Our benchmark provides a reliable and sustainable\nway to track progress in long-context understanding as LLM capabilities evolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating large language models' (LLMs) long-context understanding\ncapabilities remains challenging. We present SCALAR (Scientific Citation-based\nLive Assessment of Long-context Academic Reasoning), a novel benchmark that\nleverages academic papers and their citation networks. SCALAR features\nautomatic generation of high-quality ground truth labels without human\nannotation, controllable difficulty levels, and a dynamic updating mechanism\nthat prevents data contamination. Using ICLR 2025 papers, we evaluate 8\nstate-of-the-art LLMs, revealing key insights about their capabilities and\nlimitations in processing long scientific documents across different context\nlengths and reasoning types. Our benchmark provides a reliable and sustainable\nway to track progress in long-context understanding as LLM capabilities evolve."
                },
                "authors": [
                    {
                        "name": "Renxi Wang"
                    },
                    {
                        "name": "Honglin Mu"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Lizhi Lin"
                    },
                    {
                        "name": "Yunlong Feng"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Xudong Han"
                    },
                    {
                        "name": "Haonan Li"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Li"
                },
                "author": "Haonan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13753v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13753v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14766v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14766v2",
                "updated": "2025-02-19T14:11:10Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    11,
                    10,
                    2,
                    50,
                    0
                ],
                "published": "2024-05-23T16:33:18Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    16,
                    33,
                    18,
                    3,
                    144,
                    0
                ],
                "title": "Evaluating Large Language Models for Public Health Classification and\n  Extraction Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Large Language Models for Public Health Classification and\n  Extraction Tasks"
                },
                "summary": "Advances in Large Language Models (LLMs) have led to significant interest in\ntheir potential to support human experts across a range of domains, including\npublic health. In this work we present automated evaluations of LLMs for public\nhealth tasks involving the classification and extraction of free text. We\ncombine six externally annotated datasets with seven new internally annotated\ndatasets to evaluate LLMs for processing text related to: health burden,\nepidemiological risk factors, and public health interventions. We evaluate\neleven open-weight LLMs (7-123 billion parameters) across all tasks using\nzero-shot in-context learning. We find that Llama-3.3-70B-Instruct is the\nhighest performing model, achieving the best results on 8/16 tasks (using\nmicro-F1 scores). We see significant variation across tasks with all\nopen-weight LLMs scoring below 60% micro-F1 on some challenging tasks, such as\nContact Classification, while all LLMs achieve greater than 80% micro-F1 on\nothers, such as GI Illness Classification. For a subset of 11 tasks, we also\nevaluate three GPT-4 and GPT-4o series models and find comparable results to\nLlama-3.3-70B-Instruct. Overall, based on these initial results we find\npromising signs that LLMs may be useful tools for public health experts to\nextract information from a wide variety of free text sources, and support\npublic health surveillance, research, and interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in Large Language Models (LLMs) have led to significant interest in\ntheir potential to support human experts across a range of domains, including\npublic health. In this work we present automated evaluations of LLMs for public\nhealth tasks involving the classification and extraction of free text. We\ncombine six externally annotated datasets with seven new internally annotated\ndatasets to evaluate LLMs for processing text related to: health burden,\nepidemiological risk factors, and public health interventions. We evaluate\neleven open-weight LLMs (7-123 billion parameters) across all tasks using\nzero-shot in-context learning. We find that Llama-3.3-70B-Instruct is the\nhighest performing model, achieving the best results on 8/16 tasks (using\nmicro-F1 scores). We see significant variation across tasks with all\nopen-weight LLMs scoring below 60% micro-F1 on some challenging tasks, such as\nContact Classification, while all LLMs achieve greater than 80% micro-F1 on\nothers, such as GI Illness Classification. For a subset of 11 tasks, we also\nevaluate three GPT-4 and GPT-4o series models and find comparable results to\nLlama-3.3-70B-Instruct. Overall, based on these initial results we find\npromising signs that LLMs may be useful tools for public health experts to\nextract information from a wide variety of free text sources, and support\npublic health surveillance, research, and interventions."
                },
                "authors": [
                    {
                        "name": "Joshua Harris"
                    },
                    {
                        "name": "Timothy Laurence"
                    },
                    {
                        "name": "Leo Loman"
                    },
                    {
                        "name": "Fan Grayson"
                    },
                    {
                        "name": "Toby Nonnenmacher"
                    },
                    {
                        "name": "Harry Long"
                    },
                    {
                        "name": "Loes WalsGriffith"
                    },
                    {
                        "name": "Amy Douglas"
                    },
                    {
                        "name": "Holly Fountain"
                    },
                    {
                        "name": "Stelios Georgiou"
                    },
                    {
                        "name": "Jo Hardstaff"
                    },
                    {
                        "name": "Kathryn Hopkins"
                    },
                    {
                        "name": "Y-Ling Chi"
                    },
                    {
                        "name": "Galena Kuyumdzhieva"
                    },
                    {
                        "name": "Lesley Larkin"
                    },
                    {
                        "name": "Samuel Collins"
                    },
                    {
                        "name": "Hamish Mohammed"
                    },
                    {
                        "name": "Thomas Finnie"
                    },
                    {
                        "name": "Luke Hounsome"
                    },
                    {
                        "name": "Michael Borowitz"
                    },
                    {
                        "name": "Steven Riley"
                    }
                ],
                "author_detail": {
                    "name": "Steven Riley"
                },
                "author": "Steven Riley",
                "arxiv_comment": "36 pages. Feedback and comments are highly appreciated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14766v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14766v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13738v1",
                "updated": "2025-02-19T14:04:46Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    4,
                    46,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:04:46Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    4,
                    46,
                    2,
                    50,
                    0
                ],
                "title": "Enhancing Input-Label Mapping in In-Context Learning with Contrastive\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Input-Label Mapping in In-Context Learning with Contrastive\n  Decoding"
                },
                "summary": "Large language models (LLMs) excel at a range of tasks through in-context\nlearning (ICL), where only a few task examples guide their predictions.\nHowever, prior research highlights that LLMs often overlook input-label mapping\ninformation in ICL, relying more on their pre-trained knowledge. To address\nthis issue, we introduce In-Context Contrastive Decoding (ICCD), a novel method\nthat emphasizes input-label mapping by contrasting the output distributions\nbetween positive and negative in-context examples. Experiments on 7 natural\nlanguage understanding (NLU) tasks show that our ICCD method brings consistent\nand significant improvement (up to +2.1 improvement on average) upon 6\ndifferent scales of LLMs without requiring additional training. Our approach is\nversatile, enhancing performance with various demonstration selection methods,\ndemonstrating its broad applicability and effectiveness. The code and scripts\nwill be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at a range of tasks through in-context\nlearning (ICL), where only a few task examples guide their predictions.\nHowever, prior research highlights that LLMs often overlook input-label mapping\ninformation in ICL, relying more on their pre-trained knowledge. To address\nthis issue, we introduce In-Context Contrastive Decoding (ICCD), a novel method\nthat emphasizes input-label mapping by contrasting the output distributions\nbetween positive and negative in-context examples. Experiments on 7 natural\nlanguage understanding (NLU) tasks show that our ICCD method brings consistent\nand significant improvement (up to +2.1 improvement on average) upon 6\ndifferent scales of LLMs without requiring additional training. Our approach is\nversatile, enhancing performance with various demonstration selection methods,\ndemonstrating its broad applicability and effectiveness. The code and scripts\nwill be publicly released."
                },
                "authors": [
                    {
                        "name": "Keqin Peng"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Yuanxin Ouyang"
                    },
                    {
                        "name": "Meng Fang"
                    },
                    {
                        "name": "Yancheng Yuan"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13737v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13737v1",
                "updated": "2025-02-19T14:03:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    3,
                    36,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:03:36Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    3,
                    36,
                    2,
                    50,
                    0
                ],
                "title": "PEDRO-V: From a concurrent engineering case study to a promising phase\n  zero mission definition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PEDRO-V: From a concurrent engineering case study to a promising phase\n  zero mission definition"
                },
                "summary": "Each year, the European Space Agency (ESA) organizes challenges for\nuniversity students, from BSc to PhD levels. The ESA Concurrent Engineering\nChallange 2024 was hosted by four Concurrent Design Facilites (CDF) across\nEurope: ESEC Galazia, ISAE SUPAERO, the University of Athens, and the\nUniversity of Portsmouth. A total of 102 students participated in the event.\nOver five days, students worked on a feasibility study for a space mission,\nsimulating ESA's design session at ESTEC, the ESA headquarters. Students were\ndivided into specializes groups based on their backgrounds, reflecting ESA's\nconcurrent engineering teams. This paper discusses the design of subsystems by\nstudents, their trade-off results, and the outcomes of the CDF study. It\nhighlights the effectiveness of concurrent engineering, which enabled rapid and\nefficient results even from non-esxpert teams. The future development roadmap\nand lessons learned are also presented. The students used CDP4-Comet software\nwithin the replicated ESA CDF, resulting in the PEDRO-V mission proposal:\nPlanetary Exploration Deployment and Research Operation - Venus. The teams\ncollaboratively defined the Concept of Operations, identified actors,\nworst-case scenarios, use cases, and activities. Their output included a list\nof requirements, a draft product breakdown structure, and key subsystems\ninformation. The concurrent engineering process led to continuous improvement\nand convergence of key parameters. This approach proved to be effective by\naligning different teams' solutions and comparing them to similar missions. The\nPEDRO-V mission feasibility was confirmed, demonstrating the potential of\nconcurrent engineering in accademic settings for space missions. (summarized\nwith AI)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Each year, the European Space Agency (ESA) organizes challenges for\nuniversity students, from BSc to PhD levels. The ESA Concurrent Engineering\nChallange 2024 was hosted by four Concurrent Design Facilites (CDF) across\nEurope: ESEC Galazia, ISAE SUPAERO, the University of Athens, and the\nUniversity of Portsmouth. A total of 102 students participated in the event.\nOver five days, students worked on a feasibility study for a space mission,\nsimulating ESA's design session at ESTEC, the ESA headquarters. Students were\ndivided into specializes groups based on their backgrounds, reflecting ESA's\nconcurrent engineering teams. This paper discusses the design of subsystems by\nstudents, their trade-off results, and the outcomes of the CDF study. It\nhighlights the effectiveness of concurrent engineering, which enabled rapid and\nefficient results even from non-esxpert teams. The future development roadmap\nand lessons learned are also presented. The students used CDP4-Comet software\nwithin the replicated ESA CDF, resulting in the PEDRO-V mission proposal:\nPlanetary Exploration Deployment and Research Operation - Venus. The teams\ncollaboratively defined the Concept of Operations, identified actors,\nworst-case scenarios, use cases, and activities. Their output included a list\nof requirements, a draft product breakdown structure, and key subsystems\ninformation. The concurrent engineering process led to continuous improvement\nand convergence of key parameters. This approach proved to be effective by\naligning different teams' solutions and comparing them to similar missions. The\nPEDRO-V mission feasibility was confirmed, demonstrating the potential of\nconcurrent engineering in accademic settings for space missions. (summarized\nwith AI)"
                },
                "authors": [
                    {
                        "name": "Domenico D'Auria"
                    },
                    {
                        "name": "Arianna Rigo"
                    },
                    {
                        "name": "Luca Niero"
                    },
                    {
                        "name": "Andrei-Toma Stoica"
                    },
                    {
                        "name": "Vito Costantini"
                    },
                    {
                        "name": "Pasquale Castellano"
                    },
                    {
                        "name": "Zsofia Zita Szilagyi"
                    },
                    {
                        "name": "Nishani Vijayakumaran"
                    },
                    {
                        "name": "Ella Toppari"
                    },
                    {
                        "name": "Stefano Schiano"
                    },
                    {
                        "name": "Marco Adorno"
                    },
                    {
                        "name": "Matteo Matrone"
                    },
                    {
                        "name": "Chiara Tulli"
                    },
                    {
                        "name": "Jan Kurowski"
                    },
                    {
                        "name": "Leo Bougault"
                    },
                    {
                        "name": "Argenziano Francesco"
                    },
                    {
                        "name": "Antignano Claudia"
                    },
                    {
                        "name": "Theodoros Roumanis"
                    },
                    {
                        "name": "Victoria Kossack"
                    },
                    {
                        "name": "Spyridon Giuvalas"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Giuvalas"
                },
                "author": "Spyridon Giuvalas",
                "arxiv_comment": "ESA SECESA 2024 Conference, Strasbourg (France), 8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13737v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13737v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13734v1",
                "updated": "2025-02-19T14:02:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    2,
                    0,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T14:02:00Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    14,
                    2,
                    0,
                    2,
                    50,
                    0
                ],
                "title": "CARE: Confidence-Aware Regression Estimation of building density\n  fine-tuning EO Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARE: Confidence-Aware Regression Estimation of building density\n  fine-tuning EO Foundation Models"
                },
                "summary": "Performing accurate confidence quantification and assessment is important for\ndeep neural networks to predict their failures, improve their performance and\nenhance their capabilities in real-world applications, for their practical\ndeployment in real life. For pixel-wise regression tasks, confidence\nquantification and assessment has not been well addressed in the literature, in\ncontrast to classification tasks like semantic segmentation. The softmax output\nlayer is not used in deep neural networks that solve pixel-wise regression\nproblems. In this paper, to address these problems, we develop, train and\nevaluate the proposed model Confidence-Aware Regression Estimation (CARE). Our\nmodel CARE computes and assigns confidence to regression output results. We\nfocus on solving regression problems as downstream tasks of an AI Foundation\nModel for Earth Observation (EO). We evaluate the proposed model CARE and\nexperimental results on data from the Copernicus Sentinel-2 satellite\nconstellation for estimating the density of buildings show that the proposed\nmethod can be successfully applied to regression problems. We also show that\nour approach outperforms other methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performing accurate confidence quantification and assessment is important for\ndeep neural networks to predict their failures, improve their performance and\nenhance their capabilities in real-world applications, for their practical\ndeployment in real life. For pixel-wise regression tasks, confidence\nquantification and assessment has not been well addressed in the literature, in\ncontrast to classification tasks like semantic segmentation. The softmax output\nlayer is not used in deep neural networks that solve pixel-wise regression\nproblems. In this paper, to address these problems, we develop, train and\nevaluate the proposed model Confidence-Aware Regression Estimation (CARE). Our\nmodel CARE computes and assigns confidence to regression output results. We\nfocus on solving regression problems as downstream tasks of an AI Foundation\nModel for Earth Observation (EO). We evaluate the proposed model CARE and\nexperimental results on data from the Copernicus Sentinel-2 satellite\nconstellation for estimating the density of buildings show that the proposed\nmethod can be successfully applied to regression problems. We also show that\nour approach outperforms other methods."
                },
                "authors": [
                    {
                        "name": "Nikolaos Dionelis"
                    },
                    {
                        "name": "Jente Bosmans"
                    },
                    {
                        "name": "Nicolas Longp"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Longp"
                },
                "author": "Nicolas Longp",
                "arxiv_comment": "5 pages, 3 figures, Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13725v1",
                "updated": "2025-02-19T13:52:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    52,
                    26,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T13:52:26Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    52,
                    26,
                    2,
                    50,
                    0
                ],
                "title": "Adapting Large Language Models for Time Series Modeling via a Novel\n  Parameter-efficient Adaptation Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting Large Language Models for Time Series Modeling via a Novel\n  Parameter-efficient Adaptation Method"
                },
                "summary": "Time series modeling holds significant importance in many real-world\napplications and has been extensively studied. While pre-trained foundation\nmodels have made impressive strides in the fields of natural language\nprocessing (NLP) and computer vision (CV), their development in time series\ndomains has been constrained by data sparsity. A series of recent studies have\ndemonstrated that large language models (LLMs) possess robust pattern\nrecognition and reasoning abilities over complex sequences of tokens. However,\nthe current literature have yet striked a high-quality balance between (a)\neffectively aligning the time series and natural language modalities, and (b)\nkeeping the inference efficiency. To address the above issues, we now propose\nthe Time-LlaMA framework. Time-LlaMA first converts the time series input into\ntoken embeddings through a linear tokenization mechanism. Second, the time\nseries token embeddings are aligned with the text prompts. Third, to further\nadapt the LLM backbone for time series modeling, we have developed a dynamic\nlow-rank adaptation technique (D-LoRA). D-LoRA dynamically chooses the most\nsuitable LoRA modules at each layer of the Transformer backbone for each time\nseries input, enhancing the model's predictive capabilities. Our experimental\nresults on an extensive collection of challenging real-world time series tasks\nconfirm that our proposed method achieves the state-of-the-art (SOTA)\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series modeling holds significant importance in many real-world\napplications and has been extensively studied. While pre-trained foundation\nmodels have made impressive strides in the fields of natural language\nprocessing (NLP) and computer vision (CV), their development in time series\ndomains has been constrained by data sparsity. A series of recent studies have\ndemonstrated that large language models (LLMs) possess robust pattern\nrecognition and reasoning abilities over complex sequences of tokens. However,\nthe current literature have yet striked a high-quality balance between (a)\neffectively aligning the time series and natural language modalities, and (b)\nkeeping the inference efficiency. To address the above issues, we now propose\nthe Time-LlaMA framework. Time-LlaMA first converts the time series input into\ntoken embeddings through a linear tokenization mechanism. Second, the time\nseries token embeddings are aligned with the text prompts. Third, to further\nadapt the LLM backbone for time series modeling, we have developed a dynamic\nlow-rank adaptation technique (D-LoRA). D-LoRA dynamically chooses the most\nsuitable LoRA modules at each layer of the Transformer backbone for each time\nseries input, enhancing the model's predictive capabilities. Our experimental\nresults on an extensive collection of challenging real-world time series tasks\nconfirm that our proposed method achieves the state-of-the-art (SOTA)\nperformance."
                },
                "authors": [
                    {
                        "name": "Juyuan Zhang"
                    },
                    {
                        "name": "Wei Zhu"
                    },
                    {
                        "name": "Jiechao Gao"
                    }
                ],
                "author_detail": {
                    "name": "Jiechao Gao"
                },
                "author": "Jiechao Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13723v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13723v1",
                "updated": "2025-02-19T13:51:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    51,
                    5,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T13:51:05Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    51,
                    5,
                    2,
                    50,
                    0
                ],
                "title": "Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs\n  with Refined Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Value Optimization: Improving Chain-of-Thought Reasoning in LLMs\n  with Refined Values"
                },
                "summary": "We introduce Direct Value Optimization (DVO), an innovative reinforcement\nlearning framework for enhancing large language models in complex reasoning\ntasks. Unlike traditional methods relying on preference labels, DVO utilizes\nvalue signals at individual reasoning steps, optimizing models via a mean\nsquared error loss. The key benefit of DVO lies in its fine-grained\nsupervision, circumventing the need for labor-intensive human annotations.\nTarget values within the DVO are estimated using either Monte Carlo Tree Search\nor an outcome value model. Our empirical analysis on both mathematical and\ncommonsense reasoning tasks shows that DVO consistently outperforms existing\noffline preference optimization techniques, even with fewer training steps.\nThese findings underscore the importance of value signals in advancing\nreasoning capabilities and highlight DVO as a superior methodology under\nscenarios lacking explicit human preference information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Direct Value Optimization (DVO), an innovative reinforcement\nlearning framework for enhancing large language models in complex reasoning\ntasks. Unlike traditional methods relying on preference labels, DVO utilizes\nvalue signals at individual reasoning steps, optimizing models via a mean\nsquared error loss. The key benefit of DVO lies in its fine-grained\nsupervision, circumventing the need for labor-intensive human annotations.\nTarget values within the DVO are estimated using either Monte Carlo Tree Search\nor an outcome value model. Our empirical analysis on both mathematical and\ncommonsense reasoning tasks shows that DVO consistently outperforms existing\noffline preference optimization techniques, even with fewer training steps.\nThese findings underscore the importance of value signals in advancing\nreasoning capabilities and highlight DVO as a superior methodology under\nscenarios lacking explicit human preference information."
                },
                "authors": [
                    {
                        "name": "Hongbo Zhang"
                    },
                    {
                        "name": "Han Cui"
                    },
                    {
                        "name": "Guangsheng Bao"
                    },
                    {
                        "name": "Linyi Yang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Yue Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Zhang"
                },
                "author": "Yue Zhang",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13723v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13723v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13007v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13007v2",
                "updated": "2025-02-19T13:35:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    35,
                    48,
                    2,
                    50,
                    0
                ],
                "published": "2025-01-22T16:49:37Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    49,
                    37,
                    2,
                    22,
                    0
                ],
                "title": "PairJudge RM: Perform Best-of-N Sampling with Knockout Tournament",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PairJudge RM: Perform Best-of-N Sampling with Knockout Tournament"
                },
                "summary": "Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large\nLanguage Models (LLMs), relies on reward models to select the best candidate\nsolution from multiple generations. However, traditional reward models often\nassign arbitrary and inconsistent scores, limiting their effectiveness. To\naddress this, we propose a Pairwise Judge Reward Model (PariJudge RM) combined\nwith a knockout tournament for BoN sampling. Instead of assigning absolute\nscores, given one math problem, PariJudge RM judges two candidate solutions'\ncorrectness with chain-of-thought reasoning simultaneously. This approach\neliminates the need for scoring and enables cross-validation of solutions\nthrough parallel judgment. In the knockout tournament, PariJudge RM conducts\npairwise Judgment between candidate solutions and eliminates the incorrect ones\niteratively. We construct PairJudge-432K, a large-scale dataset of 432K\npairwise judgments derived from NumiaMath and annotated using\n\\texttt{gemini-1.5-flash}, and train the PariJudge RM via supervised\nfine-tuning. Experiments on MATH-500 and the Olympiad Bench demonstrate\nsignificant improvements over baseline reward models. And a 40\\% to 60\\%\nrelative improvement is achieved on the top 50\\% challenging problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Best-of-N (BoN) sampling, a common strategy for test-time scaling of Large\nLanguage Models (LLMs), relies on reward models to select the best candidate\nsolution from multiple generations. However, traditional reward models often\nassign arbitrary and inconsistent scores, limiting their effectiveness. To\naddress this, we propose a Pairwise Judge Reward Model (PariJudge RM) combined\nwith a knockout tournament for BoN sampling. Instead of assigning absolute\nscores, given one math problem, PariJudge RM judges two candidate solutions'\ncorrectness with chain-of-thought reasoning simultaneously. This approach\neliminates the need for scoring and enables cross-validation of solutions\nthrough parallel judgment. In the knockout tournament, PariJudge RM conducts\npairwise Judgment between candidate solutions and eliminates the incorrect ones\niteratively. We construct PairJudge-432K, a large-scale dataset of 432K\npairwise judgments derived from NumiaMath and annotated using\n\\texttt{gemini-1.5-flash}, and train the PariJudge RM via supervised\nfine-tuning. Experiments on MATH-500 and the Olympiad Bench demonstrate\nsignificant improvements over baseline reward models. And a 40\\% to 60\\%\nrelative improvement is achieved on the top 50\\% challenging problems."
                },
                "authors": [
                    {
                        "name": "Yantao Liu"
                    },
                    {
                        "name": "Zijun Yao"
                    },
                    {
                        "name": "Rui Min"
                    },
                    {
                        "name": "Yixin Cao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "in progress work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13007v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03035v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03035v3",
                "updated": "2025-02-19T13:11:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    11,
                    14,
                    2,
                    50,
                    0
                ],
                "published": "2025-01-06T14:23:02Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    14,
                    23,
                    2,
                    0,
                    6,
                    0
                ],
                "title": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization\n  Degradation for Mathematical Reasoning"
                },
                "summary": "Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. Our results demonstrate that aggressive\nquantization methods like AWQ and GPTQ introduce up to 32.39% accuracy\ndegradation (average 11.31%) on Llama-3 models, particularly in numerical\ncomputation and reasoning planning. To address this, we introduce a\nmultidimensional evaluation framework combining qualitative capability analysis\nand quantitative error assessment. We further develop targeted recovery\nstrategies, showing that fine-tuning quantized models on only 545 task-specific\nexamples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to\nnear full-precision levels. Additionally, our error assessment pipeline\nachieves 98.9% accuracy in diagnosing and localizing errors across 3,366\nfailure cases, providing actionable insights for mitigating\nquantization-induced degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have achieved significant advancements in complex\nmathematical reasoning benchmarks, such as MATH. However, their substantial\ncomputational requirements present challenges for practical deployment. Model\nquantization has emerged as an effective strategy to reduce memory usage and\ncomputational costs by employing lower precision and bit-width representations.\nIn this study, we systematically evaluate the impact of quantization on\nmathematical reasoning tasks. Our results demonstrate that aggressive\nquantization methods like AWQ and GPTQ introduce up to 32.39% accuracy\ndegradation (average 11.31%) on Llama-3 models, particularly in numerical\ncomputation and reasoning planning. To address this, we introduce a\nmultidimensional evaluation framework combining qualitative capability analysis\nand quantitative error assessment. We further develop targeted recovery\nstrategies, showing that fine-tuning quantized models on only 545 task-specific\nexamples for 3 minutes on 4 GPUs effectively restores reasoning capabilities to\nnear full-precision levels. Additionally, our error assessment pipeline\nachieves 98.9% accuracy in diagnosing and localizing errors across 3,366\nfailure cases, providing actionable insights for mitigating\nquantization-induced degradation."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Yupeng Su"
                    },
                    {
                        "name": "Runming Yang"
                    },
                    {
                        "name": "Congkai Xie"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Zhongwei Xie"
                    },
                    {
                        "name": "Ngai Wong"
                    },
                    {
                        "name": "Hongxia Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hongxia Yang"
                },
                "author": "Hongxia Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03035v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03035v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20127v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20127v2",
                "updated": "2025-02-19T13:08:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    8,
                    34,
                    2,
                    50,
                    0
                ],
                "published": "2024-12-28T12:11:28Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    12,
                    11,
                    28,
                    5,
                    363,
                    0
                ],
                "title": "M-MAD: Multidimensional Multi-Agent Debate for Advanced Machine\n  Translation Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "M-MAD: Multidimensional Multi-Agent Debate for Advanced Machine\n  Translation Evaluation"
                },
                "summary": "Recent advancements in large language models (LLMs) have given rise to the\nLLM-as-a-judge paradigm, showcasing their potential to deliver human-like\njudgments. However, in the field of machine translation (MT) evaluation,\ncurrent LLM-as-a-judge methods fall short of learned automatic metrics. In this\npaper, we propose Multidimensional Multi-Agent Debate (M-MAD), a systematic\nLLM-based multi-agent framework for advanced LLM-as-a-judge MT evaluation. Our\nfindings demonstrate that M-MAD achieves significant advancements by (1)\ndecoupling heuristic MQM criteria into distinct evaluation dimensions for\nfine-grained assessments; (2) employing multi-agent debates to harness the\ncollaborative reasoning capabilities of LLMs; (3) synthesizing\ndimension-specific results into a final evaluation judgment to ensure robust\nand reliable outcomes. Comprehensive experiments show that M-MAD not only\noutperforms all existing LLM-as-a-judge methods but also competes with\nstate-of-the-art reference-based automatic metrics, even when powered by a\nsuboptimal model like GPT-4o mini. Detailed ablations and analysis highlight\nthe superiority of our framework design, offering a fresh perspective for\nLLM-as-a-judge paradigm. Our code and data are publicly available at\nhttps://github.com/SU-JIAYUAN/M-MAD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large language models (LLMs) have given rise to the\nLLM-as-a-judge paradigm, showcasing their potential to deliver human-like\njudgments. However, in the field of machine translation (MT) evaluation,\ncurrent LLM-as-a-judge methods fall short of learned automatic metrics. In this\npaper, we propose Multidimensional Multi-Agent Debate (M-MAD), a systematic\nLLM-based multi-agent framework for advanced LLM-as-a-judge MT evaluation. Our\nfindings demonstrate that M-MAD achieves significant advancements by (1)\ndecoupling heuristic MQM criteria into distinct evaluation dimensions for\nfine-grained assessments; (2) employing multi-agent debates to harness the\ncollaborative reasoning capabilities of LLMs; (3) synthesizing\ndimension-specific results into a final evaluation judgment to ensure robust\nand reliable outcomes. Comprehensive experiments show that M-MAD not only\noutperforms all existing LLM-as-a-judge methods but also competes with\nstate-of-the-art reference-based automatic metrics, even when powered by a\nsuboptimal model like GPT-4o mini. Detailed ablations and analysis highlight\nthe superiority of our framework design, offering a fresh perspective for\nLLM-as-a-judge paradigm. Our code and data are publicly available at\nhttps://github.com/SU-JIAYUAN/M-MAD."
                },
                "authors": [
                    {
                        "name": "Jiayuan Su"
                    },
                    {
                        "name": "Zhaopeng Feng"
                    },
                    {
                        "name": "Jiamei Zheng"
                    },
                    {
                        "name": "Jiahan Ren"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Hongwei Wang"
                    },
                    {
                        "name": "Zuozhu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zuozhu Liu"
                },
                "author": "Zuozhu Liu",
                "arxiv_comment": "Work in progress. Code and data are available at\n  https://github.com/SU-JIAYUAN/M-MAD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20127v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20127v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13691v1",
                "updated": "2025-02-19T13:03:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    3,
                    6,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T13:03:06Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    13,
                    3,
                    6,
                    2,
                    50,
                    0
                ],
                "title": "Is This Collection Worth My LLM's Time? Automatically Measuring\n  Information Potential in Text Corpora",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is This Collection Worth My LLM's Time? Automatically Measuring\n  Information Potential in Text Corpora"
                },
                "summary": "As large language models (LLMs) converge towards similar capabilities, the\nkey to advancing their performance lies in identifying and incorporating\nvaluable new information sources. However, evaluating which text collections\nare worth the substantial investment required for digitization, preprocessing,\nand integration into LLM systems remains a significant challenge. We present a\nnovel approach to this challenge: an automated pipeline that evaluates the\npotential information gain from text collections without requiring model\ntraining or fine-tuning. Our method generates multiple choice questions (MCQs)\nfrom texts and measures an LLM's performance both with and without access to\nthe source material. The performance gap between these conditions serves as a\nproxy for the collection's information potential. We validate our approach\nusing three strategically selected datasets: EPFL PhD manuscripts (likely\ncontaining novel specialized knowledge), Wikipedia articles (presumably part of\ntraining data), and a synthetic baseline dataset. Our results demonstrate that\nthis method effectively identifies collections containing valuable novel\ninformation, providing a practical tool for prioritizing data acquisition and\nintegration efforts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) converge towards similar capabilities, the\nkey to advancing their performance lies in identifying and incorporating\nvaluable new information sources. However, evaluating which text collections\nare worth the substantial investment required for digitization, preprocessing,\nand integration into LLM systems remains a significant challenge. We present a\nnovel approach to this challenge: an automated pipeline that evaluates the\npotential information gain from text collections without requiring model\ntraining or fine-tuning. Our method generates multiple choice questions (MCQs)\nfrom texts and measures an LLM's performance both with and without access to\nthe source material. The performance gap between these conditions serves as a\nproxy for the collection's information potential. We validate our approach\nusing three strategically selected datasets: EPFL PhD manuscripts (likely\ncontaining novel specialized knowledge), Wikipedia articles (presumably part of\ntraining data), and a synthetic baseline dataset. Our results demonstrate that\nthis method effectively identifies collections containing valuable novel\ninformation, providing a practical tool for prioritizing data acquisition and\nintegration efforts."
                },
                "authors": [
                    {
                        "name": "Tristan Karch"
                    },
                    {
                        "name": "Luca Engel"
                    },
                    {
                        "name": "Philippe Schwaller"
                    },
                    {
                        "name": "Frdric Kaplan"
                    }
                ],
                "author_detail": {
                    "name": "Frdric Kaplan"
                },
                "author": "Frdric Kaplan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13681v1",
                "updated": "2025-02-19T12:51:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    51,
                    35,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T12:51:35Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    51,
                    35,
                    2,
                    50,
                    0
                ],
                "title": "An LLM-based Agent for Reliable Docker Environment Configuration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-based Agent for Reliable Docker Environment Configuration"
                },
                "summary": "Environment configuration is a critical yet time-consuming step in software\ndevelopment, especially when dealing with unfamiliar code repositories. While\nLarge Language Models (LLMs) demonstrate the potential to accomplish software\nengineering tasks, existing methods for environment configuration often rely on\nmanual efforts or fragile scripts, leading to inefficiencies and unreliable\noutcomes. We introduce Repo2Run, the first LLM-based agent designed to fully\nautomate environment configuration and generate executable Dockerfiles for\narbitrary Python repositories. We address two major challenges: (1) enabling\nthe LLM agent to configure environments within isolated Docker containers, and\n(2) ensuring the successful configuration process is recorded and accurately\ntransferred to a Dockerfile without error. To achieve this, we propose atomic\nconfiguration synthesis, featuring a dual-environment architecture (internal\nand external environment) with a rollback mechanism to prevent environment\n\"pollution\" from failed commands, guaranteeing atomic execution (execute fully\nor not at all) and a Dockerfile generator to transfer successful configuration\nsteps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark\nof 420 recent Python repositories with unit tests, where it achieves an 86.0%\nsuccess rate, outperforming the best baseline by 63.9%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Environment configuration is a critical yet time-consuming step in software\ndevelopment, especially when dealing with unfamiliar code repositories. While\nLarge Language Models (LLMs) demonstrate the potential to accomplish software\nengineering tasks, existing methods for environment configuration often rely on\nmanual efforts or fragile scripts, leading to inefficiencies and unreliable\noutcomes. We introduce Repo2Run, the first LLM-based agent designed to fully\nautomate environment configuration and generate executable Dockerfiles for\narbitrary Python repositories. We address two major challenges: (1) enabling\nthe LLM agent to configure environments within isolated Docker containers, and\n(2) ensuring the successful configuration process is recorded and accurately\ntransferred to a Dockerfile without error. To achieve this, we propose atomic\nconfiguration synthesis, featuring a dual-environment architecture (internal\nand external environment) with a rollback mechanism to prevent environment\n\"pollution\" from failed commands, guaranteeing atomic execution (execute fully\nor not at all) and a Dockerfile generator to transfer successful configuration\nsteps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark\nof 420 recent Python repositories with unit tests, where it achieves an 86.0%\nsuccess rate, outperforming the best baseline by 63.9%."
                },
                "authors": [
                    {
                        "name": "Ruida Hu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Xinchen Wang"
                    },
                    {
                        "name": "Cuiyun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Cuiyun Gao"
                },
                "author": "Cuiyun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13677v1",
                "updated": "2025-02-19T12:37:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    37,
                    23,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T12:37:23Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    37,
                    23,
                    2,
                    50,
                    0
                ],
                "title": "A Framework for Semantics-based Situational Awareness during Mobile\n  Robot Deployments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Semantics-based Situational Awareness during Mobile\n  Robot Deployments"
                },
                "summary": "Deployment of robots into hazardous environments typically involves a\n``Human-Robot Teaming'' (HRT) paradigm, in which a human supervisor interacts\nwith a remotely operating robot inside the hazardous zone. Situational\nAwareness (SA) is vital for enabling HRT, to support navigation, planning, and\ndecision-making. This paper explores issues of higher-level ``semantic''\ninformation and understanding in SA. In semi-autonomous, or variable-autonomy\nparadigms, different types of semantic information may be important, in\ndifferent ways, for both the human operator and an autonomous agent controlling\nthe robot. We propose a generalizable framework for acquiring and combining\nmultiple modalities of semantic-level SA during remote deployments of mobile\nrobots. We demonstrate the framework with an example application of search and\nrescue (SAR) in disaster response robotics. We propose a set of ``environment\nsemantic indicators\" that can reflect a variety of different types of semantic\ninformation, e.g. indicators of risk, or signs of human activity, as the robot\nencounters different scenes. Based on these indicators, we propose a metric to\ndescribe the overall situation of the environment called ``Situational Semantic\nRichness (SSR)\". This metric combines multiple semantic indicators to summarise\nthe overall situation. The SSR indicates if an information-rich and complex\nsituation has been encountered, which may require advanced reasoning for robots\nand humans and hence the attention of the expert human operator. The framework\nis tested on a Jackal robot in a mock-up disaster response environment.\nExperimental results demonstrate that the proposed semantic indicators are\nsensitive to changes in different modalities of semantic information in\ndifferent scenes, and the SSR metric reflects overall semantic changes in the\nsituations encountered.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deployment of robots into hazardous environments typically involves a\n``Human-Robot Teaming'' (HRT) paradigm, in which a human supervisor interacts\nwith a remotely operating robot inside the hazardous zone. Situational\nAwareness (SA) is vital for enabling HRT, to support navigation, planning, and\ndecision-making. This paper explores issues of higher-level ``semantic''\ninformation and understanding in SA. In semi-autonomous, or variable-autonomy\nparadigms, different types of semantic information may be important, in\ndifferent ways, for both the human operator and an autonomous agent controlling\nthe robot. We propose a generalizable framework for acquiring and combining\nmultiple modalities of semantic-level SA during remote deployments of mobile\nrobots. We demonstrate the framework with an example application of search and\nrescue (SAR) in disaster response robotics. We propose a set of ``environment\nsemantic indicators\" that can reflect a variety of different types of semantic\ninformation, e.g. indicators of risk, or signs of human activity, as the robot\nencounters different scenes. Based on these indicators, we propose a metric to\ndescribe the overall situation of the environment called ``Situational Semantic\nRichness (SSR)\". This metric combines multiple semantic indicators to summarise\nthe overall situation. The SSR indicates if an information-rich and complex\nsituation has been encountered, which may require advanced reasoning for robots\nand humans and hence the attention of the expert human operator. The framework\nis tested on a Jackal robot in a mock-up disaster response environment.\nExperimental results demonstrate that the proposed semantic indicators are\nsensitive to changes in different modalities of semantic information in\ndifferent scenes, and the SSR metric reflects overall semantic changes in the\nsituations encountered."
                },
                "authors": [
                    {
                        "name": "Tianshu Ruan"
                    },
                    {
                        "name": "Aniketh Ramesh"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Alix Johnstone-Morfoisse"
                    },
                    {
                        "name": "Gokcenur Altindal"
                    },
                    {
                        "name": "Paul Norman"
                    },
                    {
                        "name": "Grigoris Nikolaou"
                    },
                    {
                        "name": "Rustam Stolkin"
                    },
                    {
                        "name": "Manolis Chiou"
                    }
                ],
                "author_detail": {
                    "name": "Manolis Chiou"
                },
                "author": "Manolis Chiou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17003v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17003v4",
                "updated": "2025-02-19T12:35:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    35,
                    31,
                    2,
                    50,
                    0
                ],
                "published": "2024-08-30T04:35:59Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    4,
                    35,
                    59,
                    4,
                    243,
                    0
                ],
                "title": "Safety Layers in Aligned Large Language Models: The Key to LLM Security",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety Layers in Aligned Large Language Models: The Key to LLM Security"
                },
                "summary": "Aligned LLMs are secure, capable of recognizing and refusing to answer\nmalicious questions. However, the role of internal parameters in maintaining\nsuch security is not well understood yet, further these models can be\nvulnerable to security degradation when subjected to fine-tuning attacks. To\naddress these challenges, our work uncovers the mechanism behind security in\naligned LLMs at the parameter level, identifying a small set of contiguous\nlayers in the middle of the model that are crucial for distinguishing malicious\nqueries from normal ones, referred to as ``safety layers\". We first confirm the\nexistence of these safety layers by analyzing variations in input vectors\nwithin the model's internal layers. Additionally, we leverage the\nover-rejection phenomenon and parameters scaling analysis to precisely locate\nthe safety layers. Building on these findings, we propose a novel fine-tuning\napproach, Safely Partial-Parameter Fine-Tuning (SPPFT), that fixes the gradient\nof the safety layers during fine-tuning to address the security degradation.\nOur experiments demonstrate that the proposed approach can significantly\npreserve LLM security while maintaining performance and reducing computational\nresources compared to full fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligned LLMs are secure, capable of recognizing and refusing to answer\nmalicious questions. However, the role of internal parameters in maintaining\nsuch security is not well understood yet, further these models can be\nvulnerable to security degradation when subjected to fine-tuning attacks. To\naddress these challenges, our work uncovers the mechanism behind security in\naligned LLMs at the parameter level, identifying a small set of contiguous\nlayers in the middle of the model that are crucial for distinguishing malicious\nqueries from normal ones, referred to as ``safety layers\". We first confirm the\nexistence of these safety layers by analyzing variations in input vectors\nwithin the model's internal layers. Additionally, we leverage the\nover-rejection phenomenon and parameters scaling analysis to precisely locate\nthe safety layers. Building on these findings, we propose a novel fine-tuning\napproach, Safely Partial-Parameter Fine-Tuning (SPPFT), that fixes the gradient\nof the safety layers during fine-tuning to address the security degradation.\nOur experiments demonstrate that the proposed approach can significantly\npreserve LLM security while maintaining performance and reducing computational\nresources compared to full fine-tuning."
                },
                "authors": [
                    {
                        "name": "Shen Li"
                    },
                    {
                        "name": "Liuyi Yao"
                    },
                    {
                        "name": "Lan Zhang"
                    },
                    {
                        "name": "Yaliang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yaliang Li"
                },
                "author": "Yaliang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17003v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17003v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13674v1",
                "updated": "2025-02-19T12:31:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    31,
                    58,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T12:31:58Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    31,
                    58,
                    2,
                    50,
                    0
                ],
                "title": "SCOPE: A Self-supervised Framework for Improving Faithfulness in\n  Conditional Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: A Self-supervised Framework for Improving Faithfulness in\n  Conditional Text Generation"
                },
                "summary": "Large Language Models (LLMs), when used for conditional text generation,\noften produce hallucinations, i.e., information that is unfaithful or not\ngrounded in the input context. This issue arises in typical conditional text\ngeneration tasks, such as text summarization and data-to-text generation, where\nthe goal is to produce fluent text based on contextual input. When fine-tuned\non specific domains, LLMs struggle to provide faithful answers to a given\ncontext, often adding information or generating errors. One underlying cause of\nthis issue is that LLMs rely on statistical patterns learned from their\ntraining data. This reliance can interfere with the model's ability to stay\nfaithful to a provided context, leading to the generation of ungrounded\ninformation. We build upon this observation and introduce a novel\nself-supervised method for generating a training set of unfaithful samples. We\nthen refine the model using a training process that encourages the generation\nof grounded outputs over unfaithful ones, drawing on preference-based training.\nOur approach leads to significantly more grounded text generation,\noutperforming existing self-supervised techniques in faithfulness, as evaluated\nthrough automatic metrics, LLM-based assessments, and human evaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), when used for conditional text generation,\noften produce hallucinations, i.e., information that is unfaithful or not\ngrounded in the input context. This issue arises in typical conditional text\ngeneration tasks, such as text summarization and data-to-text generation, where\nthe goal is to produce fluent text based on contextual input. When fine-tuned\non specific domains, LLMs struggle to provide faithful answers to a given\ncontext, often adding information or generating errors. One underlying cause of\nthis issue is that LLMs rely on statistical patterns learned from their\ntraining data. This reliance can interfere with the model's ability to stay\nfaithful to a provided context, leading to the generation of ungrounded\ninformation. We build upon this observation and introduce a novel\nself-supervised method for generating a training set of unfaithful samples. We\nthen refine the model using a training process that encourages the generation\nof grounded outputs over unfaithful ones, drawing on preference-based training.\nOur approach leads to significantly more grounded text generation,\noutperforming existing self-supervised techniques in faithfulness, as evaluated\nthrough automatic metrics, LLM-based assessments, and human evaluations."
                },
                "authors": [
                    {
                        "name": "Song Duong"
                    },
                    {
                        "name": "Florian Le Bronnec"
                    },
                    {
                        "name": "Alexandre Allauzen"
                    },
                    {
                        "name": "Vincent Guigue"
                    },
                    {
                        "name": "Alberto Lumbreras"
                    },
                    {
                        "name": "Laure Soulier"
                    },
                    {
                        "name": "Patrick Gallinari"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Gallinari"
                },
                "author": "Patrick Gallinari",
                "arxiv_comment": "10 pages, ICLR 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13663v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13663v1",
                "updated": "2025-02-19T12:15:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    15,
                    32,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T12:15:32Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    15,
                    32,
                    2,
                    50,
                    0
                ],
                "title": "User Association and Coordinated Beamforming in Cognitive\n  Aerial-Terrestrial Networks: A Safe Reinforcement Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "User Association and Coordinated Beamforming in Cognitive\n  Aerial-Terrestrial Networks: A Safe Reinforcement Learning Approach"
                },
                "summary": "Cognitive aerial-terrestrial networks (CATNs) offer a solution to spectrum\nscarcity by sharing spectrum between aerial and terrestrial networks. However,\naerial users (AUs) experience significant interference from numerous\nterrestrial base stations (BSs). To alleviate such interference, we investigate\na user association and coordinated beamforming (CBF) problem in CATN, where the\naerial network serves as the primary network sharing its spectrum with the\nterrestrial network. Specifically, we maximize the sum rate of the secondary\nterrestrial users (TUs) under the interference temperature constraints of the\nAUs. Traditional iterative optimization schemes are impractical due to their\nhigh computational complexity and information exchange overhead. Although deep\nreinforcement learning (DRL) based schemes can address these challenges, their\nperformance is sensitive to the weights of the weighted penalty terms for\nviolating constraints in the reward function. Motivated by these issues, we\npropose a safe DRL-based user association and CBF scheme for CATN, eliminating\nthe need for training multiple times to find the optimal penalty weight before\nactual deployment. Specifically, the CATN is modeled as a networked constrained\npartially observable Markov game. Each TU acts as an agent to choose its\nassociated BS, and each BS acts as an agent to decide its beamforming vectors,\naiming to maximize the reward while satisfying the safety constraints\nintroduced by the interference constraints of the AUs. By exploiting a safe DRL\nalgorithm, the proposed scheme incurs lower deployment expenses than the\npenalty-based DRL schemes since only one training is required before actual\ndeployment. Simulation results show that the proposed scheme can achieve a\nhigher sum rate of TUs than a two-stage optimization scheme while the average\nreceived interference power of the AUs is generally below the threshold.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive aerial-terrestrial networks (CATNs) offer a solution to spectrum\nscarcity by sharing spectrum between aerial and terrestrial networks. However,\naerial users (AUs) experience significant interference from numerous\nterrestrial base stations (BSs). To alleviate such interference, we investigate\na user association and coordinated beamforming (CBF) problem in CATN, where the\naerial network serves as the primary network sharing its spectrum with the\nterrestrial network. Specifically, we maximize the sum rate of the secondary\nterrestrial users (TUs) under the interference temperature constraints of the\nAUs. Traditional iterative optimization schemes are impractical due to their\nhigh computational complexity and information exchange overhead. Although deep\nreinforcement learning (DRL) based schemes can address these challenges, their\nperformance is sensitive to the weights of the weighted penalty terms for\nviolating constraints in the reward function. Motivated by these issues, we\npropose a safe DRL-based user association and CBF scheme for CATN, eliminating\nthe need for training multiple times to find the optimal penalty weight before\nactual deployment. Specifically, the CATN is modeled as a networked constrained\npartially observable Markov game. Each TU acts as an agent to choose its\nassociated BS, and each BS acts as an agent to decide its beamforming vectors,\naiming to maximize the reward while satisfying the safety constraints\nintroduced by the interference constraints of the AUs. By exploiting a safe DRL\nalgorithm, the proposed scheme incurs lower deployment expenses than the\npenalty-based DRL schemes since only one training is required before actual\ndeployment. Simulation results show that the proposed scheme can achieve a\nhigher sum rate of TUs than a two-stage optimization scheme while the average\nreceived interference power of the AUs is generally below the threshold."
                },
                "authors": [
                    {
                        "name": "Zizhen Zhou"
                    },
                    {
                        "name": "Jungang Ge"
                    },
                    {
                        "name": "Ying-Chang Liang"
                    }
                ],
                "author_detail": {
                    "name": "Ying-Chang Liang"
                },
                "author": "Ying-Chang Liang",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13663v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13663v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13656v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13656v1",
                "updated": "2025-02-19T12:07:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    7,
                    53,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T12:07:53Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    12,
                    7,
                    53,
                    2,
                    50,
                    0
                ],
                "title": "Refining Sentence Embedding Model through Ranking Sentences Generation\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Sentence Embedding Model through Ranking Sentences Generation\n  with Large Language Models"
                },
                "summary": "Sentence embedding is essential for many NLP tasks, with contrastive learning\nmethods achieving strong performance using annotated datasets like NLI. Yet,\nthe reliance on manual labels limits scalability. Recent studies leverage large\nlanguage models (LLMs) to generate sentence pairs, reducing annotation\ndependency. However, they overlook ranking information crucial for fine-grained\nsemantic distinctions. To tackle this challenge, we propose a method for\ncontrolling the generation direction of LLMs in the latent space. Unlike\nunconstrained generation, the controlled approach ensures meaningful semantic\ndivergence. Then, we refine exist sentence embedding model by integrating\nranking information and semantic information. Experiments on multiple\nbenchmarks demonstrate that our method achieves new SOTA performance with a\nmodest cost in ranking sentence synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentence embedding is essential for many NLP tasks, with contrastive learning\nmethods achieving strong performance using annotated datasets like NLI. Yet,\nthe reliance on manual labels limits scalability. Recent studies leverage large\nlanguage models (LLMs) to generate sentence pairs, reducing annotation\ndependency. However, they overlook ranking information crucial for fine-grained\nsemantic distinctions. To tackle this challenge, we propose a method for\ncontrolling the generation direction of LLMs in the latent space. Unlike\nunconstrained generation, the controlled approach ensures meaningful semantic\ndivergence. Then, we refine exist sentence embedding model by integrating\nranking information and semantic information. Experiments on multiple\nbenchmarks demonstrate that our method achieves new SOTA performance with a\nmodest cost in ranking sentence synthesis."
                },
                "authors": [
                    {
                        "name": "Liyang He"
                    },
                    {
                        "name": "Chenglong Liu"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Zhenya Huang"
                    },
                    {
                        "name": "Shulan Ruan"
                    },
                    {
                        "name": "Jun Zhou"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13656v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13656v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12825v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12825v2",
                "updated": "2025-02-19T11:57:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    57,
                    19,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-18T12:46:18Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    12,
                    46,
                    18,
                    1,
                    49,
                    0
                ],
                "title": "Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment\n  Revealing Hidden Fault Lines in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning and the Trusting Behavior of DeepSeek and GPT: An Experiment\n  Revealing Hidden Fault Lines in Large Language Models"
                },
                "summary": "When encountering increasingly frequent performance improvements or cost\nreductions from a new large language model (LLM), developers of applications\nleveraging LLMs must decide whether to take advantage of these improvements or\nstay with older tried-and-tested models. Low perceived switching frictions can\nlead to choices that do not consider more subtle behavior changes that the\ntransition may induce. Our experiments use a popular game-theoretic behavioral\neconomics model of trust to show stark differences in the trusting behavior of\nOpenAI's and DeepSeek's models. We highlight a collapse in the economic trust\nbehavior of the o1-mini and o3-mini models as they reconcile profit-maximizing\nand risk-seeking with future returns from trust, and contrast it with\nDeepSeek's more sophisticated and profitable trusting behavior that stems from\nan ability to incorporate deeper concepts like forward planning and\ntheory-of-mind. As LLMs form the basis for high-stakes commercial systems, our\nresults highlight the perils of relying on LLM performance benchmarks that are\ntoo narrowly defined and suggest that careful analysis of their hidden fault\nlines should be part of any organization's AI strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When encountering increasingly frequent performance improvements or cost\nreductions from a new large language model (LLM), developers of applications\nleveraging LLMs must decide whether to take advantage of these improvements or\nstay with older tried-and-tested models. Low perceived switching frictions can\nlead to choices that do not consider more subtle behavior changes that the\ntransition may induce. Our experiments use a popular game-theoretic behavioral\neconomics model of trust to show stark differences in the trusting behavior of\nOpenAI's and DeepSeek's models. We highlight a collapse in the economic trust\nbehavior of the o1-mini and o3-mini models as they reconcile profit-maximizing\nand risk-seeking with future returns from trust, and contrast it with\nDeepSeek's more sophisticated and profitable trusting behavior that stems from\nan ability to incorporate deeper concepts like forward planning and\ntheory-of-mind. As LLMs form the basis for high-stakes commercial systems, our\nresults highlight the perils of relying on LLM performance benchmarks that are\ntoo narrowly defined and suggest that careful analysis of their hidden fault\nlines should be part of any organization's AI strategy."
                },
                "authors": [
                    {
                        "name": "Rubing Li"
                    },
                    {
                        "name": "Joo Sedoc"
                    },
                    {
                        "name": "Arun Sundararajan"
                    }
                ],
                "author_detail": {
                    "name": "Arun Sundararajan"
                },
                "author": "Arun Sundararajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12825v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12825v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13652v1",
                "updated": "2025-02-19T11:57:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    57,
                    2,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T11:57:02Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    57,
                    2,
                    2,
                    50,
                    0
                ],
                "title": "C2T: A Classifier-Based Tree Construction Method in Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C2T: A Classifier-Based Tree Construction Method in Speculative Decoding"
                },
                "summary": "The growing scale of Large Language Models (LLMs) has exacerbated inference\nlatency and computational costs. Speculative decoding methods, which aim to\nmitigate these issues, often face inefficiencies in the construction of token\ntrees and the verification of candidate tokens. Existing strategies, including\nchain mode, static tree, and dynamic tree approaches, have limitations in\naccurately preparing candidate token trees for verification. We propose a novel\nmethod named C2T that adopts a lightweight classifier to generate and prune\ntoken trees dynamically. Our classifier considers additional feature variables\nbeyond the commonly used joint probability to predict the confidence score for\neach draft token to determine whether it is the candidate token for\nverification. This method outperforms state-of-the-art (SOTA) methods such as\nEAGLE-2 on multiple benchmarks, by reducing the total number of candidate\ntokens by 25% while maintaining or even improving the acceptance length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing scale of Large Language Models (LLMs) has exacerbated inference\nlatency and computational costs. Speculative decoding methods, which aim to\nmitigate these issues, often face inefficiencies in the construction of token\ntrees and the verification of candidate tokens. Existing strategies, including\nchain mode, static tree, and dynamic tree approaches, have limitations in\naccurately preparing candidate token trees for verification. We propose a novel\nmethod named C2T that adopts a lightweight classifier to generate and prune\ntoken trees dynamically. Our classifier considers additional feature variables\nbeyond the commonly used joint probability to predict the confidence score for\neach draft token to determine whether it is the candidate token for\nverification. This method outperforms state-of-the-art (SOTA) methods such as\nEAGLE-2 on multiple benchmarks, by reducing the total number of candidate\ntokens by 25% while maintaining or even improving the acceptance length."
                },
                "authors": [
                    {
                        "name": "Feiye Huo"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Shengli Sun"
                    }
                ],
                "author_detail": {
                    "name": "Shengli Sun"
                },
                "author": "Shengli Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13648v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13648v1",
                "updated": "2025-02-19T11:49:23Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    49,
                    23,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T11:49:23Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    49,
                    23,
                    2,
                    50,
                    0
                ],
                "title": "Reliability Across Parametric and External Knowledge: Understanding\n  Knowledge Handling in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliability Across Parametric and External Knowledge: Understanding\n  Knowledge Handling in LLMs"
                },
                "summary": "Large Language Models (LLMs) enhance their problem-solving capability by\nleveraging both parametric and external knowledge. Beyond leveraging external\nknowledge to improve response accuracy, they require key capabilities for\nreliable knowledge-handling: resolving conflicts between knowledge sources,\navoiding distraction from uninformative external knowledge, and abstaining when\nsufficient knowledge is unavailable. Prior studies have examined these\nscenarios in isolation or with limited scope. To systematically evaluate these\ncapabilities, we introduce a comprehensive framework for analyzing\nknowledge-handling based on two key dimensions: the presence of parametric\nknowledge and the informativeness of external knowledge. Through analysis, we\nidentify biases in knowledge utilization and examine how the ability to handle\none scenario impacts performance in others. Furthermore, we demonstrate that\ntraining on data constructed based on the knowledge-handling scenarios improves\nLLMs' reliability in integrating and utilizing knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) enhance their problem-solving capability by\nleveraging both parametric and external knowledge. Beyond leveraging external\nknowledge to improve response accuracy, they require key capabilities for\nreliable knowledge-handling: resolving conflicts between knowledge sources,\navoiding distraction from uninformative external knowledge, and abstaining when\nsufficient knowledge is unavailable. Prior studies have examined these\nscenarios in isolation or with limited scope. To systematically evaluate these\ncapabilities, we introduce a comprehensive framework for analyzing\nknowledge-handling based on two key dimensions: the presence of parametric\nknowledge and the informativeness of external knowledge. Through analysis, we\nidentify biases in knowledge utilization and examine how the ability to handle\none scenario impacts performance in others. Furthermore, we demonstrate that\ntraining on data constructed based on the knowledge-handling scenarios improves\nLLMs' reliability in integrating and utilizing knowledge."
                },
                "authors": [
                    {
                        "name": "Youna Kim"
                    },
                    {
                        "name": "Minjoon Choi"
                    },
                    {
                        "name": "Sungmin Cho"
                    },
                    {
                        "name": "Hyuhng Joon Kim"
                    },
                    {
                        "name": "Sang-goo Lee"
                    },
                    {
                        "name": "Taeuk Kim"
                    }
                ],
                "author_detail": {
                    "name": "Taeuk Kim"
                },
                "author": "Taeuk Kim",
                "arxiv_comment": "under-review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13648v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13648v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13647v1",
                "updated": "2025-02-19T11:44:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    44,
                    27,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T11:44:27Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    44,
                    27,
                    2,
                    50,
                    0
                ],
                "title": "Instruction Tuning on Public Government and Cultural Data for\n  Low-Resource Language: a Case Study in Kazakh",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Tuning on Public Government and Cultural Data for\n  Low-Resource Language: a Case Study in Kazakh"
                },
                "summary": "Instruction tuning in low-resource languages remains underexplored due to\nlimited text data, particularly in government and cultural domains. To address\nthis, we introduce and open-source a large-scale (10,600 samples)\ninstruction-following (IFT) dataset, covering key institutional and cultural\nknowledge relevant to Kazakhstan. Our dataset enhances LLMs' understanding of\nprocedural, legal, and structural governance topics. We employ LLM-assisted\ndata generation, comparing open-weight and closed-weight models for dataset\nconstruction, and select GPT-4o as the backbone. Each entity of our dataset\nundergoes full manual verification to ensure high quality. We also show that\nfine-tuning Qwen, Falcon, and Gemma on our dataset leads to consistent\nperformance improvements in both multiple-choice and generative tasks,\ndemonstrating the potential of LLM-assisted instruction tuning for low-resource\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning in low-resource languages remains underexplored due to\nlimited text data, particularly in government and cultural domains. To address\nthis, we introduce and open-source a large-scale (10,600 samples)\ninstruction-following (IFT) dataset, covering key institutional and cultural\nknowledge relevant to Kazakhstan. Our dataset enhances LLMs' understanding of\nprocedural, legal, and structural governance topics. We employ LLM-assisted\ndata generation, comparing open-weight and closed-weight models for dataset\nconstruction, and select GPT-4o as the backbone. Each entity of our dataset\nundergoes full manual verification to ensure high quality. We also show that\nfine-tuning Qwen, Falcon, and Gemma on our dataset leads to consistent\nperformance improvements in both multiple-choice and generative tasks,\ndemonstrating the potential of LLM-assisted instruction tuning for low-resource\nlanguages."
                },
                "authors": [
                    {
                        "name": "Nurkhan Laiyk"
                    },
                    {
                        "name": "Daniil Orel"
                    },
                    {
                        "name": "Rituraj Joshi"
                    },
                    {
                        "name": "Maiya Goloburda"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Fajri Koto"
                    }
                ],
                "author_detail": {
                    "name": "Fajri Koto"
                },
                "author": "Fajri Koto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13646v1",
                "updated": "2025-02-19T11:41:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    41,
                    40,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T11:41:40Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    41,
                    40,
                    2,
                    50,
                    0
                ],
                "title": "D.Va: Validate Your Demonstration First Before You Use It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D.Va: Validate Your Demonstration First Before You Use It"
                },
                "summary": "In-context learning (ICL) has demonstrated significant potential in enhancing\nthe capabilities of large language models (LLMs) during inference. It's\nwell-established that ICL heavily relies on selecting effective demonstrations\nto generate outputs that better align with the expected results. As for\ndemonstration selection, previous approaches have typically relied on intuitive\nmetrics to evaluate the effectiveness of demonstrations, which often results in\nlimited robustness and poor cross-model generalization capabilities. To tackle\nthese challenges, we propose a novel method, \\textbf{D}emonstration\n\\textbf{VA}lidation (\\textbf{D.Va}), which integrates a demonstration\nvalidation perspective into this field. By introducing the demonstration\nvalidation mechanism, our method effectively identifies demonstrations that are\nboth effective and highly generalizable. \\textbf{D.Va} surpasses all existing\ndemonstration selection techniques across both natural language understanding\n(NLU) and natural language generation (NLG) tasks. Additionally, we demonstrate\nthe robustness and generalizability of our approach across various language\nmodels with different retrieval models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) has demonstrated significant potential in enhancing\nthe capabilities of large language models (LLMs) during inference. It's\nwell-established that ICL heavily relies on selecting effective demonstrations\nto generate outputs that better align with the expected results. As for\ndemonstration selection, previous approaches have typically relied on intuitive\nmetrics to evaluate the effectiveness of demonstrations, which often results in\nlimited robustness and poor cross-model generalization capabilities. To tackle\nthese challenges, we propose a novel method, \\textbf{D}emonstration\n\\textbf{VA}lidation (\\textbf{D.Va}), which integrates a demonstration\nvalidation perspective into this field. By introducing the demonstration\nvalidation mechanism, our method effectively identifies demonstrations that are\nboth effective and highly generalizable. \\textbf{D.Va} surpasses all existing\ndemonstration selection techniques across both natural language understanding\n(NLU) and natural language generation (NLG) tasks. Additionally, we demonstrate\nthe robustness and generalizability of our approach across various language\nmodels with different retrieval models."
                },
                "authors": [
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Zhiqing Xiao"
                    },
                    {
                        "name": "Ruixuan Xiao"
                    },
                    {
                        "name": "Lirong Gao"
                    },
                    {
                        "name": "Junbo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junbo Zhao"
                },
                "author": "Junbo Zhao",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17560v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17560v2",
                "updated": "2025-02-19T11:35:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    35,
                    43,
                    2,
                    50,
                    0
                ],
                "published": "2024-12-23T13:28:15Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    13,
                    28,
                    15,
                    0,
                    358,
                    0
                ],
                "title": "GQSA: Group Quantization and Sparsity for Accelerating Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GQSA: Group Quantization and Sparsity for Accelerating Large Language\n  Model Inference"
                },
                "summary": "Model compression has emerged as a mainstream solution to reduce memory usage\nand computational overhead. This paper presents Group Quantization and Sparse\nAcceleration (GQSA), a novel compression technique tailored for LLMs.\nTraditional methods typically focus exclusively on either quantization or\nsparsification, but relying on a single strategy often results in significant\nperformance loss at high compression rates. In contrast, GQSA integrates\nquantization and sparsification in a tightly coupled manner, leveraging\nGPU-friendly structured group sparsity and quantization for efficient\nacceleration. Building upon system-algorithm co-design principles, we propose a\ntwo-stage sparse optimization strategy that ensures the performance superiority\nof the compressed model. On the engine side, we introduce a \"task-centric\"\nparallel strategy, which, to the best of our knowledge, is the first\napplication in the domain of sparse computing. Compared to the traditional 2:4\nsparse method, the GQSA offers a more flexible and adjustable sparsity rate, as\nwell as a higher weight compression rate, and is efficiently compatible with\nweight-only quantization methods. Experimental results demonstrate that, under\nthe GQSA W4S50% compression setting, the model's accuracy surpasses that of\nboth 2:4 pruning and W2 quantization. Furthermore, at the inference level, GQSA\noutperforms W2 by 1.26$\\times$ and 2:4 pruning by 2.35$\\times$ in terms of\nspeed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model compression has emerged as a mainstream solution to reduce memory usage\nand computational overhead. This paper presents Group Quantization and Sparse\nAcceleration (GQSA), a novel compression technique tailored for LLMs.\nTraditional methods typically focus exclusively on either quantization or\nsparsification, but relying on a single strategy often results in significant\nperformance loss at high compression rates. In contrast, GQSA integrates\nquantization and sparsification in a tightly coupled manner, leveraging\nGPU-friendly structured group sparsity and quantization for efficient\nacceleration. Building upon system-algorithm co-design principles, we propose a\ntwo-stage sparse optimization strategy that ensures the performance superiority\nof the compressed model. On the engine side, we introduce a \"task-centric\"\nparallel strategy, which, to the best of our knowledge, is the first\napplication in the domain of sparse computing. Compared to the traditional 2:4\nsparse method, the GQSA offers a more flexible and adjustable sparsity rate, as\nwell as a higher weight compression rate, and is efficiently compatible with\nweight-only quantization methods. Experimental results demonstrate that, under\nthe GQSA W4S50% compression setting, the model's accuracy surpasses that of\nboth 2:4 pruning and W2 quantization. Furthermore, at the inference level, GQSA\noutperforms W2 by 1.26$\\times$ and 2:4 pruning by 2.35$\\times$ in terms of\nspeed."
                },
                "authors": [
                    {
                        "name": "Chao Zeng"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Shu Yang"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Mei"
                    },
                    {
                        "name": "Lean Fu"
                    }
                ],
                "author_detail": {
                    "name": "Lean Fu"
                },
                "author": "Lean Fu",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17560v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17560v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13640v1",
                "updated": "2025-02-19T11:33:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    33,
                    22,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T11:33:22Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    33,
                    22,
                    2,
                    50,
                    0
                ],
                "title": "Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qorgau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts"
                },
                "summary": "Large language models (LLMs) are known to have the potential to generate\nharmful content, posing risks to users. While significant progress has been\nmade in developing taxonomies for LLM risks and safety evaluation prompts, most\nstudies have focused on monolingual contexts, primarily in English. However,\nlanguage- and region-specific risks in bilingual contexts are often overlooked,\nand core findings can diverge from those in monolingual settings. In this\npaper, we introduce Qorgau, a novel dataset specifically designed for safety\nevaluation in Kazakh and Russian, reflecting the unique bilingual context in\nKazakhstan, where both Kazakh (a low-resource language) and Russian (a\nhigh-resource language) are spoken. Experiments with both multilingual and\nlanguage-specific LLMs reveal notable differences in safety performance,\nemphasizing the need for tailored, region-specific datasets to ensure the\nresponsible and safe deployment of LLMs in countries like Kazakhstan. Warning:\nthis paper contains example data that may be offensive, harmful, or biased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are known to have the potential to generate\nharmful content, posing risks to users. While significant progress has been\nmade in developing taxonomies for LLM risks and safety evaluation prompts, most\nstudies have focused on monolingual contexts, primarily in English. However,\nlanguage- and region-specific risks in bilingual contexts are often overlooked,\nand core findings can diverge from those in monolingual settings. In this\npaper, we introduce Qorgau, a novel dataset specifically designed for safety\nevaluation in Kazakh and Russian, reflecting the unique bilingual context in\nKazakhstan, where both Kazakh (a low-resource language) and Russian (a\nhigh-resource language) are spoken. Experiments with both multilingual and\nlanguage-specific LLMs reveal notable differences in safety performance,\nemphasizing the need for tailored, region-specific datasets to ensure the\nresponsible and safe deployment of LLMs in countries like Kazakhstan. Warning:\nthis paper contains example data that may be offensive, harmful, or biased."
                },
                "authors": [
                    {
                        "name": "Maiya Goloburda"
                    },
                    {
                        "name": "Nurkhan Laiyk"
                    },
                    {
                        "name": "Diana Turmakhan"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Mukhammed Togmanov"
                    },
                    {
                        "name": "Jonibek Mansurov"
                    },
                    {
                        "name": "Askhat Sametov"
                    },
                    {
                        "name": "Nurdaulet Mukhituly"
                    },
                    {
                        "name": "Minghan Wang"
                    },
                    {
                        "name": "Daniil Orel"
                    },
                    {
                        "name": "Zain Muhammad Mujahid"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Preslav Nakov"
                    }
                ],
                "author_detail": {
                    "name": "Preslav Nakov"
                },
                "author": "Preslav Nakov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13632v1",
                "updated": "2025-02-19T11:10:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    10,
                    19,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T11:10:19Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    10,
                    19,
                    2,
                    50,
                    0
                ],
                "title": "Concept Layers: Enhancing Interpretability and Intervenability via LLM\n  Conceptualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept Layers: Enhancing Interpretability and Intervenability via LLM\n  Conceptualization"
                },
                "summary": "The opaque nature of Large Language Models (LLMs) has led to significant\nresearch efforts aimed at enhancing their interpretability, primarily through\npost-hoc methods. More recent in-hoc approaches, such as Concept Bottleneck\nModels (CBMs), offer both interpretability and intervenability by incorporating\nexplicit concept representations. However, these methods suffer from key\nlimitations, including reliance on labeled concept datasets and significant\narchitectural modifications that challenges re-integration into existing system\npipelines. In this work, we introduce a new methodology for incorporating\ninterpretability and intervenability into an existing model by integrating\nConcept Layers (CLs) into its architecture. Our approach projects the model's\ninternal vector representations into a conceptual, explainable vector space\nbefore reconstructing and feeding them back into the model. Furthermore, we\neliminate the need for a human-selected concept set by algorithmically\nsearching an ontology for a set of concepts that can be either task-specific or\ntask-agnostic. We evaluate CLs across multiple tasks, demonstrating that they\nmaintain the original model's performance and agreement while enabling\nmeaningful interventions. Additionally, we present a proof of concept\nshowcasing an intervenability interface, allowing users to adjust model\nbehavior dynamically, such as mitigating biases during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The opaque nature of Large Language Models (LLMs) has led to significant\nresearch efforts aimed at enhancing their interpretability, primarily through\npost-hoc methods. More recent in-hoc approaches, such as Concept Bottleneck\nModels (CBMs), offer both interpretability and intervenability by incorporating\nexplicit concept representations. However, these methods suffer from key\nlimitations, including reliance on labeled concept datasets and significant\narchitectural modifications that challenges re-integration into existing system\npipelines. In this work, we introduce a new methodology for incorporating\ninterpretability and intervenability into an existing model by integrating\nConcept Layers (CLs) into its architecture. Our approach projects the model's\ninternal vector representations into a conceptual, explainable vector space\nbefore reconstructing and feeding them back into the model. Furthermore, we\neliminate the need for a human-selected concept set by algorithmically\nsearching an ontology for a set of concepts that can be either task-specific or\ntask-agnostic. We evaluate CLs across multiple tasks, demonstrating that they\nmaintain the original model's performance and agreement while enabling\nmeaningful interventions. Additionally, we present a proof of concept\nshowcasing an intervenability interface, allowing users to adjust model\nbehavior dynamically, such as mitigating biases during inference."
                },
                "authors": [
                    {
                        "name": "Or Raphael Bidusa"
                    },
                    {
                        "name": "Shaul Markovitch"
                    }
                ],
                "author_detail": {
                    "name": "Shaul Markovitch"
                },
                "author": "Shaul Markovitch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22118v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22118v2",
                "updated": "2025-02-19T11:10:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    10,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2024-10-29T15:19:13Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    15,
                    19,
                    13,
                    1,
                    303,
                    0
                ],
                "title": "The Impact of Inference Acceleration on Bias of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Inference Acceleration on Bias of LLMs"
                },
                "summary": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last few years have seen unprecedented advances in capabilities of Large\nLanguage Models (LLMs). These advancements promise to benefit a vast array of\napplication domains. However, due to their immense size, performing inference\nwith LLMs is both costly and slow. Consequently, a plethora of recent work has\nproposed strategies to enhance inference efficiency, e.g., quantization,\npruning, and caching. These acceleration strategies reduce the inference cost\nand latency, often by several factors, while maintaining much of the predictive\nperformance measured via common benchmarks. In this work, we explore another\ncritical aspect of LLM performance: demographic bias in model generations due\nto inference acceleration optimizations. Using a wide range of metrics, we\nprobe bias in model outputs from a number of angles. Analysis of outputs before\nand after inference acceleration shows significant change in bias. Worryingly,\nthese bias effects are complex and unpredictable. A combination of an\nacceleration strategy and bias type may show little bias change in one model\nbut may lead to a large effect in another. Our results highlight a need for\nin-depth and case-by-case evaluation of model bias after it has been modified\nto accelerate inference."
                },
                "authors": [
                    {
                        "name": "Elisabeth Kirsten"
                    },
                    {
                        "name": "Ivan Habernal"
                    },
                    {
                        "name": "Vedant Nanda"
                    },
                    {
                        "name": "Muhammad Bilal Zafar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Bilal Zafar"
                },
                "author": "Muhammad Bilal Zafar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22118v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22118v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12145v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12145v3",
                "updated": "2025-02-19T11:09:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    9,
                    15,
                    2,
                    50,
                    0
                ],
                "published": "2024-12-10T10:14:03Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    14,
                    3,
                    1,
                    345,
                    0
                ],
                "title": "Na'vi or Knave: Jailbreaking Language Models via Metaphorical Avatars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Na'vi or Knave: Jailbreaking Language Models via Metaphorical Avatars"
                },
                "summary": "Metaphor serves as an implicit approach to convey information, while enabling\nthe generalized comprehension of complex subjects. However, metaphor can\npotentially be exploited to bypass the safety alignment mechanisms of Large\nLanguage Models (LLMs), leading to the theft of harmful knowledge. In our\nstudy, we introduce a novel attack framework that exploits the imaginative\ncapacity of LLMs to achieve jailbreaking, the J\\underline{\\textbf{A}}ilbreak\n\\underline{\\textbf{V}}ia \\underline{\\textbf{A}}dversarial\nMe\\underline{\\textbf{TA}} -pho\\underline{\\textbf{R}} (\\textit{AVATAR}).\nSpecifically, to elicit the harmful response, AVATAR extracts harmful entities\nfrom a given harmful target and maps them to innocuous adversarial entities\nbased on LLM's imagination. Then, according to these metaphors, the harmful\ntarget is nested within human-like interaction for jailbreaking adaptively.\nExperimental results demonstrate that AVATAR can effectively and transferablly\njailbreak LLMs and achieve a state-of-the-art attack success rate across\nmultiple advanced LLMs. Our study exposes a security risk in LLMs from their\nendogenous imaginative capabilities. Furthermore, the analytical study reveals\nthe vulnerability of LLM to adversarial metaphors and the necessity of\ndeveloping defense methods against jailbreaking caused by the adversarial\nmetaphor. \\textcolor{orange}{ \\textbf{Warning: This paper contains potentially\nharmful content from LLMs.}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metaphor serves as an implicit approach to convey information, while enabling\nthe generalized comprehension of complex subjects. However, metaphor can\npotentially be exploited to bypass the safety alignment mechanisms of Large\nLanguage Models (LLMs), leading to the theft of harmful knowledge. In our\nstudy, we introduce a novel attack framework that exploits the imaginative\ncapacity of LLMs to achieve jailbreaking, the J\\underline{\\textbf{A}}ilbreak\n\\underline{\\textbf{V}}ia \\underline{\\textbf{A}}dversarial\nMe\\underline{\\textbf{TA}} -pho\\underline{\\textbf{R}} (\\textit{AVATAR}).\nSpecifically, to elicit the harmful response, AVATAR extracts harmful entities\nfrom a given harmful target and maps them to innocuous adversarial entities\nbased on LLM's imagination. Then, according to these metaphors, the harmful\ntarget is nested within human-like interaction for jailbreaking adaptively.\nExperimental results demonstrate that AVATAR can effectively and transferablly\njailbreak LLMs and achieve a state-of-the-art attack success rate across\nmultiple advanced LLMs. Our study exposes a security risk in LLMs from their\nendogenous imaginative capabilities. Furthermore, the analytical study reveals\nthe vulnerability of LLM to adversarial metaphors and the necessity of\ndeveloping defense methods against jailbreaking caused by the adversarial\nmetaphor. \\textcolor{orange}{ \\textbf{Warning: This paper contains potentially\nharmful content from LLMs.}}"
                },
                "authors": [
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Sheng Sun"
                    },
                    {
                        "name": "Junqi Tong"
                    },
                    {
                        "name": "Min Liu"
                    },
                    {
                        "name": "Qi Li"
                    }
                ],
                "author_detail": {
                    "name": "Qi Li"
                },
                "author": "Qi Li",
                "arxiv_comment": "We still need to polish our paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12145v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12145v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13626v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13626v1",
                "updated": "2025-02-19T11:03:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    3,
                    9,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T11:03:09Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    3,
                    9,
                    2,
                    50,
                    0
                ],
                "title": "AI-Empowered Catalyst Discovery: A Survey from Classical Machine\n  Learning Approaches to Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Empowered Catalyst Discovery: A Survey from Classical Machine\n  Learning Approaches to Large Language Models"
                },
                "summary": "Catalysts are essential for accelerating chemical reactions and enhancing\nselectivity, which is crucial for the sustainable production of energy,\nmaterials, and bioactive compounds. Catalyst discovery is fundamental yet\nchallenging in computational chemistry and has garnered significant attention\ndue to the promising performance of advanced Artificial Intelligence (AI)\ntechniques. The development of Large Language Models (LLMs) notably accelerates\nprogress in the discovery of both homogeneous and heterogeneous catalysts,\nwhere their chemical reactions differ significantly in material phases,\ntemperature, dynamics, etc. However, there is currently no comprehensive survey\nthat discusses the progress and latest developments in both areas, particularly\nwith the application of LLM techniques. To address this gap, this paper\npresents a thorough and systematic survey of AI-empowered catalyst discovery,\nemploying a unified and general categorization for homogeneous and\nheterogeneous catalysts. We examine the progress of AI-empowered catalyst\ndiscovery, highlighting their individual advantages and disadvantages, and\ndiscuss the challenges faced in this field. Furthermore, we suggest potential\ndirections for future research from the perspective of computer science. Our\ngoal is to assist researchers in computational chemistry, computer science, and\nrelated fields in easily tracking the latest advancements, providing a clear\noverview and roadmap of this area. We also organize and make accessible\nrelevant resources, including article lists and datasets, in an open repository\nat\nhttps://github.com/LuckyGirl-XU/Awesome-Artificial-Intelligence-Empowered-Catalyst-Discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catalysts are essential for accelerating chemical reactions and enhancing\nselectivity, which is crucial for the sustainable production of energy,\nmaterials, and bioactive compounds. Catalyst discovery is fundamental yet\nchallenging in computational chemistry and has garnered significant attention\ndue to the promising performance of advanced Artificial Intelligence (AI)\ntechniques. The development of Large Language Models (LLMs) notably accelerates\nprogress in the discovery of both homogeneous and heterogeneous catalysts,\nwhere their chemical reactions differ significantly in material phases,\ntemperature, dynamics, etc. However, there is currently no comprehensive survey\nthat discusses the progress and latest developments in both areas, particularly\nwith the application of LLM techniques. To address this gap, this paper\npresents a thorough and systematic survey of AI-empowered catalyst discovery,\nemploying a unified and general categorization for homogeneous and\nheterogeneous catalysts. We examine the progress of AI-empowered catalyst\ndiscovery, highlighting their individual advantages and disadvantages, and\ndiscuss the challenges faced in this field. Furthermore, we suggest potential\ndirections for future research from the perspective of computer science. Our\ngoal is to assist researchers in computational chemistry, computer science, and\nrelated fields in easily tracking the latest advancements, providing a clear\noverview and roadmap of this area. We also organize and make accessible\nrelevant resources, including article lists and datasets, in an open repository\nat\nhttps://github.com/LuckyGirl-XU/Awesome-Artificial-Intelligence-Empowered-Catalyst-Discovery."
                },
                "authors": [
                    {
                        "name": "Yuanyuan Xu"
                    },
                    {
                        "name": "Hanchen Wang"
                    },
                    {
                        "name": "Wenjie Zhang"
                    },
                    {
                        "name": "Lexing Xie"
                    },
                    {
                        "name": "Yin Chen"
                    },
                    {
                        "name": "Flora Salim"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Justin Gooding"
                    },
                    {
                        "name": "Toby Walsh"
                    }
                ],
                "author_detail": {
                    "name": "Toby Walsh"
                },
                "author": "Toby Walsh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13626v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13626v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13624v1",
                "updated": "2025-02-19T11:00:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    0,
                    34,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T11:00:34Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    11,
                    0,
                    34,
                    2,
                    50,
                    0
                ],
                "title": "CardiacMamba: A Multimodal RGB-RF Fusion Framework with State Space\n  Models for Remote Physiological Measurement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CardiacMamba: A Multimodal RGB-RF Fusion Framework with State Space\n  Models for Remote Physiological Measurement"
                },
                "summary": "Heart rate (HR) estimation via remote photoplethysmography (rPPG) offers a\nnon-invasive solution for health monitoring. However, traditional\nsingle-modality approaches (RGB or Radio Frequency (RF)) face challenges in\nbalancing robustness and accuracy due to lighting variations, motion artifacts,\nand skin tone bias. In this paper, we propose CardiacMamba, a multimodal RGB-RF\nfusion framework that leverages the complementary strengths of both modalities.\nIt introduces the Temporal Difference Mamba Module (TDMM) to capture dynamic\nchanges in RF signals using timing differences between frames, enhancing the\nextraction of local and global features. Additionally, CardiacMamba employs a\nBidirectional SSM for cross-modal alignment and a Channel-wise Fast Fourier\nTransform (CFFT) to effectively capture and refine the frequency domain\ncharacteristics of RGB and RF signals, ultimately improving heart rate\nestimation accuracy and periodicity detection. Extensive experiments on the\nEquiPleth dataset demonstrate state-of-the-art performance, achieving marked\nimprovements in accuracy and robustness. CardiacMamba significantly mitigates\nskin tone bias, reducing performance disparities across demographic groups, and\nmaintains resilience under missing-modality scenarios. By addressing critical\nchallenges in fairness, adaptability, and precision, the framework advances\nrPPG technology toward reliable real-world deployment in healthcare. The codes\nare available at: https://github.com/WuZheng42/CardiacMamba.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heart rate (HR) estimation via remote photoplethysmography (rPPG) offers a\nnon-invasive solution for health monitoring. However, traditional\nsingle-modality approaches (RGB or Radio Frequency (RF)) face challenges in\nbalancing robustness and accuracy due to lighting variations, motion artifacts,\nand skin tone bias. In this paper, we propose CardiacMamba, a multimodal RGB-RF\nfusion framework that leverages the complementary strengths of both modalities.\nIt introduces the Temporal Difference Mamba Module (TDMM) to capture dynamic\nchanges in RF signals using timing differences between frames, enhancing the\nextraction of local and global features. Additionally, CardiacMamba employs a\nBidirectional SSM for cross-modal alignment and a Channel-wise Fast Fourier\nTransform (CFFT) to effectively capture and refine the frequency domain\ncharacteristics of RGB and RF signals, ultimately improving heart rate\nestimation accuracy and periodicity detection. Extensive experiments on the\nEquiPleth dataset demonstrate state-of-the-art performance, achieving marked\nimprovements in accuracy and robustness. CardiacMamba significantly mitigates\nskin tone bias, reducing performance disparities across demographic groups, and\nmaintains resilience under missing-modality scenarios. By addressing critical\nchallenges in fairness, adaptability, and precision, the framework advances\nrPPG technology toward reliable real-world deployment in healthcare. The codes\nare available at: https://github.com/WuZheng42/CardiacMamba."
                },
                "authors": [
                    {
                        "name": "Zheng Wu"
                    },
                    {
                        "name": "Yiping Xie"
                    },
                    {
                        "name": "Bo Zhao"
                    },
                    {
                        "name": "Jiguang He"
                    },
                    {
                        "name": "Fei Luo"
                    },
                    {
                        "name": "Ning Deng"
                    },
                    {
                        "name": "Zitong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zitong Yu"
                },
                "author": "Zitong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13622v1",
                "updated": "2025-02-19T10:59:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    59,
                    5,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T10:59:05Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    59,
                    5,
                    2,
                    50,
                    0
                ],
                "title": "REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large\n  Language Models"
                },
                "summary": "Hallucinations in large language model (LLM) outputs severely limit their\nreliability in knowledge-intensive tasks such as question answering. To address\nthis challenge, we introduce REFIND (Retrieval-augmented Factuality\nhallucINation Detection), a novel framework that detects hallucinated spans\nwithin LLM outputs by directly leveraging retrieved documents. As part of the\nREFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that\nquantifies the sensitivity of LLM outputs to retrieved evidence. This\ninnovative approach enables REFIND to efficiently and accurately detect\nhallucinations, setting it apart from existing methods. In the evaluation,\nREFIND demonstrated robustness across nine languages, including low-resource\nsettings, and significantly outperformed baseline models, achieving superior\nIoU scores in identifying hallucinated spans. This work highlights the\neffectiveness of quantifying context sensitivity for hallucination detection,\nthereby paving the way for more reliable and trustworthy LLM applications\nacross diverse languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucinations in large language model (LLM) outputs severely limit their\nreliability in knowledge-intensive tasks such as question answering. To address\nthis challenge, we introduce REFIND (Retrieval-augmented Factuality\nhallucINation Detection), a novel framework that detects hallucinated spans\nwithin LLM outputs by directly leveraging retrieved documents. As part of the\nREFIND, we propose the Context Sensitivity Ratio (CSR), a novel metric that\nquantifies the sensitivity of LLM outputs to retrieved evidence. This\ninnovative approach enables REFIND to efficiently and accurately detect\nhallucinations, setting it apart from existing methods. In the evaluation,\nREFIND demonstrated robustness across nine languages, including low-resource\nsettings, and significantly outperformed baseline models, achieving superior\nIoU scores in identifying hallucinated spans. This work highlights the\neffectiveness of quantifying context sensitivity for hallucination detection,\nthereby paving the way for more reliable and trustworthy LLM applications\nacross diverse languages."
                },
                "authors": [
                    {
                        "name": "DongGeon Lee"
                    },
                    {
                        "name": "Hwanjo Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hwanjo Yu"
                },
                "author": "Hwanjo Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13619v1",
                "updated": "2025-02-19T10:56:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    56,
                    27,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T10:56:27Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    56,
                    27,
                    2,
                    50,
                    0
                ],
                "title": "Complex Ontology Matching with Large Language Model Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex Ontology Matching with Large Language Model Embeddings"
                },
                "summary": "Ontology, and more broadly, Knowledge Graph Matching is a challenging task in\nwhich expressiveness has not been fully addressed. Despite the increasing use\nof embeddings and language models for this task, approaches for generating\nexpressive correspondences still do not take full advantage of these models, in\nparticular, large language models (LLMs). This paper proposes to integrate LLMs\ninto an approach for generating expressive correspondences based on alignment\nneed and ABox-based relation discovery. The generation of correspondences is\nperformed by matching similar surroundings of instance sub-graphs. The\nintegration of LLMs results in different architectural modifications, including\nlabel similarity, sub-graph matching, and entity matching. The performance word\nembeddings, sentence embeddings, and LLM-based embeddings, was compared. The\nresults demonstrate that integrating LLMs surpasses all other models, enhancing\nthe baseline version of the approach with a 45\\% increase in F-measure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontology, and more broadly, Knowledge Graph Matching is a challenging task in\nwhich expressiveness has not been fully addressed. Despite the increasing use\nof embeddings and language models for this task, approaches for generating\nexpressive correspondences still do not take full advantage of these models, in\nparticular, large language models (LLMs). This paper proposes to integrate LLMs\ninto an approach for generating expressive correspondences based on alignment\nneed and ABox-based relation discovery. The generation of correspondences is\nperformed by matching similar surroundings of instance sub-graphs. The\nintegration of LLMs results in different architectural modifications, including\nlabel similarity, sub-graph matching, and entity matching. The performance word\nembeddings, sentence embeddings, and LLM-based embeddings, was compared. The\nresults demonstrate that integrating LLMs surpasses all other models, enhancing\nthe baseline version of the approach with a 45\\% increase in F-measure."
                },
                "authors": [
                    {
                        "name": "Guilherme Sousa"
                    },
                    {
                        "name": "Rinaldo Lima"
                    },
                    {
                        "name": "Cassia Trojahn"
                    }
                ],
                "author_detail": {
                    "name": "Cassia Trojahn"
                },
                "author": "Cassia Trojahn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16130v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16130v2",
                "updated": "2025-02-19T10:49:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    49,
                    41,
                    2,
                    50,
                    0
                ],
                "published": "2024-04-24T18:38:11Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    18,
                    38,
                    11,
                    2,
                    115,
                    0
                ],
                "title": "From Local to Global: A Graph RAG Approach to Query-Focused\n  Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Local to Global: A Graph RAG Approach to Query-Focused\n  Summarization"
                },
                "summary": "The use of retrieval-augmented generation (RAG) to retrieve relevant\ninformation from an external knowledge source enables large language models\n(LLMs) to answer questions over private and/or previously unseen document\ncollections. However, RAG fails on global questions directed at an entire text\ncorpus, such as \"What are the main themes in the dataset?\", since this is\ninherently a query-focused summarization (QFS) task, rather than an explicit\nretrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of\ntext indexed by typical RAG systems. To combine the strengths of these\ncontrasting methods, we propose GraphRAG, a graph-based approach to question\nanswering over private text corpora that scales with both the generality of\nuser questions and the quantity of source text. Our approach uses an LLM to\nbuild a graph index in two stages: first, to derive an entity knowledge graph\nfrom the source documents, then to pregenerate community summaries for all\ngroups of closely related entities. Given a question, each community summary is\nused to generate a partial response, before all partial responses are again\nsummarized in a final response to the user. For a class of global sensemaking\nquestions over datasets in the 1 million token range, we show that GraphRAG\nleads to substantial improvements over a conventional RAG baseline for both the\ncomprehensiveness and diversity of generated answers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of retrieval-augmented generation (RAG) to retrieve relevant\ninformation from an external knowledge source enables large language models\n(LLMs) to answer questions over private and/or previously unseen document\ncollections. However, RAG fails on global questions directed at an entire text\ncorpus, such as \"What are the main themes in the dataset?\", since this is\ninherently a query-focused summarization (QFS) task, rather than an explicit\nretrieval task. Prior QFS methods, meanwhile, do not scale to the quantities of\ntext indexed by typical RAG systems. To combine the strengths of these\ncontrasting methods, we propose GraphRAG, a graph-based approach to question\nanswering over private text corpora that scales with both the generality of\nuser questions and the quantity of source text. Our approach uses an LLM to\nbuild a graph index in two stages: first, to derive an entity knowledge graph\nfrom the source documents, then to pregenerate community summaries for all\ngroups of closely related entities. Given a question, each community summary is\nused to generate a partial response, before all partial responses are again\nsummarized in a final response to the user. For a class of global sensemaking\nquestions over datasets in the 1 million token range, we show that GraphRAG\nleads to substantial improvements over a conventional RAG baseline for both the\ncomprehensiveness and diversity of generated answers."
                },
                "authors": [
                    {
                        "name": "Darren Edge"
                    },
                    {
                        "name": "Ha Trinh"
                    },
                    {
                        "name": "Newman Cheng"
                    },
                    {
                        "name": "Joshua Bradley"
                    },
                    {
                        "name": "Alex Chao"
                    },
                    {
                        "name": "Apurva Mody"
                    },
                    {
                        "name": "Steven Truitt"
                    },
                    {
                        "name": "Dasha Metropolitansky"
                    },
                    {
                        "name": "Robert Osazuwa Ness"
                    },
                    {
                        "name": "Jonathan Larson"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Larson"
                },
                "author": "Jonathan Larson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16130v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16130v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07365v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07365v2",
                "updated": "2025-02-19T10:49:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    49,
                    24,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-11T08:37:16Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    8,
                    37,
                    16,
                    1,
                    42,
                    0
                ],
                "title": "LongReD: Mitigating Short-Text Degradation of Long-Context Large\n  Language Models via Restoration Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongReD: Mitigating Short-Text Degradation of Long-Context Large\n  Language Models via Restoration Distillation"
                },
                "summary": "Large language models (LLMs) have gained extended context windows through\nscaling positional encodings and lightweight continual pre-training. However,\nthis often leads to degraded performance on short-text tasks, while the reasons\nfor this degradation remain insufficiently explored. In this work, we identify\ntwo primary factors contributing to this issue: distribution drift in hidden\nstates and attention scores, and catastrophic forgetting during continual\npre-training. To address these challenges, we propose Long Context Pre-training\nwith Restoration Distillation (LongReD), a novel approach designed to mitigate\nshort-text performance degradation through minimizing the distribution\ndiscrepancy between the extended and original models. Besides training on long\ntexts, LongReD distills the hidden state of selected layers from the original\nmodel on short texts. Additionally, LongReD also introduces a short-to-long\ndistillation, aligning the output distribution on short texts with that on long\ntexts by leveraging skipped positional indices. Experiments on common text\nbenchmarks demonstrate that LongReD effectively preserves the model's\nshort-text performance while maintaining comparable or even better capacity to\nhandle long texts than baselines. Our code is available at\nhttps://github.com/RUCAIBox/LongReD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained extended context windows through\nscaling positional encodings and lightweight continual pre-training. However,\nthis often leads to degraded performance on short-text tasks, while the reasons\nfor this degradation remain insufficiently explored. In this work, we identify\ntwo primary factors contributing to this issue: distribution drift in hidden\nstates and attention scores, and catastrophic forgetting during continual\npre-training. To address these challenges, we propose Long Context Pre-training\nwith Restoration Distillation (LongReD), a novel approach designed to mitigate\nshort-text performance degradation through minimizing the distribution\ndiscrepancy between the extended and original models. Besides training on long\ntexts, LongReD distills the hidden state of selected layers from the original\nmodel on short texts. Additionally, LongReD also introduces a short-to-long\ndistillation, aligning the output distribution on short texts with that on long\ntexts by leveraging skipped positional indices. Experiments on common text\nbenchmarks demonstrate that LongReD effectively preserves the model's\nshort-text performance while maintaining comparable or even better capacity to\nhandle long texts than baselines. Our code is available at\nhttps://github.com/RUCAIBox/LongReD."
                },
                "authors": [
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07365v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07365v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08638v2",
                "updated": "2025-02-19T10:46:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    46,
                    14,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-12T18:54:37Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    18,
                    54,
                    37,
                    2,
                    43,
                    0
                ],
                "title": "Examining Multilingual Embedding Models Cross-Lingually Through\n  LLM-Generated Adversarial Examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining Multilingual Embedding Models Cross-Lingually Through\n  LLM-Generated Adversarial Examples"
                },
                "summary": "The evaluation of cross-lingual semantic search capabilities of models is\noften limited to existing datasets from tasks such as information retrieval and\nsemantic textual similarity. To allow for domain-specific evaluation, we\nintroduce Cross Lingual Semantic Discrimination (CLSD), a novel cross-lingual\nsemantic search task that requires only a set of parallel sentence pairs of the\nlanguage pair of interest within the target domain. This task focuses on the\nability of a model to cross-lingually rank the true parallel sentence higher\nthan hard negatives generated by a large language model. We create four\ninstances of our introduced CLSD task for the language pair German-French\nwithin the domain of news. Within this case study, we find that models that are\nalso fine-tuned for retrieval tasks (e.g., multilingual E5) benefit from using\nEnglish as the pivot language, while bitext mining models such as LaBSE perform\nbest directly cross-lingually. We also show a fine-grained similarity analysis\nenabled by our distractor generation strategy, indicating that different\nembedding models are sensitive to different types of perturbations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evaluation of cross-lingual semantic search capabilities of models is\noften limited to existing datasets from tasks such as information retrieval and\nsemantic textual similarity. To allow for domain-specific evaluation, we\nintroduce Cross Lingual Semantic Discrimination (CLSD), a novel cross-lingual\nsemantic search task that requires only a set of parallel sentence pairs of the\nlanguage pair of interest within the target domain. This task focuses on the\nability of a model to cross-lingually rank the true parallel sentence higher\nthan hard negatives generated by a large language model. We create four\ninstances of our introduced CLSD task for the language pair German-French\nwithin the domain of news. Within this case study, we find that models that are\nalso fine-tuned for retrieval tasks (e.g., multilingual E5) benefit from using\nEnglish as the pivot language, while bitext mining models such as LaBSE perform\nbest directly cross-lingually. We also show a fine-grained similarity analysis\nenabled by our distractor generation strategy, indicating that different\nembedding models are sensitive to different types of perturbations."
                },
                "authors": [
                    {
                        "name": "Andrianos Michail"
                    },
                    {
                        "name": "Simon Clematide"
                    },
                    {
                        "name": "Rico Sennrich"
                    }
                ],
                "author_detail": {
                    "name": "Rico Sennrich"
                },
                "author": "Rico Sennrich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13606v1",
                "updated": "2025-02-19T10:37:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    37,
                    4,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T10:37:04Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    37,
                    4,
                    2,
                    50,
                    0
                ],
                "title": "LaVCa: LLM-assisted Visual Cortex Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaVCa: LLM-assisted Visual Cortex Captioning"
                },
                "summary": "Understanding the property of neural populations (or voxels) in the human\nbrain can advance our comprehension of human perceptual and cognitive\nprocessing capabilities and contribute to developing brain-inspired computer\nmodels. Recent encoding models using deep neural networks (DNNs) have\nsuccessfully predicted voxel-wise activity. However, interpreting the\nproperties that explain voxel responses remains challenging because of the\nblack-box nature of DNNs. As a solution, we propose LLM-assisted Visual Cortex\nCaptioning (LaVCa), a data-driven approach that uses large language models\n(LLMs) to generate natural-language captions for images to which voxels are\nselective. By applying LaVCa for image-evoked brain activity, we demonstrate\nthat LaVCa generates captions that describe voxel selectivity more accurately\nthan the previously proposed method. Furthermore, the captions generated by\nLaVCa quantitatively capture more detailed properties than the existing method\nat both the inter-voxel and intra-voxel levels. Furthermore, a more detailed\nanalysis of the voxel-specific properties generated by LaVCa reveals\nfine-grained functional differentiation within regions of interest (ROIs) in\nthe visual cortex and voxels that simultaneously represent multiple distinct\nconcepts. These findings offer profound insights into human visual\nrepresentations by assigning detailed captions throughout the visual cortex\nwhile highlighting the potential of LLM-based methods in understanding brain\nrepresentations. Please check out our webpage at\nhttps://sites.google.com/view/lavca-llm/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the property of neural populations (or voxels) in the human\nbrain can advance our comprehension of human perceptual and cognitive\nprocessing capabilities and contribute to developing brain-inspired computer\nmodels. Recent encoding models using deep neural networks (DNNs) have\nsuccessfully predicted voxel-wise activity. However, interpreting the\nproperties that explain voxel responses remains challenging because of the\nblack-box nature of DNNs. As a solution, we propose LLM-assisted Visual Cortex\nCaptioning (LaVCa), a data-driven approach that uses large language models\n(LLMs) to generate natural-language captions for images to which voxels are\nselective. By applying LaVCa for image-evoked brain activity, we demonstrate\nthat LaVCa generates captions that describe voxel selectivity more accurately\nthan the previously proposed method. Furthermore, the captions generated by\nLaVCa quantitatively capture more detailed properties than the existing method\nat both the inter-voxel and intra-voxel levels. Furthermore, a more detailed\nanalysis of the voxel-specific properties generated by LaVCa reveals\nfine-grained functional differentiation within regions of interest (ROIs) in\nthe visual cortex and voxels that simultaneously represent multiple distinct\nconcepts. These findings offer profound insights into human visual\nrepresentations by assigning detailed captions throughout the visual cortex\nwhile highlighting the potential of LLM-based methods in understanding brain\nrepresentations. Please check out our webpage at\nhttps://sites.google.com/view/lavca-llm/"
                },
                "authors": [
                    {
                        "name": "Takuya Matsuyama"
                    },
                    {
                        "name": "Shinji Nishimoto"
                    },
                    {
                        "name": "Yu Takagi"
                    }
                ],
                "author_detail": {
                    "name": "Yu Takagi"
                },
                "author": "Yu Takagi",
                "arxiv_comment": "33 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13603v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13603v1",
                "updated": "2025-02-19T10:33:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    33,
                    18,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T10:33:18Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    33,
                    18,
                    2,
                    50,
                    0
                ],
                "title": "Efficient Safety Retrofitting Against Jailbreaking for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Safety Retrofitting Against Jailbreaking for LLMs"
                },
                "summary": "Direct Preference Optimization (DPO) is an efficient alignment technique that\nsteers LLMs towards preferable outputs by training on preference data,\nbypassing the need for explicit reward models. Its simplicity enables easy\nadaptation to various domains and safety requirements. This paper examines\nDPO's effectiveness in model safety against jailbreaking attacks while\nminimizing data requirements and training costs. We introduce Egida, a dataset\nexpanded from multiple sources, which includes 27 different safety topics and\n18 different attack styles, complemented with synthetic and human labels. This\ndata is used to boost the safety of state-of-the-art LLMs\n(Llama-3.1-8B/70B-Instruct, Qwen-2.5-7B/72B-Instruct) across topics and attack\nstyles. In addition to safety evaluations, we assess their post-alignment\nperformance degradation in general purpose tasks, and their tendency to over\nrefusal. Following the proposed methodology, trained models reduce their Attack\nSuccess Rate by 10%-30%, using small training efforts (2,000 samples) with low\ncomputational cost (3\\$ for 8B models, 20\\$ for 72B models). Safety aligned\nmodels generalize to unseen topics and attack styles, with the most successful\nattack style reaching a success rate around 5%. Size and family are found to\nstrongly influence model malleability towards safety, pointing at the\nimportance of pre-training choices. To validate our findings, a large\nindependent assessment of human preference agreement with Llama-Guard-3-8B is\nconducted by the authors and the associated dataset Egida-HSafe is released.\nOverall, this study illustrates how affordable and accessible it is to enhance\nLLM safety using DPO while outlining its current limitations. All datasets and\nmodels are released to enable reproducibility and further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) is an efficient alignment technique that\nsteers LLMs towards preferable outputs by training on preference data,\nbypassing the need for explicit reward models. Its simplicity enables easy\nadaptation to various domains and safety requirements. This paper examines\nDPO's effectiveness in model safety against jailbreaking attacks while\nminimizing data requirements and training costs. We introduce Egida, a dataset\nexpanded from multiple sources, which includes 27 different safety topics and\n18 different attack styles, complemented with synthetic and human labels. This\ndata is used to boost the safety of state-of-the-art LLMs\n(Llama-3.1-8B/70B-Instruct, Qwen-2.5-7B/72B-Instruct) across topics and attack\nstyles. In addition to safety evaluations, we assess their post-alignment\nperformance degradation in general purpose tasks, and their tendency to over\nrefusal. Following the proposed methodology, trained models reduce their Attack\nSuccess Rate by 10%-30%, using small training efforts (2,000 samples) with low\ncomputational cost (3\\$ for 8B models, 20\\$ for 72B models). Safety aligned\nmodels generalize to unseen topics and attack styles, with the most successful\nattack style reaching a success rate around 5%. Size and family are found to\nstrongly influence model malleability towards safety, pointing at the\nimportance of pre-training choices. To validate our findings, a large\nindependent assessment of human preference agreement with Llama-Guard-3-8B is\nconducted by the authors and the associated dataset Egida-HSafe is released.\nOverall, this study illustrates how affordable and accessible it is to enhance\nLLM safety using DPO while outlining its current limitations. All datasets and\nmodels are released to enable reproducibility and further research."
                },
                "authors": [
                    {
                        "name": "Dario Garcia-Gasulla"
                    },
                    {
                        "name": "Anna Arias-Duart"
                    },
                    {
                        "name": "Adrian Tormos"
                    },
                    {
                        "name": "Daniel Hinjos"
                    },
                    {
                        "name": "Oscar Molina-Sedano"
                    },
                    {
                        "name": "Ashwin Kumar Gururajan"
                    },
                    {
                        "name": "Maria Eugenia Cardello"
                    }
                ],
                "author_detail": {
                    "name": "Maria Eugenia Cardello"
                },
                "author": "Maria Eugenia Cardello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13603v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13603v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13595v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13595v1",
                "updated": "2025-02-19T10:13:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    13,
                    43,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T10:13:43Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    13,
                    43,
                    2,
                    50,
                    0
                ],
                "title": "MMTEB: Massive Multilingual Text Embedding Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMTEB: Massive Multilingual Text Embedding Benchmark"
                },
                "summary": "Text embeddings are typically evaluated on a limited set of tasks, which are\nconstrained by language, domain, and task diversity. To address these\nlimitations and provide a more comprehensive evaluation, we introduce the\nMassive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale,\ncommunity-driven expansion of MTEB, covering over 500 quality-controlled\nevaluation tasks across 250+ languages. MMTEB includes a diverse set of\nchallenging, novel tasks such as instruction following, long-document\nretrieval, and code retrieval, representing the largest multilingual collection\nof evaluation tasks for embedding models to date. Using this collection, we\ndevelop several highly multilingual benchmarks, which we use to evaluate a\nrepresentative set of models. We find that while large language models (LLMs)\nwith billions of parameters can achieve state-of-the-art performance on certain\nlanguage subsets and task categories, the best-performing publicly available\nmodel is multilingual-e5-large-instruct with only 560 million parameters. To\nfacilitate accessibility and reduce computational cost, we introduce a novel\ndownsampling method based on inter-task correlation, ensuring a diverse\nselection while preserving relative model rankings. Furthermore, we optimize\ntasks such as retrieval by sampling hard negatives, creating smaller but\neffective splits. These optimizations allow us to introduce benchmarks that\ndrastically reduce computational demands. For instance, our newly introduced\nzero-shot English benchmark maintains a ranking order similar to the full-scale\nversion but at a fraction of the computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text embeddings are typically evaluated on a limited set of tasks, which are\nconstrained by language, domain, and task diversity. To address these\nlimitations and provide a more comprehensive evaluation, we introduce the\nMassive Multilingual Text Embedding Benchmark (MMTEB) - a large-scale,\ncommunity-driven expansion of MTEB, covering over 500 quality-controlled\nevaluation tasks across 250+ languages. MMTEB includes a diverse set of\nchallenging, novel tasks such as instruction following, long-document\nretrieval, and code retrieval, representing the largest multilingual collection\nof evaluation tasks for embedding models to date. Using this collection, we\ndevelop several highly multilingual benchmarks, which we use to evaluate a\nrepresentative set of models. We find that while large language models (LLMs)\nwith billions of parameters can achieve state-of-the-art performance on certain\nlanguage subsets and task categories, the best-performing publicly available\nmodel is multilingual-e5-large-instruct with only 560 million parameters. To\nfacilitate accessibility and reduce computational cost, we introduce a novel\ndownsampling method based on inter-task correlation, ensuring a diverse\nselection while preserving relative model rankings. Furthermore, we optimize\ntasks such as retrieval by sampling hard negatives, creating smaller but\neffective splits. These optimizations allow us to introduce benchmarks that\ndrastically reduce computational demands. For instance, our newly introduced\nzero-shot English benchmark maintains a ranking order similar to the full-scale\nversion but at a fraction of the computational cost."
                },
                "authors": [
                    {
                        "name": "Kenneth Enevoldsen"
                    },
                    {
                        "name": "Isaac Chung"
                    },
                    {
                        "name": "Imene Kerboua"
                    },
                    {
                        "name": "Mrton Kardos"
                    },
                    {
                        "name": "Ashwin Mathur"
                    },
                    {
                        "name": "David Stap"
                    },
                    {
                        "name": "Jay Gala"
                    },
                    {
                        "name": "Wissam Siblini"
                    },
                    {
                        "name": "Dominik Krzemiski"
                    },
                    {
                        "name": "Genta Indra Winata"
                    },
                    {
                        "name": "Saba Sturua"
                    },
                    {
                        "name": "Saiteja Utpala"
                    },
                    {
                        "name": "Mathieu Ciancone"
                    },
                    {
                        "name": "Marion Schaeffer"
                    },
                    {
                        "name": "Gabriel Sequeira"
                    },
                    {
                        "name": "Diganta Misra"
                    },
                    {
                        "name": "Shreeya Dhakal"
                    },
                    {
                        "name": "Jonathan Rystrm"
                    },
                    {
                        "name": "Roman Solomatin"
                    },
                    {
                        "name": "mer aatan"
                    },
                    {
                        "name": "Akash Kundu"
                    },
                    {
                        "name": "Martin Bernstorff"
                    },
                    {
                        "name": "Shitao Xiao"
                    },
                    {
                        "name": "Akshita Sukhlecha"
                    },
                    {
                        "name": "Bhavish Pahwa"
                    },
                    {
                        "name": "Rafa Powiata"
                    },
                    {
                        "name": "Kranthi Kiran GV"
                    },
                    {
                        "name": "Shawon Ashraf"
                    },
                    {
                        "name": "Daniel Auras"
                    },
                    {
                        "name": "Bjrn Plster"
                    },
                    {
                        "name": "Jan Philipp Harries"
                    },
                    {
                        "name": "Loc Magne"
                    },
                    {
                        "name": "Isabelle Mohr"
                    },
                    {
                        "name": "Mariya Hendriksen"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Hippolyte Gisserot-Boukhlef"
                    },
                    {
                        "name": "Tom Aarsen"
                    },
                    {
                        "name": "Jan Kostkan"
                    },
                    {
                        "name": "Konrad Wojtasik"
                    },
                    {
                        "name": "Taemin Lee"
                    },
                    {
                        "name": "Marek uppa"
                    },
                    {
                        "name": "Crystina Zhang"
                    },
                    {
                        "name": "Roberta Rocca"
                    },
                    {
                        "name": "Mohammed Hamdy"
                    },
                    {
                        "name": "Andrianos Michail"
                    },
                    {
                        "name": "John Yang"
                    },
                    {
                        "name": "Manuel Faysse"
                    },
                    {
                        "name": "Aleksei Vatolin"
                    },
                    {
                        "name": "Nandan Thakur"
                    },
                    {
                        "name": "Manan Dey"
                    },
                    {
                        "name": "Dipam Vasani"
                    },
                    {
                        "name": "Pranjal Chitale"
                    },
                    {
                        "name": "Simone Tedeschi"
                    },
                    {
                        "name": "Nguyen Tai"
                    },
                    {
                        "name": "Artem Snegirev"
                    },
                    {
                        "name": "Michael Gnther"
                    },
                    {
                        "name": "Mengzhou Xia"
                    },
                    {
                        "name": "Weijia Shi"
                    },
                    {
                        "name": "Xing Han L"
                    },
                    {
                        "name": "Jordan Clive"
                    },
                    {
                        "name": "Gayatri Krishnakumar"
                    },
                    {
                        "name": "Anna Maksimova"
                    },
                    {
                        "name": "Silvan Wehrli"
                    },
                    {
                        "name": "Maria Tikhonova"
                    },
                    {
                        "name": "Henil Panchal"
                    },
                    {
                        "name": "Aleksandr Abramov"
                    },
                    {
                        "name": "Malte Ostendorff"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Simon Clematide"
                    },
                    {
                        "name": "Lester James Miranda"
                    },
                    {
                        "name": "Alena Fenogenova"
                    },
                    {
                        "name": "Guangyu Song"
                    },
                    {
                        "name": "Ruqiya Bin Safi"
                    },
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Alessia Borghini"
                    },
                    {
                        "name": "Federico Cassano"
                    },
                    {
                        "name": "Hongjin Su"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Howard Yen"
                    },
                    {
                        "name": "Lasse Hansen"
                    },
                    {
                        "name": "Sara Hooker"
                    },
                    {
                        "name": "Chenghao Xiao"
                    },
                    {
                        "name": "Vaibhav Adlakha"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Muennighoff"
                },
                "author": "Niklas Muennighoff",
                "arxiv_comment": "Accepted for ICLR: https://openreview.net/forum?id=zl3pfz4VCV",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13595v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13592v1",
                "updated": "2025-02-19T10:10:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    10,
                    43,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T10:10:43Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    10,
                    10,
                    43,
                    2,
                    50,
                    0
                ],
                "title": "Don't Stop the Multi-Party! On Generating Synthetic Multi-Party\n  Conversations with Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Stop the Multi-Party! On Generating Synthetic Multi-Party\n  Conversations with Constraints"
                },
                "summary": "Multi-Party Conversations (MPCs) are widely studied across disciplines, with\nsocial media as a primary data source due to their accessibility. However,\nthese datasets raise privacy concerns and often reflect platform-specific\nproperties. For example, interactions between speakers may be limited due to\nrigid platform structures (e.g., threads, tree-like discussions), which yield\noverly simplistic interaction patterns (e.g., as a consequence of ``reply-to''\nlinks). This work explores the feasibility of generating diverse MPCs with\ninstruction-tuned Large Language Models (LLMs) by providing deterministic\nconstraints such as dialogue structure and participants' stance. We investigate\ntwo complementary strategies of leveraging LLMs in this context: (i.) LLMs as\nMPC generators, where we task the LLM to generate a whole MPC at once and (ii.)\nLLMs as MPC parties, where the LLM generates one turn of the conversation at a\ntime, provided the conversation history. We next introduce an analytical\nframework to evaluate compliance with the constraints, content quality, and\ninteraction complexity for both strategies. Finally, we assess the quality of\nobtained MPCs via human annotation and LLM-as-a-judge evaluations. We find\nstark differences among LLMs, with only some being able to generate\nhigh-quality MPCs. We also find that turn-by-turn generation yields better\nconformance to constraints and higher linguistic variability than generating\nMPCs in one pass. Nonetheless, our structural and qualitative evaluation\nindicates that both generation strategies can yield high-quality MPCs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Party Conversations (MPCs) are widely studied across disciplines, with\nsocial media as a primary data source due to their accessibility. However,\nthese datasets raise privacy concerns and often reflect platform-specific\nproperties. For example, interactions between speakers may be limited due to\nrigid platform structures (e.g., threads, tree-like discussions), which yield\noverly simplistic interaction patterns (e.g., as a consequence of ``reply-to''\nlinks). This work explores the feasibility of generating diverse MPCs with\ninstruction-tuned Large Language Models (LLMs) by providing deterministic\nconstraints such as dialogue structure and participants' stance. We investigate\ntwo complementary strategies of leveraging LLMs in this context: (i.) LLMs as\nMPC generators, where we task the LLM to generate a whole MPC at once and (ii.)\nLLMs as MPC parties, where the LLM generates one turn of the conversation at a\ntime, provided the conversation history. We next introduce an analytical\nframework to evaluate compliance with the constraints, content quality, and\ninteraction complexity for both strategies. Finally, we assess the quality of\nobtained MPCs via human annotation and LLM-as-a-judge evaluations. We find\nstark differences among LLMs, with only some being able to generate\nhigh-quality MPCs. We also find that turn-by-turn generation yields better\nconformance to constraints and higher linguistic variability than generating\nMPCs in one pass. Nonetheless, our structural and qualitative evaluation\nindicates that both generation strategies can yield high-quality MPCs."
                },
                "authors": [
                    {
                        "name": "Nicol Penzo"
                    },
                    {
                        "name": "Marco Guerini"
                    },
                    {
                        "name": "Bruno Lepri"
                    },
                    {
                        "name": "Goran Glava"
                    },
                    {
                        "name": "Sara Tonelli"
                    }
                ],
                "author_detail": {
                    "name": "Sara Tonelli"
                },
                "author": "Sara Tonelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13577v1",
                "updated": "2025-02-19T09:33:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    33,
                    16,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T09:33:16Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    33,
                    16,
                    2,
                    50,
                    0
                ],
                "title": "Unraveling the Localized Latents: Learning Stratified Manifold\n  Structures in LLM Embedding Space with Sparse Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unraveling the Localized Latents: Learning Stratified Manifold\n  Structures in LLM Embedding Space with Sparse Mixture-of-Experts"
                },
                "summary": "However, real-world data often exhibit complex local structures that can be\nchallenging for single-model approaches with a smooth global manifold in the\nembedding space to unravel. In this work, we conjecture that in the latent\nspace of these large language models, the embeddings live in a local manifold\nstructure with different dimensions depending on the perplexities and domains\nof the input data, commonly referred to as a Stratified Manifold structure,\nwhich in combination form a structured space known as a Stratified Space. To\ninvestigate the validity of this structural claim, we propose an analysis\nframework based on a Mixture-of-Experts (MoE) model where each expert is\nimplemented with a simple dictionary learning algorithm at varying sparsity\nlevels. By incorporating an attention-based soft-gating network, we verify that\nour model learns specialized sub-manifolds for an ensemble of input data\nsources, reflecting the semantic stratification in LLM embedding space. We\nfurther analyze the intrinsic dimensions of these stratified sub-manifolds and\npresent extensive statistics on expert assignments, gating entropy, and\ninter-expert distances. Our experimental results demonstrate that our method\nnot only validates the claim of a stratified manifold structure in the LLM\nembedding space, but also provides interpretable clusters that align with the\nintrinsic semantic variations of the input data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "However, real-world data often exhibit complex local structures that can be\nchallenging for single-model approaches with a smooth global manifold in the\nembedding space to unravel. In this work, we conjecture that in the latent\nspace of these large language models, the embeddings live in a local manifold\nstructure with different dimensions depending on the perplexities and domains\nof the input data, commonly referred to as a Stratified Manifold structure,\nwhich in combination form a structured space known as a Stratified Space. To\ninvestigate the validity of this structural claim, we propose an analysis\nframework based on a Mixture-of-Experts (MoE) model where each expert is\nimplemented with a simple dictionary learning algorithm at varying sparsity\nlevels. By incorporating an attention-based soft-gating network, we verify that\nour model learns specialized sub-manifolds for an ensemble of input data\nsources, reflecting the semantic stratification in LLM embedding space. We\nfurther analyze the intrinsic dimensions of these stratified sub-manifolds and\npresent extensive statistics on expert assignments, gating entropy, and\ninter-expert distances. Our experimental results demonstrate that our method\nnot only validates the claim of a stratified manifold structure in the LLM\nembedding space, but also provides interpretable clusters that align with the\nintrinsic semantic variations of the input data."
                },
                "authors": [
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Anand Sarwate"
                    }
                ],
                "author_detail": {
                    "name": "Anand Sarwate"
                },
                "author": "Anand Sarwate",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07503v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07503v2",
                "updated": "2025-02-19T09:24:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    24,
                    45,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-11T12:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    12,
                    11,
                    40,
                    1,
                    42,
                    0
                ],
                "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in\n  Language and Multimodal Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recursive Inference Scaling: A Winning Path to Scalable Inference in\n  Language and Multimodal Systems"
                },
                "summary": "Recent research in language modeling reveals two scaling effects: the\nwell-known improvement from increased training compute, and a lesser-known\nboost from applying more sophisticated or computationally intensive inference\nmethods. Inspired by recent findings on the fractal geometry of language, we\nintroduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe\nfor scaling inference time. For a given fixed model architecture and training\ncompute budget, RINS substantially improves language modeling performance. It\nalso generalizes beyond pure language tasks, delivering gains in multimodal\nsystems, including a +2% improvement in 0-shot ImageNet accuracy for\nSigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS\nimproves both the asymptotic performance limits and the scaling exponents.\nThese advantages are maintained even when compared to state-of-the-art\nrecursive techniques like the \"repeat-all-over\" (RAO) strategy in Mobile LLM.\nFinally, stochastic RINS not only can enhance performance further but also\nprovides the flexibility to optionally forgo increased inference computation at\ntest time with minimal performance degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research in language modeling reveals two scaling effects: the\nwell-known improvement from increased training compute, and a lesser-known\nboost from applying more sophisticated or computationally intensive inference\nmethods. Inspired by recent findings on the fractal geometry of language, we\nintroduce Recursive INference Scaling (RINS) as a complementary, plug-in recipe\nfor scaling inference time. For a given fixed model architecture and training\ncompute budget, RINS substantially improves language modeling performance. It\nalso generalizes beyond pure language tasks, delivering gains in multimodal\nsystems, including a +2% improvement in 0-shot ImageNet accuracy for\nSigLIP-B/16. Additionally, by deriving data scaling laws, we show that RINS\nimproves both the asymptotic performance limits and the scaling exponents.\nThese advantages are maintained even when compared to state-of-the-art\nrecursive techniques like the \"repeat-all-over\" (RAO) strategy in Mobile LLM.\nFinally, stochastic RINS not only can enhance performance further but also\nprovides the flexibility to optionally forgo increased inference computation at\ntest time with minimal performance degradation."
                },
                "authors": [
                    {
                        "name": "Ibrahim Alabdulmohsin"
                    },
                    {
                        "name": "Xiaohua Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohua Zhai"
                },
                "author": "Xiaohua Zhai",
                "arxiv_comment": "18 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.07503v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07503v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13566v1",
                "updated": "2025-02-19T09:17:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    17,
                    41,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T09:17:41Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    17,
                    41,
                    2,
                    50,
                    0
                ],
                "title": "Extracting Social Connections from Finnish Karelian Refugee Interviews\n  Using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting Social Connections from Finnish Karelian Refugee Interviews\n  Using LLMs"
                },
                "summary": "We performed a zero-shot information extraction study on a historical\ncollection of 89,339 brief Finnish-language interviews of refugee families\nrelocated post-WWII from Finnish Eastern Karelia. Our research objective is\ntwo-fold. First, we aim to extract social organizations and hobbies from the\nfree text of the interviews, separately for each family member. These can act\nas a proxy variable indicating the degree of social integration of refugees in\ntheir new environment. Second, we aim to evaluate several alternative ways to\napproach this task, comparing a number of generative models and a supervised\nlearning approach, to gain a broader insight into the relative merits of these\ndifferent approaches and their applicability in similar studies.\n  We find that the best generative model (GPT-4) is roughly on par with human\nperformance, at an F-score of 88.8%. Interestingly, the best open generative\nmodel (Llama-3-70B-Instruct) reaches almost the same performance, at 87.7%\nF-score, demonstrating that open models are becoming a viable alternative for\nsome practical tasks even on non-English data. Additionally, we test a\nsupervised learning alternative, where we fine-tune a Finnish BERT model\n(FinBERT) using GPT-4 generated training data. By this method, we achieved an\nF-score of 84.1% already with 6K interviews up to an F-score of 86.3% with 30k\ninterviews. Such an approach would be particularly appealing in cases where the\ncomputational resources are limited, or there is a substantial mass of data to\nprocess.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We performed a zero-shot information extraction study on a historical\ncollection of 89,339 brief Finnish-language interviews of refugee families\nrelocated post-WWII from Finnish Eastern Karelia. Our research objective is\ntwo-fold. First, we aim to extract social organizations and hobbies from the\nfree text of the interviews, separately for each family member. These can act\nas a proxy variable indicating the degree of social integration of refugees in\ntheir new environment. Second, we aim to evaluate several alternative ways to\napproach this task, comparing a number of generative models and a supervised\nlearning approach, to gain a broader insight into the relative merits of these\ndifferent approaches and their applicability in similar studies.\n  We find that the best generative model (GPT-4) is roughly on par with human\nperformance, at an F-score of 88.8%. Interestingly, the best open generative\nmodel (Llama-3-70B-Instruct) reaches almost the same performance, at 87.7%\nF-score, demonstrating that open models are becoming a viable alternative for\nsome practical tasks even on non-English data. Additionally, we test a\nsupervised learning alternative, where we fine-tune a Finnish BERT model\n(FinBERT) using GPT-4 generated training data. By this method, we achieved an\nF-score of 84.1% already with 6K interviews up to an F-score of 86.3% with 30k\ninterviews. Such an approach would be particularly appealing in cases where the\ncomputational resources are limited, or there is a substantial mass of data to\nprocess."
                },
                "authors": [
                    {
                        "name": "Joonatan Laato"
                    },
                    {
                        "name": "Jenna Kanerva"
                    },
                    {
                        "name": "John Loehr"
                    },
                    {
                        "name": "Virpi Lummaa"
                    },
                    {
                        "name": "Filip Ginter"
                    }
                ],
                "author_detail": {
                    "name": "Filip Ginter"
                },
                "author": "Filip Ginter",
                "arxiv_comment": "Published at Proceedings of Fifth Conference on Computational\n  Humanities Research (CHR'2024), December 2024\n  https://ceur-ws.org/Vol-3834/paper52.pdf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13564v1",
                "updated": "2025-02-19T09:17:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    17,
                    7,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T09:17:07Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    17,
                    7,
                    2,
                    50,
                    0
                ],
                "title": "PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language\n  Models"
                },
                "summary": "The rapid development of large language models (LLMs) is redefining the\nlandscape of human-computer interaction, and their integration into various\nuser-service applications is becoming increasingly prevalent. However,\ntransmitting user data to cloud-based LLMs presents significant risks of data\nbreaches and unauthorized access to personal identification information. In\nthis paper, we propose a privacy preservation pipeline for protecting privacy\nand sensitive information during interactions between users and LLMs in\npractical LLM usage scenarios. We construct SensitiveQA, the first privacy\nopen-ended question-answering dataset. It comprises 57k interactions in Chinese\nand English, encompassing a diverse range of user-sensitive information within\nthe conversations. Our proposed solution employs a multi-stage strategy aimed\nat preemptively securing user information while simultaneously preserving the\nresponse quality of cloud-based LLMs. Experimental validation underscores our\nmethod's efficacy in balancing privacy protection with maintaining robust\ninteraction quality. The code and dataset are available at\nhttps://github.com/ligw1998/PRIV-QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language models (LLMs) is redefining the\nlandscape of human-computer interaction, and their integration into various\nuser-service applications is becoming increasingly prevalent. However,\ntransmitting user data to cloud-based LLMs presents significant risks of data\nbreaches and unauthorized access to personal identification information. In\nthis paper, we propose a privacy preservation pipeline for protecting privacy\nand sensitive information during interactions between users and LLMs in\npractical LLM usage scenarios. We construct SensitiveQA, the first privacy\nopen-ended question-answering dataset. It comprises 57k interactions in Chinese\nand English, encompassing a diverse range of user-sensitive information within\nthe conversations. Our proposed solution employs a multi-stage strategy aimed\nat preemptively securing user information while simultaneously preserving the\nresponse quality of cloud-based LLMs. Experimental validation underscores our\nmethod's efficacy in balancing privacy protection with maintaining robust\ninteraction quality. The code and dataset are available at\nhttps://github.com/ligw1998/PRIV-QA."
                },
                "authors": [
                    {
                        "name": "Guangwei Li"
                    },
                    {
                        "name": "Yuansen Zhang"
                    },
                    {
                        "name": "Yinggui Wang"
                    },
                    {
                        "name": "Shoumeng Yan"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Tao Wei"
                    }
                ],
                "author_detail": {
                    "name": "Tao Wei"
                },
                "author": "Tao Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13562v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13562v1",
                "updated": "2025-02-19T09:14:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    14,
                    19,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T09:14:19Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    14,
                    19,
                    2,
                    50,
                    0
                ],
                "title": "Are Large Language Models In-Context Graph Learners?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models In-Context Graph Learners?"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable in-context\nreasoning capabilities across a wide range of tasks, particularly with\nunstructured inputs such as language or images. However, LLMs struggle to\nhandle structured data, such as graphs, due to their lack of understanding of\nnon-Euclidean structures. As a result, without additional fine-tuning, their\nperformance significantly lags behind that of graph neural networks (GNNs) in\ngraph learning tasks. In this paper, we show that learning on graph data can be\nconceptualized as a retrieval-augmented generation (RAG) process, where\nspecific instances (e.g., nodes or edges) act as queries, and the graph itself\nserves as the retrieved context. Building on this insight, we propose a series\nof RAG frameworks to enhance the in-context learning capabilities of LLMs for\ngraph learning tasks. Comprehensive evaluations demonstrate that our proposed\nRAG frameworks significantly improve LLM performance on graph-based tasks,\nparticularly in scenarios where a pretrained LLM must be used without\nmodification or accessed via an API.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable in-context\nreasoning capabilities across a wide range of tasks, particularly with\nunstructured inputs such as language or images. However, LLMs struggle to\nhandle structured data, such as graphs, due to their lack of understanding of\nnon-Euclidean structures. As a result, without additional fine-tuning, their\nperformance significantly lags behind that of graph neural networks (GNNs) in\ngraph learning tasks. In this paper, we show that learning on graph data can be\nconceptualized as a retrieval-augmented generation (RAG) process, where\nspecific instances (e.g., nodes or edges) act as queries, and the graph itself\nserves as the retrieved context. Building on this insight, we propose a series\nof RAG frameworks to enhance the in-context learning capabilities of LLMs for\ngraph learning tasks. Comprehensive evaluations demonstrate that our proposed\nRAG frameworks significantly improve LLM performance on graph-based tasks,\nparticularly in scenarios where a pretrained LLM must be used without\nmodification or accessed via an API."
                },
                "authors": [
                    {
                        "name": "Jintang Li"
                    },
                    {
                        "name": "Ruofan Wu"
                    },
                    {
                        "name": "Yuchang Zhu"
                    },
                    {
                        "name": "Huizhe Zhang"
                    },
                    {
                        "name": "Liang Chen"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13562v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13562v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07380v2",
                "updated": "2025-02-19T09:01:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    1,
                    59,
                    2,
                    50,
                    0
                ],
                "published": "2024-12-10T10:27:41Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    10,
                    27,
                    41,
                    1,
                    345,
                    0
                ],
                "title": "SpecFuse: Ensembling Large Language Models via Next-Segment Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecFuse: Ensembling Large Language Models via Next-Segment Prediction"
                },
                "summary": "Ensembles of generative large language models (LLMs) can integrate the\nstrengths of different LLMs to compensate for the limitations of individual\nmodels. However, recent work has focused on training an additional fusion model\nto combine complete responses from multiple LLMs, failing to tap into their\ncollaborative potential to generate higher-quality responses. Moreover, as the\nadditional fusion model is trained on a specialized dataset, these methods\nstruggle with generalizing to open-domain queries from online users. In this\npaper, we propose SpecFuse, a novel ensemble framework that outputs the fused\nresult by iteratively producing the next segment through collaboration among\nLLMs. This is achieved through cyclic execution of its inference and\nverification components. In each round, the inference component invokes each\nbase LLM to generate candidate segments in parallel, and the verify component\ncalls these LLMs again to predict the ranking of the segments. The top-ranked\nsegment is then broadcast to all LLMs, encouraging them to generate\nhigher-quality segments in the next round. This approach also allows the base\nLLMs to be plug-and-play, without any training or adaptation, avoiding\ngeneralization limitations. Furthermore, to conserve computational resources,\nwe propose a model exit mechanism that dynamically excludes models exhibiting\npoor performance in previous rounds during each query response. In this way, it\neffectively reduces the number of model calls while maintaining overall\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensembles of generative large language models (LLMs) can integrate the\nstrengths of different LLMs to compensate for the limitations of individual\nmodels. However, recent work has focused on training an additional fusion model\nto combine complete responses from multiple LLMs, failing to tap into their\ncollaborative potential to generate higher-quality responses. Moreover, as the\nadditional fusion model is trained on a specialized dataset, these methods\nstruggle with generalizing to open-domain queries from online users. In this\npaper, we propose SpecFuse, a novel ensemble framework that outputs the fused\nresult by iteratively producing the next segment through collaboration among\nLLMs. This is achieved through cyclic execution of its inference and\nverification components. In each round, the inference component invokes each\nbase LLM to generate candidate segments in parallel, and the verify component\ncalls these LLMs again to predict the ranking of the segments. The top-ranked\nsegment is then broadcast to all LLMs, encouraging them to generate\nhigher-quality segments in the next round. This approach also allows the base\nLLMs to be plug-and-play, without any training or adaptation, avoiding\ngeneralization limitations. Furthermore, to conserve computational resources,\nwe propose a model exit mechanism that dynamically excludes models exhibiting\npoor performance in previous rounds during each query response. In this way, it\neffectively reduces the number of model calls while maintaining overall\nperformance."
                },
                "authors": [
                    {
                        "name": "Bo Lv"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Yanan Zhang"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "15 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13555v1",
                "updated": "2025-02-19T09:00:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    0,
                    32,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T09:00:32Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    9,
                    0,
                    32,
                    2,
                    50,
                    0
                ],
                "title": "Democratizing Large Language Model-Based Graph Data Augmentation via\n  Latent Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Democratizing Large Language Model-Based Graph Data Augmentation via\n  Latent Knowledge Graphs"
                },
                "summary": "Data augmentation is necessary for graph representation learning due to the\nscarcity and noise present in graph data. Most of the existing augmentation\nmethods overlook the context information inherited from the dataset as they\nrely solely on the graph structure for augmentation. Despite the success of\nsome large language model-based (LLM) graph learning methods, they are mostly\nwhite-box which require access to the weights or latent features from the\nopen-access LLMs, making them difficult to be democratized for everyone as\nexisting LLMs are mostly closed-source for commercial considerations. To\novercome these limitations, we propose a black-box context-driven graph data\naugmentation approach, with the guidance of LLMs -- DemoGraph. Leveraging the\ntext prompt as context-related information, we task the LLM with generating\nknowledge graphs (KGs), which allow us to capture the structural interactions\nfrom the text outputs. We then design a dynamic merging schema to\nstochastically integrate the LLM-generated KGs into the original graph during\ntraining. To control the sparsity of the augmented graph, we further devise a\ngranularity-aware prompting strategy and an instruction fine-tuning module,\nwhich seamlessly generates text prompts according to different granularity\nlevels of the dataset. Extensive experiments on various graph learning tasks\nvalidate the effectiveness of our method over existing graph data augmentation\nmethods. Notably, our approach excels in scenarios involving electronic health\nrecords (EHRs), which validates its maximal utilization of contextual\nknowledge, leading to enhanced predictive performance and interpretability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data augmentation is necessary for graph representation learning due to the\nscarcity and noise present in graph data. Most of the existing augmentation\nmethods overlook the context information inherited from the dataset as they\nrely solely on the graph structure for augmentation. Despite the success of\nsome large language model-based (LLM) graph learning methods, they are mostly\nwhite-box which require access to the weights or latent features from the\nopen-access LLMs, making them difficult to be democratized for everyone as\nexisting LLMs are mostly closed-source for commercial considerations. To\novercome these limitations, we propose a black-box context-driven graph data\naugmentation approach, with the guidance of LLMs -- DemoGraph. Leveraging the\ntext prompt as context-related information, we task the LLM with generating\nknowledge graphs (KGs), which allow us to capture the structural interactions\nfrom the text outputs. We then design a dynamic merging schema to\nstochastically integrate the LLM-generated KGs into the original graph during\ntraining. To control the sparsity of the augmented graph, we further devise a\ngranularity-aware prompting strategy and an instruction fine-tuning module,\nwhich seamlessly generates text prompts according to different granularity\nlevels of the dataset. Extensive experiments on various graph learning tasks\nvalidate the effectiveness of our method over existing graph data augmentation\nmethods. Notably, our approach excels in scenarios involving electronic health\nrecords (EHRs), which validates its maximal utilization of contextual\nknowledge, leading to enhanced predictive performance and interpretability."
                },
                "authors": [
                    {
                        "name": "Yushi Feng"
                    },
                    {
                        "name": "Tsai Hor Chan"
                    },
                    {
                        "name": "Guosheng Yin"
                    },
                    {
                        "name": "Lequan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Lequan Yu"
                },
                "author": "Lequan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13550v1",
                "updated": "2025-02-19T08:58:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    58,
                    44,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T08:58:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    58,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "STaR-SQL: Self-Taught Reasoner for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STaR-SQL: Self-Taught Reasoner for Text-to-SQL"
                },
                "summary": "Generating step-by-step \"chain-of-thought\" rationales has proven effective\nfor improving the performance of large language models on complex reasoning\ntasks. However, applying such techniques to structured tasks, such as\ntext-to-SQL, remains largely unexplored. In this paper, we introduce\nSelf-Taught Reasoner for text-to-SQL (STaR-SQL), a novel approach that reframes\nSQL query generation as a reasoning-driven process. Our method prompts the LLM\nto produce detailed reasoning steps for SQL queries and fine-tunes it on\nrationales that lead to correct outcomes. Unlike traditional methods, STaR-SQL\ndedicates additional test-time computation to reasoning, thereby positioning\nLLMs as spontaneous reasoners rather than mere prompt-based agents. To further\nscale the inference process, we incorporate an outcome-supervised reward model\n(ORM) as a verifier, which enhances SQL query accuracy. Experimental results on\nthe challenging Spider benchmark demonstrate that STaR-SQL significantly\nimproves text-to-SQL performance, achieving an execution accuracy of 86.6%.\nThis surpasses a few-shot baseline by 31.6% and a baseline fine-tuned to\npredict answers directly by 18.0%. Additionally, STaR-SQL outperforms\nagent-like prompting methods that leverage more powerful yet closed-source\nmodels such as GPT-4. These findings underscore the potential of\nreasoning-augmented training for structured tasks and open the door to\nextending self-improving reasoning models to text-to-SQL generation and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating step-by-step \"chain-of-thought\" rationales has proven effective\nfor improving the performance of large language models on complex reasoning\ntasks. However, applying such techniques to structured tasks, such as\ntext-to-SQL, remains largely unexplored. In this paper, we introduce\nSelf-Taught Reasoner for text-to-SQL (STaR-SQL), a novel approach that reframes\nSQL query generation as a reasoning-driven process. Our method prompts the LLM\nto produce detailed reasoning steps for SQL queries and fine-tunes it on\nrationales that lead to correct outcomes. Unlike traditional methods, STaR-SQL\ndedicates additional test-time computation to reasoning, thereby positioning\nLLMs as spontaneous reasoners rather than mere prompt-based agents. To further\nscale the inference process, we incorporate an outcome-supervised reward model\n(ORM) as a verifier, which enhances SQL query accuracy. Experimental results on\nthe challenging Spider benchmark demonstrate that STaR-SQL significantly\nimproves text-to-SQL performance, achieving an execution accuracy of 86.6%.\nThis surpasses a few-shot baseline by 31.6% and a baseline fine-tuned to\npredict answers directly by 18.0%. Additionally, STaR-SQL outperforms\nagent-like prompting methods that leverage more powerful yet closed-source\nmodels such as GPT-4. These findings underscore the potential of\nreasoning-augmented training for structured tasks and open the door to\nextending self-improving reasoning models to text-to-SQL generation and beyond."
                },
                "authors": [
                    {
                        "name": "Mingqian He"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Qiuying Peng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Weiming Lu"
                    }
                ],
                "author_detail": {
                    "name": "Weiming Lu"
                },
                "author": "Weiming Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12568v2",
                "updated": "2025-02-19T08:58:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    58,
                    13,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-18T06:12:14Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    6,
                    12,
                    14,
                    1,
                    49,
                    0
                ],
                "title": "A Cognitive Writing Perspective for Constrained Long-Form Text\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cognitive Writing Perspective for Constrained Long-Form Text\n  Generation"
                },
                "summary": "Like humans, Large Language Models (LLMs) struggle to generate high-quality\nlong-form text that adheres to strict requirements in a single pass. This\nchallenge is unsurprising, as successful human writing, according to the\nCognitive Writing Theory, is a complex cognitive process involving iterative\nplanning, translating, reviewing, and monitoring. Motivated by these cognitive\nprinciples, we aim to equip LLMs with human-like cognitive writing capabilities\nthrough CogWriter, a novel training-free framework that transforms LLM\nconstrained long-form text generation into a systematic cognitive writing\nparadigm. Our framework consists of two key modules: (1) a Planning Agent that\nperforms hierarchical planning to decompose the task, and (2) multiple\nGeneration Agents that execute these plans in parallel. The system maintains\nquality via continuous monitoring and reviewing mechanisms, which evaluate\noutputs against specified requirements and trigger necessary revisions.\nCogWriter demonstrates exceptional performance on LongGenBench, a benchmark for\ncomplex constrained long-form text generation. Even when using Qwen-2.5-14B as\nits backbone, CogWriter surpasses GPT-4o by 22% in complex instruction\ncompletion accuracy while reliably generating texts exceeding 10,000 words. We\nhope this cognitive science-inspired approach provides a paradigm for LLM\nwriting advancements:\n\\href{https://github.com/KaiyangWan/CogWriter}{CogWriter}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Like humans, Large Language Models (LLMs) struggle to generate high-quality\nlong-form text that adheres to strict requirements in a single pass. This\nchallenge is unsurprising, as successful human writing, according to the\nCognitive Writing Theory, is a complex cognitive process involving iterative\nplanning, translating, reviewing, and monitoring. Motivated by these cognitive\nprinciples, we aim to equip LLMs with human-like cognitive writing capabilities\nthrough CogWriter, a novel training-free framework that transforms LLM\nconstrained long-form text generation into a systematic cognitive writing\nparadigm. Our framework consists of two key modules: (1) a Planning Agent that\nperforms hierarchical planning to decompose the task, and (2) multiple\nGeneration Agents that execute these plans in parallel. The system maintains\nquality via continuous monitoring and reviewing mechanisms, which evaluate\noutputs against specified requirements and trigger necessary revisions.\nCogWriter demonstrates exceptional performance on LongGenBench, a benchmark for\ncomplex constrained long-form text generation. Even when using Qwen-2.5-14B as\nits backbone, CogWriter surpasses GPT-4o by 22% in complex instruction\ncompletion accuracy while reliably generating texts exceeding 10,000 words. We\nhope this cognitive science-inspired approach provides a paradigm for LLM\nwriting advancements:\n\\href{https://github.com/KaiyangWan/CogWriter}{CogWriter}."
                },
                "authors": [
                    {
                        "name": "Kaiyang Wan"
                    },
                    {
                        "name": "Honglin Mu"
                    },
                    {
                        "name": "Rui Hao"
                    },
                    {
                        "name": "Haoran Luo"
                    },
                    {
                        "name": "Tianle Gu"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13544v1",
                "updated": "2025-02-19T08:52:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    52,
                    45,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T08:52:45Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    52,
                    45,
                    2,
                    50,
                    0
                ],
                "title": "From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap\n  for Text Length Control via MARKERGEN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Sub-Ability Diagnosis to Human-Aligned Generation: Bridging the Gap\n  for Text Length Control via MARKERGEN"
                },
                "summary": "Despite the rapid progress of large language models (LLMs), their\nlength-controllable text generation (LCTG) ability remains below expectations,\nposing a major limitation for practical applications. Existing methods mainly\nfocus on end-to-end training to reinforce adherence to length constraints.\nHowever, the lack of decomposition and targeted enhancement of LCTG\nsub-abilities restricts further progress.To bridge this gap, we conduct a\nbottom-up decomposition of LCTG sub-abilities with human patterns as reference\nand perform a detailed error analysis.On this basis, we propose MarkerGen, a\nsimple-yet-effective plug-and-play approach that:(1) mitigates LLM fundamental\ndeficiencies via external tool integration;(2) conducts explicit length\nmodeling with dynamically inserted markers;(3) employs a three-stage generation\nscheme to better align length constraints while maintaining content\nquality.Comprehensive experiments demonstrate that MarkerGen significantly\nimproves LCTG across various settings, exhibiting outstanding effectiveness and\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the rapid progress of large language models (LLMs), their\nlength-controllable text generation (LCTG) ability remains below expectations,\nposing a major limitation for practical applications. Existing methods mainly\nfocus on end-to-end training to reinforce adherence to length constraints.\nHowever, the lack of decomposition and targeted enhancement of LCTG\nsub-abilities restricts further progress.To bridge this gap, we conduct a\nbottom-up decomposition of LCTG sub-abilities with human patterns as reference\nand perform a detailed error analysis.On this basis, we propose MarkerGen, a\nsimple-yet-effective plug-and-play approach that:(1) mitigates LLM fundamental\ndeficiencies via external tool integration;(2) conducts explicit length\nmodeling with dynamically inserted markers;(3) employs a three-stage generation\nscheme to better align length constraints while maintaining content\nquality.Comprehensive experiments demonstrate that MarkerGen significantly\nimproves LCTG across various settings, exhibiting outstanding effectiveness and\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Peiwen Yuan"
                    },
                    {
                        "name": "Chuyi Tan"
                    },
                    {
                        "name": "Shaoxiong Feng"
                    },
                    {
                        "name": "Yiwei Li"
                    },
                    {
                        "name": "Xinglin Wang"
                    },
                    {
                        "name": "Yueqi Zhang"
                    },
                    {
                        "name": "Jiayi Shi"
                    },
                    {
                        "name": "Boyuan Pan"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kan Li"
                    }
                ],
                "author_detail": {
                    "name": "Kan Li"
                },
                "author": "Kan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13542v1",
                "updated": "2025-02-19T08:50:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T08:50:44Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    50,
                    44,
                    2,
                    50,
                    0
                ],
                "title": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation-aware Probe-Query: Effective Key-Value Retrieval for\n  Long-Context LLMs Inference"
                },
                "summary": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have showcased exceptional\nperformance in long-context tasks, while facing significant inference\nefficiency challenges with limited GPU memory. Existing solutions first\nproposed the sliding-window approach to accumulate a set of historical\n\\textbf{key-value} (KV) pairs for reuse, then further improvements selectively\nretain its subsets at each step. However, due to the sparse attention\ndistribution across a long context, it is hard to identify and recall relevant\nKV pairs, as the attention is distracted by massive candidate pairs.\nAdditionally, we found it promising to select representative tokens as\nprobe-Query in each sliding window to effectively represent the entire context,\nwhich is an approach overlooked by existing methods. Thus, we propose\n\\textbf{ActQKV}, a training-free, \\textbf{Act}ivation-aware approach that\ndynamically determines probe-\\textbf{Q}uery and leverages it to retrieve the\nrelevant \\textbf{KV} pairs for inference. Specifically, ActQKV monitors a\ntoken-level indicator, Activation Bias, within each context window, enabling\nthe proper construction of probe-Query for retrieval at pre-filling stage. To\naccurately recall the relevant KV pairs and minimize the irrelevant ones, we\ndesign a dynamic KV cut-off mechanism guided by information density across\nlayers at the decoding stage. Experiments on the Long-Bench and $\\infty$\nBenchmarks demonstrate its state-of-the-art performance with competitive\ninference quality and resource efficiency."
                },
                "authors": [
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Jiachuan Wang"
                    },
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiaqi Tang"
                    },
                    {
                        "name": "Shuangyin Li"
                    },
                    {
                        "name": "Yongqi Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13539v1",
                "updated": "2025-02-19T08:47:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    47,
                    42,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T08:47:42Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    47,
                    42,
                    2,
                    50,
                    0
                ],
                "title": "Bursting Filter Bubble: Enhancing Serendipity Recommendations with\n  Aligned Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bursting Filter Bubble: Enhancing Serendipity Recommendations with\n  Aligned Large Language Models"
                },
                "summary": "Recommender systems (RSs) often suffer from the feedback loop phenomenon,\ne.g., RSs are trained on data biased by their recommendations. This leads to\nthe filter bubble effect that reinforces homogeneous content and reduces user\nsatisfaction. To this end, serendipity recommendations, which offer unexpected\nyet relevant items, are proposed. Recently, large language models (LLMs) have\nshown potential in serendipity prediction due to their extensive world\nknowledge and reasoning capabilities. However, they still face challenges in\naligning serendipity judgments with human assessments, handling long user\nbehavior sequences, and meeting the latency requirements of industrial RSs. To\naddress these issues, we propose SERAL (Serendipity Recommendations with\nAligned Large Language Models), a framework comprising three stages: (1)\nCognition Profile Generation to compress user behavior into multi-level\nprofiles; (2) SerenGPT Alignment to align serendipity judgments with human\npreferences using enriched training data; and (3) Nearline Adaptation to\nintegrate SerenGPT into industrial RSs pipelines efficiently. Online\nexperiments demonstrate that SERAL improves exposure ratio (PVR), clicks, and\ntransactions of serendipitous items by 5.7%, 29.56%, and 27.6%, enhancing user\nexperience without much impact on overall revenue. Now, it has been fully\ndeployed in the \"Guess What You Like\" of the Taobao App homepage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems (RSs) often suffer from the feedback loop phenomenon,\ne.g., RSs are trained on data biased by their recommendations. This leads to\nthe filter bubble effect that reinforces homogeneous content and reduces user\nsatisfaction. To this end, serendipity recommendations, which offer unexpected\nyet relevant items, are proposed. Recently, large language models (LLMs) have\nshown potential in serendipity prediction due to their extensive world\nknowledge and reasoning capabilities. However, they still face challenges in\naligning serendipity judgments with human assessments, handling long user\nbehavior sequences, and meeting the latency requirements of industrial RSs. To\naddress these issues, we propose SERAL (Serendipity Recommendations with\nAligned Large Language Models), a framework comprising three stages: (1)\nCognition Profile Generation to compress user behavior into multi-level\nprofiles; (2) SerenGPT Alignment to align serendipity judgments with human\npreferences using enriched training data; and (3) Nearline Adaptation to\nintegrate SerenGPT into industrial RSs pipelines efficiently. Online\nexperiments demonstrate that SERAL improves exposure ratio (PVR), clicks, and\ntransactions of serendipitous items by 5.7%, 29.56%, and 27.6%, enhancing user\nexperience without much impact on overall revenue. Now, it has been fully\ndeployed in the \"Guess What You Like\" of the Taobao App homepage."
                },
                "authors": [
                    {
                        "name": "Yunjia Xi"
                    },
                    {
                        "name": "Muyan Weng"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Chao Yi"
                    },
                    {
                        "name": "Dian Chen"
                    },
                    {
                        "name": "Gaoyang Guo"
                    },
                    {
                        "name": "Mao Zhang"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Yuning Jiang"
                    },
                    {
                        "name": "Qingwen Liu"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08904v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08904v2",
                "updated": "2025-02-19T08:42:33Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    42,
                    33,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-13T02:40:33Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    2,
                    40,
                    33,
                    3,
                    44,
                    0
                ],
                "title": "MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via\n  Event-Driven Text-Code Cyclic Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIH-TCCT: Mitigating Inconsistent Hallucinations in LLMs via\n  Event-Driven Text-Code Cyclic Training"
                },
                "summary": "Recent methodologies utilizing synthetic datasets have aimed to address\ninconsistent hallucinations in large language models (LLMs); however,these\napproaches are primarily tailored to specific tasks, limiting their\ngeneralizability. Inspired by the strong performance of code-trained models in\nlogic-intensive domains, we propose a novel framework that leverages\nevent-based text to generate corresponding code and employs cyclic training to\ntransfer the logical consistency of code to natural language effectively. Our\nmethod significantly reduces inconsistent hallucinations across three leading\nLLMs and two categories of natural language tasks while maintaining overall\nperformance. This framework effectively alleviates hallucinations without\nnecessitating adaptation to downstream tasks, demonstrating generality and\nproviding new perspectives to tackle the challenge of inconsistent\nhallucinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent methodologies utilizing synthetic datasets have aimed to address\ninconsistent hallucinations in large language models (LLMs); however,these\napproaches are primarily tailored to specific tasks, limiting their\ngeneralizability. Inspired by the strong performance of code-trained models in\nlogic-intensive domains, we propose a novel framework that leverages\nevent-based text to generate corresponding code and employs cyclic training to\ntransfer the logical consistency of code to natural language effectively. Our\nmethod significantly reduces inconsistent hallucinations across three leading\nLLMs and two categories of natural language tasks while maintaining overall\nperformance. This framework effectively alleviates hallucinations without\nnecessitating adaptation to downstream tasks, demonstrating generality and\nproviding new perspectives to tackle the challenge of inconsistent\nhallucinations."
                },
                "authors": [
                    {
                        "name": "Xinxin You"
                    },
                    {
                        "name": "Xien Liu"
                    },
                    {
                        "name": "Qixin Sun"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Kaiyin Zhou"
                    },
                    {
                        "name": "Shaohui Liu"
                    },
                    {
                        "name": "GuoPing Hu"
                    },
                    {
                        "name": "ShiJin Wang"
                    },
                    {
                        "name": "Si Liu"
                    },
                    {
                        "name": "Ji Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ji Wu"
                },
                "author": "Ji Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.08904v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08904v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13533v1",
                "updated": "2025-02-19T08:39:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    39,
                    15,
                    2,
                    50,
                    0
                ],
                "published": "2025-02-19T08:39:15Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    8,
                    39,
                    15,
                    2,
                    50,
                    0
                ],
                "title": "Train Small, Infer Large: Memory-Efficient LoRA Training for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Train Small, Infer Large: Memory-Efficient LoRA Training for Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing with exceptional task generalization capabilities. Low-Rank Adaption\n(LoRA) offers a cost-effective fine-tuning solution, freezing the original\nmodel parameters and training only lightweight, low-rank adapter matrices.\nHowever, the memory footprint of LoRA is largely dominated by the original\nmodel parameters. To mitigate this, we propose LoRAM, a memory-efficient LoRA\ntraining scheme founded on the intuition that many neurons in\nover-parameterized LLMs have low training utility but are essential for\ninference. LoRAM presents a unique twist: it trains on a pruned (small) model\nto obtain pruned low-rank matrices, which are then recovered and utilized with\nthe original (large) model for inference. Additionally, minimal-cost continual\npre-training, performed by the model publishers in advance, aligns the\nknowledge discrepancy between pruned and original models. Our extensive\nexperiments demonstrate the efficacy of LoRAM across various pruning strategies\nand downstream tasks. For a model with 70 billion parameters, LoRAM enables\ntraining on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA\ntraining and 15 GPUs for full fine-tuning. Specifically, QLoRAM implemented by\nstructured pruning combined with 4-bit quantization, for LLaMA-3.1-70B\n(LLaMA-2-70B), reduces the parameter storage cost that dominates the memory\nusage in low-rank matrix training by 15.81$\\times$ (16.95$\\times$), while\nachieving dominant performance gains over both the original LLaMA-3.1-70B\n(LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced natural language\nprocessing with exceptional task generalization capabilities. Low-Rank Adaption\n(LoRA) offers a cost-effective fine-tuning solution, freezing the original\nmodel parameters and training only lightweight, low-rank adapter matrices.\nHowever, the memory footprint of LoRA is largely dominated by the original\nmodel parameters. To mitigate this, we propose LoRAM, a memory-efficient LoRA\ntraining scheme founded on the intuition that many neurons in\nover-parameterized LLMs have low training utility but are essential for\ninference. LoRAM presents a unique twist: it trains on a pruned (small) model\nto obtain pruned low-rank matrices, which are then recovered and utilized with\nthe original (large) model for inference. Additionally, minimal-cost continual\npre-training, performed by the model publishers in advance, aligns the\nknowledge discrepancy between pruned and original models. Our extensive\nexperiments demonstrate the efficacy of LoRAM across various pruning strategies\nand downstream tasks. For a model with 70 billion parameters, LoRAM enables\ntraining on a GPU with only 20G HBM, replacing an A100-80G GPU for LoRA\ntraining and 15 GPUs for full fine-tuning. Specifically, QLoRAM implemented by\nstructured pruning combined with 4-bit quantization, for LLaMA-3.1-70B\n(LLaMA-2-70B), reduces the parameter storage cost that dominates the memory\nusage in low-rank matrix training by 15.81$\\times$ (16.95$\\times$), while\nachieving dominant performance gains over both the original LLaMA-3.1-70B\n(LLaMA-2-70B) and LoRA-trained LLaMA-3.1-8B (LLaMA-2-13B)."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Jue Wang"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Lidan Shou"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Guiming Xie"
                    },
                    {
                        "name": "Xuejian Gong"
                    },
                    {
                        "name": "Kunlong Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Kunlong Zhou"
                },
                "author": "Kunlong Zhou",
                "arxiv_comment": "Accepted at ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]