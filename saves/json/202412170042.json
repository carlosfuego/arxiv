[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v2",
                "updated": "2024-12-13T17:53:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    53,
                    25,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10302v1",
                "updated": "2024-12-13T17:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding"
                },
                "summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yukun Li"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v2",
                "updated": "2024-12-13T16:13:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Köstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v1",
                "updated": "2024-12-13T14:11:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v2",
                "updated": "2024-12-13T14:08:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    8,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thießen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.55",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.55",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper in ISAAC 2024; minor changes",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 18 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09474v1",
                "updated": "2024-12-12T17:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance"
                },
                "summary": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments."
                },
                "authors": [
                    {
                        "name": "Md Nurul Absur"
                    },
                    {
                        "name": "Sourya Saha"
                    },
                    {
                        "name": "Sifat Nawrin Nova"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Md Rahat Ul Nasib"
                    }
                ],
                "author_detail": {
                    "name": "Md Rahat Ul Nasib"
                },
                "author": "Md Rahat Ul Nasib",
                "arxiv_comment": "6 Pages, 10 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v1",
                "updated": "2024-12-12T16:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v3",
                "updated": "2024-12-12T15:39:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    39,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v2",
                "updated": "2024-12-12T14:43:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtárik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtárik"
                },
                "author": "Peter Richtárik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06282v3",
                "updated": "2024-12-12T12:24:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    24,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-10T14:01:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    1,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone"
                },
                "summary": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v3",
                "updated": "2024-12-12T12:03:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    3,
                    19,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v2",
                "updated": "2024-12-12T10:07:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    7,
                    17,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v1",
                "updated": "2024-12-12T08:33:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09036v1",
                "updated": "2024-12-12T07:52:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T07:52:56Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty"
                },
                "summary": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance."
                },
                "authors": [
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v3",
                "updated": "2024-12-12T03:21:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    21,
                    13,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_doi": "10.1145/3669940.3707265",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3669940.3707265",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08890v1",
                "updated": "2024-12-12T03:00:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T03:00:29Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "title": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries"
                },
                "summary": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v1",
                "updated": "2024-12-11T18:03:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TURBOATTENTION: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TURBOATTENTION: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v1",
                "updated": "2024-12-11T16:35:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v3",
                "updated": "2024-12-11T12:03:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    3,
                    40,
                    2,
                    346,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Pushing the Limits of In-Network Caching for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of In-Network Caching for Key-Value Stores"
                },
                "summary": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "arxiv_comment": "To be appeared in USENIX NSDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08176v1",
                "updated": "2024-12-11T08:07:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:07:12Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "title": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning"
                },
                "summary": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner"
                },
                "authors": [
                    {
                        "name": "Jingjing Xie"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Zhaohong Huang"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08063v1",
                "updated": "2024-12-11T03:15:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T03:15:49Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "title": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates."
                },
                "authors": [
                    {
                        "name": "Zhanming Guan"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Jierui Liu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Dexin Liu"
                    },
                    {
                        "name": "Ningyuan Sun"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Wenchao Li"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Hang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhu"
                },
                "author": "Hang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12952v2",
                "updated": "2024-12-10T22:53:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    22,
                    53,
                    16,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-19T17:54:34Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    54,
                    34,
                    1,
                    79,
                    0
                ],
                "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models"
                },
                "summary": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements."
                },
                "authors": [
                    {
                        "name": "Elaine Sui"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy",
                "arxiv_comment": "Accepted at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v1",
                "updated": "2024-12-10T18:59:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Causal Video Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Causal Video Generators"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v1",
                "updated": "2024-12-10T18:50:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v1",
                "updated": "2024-12-10T18:13:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14485v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14485v4",
                "updated": "2024-12-10T12:45:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    45,
                    31,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-22T15:13:31Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    13,
                    31,
                    6,
                    266,
                    0
                ],
                "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding"
                },
                "summary": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14485v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14485v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v3",
                "updated": "2024-12-09T01:44:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    44,
                    10,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01844v3",
                "updated": "2024-12-09T01:39:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    39,
                    15,
                    0,
                    344,
                    0
                ],
                "published": "2024-05-03T04:27:32Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    4,
                    27,
                    32,
                    4,
                    124,
                    0
                ],
                "title": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges"
                },
                "summary": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Shazia Riaz"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "arxiv_doi": "10.1145/3706630",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706630",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.01844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05896v1",
                "updated": "2024-12-08T11:32:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T11:32:08Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "title": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference"
                },
                "summary": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x."
                },
                "authors": [
                    {
                        "name": "Weizhuo Li"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v1",
                "updated": "2024-12-08T06:37:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "4 pages + 1 reference page, 2 figures, 2 tables. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05704v1",
                "updated": "2024-12-07T17:22:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T17:22:14Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "title": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse"
                },
                "summary": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state."
                },
                "authors": [
                    {
                        "name": "A. A. Melnikov"
                    },
                    {
                        "name": "Yu. G. Selivanov"
                    },
                    {
                        "name": "D. G. Poydashev"
                    },
                    {
                        "name": "S. V. Chekalin"
                    }
                ],
                "author_detail": {
                    "name": "S. V. Chekalin"
                },
                "author": "S. V. Chekalin",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v1",
                "updated": "2024-12-07T16:41:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06567v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-06-03T13:28:43Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    13,
                    28,
                    43,
                    0,
                    155,
                    0
                ],
                "title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion"
                },
                "summary": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yu Sun"
                },
                "author": "Yu Sun",
                "arxiv_comment": "Accepted at NeurIPS 2024 10 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v2",
                "updated": "2024-12-07T04:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    4,
                    8,
                    56,
                    5,
                    342,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05392v1",
                "updated": "2024-12-06T19:35:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T19:35:52Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "title": "Effect of electric field on excitons in wide quantum wells",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effect of electric field on excitons in wide quantum wells"
                },
                "summary": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons."
                },
                "authors": [
                    {
                        "name": "Shiming Zheng"
                    },
                    {
                        "name": "E. S. Khramtsov"
                    },
                    {
                        "name": "I. V. Ignatiev"
                    }
                ],
                "author_detail": {
                    "name": "I. V. Ignatiev"
                },
                "author": "I. V. Ignatiev",
                "arxiv_comment": "12 pages, 8 figures, to be published in Physical Review B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v1",
                "updated": "2024-12-06T17:58:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Selçuk Köse"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02031v2",
                "updated": "2024-12-06T11:47:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    47,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-02T07:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    59,
                    8,
                    1,
                    184,
                    0
                ],
                "title": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules"
                },
                "summary": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality."
                },
                "authors": [
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Hanfeng Lu"
                    },
                    {
                        "name": "Dakai An"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04757v1",
                "updated": "2024-12-06T03:46:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T03:46:06Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "title": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern"
                },
                "summary": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference."
                },
                "authors": [
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Di Xiu"
                    },
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Xiurui Geng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04698v1",
                "updated": "2024-12-06T01:20:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T01:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "One-Hop Sub-Query Result Caches for Graph Database Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Hop Sub-Query Result Caches for Graph Database Systems"
                },
                "summary": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly."
                },
                "authors": [
                    {
                        "name": "Hieu Nguyen"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Shahram Ghandeharizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Ghandeharizadeh"
                },
                "author": "Shahram Ghandeharizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04652v1",
                "updated": "2024-12-05T22:47:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:47:17Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference"
                },
                "summary": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP"
                },
                "authors": [
                    {
                        "name": "Xiaohuan Pei"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04634v1",
                "updated": "2024-12-05T22:06:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:06:23Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "title": "Neural Two-Level Monte Carlo Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Two-Level Monte Carlo Real-Time Rendering"
                },
                "summary": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques."
                },
                "authors": [
                    {
                        "name": "Mikhail Dereviannykh"
                    },
                    {
                        "name": "Dmitrii Klepikov"
                    },
                    {
                        "name": "Johannes Hanika"
                    },
                    {
                        "name": "Carsten Dachsbacher"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Dachsbacher"
                },
                "author": "Carsten Dachsbacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v1",
                "updated": "2024-12-05T18:58:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Ji Qi"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Technical Report; Code released at https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v2",
                "updated": "2024-12-05T14:56:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    56,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19574v2",
                "updated": "2024-12-05T12:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    19,
                    38,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-29T09:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "title": "KV Shifting Attention Enhances Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Shifting Attention Enhances Language Modeling"
                },
                "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters."
                },
                "authors": [
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v2",
                "updated": "2024-12-05T06:52:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    52,
                    42,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v3",
                "updated": "2024-12-05T04:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    29,
                    49,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "01. AI"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Ethan Dai"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Zhang"
                },
                "author": "Zirui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v2",
                "updated": "2024-12-05T01:50:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    1,
                    50,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "F2: Designing a Key-Value Store for Large Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2: Designing a Key-Value Store for Large Skewed Workloads"
                },
                "summary": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v2",
                "updated": "2024-12-04T18:40:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    40,
                    24,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03361v1",
                "updated": "2024-12-04T14:47:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "title": "Measurement of electron beam induced sample heating in SEM experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of electron beam induced sample heating in SEM experiments"
                },
                "summary": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact."
                },
                "authors": [
                    {
                        "name": "Christina Koenig"
                    },
                    {
                        "name": "Alice Bastos da Silva Fanta"
                    },
                    {
                        "name": "Joerg R. Jinschek"
                    }
                ],
                "author_detail": {
                    "name": "Joerg R. Jinschek"
                },
                "author": "Joerg R. Jinschek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v1",
                "updated": "2024-12-04T10:58:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v1",
                "updated": "2024-12-04T08:51:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.08066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.08066v2",
                "updated": "2024-12-04T05:32:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    5,
                    32,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2023-02-06T13:46:08Z",
                "published_parsed": [
                    2023,
                    2,
                    6,
                    13,
                    46,
                    8,
                    0,
                    37,
                    0
                ],
                "title": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems"
                },
                "summary": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level."
                },
                "authors": [
                    {
                        "name": "Xijun Li"
                    },
                    {
                        "name": "Yunfan Zhou"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_comment": "for modifying part of contents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.08066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.08066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03023v1",
                "updated": "2024-12-04T04:29:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T04:29:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis"
                },
                "summary": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in"
                },
                "authors": [
                    {
                        "name": "Cebajel Tanan"
                    },
                    {
                        "name": "Sameer G. Kulkarni"
                    },
                    {
                        "name": "Tamal Das"
                    },
                    {
                        "name": "Manjesh K. Hanawal"
                    }
                ],
                "author_detail": {
                    "name": "Manjesh K. Hanawal"
                },
                "author": "Manjesh K. Hanawal",
                "arxiv_comment": "Presented at ICIE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12622v2",
                "updated": "2024-12-03T22:48:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    48,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2023-10-19T10:02:52Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    10,
                    2,
                    52,
                    3,
                    292,
                    0
                ],
                "title": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching"
                },
                "summary": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Linchang Xiao"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Quan Z. Sheng"
                },
                "author": "Quan Z. Sheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02867v1",
                "updated": "2024-12-03T22:02:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T22:02:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum"
                },
                "summary": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin."
                },
                "authors": [
                    {
                        "name": "Cynthia Marcelino"
                    },
                    {
                        "name": "Jack Shahhoud"
                    },
                    {
                        "name": "Stefan Nastic"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Nastic"
                },
                "author": "Stefan Nastic",
                "arxiv_doi": "10.1145/3703790.3703797",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3703790.3703797",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.02867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14th International Conference on the Internet of Things (IoT 2024),\n  November 19--22, 2024, Oulu, Finland",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v2",
                "updated": "2024-12-03T16:12:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    12,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaoshen Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v3",
                "updated": "2024-12-03T12:36:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    36,
                    19,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v1",
                "updated": "2024-12-03T08:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02122v1",
                "updated": "2024-12-03T03:20:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:20:40Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "title": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior"
                },
                "summary": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior."
                },
                "authors": [
                    {
                        "name": "Luyi Ma"
                    },
                    {
                        "name": "Aashika Padmanabhan"
                    },
                    {
                        "name": "Anjana Ganesh"
                    },
                    {
                        "name": "Shengwei Tang"
                    },
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Lalitesh Morishetti"
                    },
                    {
                        "name": "Kaushiki Nag"
                    },
                    {
                        "name": "Malay Patel"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "6 pages, IEEE BigData 2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v1",
                "updated": "2024-12-02T20:39:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v1",
                "updated": "2024-12-02T18:59:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01659v1",
                "updated": "2024-12-02T16:10:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T16:10:26Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "title": "Local and Regional Contributions to Tropospheric Ozone Concentrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local and Regional Contributions to Tropospheric Ozone Concentrations"
                },
                "summary": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited."
                },
                "authors": [
                    {
                        "name": "Callum E. Flowerday"
                    },
                    {
                        "name": "Ryan Thalman"
                    },
                    {
                        "name": "Jaron C. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Jaron C. Hansen"
                },
                "author": "Jaron C. Hansen",
                "arxiv_doi": "10.3390/atmos14081262",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/atmos14081262",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.01659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Atmosphere 2023, 14, 1262",
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06892v2",
                "updated": "2024-12-02T11:24:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    24,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-03-11T16:48:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    16,
                    48,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head"
                },
                "summary": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}"
                },
                "authors": [
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Kyusong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyusong Lee"
                },
                "author": "Kyusong Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v1",
                "updated": "2024-12-02T11:07:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 3 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01195v1",
                "updated": "2024-12-02T06:57:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T06:57:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification"
                },
                "summary": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs."
                },
                "authors": [
                    {
                        "name": "Bei Liu"
                    },
                    {
                        "name": "Yanmin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Yanmin Qian"
                },
                "author": "Yanmin Qian",
                "arxiv_comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v2",
                "updated": "2024-12-02T06:30:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    30,
                    4,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Zemin Sun"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00977v1",
                "updated": "2024-12-01T21:47:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T21:47:35Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "title": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications"
                },
                "summary": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications."
                },
                "authors": [
                    {
                        "name": "Aditya Powari"
                    },
                    {
                        "name": "Daniel K. C. So"
                    }
                ],
                "author_detail": {
                    "name": "Daniel K. C. So"
                },
                "author": "Daniel K. C. So",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v1",
                "updated": "2024-12-01T15:45:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02532v3",
                "updated": "2024-11-30T21:33:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    21,
                    33,
                    59,
                    5,
                    335,
                    0
                ],
                "published": "2024-06-04T17:53:36Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    53,
                    36,
                    1,
                    156,
                    0
                ],
                "title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices"
                },
                "summary": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights."
                },
                "authors": [
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00209v1",
                "updated": "2024-11-29T19:14:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T19:14:45Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "title": "Digital Twin in Industries: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin in Industries: A Comprehensive Survey"
                },
                "summary": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area."
                },
                "authors": [
                    {
                        "name": "Md Bokhtiar Al Zami"
                    },
                    {
                        "name": "Shaba Shaon"
                    },
                    {
                        "name": "Vu Khanh Quy"
                    },
                    {
                        "name": "Dinh C. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Dinh C. Nguyen"
                },
                "author": "Dinh C. Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19730v1",
                "updated": "2024-11-29T14:23:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T14:23:25Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "title": "Ten Ways in which Virtual Reality Differs from Video Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ten Ways in which Virtual Reality Differs from Video Streaming"
                },
                "summary": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR."
                },
                "authors": [
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "Sonia Fahmy"
                    },
                    {
                        "name": "George Kesidis"
                    },
                    {
                        "name": "Voicu Popescu"
                    }
                ],
                "author_detail": {
                    "name": "Voicu Popescu"
                },
                "author": "Voicu Popescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01852v1",
                "updated": "2024-11-29T10:21:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T10:21:12Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "title": "Communication efficient application of sequences of planar rotations to\n  a matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication efficient application of sequences of planar rotations to\n  a matrix"
                },
                "summary": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware."
                },
                "authors": [
                    {
                        "name": "Thijs Steel"
                    },
                    {
                        "name": "Julien Langou"
                    }
                ],
                "author_detail": {
                    "name": "Julien Langou"
                },
                "author": "Julien Langou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F15, 65Y05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v3",
                "updated": "2024-11-29T08:48:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    48,
                    1,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Küpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024). Publication by IEEE still pending",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v2",
                "updated": "2024-11-29T08:33:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    33,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v1",
                "updated": "2024-11-29T05:57:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Many LLM tasks are performed in large batches or even offline, and the\nperformance indictor for which is throughput. These tasks usually show the\ncharacteristic of prefix sharing, where different prompt input can partially\nshow the common prefix. However, the existing LLM inference engines tend to\noptimize the streaming requests and show limitations of supporting the large\nbatched tasks with the prefix sharing characteristic. The existing solutions\nuse the LRU-based cache to reuse the KV context of common prefix. The KV\ncontext that is about to be reused may prematurely be evicted with the implicit\ncache management. Even if not evicted, the lifetime of the shared KV context is\nextended since requests sharing the same context are not scheduled together,\nresulting in larger memory usage. These streaming oriented systems schedule the\nrequests in the first-come-first-serve or similar order. As a result, the\nrequests with larger ratio of decoding steps may be scheduled too late to be\nable to mix with the prefill chunks to increase the hardware utilization.\nBesides, the token and request number based batching can limit the size of\ntoken-batch, which keeps the GPU from saturating for the iterations dominated\nby decoding tokens. We propose BatchLLM to address the above problems. BatchLLM\nexplicitly identifies the common prefixes globally. The requests sharing the\nsame prefix will be scheduled together to reuse the KV context the best, which\nalso shrinks the lifetime of common KV memory. BatchLLM reorders the requests\nand schedules the requests with larger ratio of decoding first to better mix\nthe decoding tokens with the latter prefill chunks and applies memory-centric\ntoken batching to enlarge the token-batch sizes, which helps to increase the\nGPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by\n1.1x to 2x on a set of microbenchmarks and two typical industry workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many LLM tasks are performed in large batches or even offline, and the\nperformance indictor for which is throughput. These tasks usually show the\ncharacteristic of prefix sharing, where different prompt input can partially\nshow the common prefix. However, the existing LLM inference engines tend to\noptimize the streaming requests and show limitations of supporting the large\nbatched tasks with the prefix sharing characteristic. The existing solutions\nuse the LRU-based cache to reuse the KV context of common prefix. The KV\ncontext that is about to be reused may prematurely be evicted with the implicit\ncache management. Even if not evicted, the lifetime of the shared KV context is\nextended since requests sharing the same context are not scheduled together,\nresulting in larger memory usage. These streaming oriented systems schedule the\nrequests in the first-come-first-serve or similar order. As a result, the\nrequests with larger ratio of decoding steps may be scheduled too late to be\nable to mix with the prefill chunks to increase the hardware utilization.\nBesides, the token and request number based batching can limit the size of\ntoken-batch, which keeps the GPU from saturating for the iterations dominated\nby decoding tokens. We propose BatchLLM to address the above problems. BatchLLM\nexplicitly identifies the common prefixes globally. The requests sharing the\nsame prefix will be scheduled together to reuse the KV context the best, which\nalso shrinks the lifetime of common KV memory. BatchLLM reorders the requests\nand schedules the requests with larger ratio of decoding first to better mix\nthe decoding tokens with the latter prefill chunks and applies memory-centric\ntoken batching to enlarge the token-batch sizes, which helps to increase the\nGPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by\n1.1x to 2x on a set of microbenchmarks and two typical industry workloads."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19248v1",
                "updated": "2024-11-28T16:35:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T16:35:22Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "title": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching"
                },
                "summary": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF."
                },
                "authors": [
                    {
                        "name": "Xiaofan Niu"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Robert Caiming Qiu"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "The short version of this paper was presented in 2024 IEEE\n  Information Theory Workshop, Nov. 24-28, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12468v2",
                "updated": "2024-11-28T14:42:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    14,
                    42,
                    54,
                    3,
                    333,
                    0
                ],
                "published": "2024-04-18T19:04:33Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    19,
                    4,
                    33,
                    3,
                    109,
                    0
                ],
                "title": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits"
                },
                "summary": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v1",
                "updated": "2024-11-28T12:50:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v2",
                "updated": "2024-11-28T02:01:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    2,
                    1,
                    50,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v1",
                "updated": "2024-11-27T18:59:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v4",
                "updated": "2024-11-27T18:05:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    5,
                    57,
                    2,
                    332,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Published in PVLDB Volume 18, Issue 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18424v1",
                "updated": "2024-11-27T15:07:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:07:28Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "title": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving"
                },
                "summary": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT."
                },
                "authors": [
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Zhiyao Li"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v2",
                "updated": "2024-11-27T14:43:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    43,
                    46,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Accelerating Vision Diffusion Transformers with Skip Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Vision Diffusion Transformers with Skip Branches"
                },
                "summary": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v2",
                "updated": "2024-11-27T08:21:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    21,
                    47,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15785v2",
                "updated": "2024-11-27T03:07:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    7,
                    20,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-24T11:30:00Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    11,
                    30,
                    0,
                    6,
                    329,
                    0
                ],
                "title": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity"
                },
                "summary": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models."
                },
                "authors": [
                    {
                        "name": "Zhonghua Yi"
                    },
                    {
                        "name": "Ge Niu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Liqiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liqiu Zhang"
                },
                "author": "Liqiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17685v1",
                "updated": "2024-11-26T18:52:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:52:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "Attamba: Attending To Multi-Token States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attamba: Attending To Multi-Token States"
                },
                "summary": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains."
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Safeen Huda"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17800v1",
                "updated": "2024-11-26T18:42:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:42:42Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "title": "STAR: Synthesis of Tailored Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR: Synthesis of Tailored Architectures"
                },
                "summary": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling."
                },
                "authors": [
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Rom Parnichkun"
                    },
                    {
                        "name": "Alexander Amini"
                    },
                    {
                        "name": "Stefano Massaroli"
                    },
                    {
                        "name": "Michael Poli"
                    }
                ],
                "author_detail": {
                    "name": "Michael Poli"
                },
                "author": "Michael Poli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08652v1",
                "updated": "2024-11-26T17:52:21Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    52,
                    21,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T17:52:21Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    52,
                    21,
                    1,
                    331,
                    0
                ],
                "title": "Twenty-Year Review of Outdoor Air Quality in Utah, USA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twenty-Year Review of Outdoor Air Quality in Utah, USA"
                },
                "summary": "Air quality is a prevalent concern due to its imposing health risks. The\nstate of Utah, USA, has, at times over the last 20 years, experienced some of\nthe worst air quality in the nation. The propensity for Utah to experience\nelevated concentrations of particulate matter ($\\mathrm{PM_{2.5}}$) and ozone\n($\\mathrm{O_3}$) can, in part, be attributed to its unique geography, which\nfeatures dry, mountainous terrain. Valleys in Utah create ideal environments\nfor extended cold-pool events. In this review, we summarize air quality\nresearch conducted in Utah over the past 20 years (2002-2022) by dividing the\nstate into six regions: Utah Valley, Summit County, Southern Utah (regions\nsouth of Utah Valley), Cache Valley, Uinta Basin, and Salt Lake Valley. We\nreview the published literature chronologically and provide a summary for each\nregion, identifying areas where additional research is warranted. We found that\nresearch efforts are heavily weighted toward the Uinta Basin and Salt Lake\nValley, with the remaining regions collectively accounting for only 20% of\nstudies. We identified the need for more source apportionment studies,\nspeciated volatile organic compound (VOC) analyses, and ozone isopleths. Where\nozone isopleths cannot be created, measurements of glyoxal ($\\mathrm{CHOCHO}$)\nand formaldehyde ($\\mathrm{HCHO}$) concentrations could serve as cost-effective\nsurrogates to inform ozone mitigation policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Air quality is a prevalent concern due to its imposing health risks. The\nstate of Utah, USA, has, at times over the last 20 years, experienced some of\nthe worst air quality in the nation. The propensity for Utah to experience\nelevated concentrations of particulate matter ($\\mathrm{PM_{2.5}}$) and ozone\n($\\mathrm{O_3}$) can, in part, be attributed to its unique geography, which\nfeatures dry, mountainous terrain. Valleys in Utah create ideal environments\nfor extended cold-pool events. In this review, we summarize air quality\nresearch conducted in Utah over the past 20 years (2002-2022) by dividing the\nstate into six regions: Utah Valley, Summit County, Southern Utah (regions\nsouth of Utah Valley), Cache Valley, Uinta Basin, and Salt Lake Valley. We\nreview the published literature chronologically and provide a summary for each\nregion, identifying areas where additional research is warranted. We found that\nresearch efforts are heavily weighted toward the Uinta Basin and Salt Lake\nValley, with the remaining regions collectively accounting for only 20% of\nstudies. We identified the need for more source apportionment studies,\nspeciated volatile organic compound (VOC) analyses, and ozone isopleths. Where\nozone isopleths cannot be created, measurements of glyoxal ($\\mathrm{CHOCHO}$)\nand formaldehyde ($\\mathrm{HCHO}$) concentrations could serve as cost-effective\nsurrogates to inform ozone mitigation policies."
                },
                "authors": [
                    {
                        "name": "Callum E. Flowerday"
                    },
                    {
                        "name": "Ryan Thalman"
                    },
                    {
                        "name": "Jaron C. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Jaron C. Hansen"
                },
                "author": "Jaron C. Hansen",
                "arxiv_doi": "10.3390/atmos14101496",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/atmos14101496",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.08652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Atmosphere 2023, 14, 1496",
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v3",
                "updated": "2024-11-26T17:28:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17559v1",
                "updated": "2024-11-26T16:21:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:21:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces"
                },
                "summary": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest."
                },
                "authors": [
                    {
                        "name": "Abolfazl Changizi"
                    },
                    {
                        "name": "Ali H. Abdollahi Bafghi"
                    },
                    {
                        "name": "Masoumeh Nasiri-Kenari"
                    }
                ],
                "author_detail": {
                    "name": "Masoumeh Nasiri-Kenari"
                },
                "author": "Masoumeh Nasiri-Kenari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17786v1",
                "updated": "2024-11-26T15:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching"
                },
                "summary": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models."
                },
                "authors": [
                    {
                        "name": "Emanuele Aiello"
                    },
                    {
                        "name": "Umberto Michieli"
                    },
                    {
                        "name": "Diego Valsesia"
                    },
                    {
                        "name": "Mete Ozay"
                    },
                    {
                        "name": "Enrico Magli"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Magli"
                },
                "author": "Enrico Magli",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v1",
                "updated": "2024-11-26T05:10:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v1",
                "updated": "2024-11-26T04:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v1",
                "updated": "2024-11-25T13:33:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Technical Report. Code is available at\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v2",
                "updated": "2024-11-25T12:14:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    14,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v2",
                "updated": "2024-11-24T21:57:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    21,
                    57,
                    29,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17741v1",
                "updated": "2024-11-24T16:20:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T16:20:57Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "title": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments"
                },
                "summary": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Nikoleta Iliakopoulou"
                    },
                    {
                        "name": "Jovan Stojkovic"
                    },
                    {
                        "name": "Chloe Alverti"
                    },
                    {
                        "name": "Tianyin Xu"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "author": "Josep Torrellas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.0; D.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15735v1",
                "updated": "2024-11-24T06:43:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T06:43:38Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "title": "Test-time Alignment-Enhanced Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Alignment-Enhanced Adapter for Vision-Language Models"
                },
                "summary": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA."
                },
                "authors": [
                    {
                        "name": "Baoshun Tong"
                    },
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.10373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10373v1",
                "updated": "2024-12-13T18:59:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    59,
                    54,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T18:59:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    59,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "GaussianWorld: Gaussian World Model for Streaming 3D Occupancy\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaussianWorld: Gaussian World Model for Streaming 3D Occupancy\n  Prediction"
                },
                "summary": "3D occupancy prediction is important for autonomous driving due to its\ncomprehensive perception of the surroundings. To incorporate sequential inputs,\nmost existing methods fuse representations from previous frames to infer the\ncurrent 3D occupancy. However, they fail to consider the continuity of driving\nscenarios and ignore the strong prior provided by the evolution of 3D scenes\n(e.g., only dynamic objects move). In this paper, we propose a\nworld-model-based framework to exploit the scene evolution for perception. We\nreformulate 3D occupancy prediction as a 4D occupancy forecasting problem\nconditioned on the current sensor input. We decompose the scene evolution into\nthree factors: 1) ego motion alignment of static scenes; 2) local movements of\ndynamic objects; and 3) completion of newly-observed scenes. We then employ a\nGaussian world model (GaussianWorld) to explicitly exploit these priors and\ninfer the scene evolution in the 3D Gaussian space considering the current RGB\nobservation. We evaluate the effectiveness of our framework on the widely used\nnuScenes dataset. Our GaussianWorld improves the performance of the\nsingle-frame counterpart by over 2% in mIoU without introducing additional\ncomputations. Code: https://github.com/zuosc19/GaussianWorld.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D occupancy prediction is important for autonomous driving due to its\ncomprehensive perception of the surroundings. To incorporate sequential inputs,\nmost existing methods fuse representations from previous frames to infer the\ncurrent 3D occupancy. However, they fail to consider the continuity of driving\nscenarios and ignore the strong prior provided by the evolution of 3D scenes\n(e.g., only dynamic objects move). In this paper, we propose a\nworld-model-based framework to exploit the scene evolution for perception. We\nreformulate 3D occupancy prediction as a 4D occupancy forecasting problem\nconditioned on the current sensor input. We decompose the scene evolution into\nthree factors: 1) ego motion alignment of static scenes; 2) local movements of\ndynamic objects; and 3) completion of newly-observed scenes. We then employ a\nGaussian world model (GaussianWorld) to explicitly exploit these priors and\ninfer the scene evolution in the 3D Gaussian space considering the current RGB\nobservation. We evaluate the effectiveness of our framework on the widely used\nnuScenes dataset. Our GaussianWorld improves the performance of the\nsingle-frame counterpart by over 2% in mIoU without introducing additional\ncomputations. Code: https://github.com/zuosc19/GaussianWorld."
                },
                "authors": [
                    {
                        "name": "Sicheng Zuo"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Yuanhui Huang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Code is available at: https://github.com/zuosc19/GaussianWorld",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08760v2",
                "updated": "2024-12-13T18:59:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    59,
                    45,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-11T20:25:18Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    20,
                    25,
                    18,
                    2,
                    346,
                    0
                ],
                "title": "Bayesian evidence for two peaks in the sound speed in cold dense QCD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian evidence for two peaks in the sound speed in cold dense QCD"
                },
                "summary": "I show that in addition to the well-known peak inside massive neutron stars,\nthe sound speed in cold dense QCD matter likely exhibits another peak above\nneutron star densities before it asymptotes to $c_s=\\sqrt{C_s}=\\sqrt{1/3}$.\nBased on the framework reported in arxiv:2408.16738, this approach does not\nrely on any assumption about the ultra-dense matter not realized in nature.\nCurrent multimessenger observation of neutron stars favors the two-peak\nscenario with a Bayes factor $5.1_{-0.7}^{+0.9}$, where the uncertainties are\nsystematics due to models of neutron star inner cores. This evidence grows to\n$27_{-8}^{+11}$ if the $2.6M_\\odot$ component of GW190814 is a neutron star.\nThe trough in $C_s$ separating the two peaks is inferred to be below\n$0.05^{+0.04}_{-0.02}$ (at the $50\\%$ level) if $2.6M_\\odot$ neutron stars\nexist. The second peak above $1/3$ beyond baryon chemical potential\n$\\mu_B=1.6-1.9$ GeV most likely signals non-perturbative effects in cold quark\nmatter, for instance color superconductivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I show that in addition to the well-known peak inside massive neutron stars,\nthe sound speed in cold dense QCD matter likely exhibits another peak above\nneutron star densities before it asymptotes to $c_s=\\sqrt{C_s}=\\sqrt{1/3}$.\nBased on the framework reported in arxiv:2408.16738, this approach does not\nrely on any assumption about the ultra-dense matter not realized in nature.\nCurrent multimessenger observation of neutron stars favors the two-peak\nscenario with a Bayes factor $5.1_{-0.7}^{+0.9}$, where the uncertainties are\nsystematics due to models of neutron star inner cores. This evidence grows to\n$27_{-8}^{+11}$ if the $2.6M_\\odot$ component of GW190814 is a neutron star.\nThe trough in $C_s$ separating the two peaks is inferred to be below\n$0.05^{+0.04}_{-0.02}$ (at the $50\\%$ level) if $2.6M_\\odot$ neutron stars\nexist. The second peak above $1/3$ beyond baryon chemical potential\n$\\mu_B=1.6-1.9$ GeV most likely signals non-perturbative effects in cold quark\nmatter, for instance color superconductivity."
                },
                "authors": [
                    {
                        "name": "Dake Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Dake Zhou"
                },
                "author": "Dake Zhou",
                "arxiv_comment": "v2: clarify the convexity of functions in the reasoning by\n  contradiction, typos fixed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-th",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10372v1",
                "updated": "2024-12-13T18:59:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    59,
                    40,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T18:59:40Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    59,
                    40,
                    4,
                    348,
                    0
                ],
                "title": "UniMed-CLIP: Towards a Unified Image-Text Pretraining Paradigm for\n  Diverse Medical Imaging Modalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniMed-CLIP: Towards a Unified Image-Text Pretraining Paradigm for\n  Diverse Medical Imaging Modalities"
                },
                "summary": "Vision-Language Models (VLMs) trained via contrastive learning have achieved\nnotable success in natural image tasks. However, their application in the\nmedical domain remains limited due to the scarcity of openly accessible,\nlarge-scale medical image-text datasets. Existing medical VLMs either train on\nclosed-source proprietary or relatively small open-source datasets that do not\ngeneralize well. Similarly, most models remain specific to a single or limited\nnumber of medical imaging domains, again restricting their applicability to\nother modalities. To address this gap, we introduce UniMed, a large-scale,\nopen-source multi-modal medical dataset comprising over 5.3 million image-text\npairs across six diverse imaging modalities: X-ray, CT, MRI, Ultrasound,\nPathology, and Fundus. UniMed is developed using a data-collection framework\nthat leverages Large Language Models (LLMs) to transform modality-specific\nclassification datasets into image-text formats while incorporating existing\nimage-text data from the medical domain, facilitating scalable VLM pretraining.\nUsing UniMed, we trained UniMed-CLIP, a unified VLM for six modalities that\nsignificantly outperforms existing generalist VLMs and matches\nmodality-specific medical VLMs, achieving notable gains in zero-shot\nevaluations. For instance, UniMed-CLIP improves over BiomedCLIP (trained on\nproprietary data) by an absolute gain of +12.61, averaged over 21 datasets,\nwhile using 3x less training data. To facilitate future research, we release\nUniMed dataset, training codes, and models at\nhttps://github.com/mbzuai-oryx/UniMed-CLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) trained via contrastive learning have achieved\nnotable success in natural image tasks. However, their application in the\nmedical domain remains limited due to the scarcity of openly accessible,\nlarge-scale medical image-text datasets. Existing medical VLMs either train on\nclosed-source proprietary or relatively small open-source datasets that do not\ngeneralize well. Similarly, most models remain specific to a single or limited\nnumber of medical imaging domains, again restricting their applicability to\nother modalities. To address this gap, we introduce UniMed, a large-scale,\nopen-source multi-modal medical dataset comprising over 5.3 million image-text\npairs across six diverse imaging modalities: X-ray, CT, MRI, Ultrasound,\nPathology, and Fundus. UniMed is developed using a data-collection framework\nthat leverages Large Language Models (LLMs) to transform modality-specific\nclassification datasets into image-text formats while incorporating existing\nimage-text data from the medical domain, facilitating scalable VLM pretraining.\nUsing UniMed, we trained UniMed-CLIP, a unified VLM for six modalities that\nsignificantly outperforms existing generalist VLMs and matches\nmodality-specific medical VLMs, achieving notable gains in zero-shot\nevaluations. For instance, UniMed-CLIP improves over BiomedCLIP (trained on\nproprietary data) by an absolute gain of +12.61, averaged over 21 datasets,\nwhile using 3x less training data. To facilitate future research, we release\nUniMed dataset, training codes, and models at\nhttps://github.com/mbzuai-oryx/UniMed-CLIP."
                },
                "authors": [
                    {
                        "name": "Muhammad Uzair Khattak"
                    },
                    {
                        "name": "Shahina Kunhimon"
                    },
                    {
                        "name": "Muzammal Naseer"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Shahbaz Khan"
                },
                "author": "Fahad Shahbaz Khan",
                "arxiv_comment": "Code, models and demo available at\n  https://github.com/mbzuai-oryx/UniMed-CLIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10362v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10362v1",
                "updated": "2024-12-13T18:55:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    55,
                    19,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T18:55:19Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    55,
                    19,
                    4,
                    348,
                    0
                ],
                "title": "OP-LoRA: The Blessing of Dimensionality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OP-LoRA: The Blessing of Dimensionality"
                },
                "summary": "Low-rank adapters enable fine-tuning of large models with only a small number\nof parameters, thus reducing storage costs and minimizing the risk of\ncatastrophic forgetting. However, they often pose optimization challenges, with\npoor convergence. To overcome these challenges, we introduce an\nover-parameterized approach that accelerates training without increasing\ninference costs. This method reparameterizes low-rank adaptation by employing a\nseparate MLP and learned embedding for each layer. The learned embedding is\ninput to the MLP, which generates the adapter parameters. Such\noverparamaterization has been shown to implicitly function as an adaptive\nlearning rate and momentum, accelerating optimization. At inference time, the\nMLP can be discarded, leaving behind a standard low-rank adapter. To study the\neffect of MLP overparameterization on a small yet difficult proxy task, we\nimplement it for matrix factorization, and find it achieves faster convergence\nand lower final loss. Extending this approach to larger-scale tasks, we observe\nconsistent performance gains across domains. We achieve improvements in\nvision-language tasks and especially notable increases in image generation,\nwith CMMD scores improving by up to 15 points.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-rank adapters enable fine-tuning of large models with only a small number\nof parameters, thus reducing storage costs and minimizing the risk of\ncatastrophic forgetting. However, they often pose optimization challenges, with\npoor convergence. To overcome these challenges, we introduce an\nover-parameterized approach that accelerates training without increasing\ninference costs. This method reparameterizes low-rank adaptation by employing a\nseparate MLP and learned embedding for each layer. The learned embedding is\ninput to the MLP, which generates the adapter parameters. Such\noverparamaterization has been shown to implicitly function as an adaptive\nlearning rate and momentum, accelerating optimization. At inference time, the\nMLP can be discarded, leaving behind a standard low-rank adapter. To study the\neffect of MLP overparameterization on a small yet difficult proxy task, we\nimplement it for matrix factorization, and find it achieves faster convergence\nand lower final loss. Extending this approach to larger-scale tasks, we observe\nconsistent performance gains across domains. We achieve improvements in\nvision-language tasks and especially notable increases in image generation,\nwith CMMD scores improving by up to 15 points."
                },
                "authors": [
                    {
                        "name": "Piotr Teterwak"
                    },
                    {
                        "name": "Kate Saenko"
                    },
                    {
                        "name": "Bryan A. Plummer"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    }
                ],
                "author_detail": {
                    "name": "Ser-Nam Lim"
                },
                "author": "Ser-Nam Lim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10362v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05213v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05213v2",
                "updated": "2024-12-13T18:53:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    53,
                    59,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-06T17:43:51Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    43,
                    51,
                    4,
                    341,
                    0
                ],
                "title": "Probing neutrino mass ordering with supernova neutrinos at NO$ν$A\n  including the effect of sterile neutrinos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing neutrino mass ordering with supernova neutrinos at NO$ν$A\n  including the effect of sterile neutrinos"
                },
                "summary": "In this work, we explore the possibility of probing the mass ordering\nsensitivity as a function of supernova distance in the context of the ongoing\nneutrino experiment NO$\\nu$A. We provide a detailed study of the active-active\nand active-sterile mixing frameworks, illustrating how supernova neutrinos can\nbe used to realize the existence of sterile neutrinos. Interestingly, we infer\nthat observation of the NC channel alone can differentiate between the presence\nand absence of sterile neutrinos. Our results indicate that the primary channel\nof NO$\\nu$A can distinguish normal mass hierarchy from inverted mass hierarchy\nat $5 \\sigma$ confidence level for a supernova explosion occurring at a\ndistance of 5 kpc. Additionally, we examine the impact of systematic\nuncertainties on mass hierarchy sensitivity, showing that higher levels of\nsystematics lead to a reduction in sensitivity. Similarly, the inclusion of\nenergy smearing significantly diminishes hierarchy sensitivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we explore the possibility of probing the mass ordering\nsensitivity as a function of supernova distance in the context of the ongoing\nneutrino experiment NO$\\nu$A. We provide a detailed study of the active-active\nand active-sterile mixing frameworks, illustrating how supernova neutrinos can\nbe used to realize the existence of sterile neutrinos. Interestingly, we infer\nthat observation of the NC channel alone can differentiate between the presence\nand absence of sterile neutrinos. Our results indicate that the primary channel\nof NO$\\nu$A can distinguish normal mass hierarchy from inverted mass hierarchy\nat $5 \\sigma$ confidence level for a supernova explosion occurring at a\ndistance of 5 kpc. Additionally, we examine the impact of systematic\nuncertainties on mass hierarchy sensitivity, showing that higher levels of\nsystematics lead to a reduction in sensitivity. Similarly, the inclusion of\nenergy smearing significantly diminishes hierarchy sensitivity."
                },
                "authors": [
                    {
                        "name": "Papia Panda"
                    },
                    {
                        "name": "Rukmani Mohanta"
                    }
                ],
                "author_detail": {
                    "name": "Rukmani Mohanta"
                },
                "author": "Rukmani Mohanta",
                "arxiv_comment": "25 pages, 9 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05213v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05213v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10351v1",
                "updated": "2024-12-13T18:47:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    47,
                    11,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T18:47:11Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    47,
                    11,
                    4,
                    348,
                    0
                ],
                "title": "VibrantVS: A high-resolution multi-task transformer for forest canopy\n  height estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VibrantVS: A high-resolution multi-task transformer for forest canopy\n  height estimation"
                },
                "summary": "This paper explores the application of a novel multi-task vision transformer\n(ViT) model for the estimation of canopy height models (CHMs) using 4-band\nNational Agriculture Imagery Program (NAIP) imagery across the western United\nStates. We compare the effectiveness of this model in terms of accuracy and\nprecision aggregated across ecoregions and class heights versus three other\nbenchmark peer-reviewed models. Key findings suggest that, while other\nbenchmark models can provide high precision in localized areas, the VibrantVS\nmodel has substantial advantages across a broad reach of ecoregions in the\nwestern United States with higher accuracy, higher precision, the ability to\ngenerate updated inference at a cadence of three years or less, and high\nspatial resolution. The VibrantVS model provides significant value for\necological monitoring and land management decisions for wildfire mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the application of a novel multi-task vision transformer\n(ViT) model for the estimation of canopy height models (CHMs) using 4-band\nNational Agriculture Imagery Program (NAIP) imagery across the western United\nStates. We compare the effectiveness of this model in terms of accuracy and\nprecision aggregated across ecoregions and class heights versus three other\nbenchmark peer-reviewed models. Key findings suggest that, while other\nbenchmark models can provide high precision in localized areas, the VibrantVS\nmodel has substantial advantages across a broad reach of ecoregions in the\nwestern United States with higher accuracy, higher precision, the ability to\ngenerate updated inference at a cadence of three years or less, and high\nspatial resolution. The VibrantVS model provides significant value for\necological monitoring and land management decisions for wildfire mitigation."
                },
                "authors": [
                    {
                        "name": "Tony Chang"
                    },
                    {
                        "name": "Kiarie Ndegwa"
                    },
                    {
                        "name": "Andreas Gros"
                    },
                    {
                        "name": "Vincent A. Landau"
                    },
                    {
                        "name": "Luke J. Zachmann"
                    },
                    {
                        "name": "Bogdan State"
                    },
                    {
                        "name": "Mitchell A. Gritts"
                    },
                    {
                        "name": "Colton W. Miller"
                    },
                    {
                        "name": "Nathan E. Rutenbeck"
                    },
                    {
                        "name": "Scott Conway"
                    },
                    {
                        "name": "Guy Bayes"
                    }
                ],
                "author_detail": {
                    "name": "Guy Bayes"
                },
                "author": "Guy Bayes",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10345v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10345v1",
                "updated": "2024-12-13T18:40:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    40,
                    51,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T18:40:51Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    40,
                    51,
                    4,
                    348,
                    0
                ],
                "title": "TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for\n  Generalist Robotic Policies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for\n  Generalist Robotic Policies"
                },
                "summary": "Although large vision-language-action (VLA) models pretrained on extensive\nrobot datasets offer promising generalist policies for robotic learning, they\nstill struggle with spatial-temporal dynamics in interactive robotics, making\nthem less effective in handling complex tasks, such as manipulation. In this\nwork, we introduce visual trace prompting, a simple yet effective approach to\nfacilitate VLA models' spatial-temporal awareness for action prediction by\nencoding state-action trajectories visually. We develop a new TraceVLA model by\nfinetuning OpenVLA on our own collected dataset of 150K robot manipulation\ntrajectories using visual trace prompting. Evaluations of TraceVLA across 137\nconfigurations in SimplerEnv and 4 tasks on a physical WidowX robot demonstrate\nstate-of-the-art performance, outperforming OpenVLA by 10% on SimplerEnv and\n3.5x on real-robot tasks and exhibiting robust generalization across diverse\nembodiments and scenarios. To further validate the effectiveness and generality\nof our method, we present a compact VLA model based on 4B Phi-3-Vision,\npretrained on the Open-X-Embodiment and finetuned on our dataset, rivals the 7B\nOpenVLA baseline while significantly improving inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large vision-language-action (VLA) models pretrained on extensive\nrobot datasets offer promising generalist policies for robotic learning, they\nstill struggle with spatial-temporal dynamics in interactive robotics, making\nthem less effective in handling complex tasks, such as manipulation. In this\nwork, we introduce visual trace prompting, a simple yet effective approach to\nfacilitate VLA models' spatial-temporal awareness for action prediction by\nencoding state-action trajectories visually. We develop a new TraceVLA model by\nfinetuning OpenVLA on our own collected dataset of 150K robot manipulation\ntrajectories using visual trace prompting. Evaluations of TraceVLA across 137\nconfigurations in SimplerEnv and 4 tasks on a physical WidowX robot demonstrate\nstate-of-the-art performance, outperforming OpenVLA by 10% on SimplerEnv and\n3.5x on real-robot tasks and exhibiting robust generalization across diverse\nembodiments and scenarios. To further validate the effectiveness and generality\nof our method, we present a compact VLA model based on 4B Phi-3-Vision,\npretrained on the Open-X-Embodiment and finetuned on our dataset, rivals the 7B\nOpenVLA baseline while significantly improving inference efficiency."
                },
                "authors": [
                    {
                        "name": "Ruijie Zheng"
                    },
                    {
                        "name": "Yongyuan Liang"
                    },
                    {
                        "name": "Shuaiyi Huang"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Hal Daumé III"
                    },
                    {
                        "name": "Andrey Kolobov"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Jianwei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jianwei Yang"
                },
                "author": "Jianwei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10345v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10343v1",
                "updated": "2024-12-13T18:40:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    40,
                    25,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T18:40:25Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    40,
                    25,
                    4,
                    348,
                    0
                ],
                "title": "Accretion Disc-Jet Decomposition from the Optical-Near Infrared\n  Monitoring of Fermi Blazars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accretion Disc-Jet Decomposition from the Optical-Near Infrared\n  Monitoring of Fermi Blazars"
                },
                "summary": "We study the variability of the thermal (accretion disc) and non-thermal\n(jet) emission of thirteen flat spectrum radio quasars in the optical and near\ninfrared (OIR) regime using light curves spanning years with an average\nsampling of three observations per week. We fit a combination of a blackbody\nand a power-law function to the OIR data, in the blazar rest frame, to extract\nthe corresponding thermal (disc) and non-thermal (jet) components from the\ntotal flux. We carry out this analysis for the entire duration of the light\ncurves to obtain the variation of the disc and jet components over years.\nReliability of our fits have been affirmed by successfully retrieving accurate\nparameters by employing our method to simulated data and by comparing our\nresults with published disc luminosity obtained by other methods for a few\nwell-observed blazars. In blazars, the thermal (disc) emission is difficult to\nextract because the relativistically beamed radiation of the jet dominates at\nall wavelengths. By employing this method, the disc emission in blazars may be\nestimated directly from photometric data at OIR bands instead of indirect\nmethods, such as, inferring it from the emission line luminosities. We find\nthat the variability of the disc and jet emission obtained by the above method\nare strongly correlated in most cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the variability of the thermal (accretion disc) and non-thermal\n(jet) emission of thirteen flat spectrum radio quasars in the optical and near\ninfrared (OIR) regime using light curves spanning years with an average\nsampling of three observations per week. We fit a combination of a blackbody\nand a power-law function to the OIR data, in the blazar rest frame, to extract\nthe corresponding thermal (disc) and non-thermal (jet) components from the\ntotal flux. We carry out this analysis for the entire duration of the light\ncurves to obtain the variation of the disc and jet components over years.\nReliability of our fits have been affirmed by successfully retrieving accurate\nparameters by employing our method to simulated data and by comparing our\nresults with published disc luminosity obtained by other methods for a few\nwell-observed blazars. In blazars, the thermal (disc) emission is difficult to\nextract because the relativistically beamed radiation of the jet dominates at\nall wavelengths. By employing this method, the disc emission in blazars may be\nestimated directly from photometric data at OIR bands instead of indirect\nmethods, such as, inferring it from the emission line luminosities. We find\nthat the variability of the disc and jet emission obtained by the above method\nare strongly correlated in most cases."
                },
                "authors": [
                    {
                        "name": "Garima Rajguru"
                    },
                    {
                        "name": "Ritaban Chatterjee"
                    }
                ],
                "author_detail": {
                    "name": "Ritaban Chatterjee"
                },
                "arxiv_affiliation": "Presidency U. Kolkata",
                "author": "Ritaban Chatterjee",
                "arxiv_doi": "10.1093/mnras/stae2608",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/stae2608",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.10343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in MNRAS",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10342v1",
                "updated": "2024-12-13T18:40:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    40,
                    10,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T18:40:10Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    40,
                    10,
                    4,
                    348,
                    0
                ],
                "title": "Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining"
                },
                "summary": "Digital agents are increasingly employed to automate tasks in interactive\ndigital environments such as web pages, software applications, and operating\nsystems. While text-based agents built on Large Language Models (LLMs) often\nrequire frequent updates due to platform-specific APIs, visual agents\nleveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability\nby interacting directly with Graphical User Interfaces (GUIs). However, these\nagents face significant challenges in visual perception, particularly when\nhandling high-resolution, visually complex digital environments. This paper\nintroduces Iris, a foundational visual agent that addresses these challenges\nthrough two key innovations: Information-Sensitive Cropping (ISC) and\nSelf-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes\nvisually dense regions using a edge detection algorithm, enabling efficient\nprocessing by allocating more computational resources to areas with higher\ninformation density. SRDL enhances the agent's ability to handle complex tasks\nby leveraging a dual-learning loop, where improvements in referring (describing\nUI elements) reinforce grounding (locating elements) and vice versa, all\nwithout requiring additional annotated data. Empirical evaluations demonstrate\nthat Iris achieves state-of-the-art performance across multiple benchmarks with\nonly 850K GUI annotations, outperforming methods using 10x more training data.\nThese improvements further translate to significant gains in both web and OS\nagent downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital agents are increasingly employed to automate tasks in interactive\ndigital environments such as web pages, software applications, and operating\nsystems. While text-based agents built on Large Language Models (LLMs) often\nrequire frequent updates due to platform-specific APIs, visual agents\nleveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability\nby interacting directly with Graphical User Interfaces (GUIs). However, these\nagents face significant challenges in visual perception, particularly when\nhandling high-resolution, visually complex digital environments. This paper\nintroduces Iris, a foundational visual agent that addresses these challenges\nthrough two key innovations: Information-Sensitive Cropping (ISC) and\nSelf-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes\nvisually dense regions using a edge detection algorithm, enabling efficient\nprocessing by allocating more computational resources to areas with higher\ninformation density. SRDL enhances the agent's ability to handle complex tasks\nby leveraging a dual-learning loop, where improvements in referring (describing\nUI elements) reinforce grounding (locating elements) and vice versa, all\nwithout requiring additional annotated data. Empirical evaluations demonstrate\nthat Iris achieves state-of-the-art performance across multiple benchmarks with\nonly 850K GUI annotations, outperforming methods using 10x more training data.\nThese improvements further translate to significant gains in both web and OS\nagent downstream tasks."
                },
                "authors": [
                    {
                        "name": "Zhiqi Ge"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Xinglei Pang"
                    },
                    {
                        "name": "Minghe Gao"
                    },
                    {
                        "name": "Kaihang Pan"
                    },
                    {
                        "name": "Wang Lin"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10321v1",
                "updated": "2024-12-13T18:00:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    0,
                    57,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T18:00:57Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    0,
                    57,
                    4,
                    348,
                    0
                ],
                "title": "AdvPrefix: An Objective for Nuanced LLM Jailbreaks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdvPrefix: An Objective for Nuanced LLM Jailbreaks"
                },
                "summary": "Many jailbreak attacks on large language models (LLMs) rely on a common\nobjective: making the model respond with the prefix \"Sure, here is (harmful\nrequest)\". While straightforward, this objective has two limitations: limited\ncontrol over model behaviors, often resulting in incomplete or unrealistic\nresponses, and a rigid format that hinders optimization. To address these\nlimitations, we introduce AdvPrefix, a new prefix-forcing objective that\nenables more nuanced control over model behavior while being easy to optimize.\nOur objective leverages model-dependent prefixes, automatically selected based\non two criteria: high prefilling attack success rates and low negative\nlog-likelihood. It can further simplify optimization by using multiple prefixes\nfor a single user request. AdvPrefix can integrate seamlessly into existing\njailbreak attacks to improve their performance for free. For example, simply\nreplacing GCG attack's target prefixes with ours on Llama-3 improves nuanced\nattack success rates from 14% to 80%, suggesting that current alignment\nstruggles to generalize to unseen prefixes. Our work demonstrates the\nimportance of jailbreak objectives in achieving nuanced jailbreaks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many jailbreak attacks on large language models (LLMs) rely on a common\nobjective: making the model respond with the prefix \"Sure, here is (harmful\nrequest)\". While straightforward, this objective has two limitations: limited\ncontrol over model behaviors, often resulting in incomplete or unrealistic\nresponses, and a rigid format that hinders optimization. To address these\nlimitations, we introduce AdvPrefix, a new prefix-forcing objective that\nenables more nuanced control over model behavior while being easy to optimize.\nOur objective leverages model-dependent prefixes, automatically selected based\non two criteria: high prefilling attack success rates and low negative\nlog-likelihood. It can further simplify optimization by using multiple prefixes\nfor a single user request. AdvPrefix can integrate seamlessly into existing\njailbreak attacks to improve their performance for free. For example, simply\nreplacing GCG attack's target prefixes with ours on Llama-3 improves nuanced\nattack success rates from 14% to 80%, suggesting that current alignment\nstruggles to generalize to unseen prefixes. Our work demonstrates the\nimportance of jailbreak objectives in achieving nuanced jailbreaks."
                },
                "authors": [
                    {
                        "name": "Sicheng Zhu"
                    },
                    {
                        "name": "Brandon Amos"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Chuan Guo"
                    },
                    {
                        "name": "Ivan Evtimov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Evtimov"
                },
                "author": "Ivan Evtimov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v2",
                "updated": "2024-12-13T17:53:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    53,
                    25,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08722v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08722v2",
                "updated": "2024-12-13T17:49:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    49,
                    29,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-11T19:00:06Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    19,
                    0,
                    6,
                    2,
                    346,
                    0
                ],
                "title": "$\\texttt{galsbi}$: A Python package for the GalSBI galaxy population\n  model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\texttt{galsbi}$: A Python package for the GalSBI galaxy population\n  model"
                },
                "summary": "Large-scale structure surveys measure the shape and position of millions of\ngalaxies in order to constrain the cosmological model with high precision. The\nresulting large data volume poses a challenge for the analysis of the data,\nfrom the estimation of photometric redshifts to the calibration of shape\nmeasurements. We present GalSBI, a model for the galaxy population, to address\nthese challenges. This phenomenological model is constrained by observational\ndata using simulation-based inference (SBI). The $\\texttt{galsbi}$ Python\npackage provides an easy interface to generate catalogs of galaxies based on\nthe GalSBI model, including their photometric properties, and to simulate\nrealistic images of these galaxies using the $\\texttt{UFig}$ package.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale structure surveys measure the shape and position of millions of\ngalaxies in order to constrain the cosmological model with high precision. The\nresulting large data volume poses a challenge for the analysis of the data,\nfrom the estimation of photometric redshifts to the calibration of shape\nmeasurements. We present GalSBI, a model for the galaxy population, to address\nthese challenges. This phenomenological model is constrained by observational\ndata using simulation-based inference (SBI). The $\\texttt{galsbi}$ Python\npackage provides an easy interface to generate catalogs of galaxies based on\nthe GalSBI model, including their photometric properties, and to simulate\nrealistic images of these galaxies using the $\\texttt{UFig}$ package."
                },
                "authors": [
                    {
                        "name": "Silvan Fischbacher"
                    },
                    {
                        "name": "Beatrice Moser"
                    },
                    {
                        "name": "Tomasz Kacprzak"
                    },
                    {
                        "name": "Joerg Herbel"
                    },
                    {
                        "name": "Luca Tortorelli"
                    },
                    {
                        "name": "Uwe Schmitt"
                    },
                    {
                        "name": "Alexandre Refregier"
                    },
                    {
                        "name": "Adam Amara"
                    }
                ],
                "author_detail": {
                    "name": "Adam Amara"
                },
                "author": "Adam Amara",
                "arxiv_comment": "10 pages, 5 figures, submitted to JOSS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08722v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08722v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10310v1",
                "updated": "2024-12-13T17:49:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    49,
                    4,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:49:04Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    49,
                    4,
                    4,
                    348,
                    0
                ],
                "title": "Simulation-based inference on warm dark matter from HERA forecasts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-based inference on warm dark matter from HERA forecasts"
                },
                "summary": "The redshifted 21cm signal from Cosmic Dawn promises to open a new window\ninto the early history of our universe and enable the probing of an\nunprecedented comoving survey volume. In this work, we revisit the imprint of\nWarm Dark Matter (WDM) on the 21cm signal power spectrum using an updated\nimplementation of the WDM effect in the public code $\\texttt{21cmFast}$ and\nconsidering a single population of cosmic dawn galaxies. By focusing on\ninferring the WDM mass, we analyze the degeneracies between the latter and the\nastrophysics parameters characterizing star formation and X-ray heating and we\nemphasize the role of the threshold mass for star-forming galaxies, $M_{\\rm\nturn}$. We study the capability of the recently built HERA telescope to\nreconstruct the WDM mass by adopting the statistical approach of\nsimulation-based inference. We include a comparison of the per-parameter\nreconstruction quality for different number of simulations used in the training\nof the algorithm. Our results indicate that HERA could surpass current\nLyman-$\\alpha$ forest constraints if Cosmic Dawn galaxies exhibit a threshold\nmass $M_{\\rm turn}\\lesssim 10^{8}\\, M_\\odot$. The X-ray source properties\nconsidered in this study may also influence the strength of the WDM constraint\nfor lower threshold masses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The redshifted 21cm signal from Cosmic Dawn promises to open a new window\ninto the early history of our universe and enable the probing of an\nunprecedented comoving survey volume. In this work, we revisit the imprint of\nWarm Dark Matter (WDM) on the 21cm signal power spectrum using an updated\nimplementation of the WDM effect in the public code $\\texttt{21cmFast}$ and\nconsidering a single population of cosmic dawn galaxies. By focusing on\ninferring the WDM mass, we analyze the degeneracies between the latter and the\nastrophysics parameters characterizing star formation and X-ray heating and we\nemphasize the role of the threshold mass for star-forming galaxies, $M_{\\rm\nturn}$. We study the capability of the recently built HERA telescope to\nreconstruct the WDM mass by adopting the statistical approach of\nsimulation-based inference. We include a comparison of the per-parameter\nreconstruction quality for different number of simulations used in the training\nof the algorithm. Our results indicate that HERA could surpass current\nLyman-$\\alpha$ forest constraints if Cosmic Dawn galaxies exhibit a threshold\nmass $M_{\\rm turn}\\lesssim 10^{8}\\, M_\\odot$. The X-ray source properties\nconsidered in this study may also influence the strength of the WDM constraint\nfor lower threshold masses."
                },
                "authors": [
                    {
                        "name": "Quentin Decant"
                    },
                    {
                        "name": "Androniki Dimitriou"
                    },
                    {
                        "name": "Laura Lopez Honorez"
                    },
                    {
                        "name": "Bryan Zaldivar"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Zaldivar"
                },
                "author": "Bryan Zaldivar",
                "arxiv_comment": "34 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08716v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08716v2",
                "updated": "2024-12-13T17:42:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    42,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-11T19:00:03Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    19,
                    0,
                    3,
                    2,
                    346,
                    0
                ],
                "title": "$\\texttt{UFig v1}$: The ultra-fast image generator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\texttt{UFig v1}$: The ultra-fast image generator"
                },
                "summary": "With the rise of simulation-based inference (SBI) methods, simulations need\nto be fast as well as realistic. $\\texttt{UFig v1}$ is a public Python package\nthat generates simulated astronomical images with exceptional speed - taking\napproximately the same time as source extraction. This makes it particularly\nwell-suited for simulation-based inference (SBI) methods where computational\nefficiency is crucial. To render an image, $\\texttt{UFig}$ requires a galaxy\ncatalog and a description of the point spread function (PSF). It can also add\nbackground noise, sample stars using the Besan\\c{c}on model of the Milky Way,\nand run $\\texttt{SExtractor}$ to extract sources from the rendered image. The\nextracted sources can be matched to the intrinsic catalog, flagged based on\n$\\texttt{SExtractor}$ output and survey masks, and emulators can be used to\nbypass the image simulation and extraction steps. A first version of\n$\\texttt{UFig}$ was presented in Berg\\'e et al. (2013) and the software has\nsince been used and further developed in a variety of forward modelling\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of simulation-based inference (SBI) methods, simulations need\nto be fast as well as realistic. $\\texttt{UFig v1}$ is a public Python package\nthat generates simulated astronomical images with exceptional speed - taking\napproximately the same time as source extraction. This makes it particularly\nwell-suited for simulation-based inference (SBI) methods where computational\nefficiency is crucial. To render an image, $\\texttt{UFig}$ requires a galaxy\ncatalog and a description of the point spread function (PSF). It can also add\nbackground noise, sample stars using the Besan\\c{c}on model of the Milky Way,\nand run $\\texttt{SExtractor}$ to extract sources from the rendered image. The\nextracted sources can be matched to the intrinsic catalog, flagged based on\n$\\texttt{SExtractor}$ output and survey masks, and emulators can be used to\nbypass the image simulation and extraction steps. A first version of\n$\\texttt{UFig}$ was presented in Berg\\'e et al. (2013) and the software has\nsince been used and further developed in a variety of forward modelling\napplications."
                },
                "authors": [
                    {
                        "name": "Silvan Fischbacher"
                    },
                    {
                        "name": "Beatrice Moser"
                    },
                    {
                        "name": "Tomasz Kacprzak"
                    },
                    {
                        "name": "Luca Tortorelli"
                    },
                    {
                        "name": "Joerg Herbel"
                    },
                    {
                        "name": "Claudio Bruderer"
                    },
                    {
                        "name": "Uwe Schmitt"
                    },
                    {
                        "name": "Alexandre Refregier"
                    },
                    {
                        "name": "Joel Berge"
                    },
                    {
                        "name": "Lukas Gamper"
                    },
                    {
                        "name": "Adam Amara"
                    }
                ],
                "author_detail": {
                    "name": "Adam Amara"
                },
                "author": "Adam Amara",
                "arxiv_comment": "8 pages, 4 figures, submitted to JOSS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08716v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08716v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10307v1",
                "updated": "2024-12-13T17:41:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    41,
                    57,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:41:57Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    41,
                    57,
                    4,
                    348,
                    0
                ],
                "title": "A Note On Square-free Sequences and Anti-unification Type",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Note On Square-free Sequences and Anti-unification Type"
                },
                "summary": "Anti-unification is a fundamental operation used for inductive inference. It\nis abstractly defined as a process deriving from a set of symbolic expressions\na new symbolic expression possessing certain commonalities shared between its\nmembers. We consider anti-unification over term algebras where some function\nsymbols are interpreted as associative-idempotent ($f(x,f(y,z)) = f(f(x,y),z)$\nand $f(x,x)=x$, respectively) and show that there exists generalization\nproblems for which a minimal complete set of solutions does not exist\n(Nullary), that is every complete set must contain comparable elements with\nrespect to the generality relation. In contrast to earlier techniques for\nshowing the nullarity of a generalization problem, we exploit combinatorial\nproperties of complete sets of solutions to show that comparable elements are\nnot avoidable. We show that every complete set of solutions contains an\ninfinite chain of comparable generalizations whose structure is isomorphic to a\nsubsequence of an infinite square-free sequence over three symbols.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anti-unification is a fundamental operation used for inductive inference. It\nis abstractly defined as a process deriving from a set of symbolic expressions\na new symbolic expression possessing certain commonalities shared between its\nmembers. We consider anti-unification over term algebras where some function\nsymbols are interpreted as associative-idempotent ($f(x,f(y,z)) = f(f(x,y),z)$\nand $f(x,x)=x$, respectively) and show that there exists generalization\nproblems for which a minimal complete set of solutions does not exist\n(Nullary), that is every complete set must contain comparable elements with\nrespect to the generality relation. In contrast to earlier techniques for\nshowing the nullarity of a generalization problem, we exploit combinatorial\nproperties of complete sets of solutions to show that comparable elements are\nnot avoidable. We show that every complete set of solutions contains an\ninfinite chain of comparable generalizations whose structure is isomorphic to a\nsubsequence of an infinite square-free sequence over three symbols."
                },
                "authors": [
                    {
                        "name": "David M. Cerna"
                    }
                ],
                "author_detail": {
                    "name": "David M. Cerna"
                },
                "author": "David M. Cerna",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10304v1",
                "updated": "2024-12-13T17:37:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    57,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:57Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    57,
                    4,
                    348,
                    0
                ],
                "title": "A Neyman-Orthogonalization Approach to the Incidental Parameter Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Neyman-Orthogonalization Approach to the Incidental Parameter Problem"
                },
                "summary": "A popular approach to perform inference on a target parameter in the presence\nof nuisance parameters is to construct estimating equations that are orthogonal\nto the nuisance parameters, in the sense that their expected first derivative\nis zero. Such first-order orthogonalization may, however, not suffice when the\nnuisance parameters are very imprecisely estimated. Leading examples where this\nis the case are models for panel and network data that feature fixed effects.\nIn this paper, we show how, in the conditional-likelihood setting, estimating\nequations can be constructed that are orthogonal to any chosen order. Combining\nthese equations with sample splitting yields higher-order bias-corrected\nestimators of target parameters. In an empirical application we apply our\nmethod to a fixed-effect model of team production and obtain estimates of\ncomplementarity in production and impacts of counterfactual re-allocations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A popular approach to perform inference on a target parameter in the presence\nof nuisance parameters is to construct estimating equations that are orthogonal\nto the nuisance parameters, in the sense that their expected first derivative\nis zero. Such first-order orthogonalization may, however, not suffice when the\nnuisance parameters are very imprecisely estimated. Leading examples where this\nis the case are models for panel and network data that feature fixed effects.\nIn this paper, we show how, in the conditional-likelihood setting, estimating\nequations can be constructed that are orthogonal to any chosen order. Combining\nthese equations with sample splitting yields higher-order bias-corrected\nestimators of target parameters. In an empirical application we apply our\nmethod to a fixed-effect model of team production and obtain estimates of\ncomplementarity in production and impacts of counterfactual re-allocations."
                },
                "authors": [
                    {
                        "name": "Stéphane Bonhomme"
                    },
                    {
                        "name": "Koen Jochmans"
                    },
                    {
                        "name": "Martin Weidner"
                    }
                ],
                "author_detail": {
                    "name": "Martin Weidner"
                },
                "author": "Martin Weidner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10302v1",
                "updated": "2024-12-13T17:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding"
                },
                "summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yukun Li"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10299v1",
                "updated": "2024-12-13T17:35:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    35,
                    4,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:35:04Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    35,
                    4,
                    4,
                    348,
                    0
                ],
                "title": "Towards an exact approach to pulsar timing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards an exact approach to pulsar timing"
                },
                "summary": "The pulsar timing technique, which compares the observed arrival times of\nelectromagnetic radiation from a pulsar with the predicted arrival times\nderived from a theoretical model of the pulsar system, is used in pulsar\nastronomy to infer a multitude of physical information and to constrain\npossible corrections to General Relativity (GR). The propagation delay is\nusually computed using formulas based on a post-Newtonian approach, for both\nthe light trajectory and the orbital motion. However, evidence has recently\nemerged that this approximation may no longer be sufficient when the companion\nobject is a supermassive black hole; deviations from a full GR computation of\nthe propagation delay can reach a few seconds. In this paper, we analyze the\ncase of binary pulsars with a stellar or intermediate black hole companion,\nwhose discovery and timing are key goals of SKA. With a numerical algorithm, we\nhave found that in this case, the full GR value depends only on the semi-major\naxis of the relative orbit and on the mass of the black hole companion. If the\nmass of the latter is sufficiently large ($100 M_{\\odot}$), the maximum\ndifference between the two approaches is significant ($\\sim10^{-7}$ s) even for\nlarge binaries ($\\sim10^{16}$ cm), and increases up to $\\sim 10^{-4}$ s when\nthe mass is $10^5 M_{\\odot}$. We also consider relativistic corrections to the\norbital motion, and discover that they can strongly affect the value of the\npropagation delay. We conclude that in the future, post-Newtonian formulas\nshould be replaced with a more accurate approach in these systems, especially\nin view of future discoveries made by new large telescopes such as SKA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pulsar timing technique, which compares the observed arrival times of\nelectromagnetic radiation from a pulsar with the predicted arrival times\nderived from a theoretical model of the pulsar system, is used in pulsar\nastronomy to infer a multitude of physical information and to constrain\npossible corrections to General Relativity (GR). The propagation delay is\nusually computed using formulas based on a post-Newtonian approach, for both\nthe light trajectory and the orbital motion. However, evidence has recently\nemerged that this approximation may no longer be sufficient when the companion\nobject is a supermassive black hole; deviations from a full GR computation of\nthe propagation delay can reach a few seconds. In this paper, we analyze the\ncase of binary pulsars with a stellar or intermediate black hole companion,\nwhose discovery and timing are key goals of SKA. With a numerical algorithm, we\nhave found that in this case, the full GR value depends only on the semi-major\naxis of the relative orbit and on the mass of the black hole companion. If the\nmass of the latter is sufficiently large ($100 M_{\\odot}$), the maximum\ndifference between the two approaches is significant ($\\sim10^{-7}$ s) even for\nlarge binaries ($\\sim10^{16}$ cm), and increases up to $\\sim 10^{-4}$ s when\nthe mass is $10^5 M_{\\odot}$. We also consider relativistic corrections to the\norbital motion, and discover that they can strongly affect the value of the\npropagation delay. We conclude that in the future, post-Newtonian formulas\nshould be replaced with a more accurate approach in these systems, especially\nin view of future discoveries made by new large telescopes such as SKA."
                },
                "authors": [
                    {
                        "name": "Amodio Carleo"
                    },
                    {
                        "name": "Delphine Perrodin"
                    },
                    {
                        "name": "Andrea Possenti"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Possenti"
                },
                "author": "Andrea Possenti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10296v1",
                "updated": "2024-12-13T17:31:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    31,
                    50,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:31:50Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    31,
                    50,
                    4,
                    348,
                    0
                ],
                "title": "My Statistics is Better than Yours",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "My Statistics is Better than Yours"
                },
                "summary": "When performing data analysis, a researcher often faces a choice between\nFrequentist and Bayesian approaches, each of which offers distinct principles\nand prescribed methods. Frequentism operates under the assumption of repeated\nsampling, aiming for so-called objective inferences through significance tests\nand efficient estimators. Bayesianism, on the other hand, integrates a\nresearcher's prior beliefs about a hypothesis while updating these with new\nevidence to produce posterior distributions. Despite the technical rigour of\nboth methods, neither approach appears universally applicable. A single,\n\"correct\" statistical school may seem like an objective ideal. However, we will\nsee that it becomes impossible to choose between the two schools, even when we\ntry our best to fulfil this ideal.\n  Instead, this essay proposes a context-dependent approach to guide the\nselection of an appropriate statistical school. This approach style is not\nnovel. Worsdale & Wright (2021) presents Douglas (2004)'s \"operational\"\nobjectivity in the search for an objective gender inequality index. The authors\npoint out the worrying obsession researchers have to find a single universal\ntrue measure of gender inequality. Rather, Worsdale & Wright (2021) recommend\ntaking the research goals and context into \"objectivity\", making a\ncontext-dependent objectivity. I take the same idea and apply it to the search\nfor a normative system of statistics: contextualizing statistical norms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When performing data analysis, a researcher often faces a choice between\nFrequentist and Bayesian approaches, each of which offers distinct principles\nand prescribed methods. Frequentism operates under the assumption of repeated\nsampling, aiming for so-called objective inferences through significance tests\nand efficient estimators. Bayesianism, on the other hand, integrates a\nresearcher's prior beliefs about a hypothesis while updating these with new\nevidence to produce posterior distributions. Despite the technical rigour of\nboth methods, neither approach appears universally applicable. A single,\n\"correct\" statistical school may seem like an objective ideal. However, we will\nsee that it becomes impossible to choose between the two schools, even when we\ntry our best to fulfil this ideal.\n  Instead, this essay proposes a context-dependent approach to guide the\nselection of an appropriate statistical school. This approach style is not\nnovel. Worsdale & Wright (2021) presents Douglas (2004)'s \"operational\"\nobjectivity in the search for an objective gender inequality index. The authors\npoint out the worrying obsession researchers have to find a single universal\ntrue measure of gender inequality. Rather, Worsdale & Wright (2021) recommend\ntaking the research goals and context into \"objectivity\", making a\ncontext-dependent objectivity. I take the same idea and apply it to the search\nfor a normative system of statistics: contextualizing statistical norms."
                },
                "authors": [
                    {
                        "name": "Simon Benhaïem"
                    }
                ],
                "author_detail": {
                    "name": "Simon Benhaïem"
                },
                "author": "Simon Benhaïem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.OT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10295v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10295v1",
                "updated": "2024-12-13T17:29:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    29,
                    56,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:29:56Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    29,
                    56,
                    4,
                    348,
                    0
                ],
                "title": "The Stein-log-Sobolev inequality and the exponential rate of convergence\n  for the continuous Stein variational gradient descent method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Stein-log-Sobolev inequality and the exponential rate of convergence\n  for the continuous Stein variational gradient descent method"
                },
                "summary": "The Stein Variational Gradient Descent method is a variational inference\nmethod in statistics that has recently received a lot of attention. The method\nprovides a deterministic approximation of the target distribution, by\nintroducing a nonlocal interaction with a kernel. Despite the significant\ninterest, the exponential rate of convergence for the continuous method has\nremained an open problem, due to the difficulty of establishing the related\nso-called Stein-log-Sobolev inequality. Here, we prove that the inequality is\nsatisfied for each space dimension and every kernel whose Fourier transform has\na quadratic decay at infinity and is locally bounded away from zero and\ninfinity. Moreover, we construct weak solutions to the related PDE satisfying\nexponential rate of decay towards the equilibrium. The main novelty in our\napproach is to interpret the Stein-Fisher information, also called the squared\nStein discrepancy, as a duality pairing between $H^{-1}(\\mathbb{R}^d)$ and\n$H^{1}(\\mathbb{R}^d)$, which allows us to employ the Fourier transform. We also\nprovide several examples of kernels for which the Stein-log-Sobolev inequality\nfails, partially showing the necessity of our assumptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Stein Variational Gradient Descent method is a variational inference\nmethod in statistics that has recently received a lot of attention. The method\nprovides a deterministic approximation of the target distribution, by\nintroducing a nonlocal interaction with a kernel. Despite the significant\ninterest, the exponential rate of convergence for the continuous method has\nremained an open problem, due to the difficulty of establishing the related\nso-called Stein-log-Sobolev inequality. Here, we prove that the inequality is\nsatisfied for each space dimension and every kernel whose Fourier transform has\na quadratic decay at infinity and is locally bounded away from zero and\ninfinity. Moreover, we construct weak solutions to the related PDE satisfying\nexponential rate of decay towards the equilibrium. The main novelty in our\napproach is to interpret the Stein-Fisher information, also called the squared\nStein discrepancy, as a duality pairing between $H^{-1}(\\mathbb{R}^d)$ and\n$H^{1}(\\mathbb{R}^d)$, which allows us to employ the Fourier transform. We also\nprovide several examples of kernels for which the Stein-log-Sobolev inequality\nfails, partially showing the necessity of our assumptions."
                },
                "authors": [
                    {
                        "name": "José A. Carrillo"
                    },
                    {
                        "name": "Jakub Skrzeczkowski"
                    },
                    {
                        "name": "Jethro Warnett"
                    }
                ],
                "author_detail": {
                    "name": "Jethro Warnett"
                },
                "author": "Jethro Warnett",
                "arxiv_comment": "65 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10295v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10295v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "35Q62, 35Q68, 35B40, 62-08, 62D05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06223v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06223v3",
                "updated": "2024-12-13T17:29:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    29,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-10T05:26:53Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    5,
                    26,
                    53,
                    1,
                    254,
                    0
                ],
                "title": "Enhancing Temporal Understanding in Audio Question Answering for Large\n  Audio Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Temporal Understanding in Audio Question Answering for Large\n  Audio Language Models"
                },
                "summary": "The Audio Question Answering (AQA) task includes audio event classification,\naudio captioning, and open-ended reasoning. Recently, AQA has garnered\nattention due to the advent of Large Audio Language Models (LALMs). Current\nliterature focuses on constructing LALMs by integrating audio encoders with\ntext-only Large Language Models (LLMs) through a projection module. While LALMs\nexcel in general audio understanding, they are limited in temporal reasoning,\nwhich may hinder their commercial applications and on-device deployment. This\npaper addresses these challenges and limitations in audio temporal reasoning.\nFirst, we introduce a data augmentation technique for generating reliable audio\ntemporal questions and answers using an LLM. Second, we perform a further\nfine-tuning of an existing baseline using curriculum learning strategy to\nspecialize in temporal reasoning without compromising performance on fine-tuned\ntasks. We demonstrate the performance of our model using state-of-the-art LALMs\non public audio benchmark datasets. Third, we implement our AQA model on-device\nlocally and investigate its CPU inference for edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Audio Question Answering (AQA) task includes audio event classification,\naudio captioning, and open-ended reasoning. Recently, AQA has garnered\nattention due to the advent of Large Audio Language Models (LALMs). Current\nliterature focuses on constructing LALMs by integrating audio encoders with\ntext-only Large Language Models (LLMs) through a projection module. While LALMs\nexcel in general audio understanding, they are limited in temporal reasoning,\nwhich may hinder their commercial applications and on-device deployment. This\npaper addresses these challenges and limitations in audio temporal reasoning.\nFirst, we introduce a data augmentation technique for generating reliable audio\ntemporal questions and answers using an LLM. Second, we perform a further\nfine-tuning of an existing baseline using curriculum learning strategy to\nspecialize in temporal reasoning without compromising performance on fine-tuned\ntasks. We demonstrate the performance of our model using state-of-the-art LALMs\non public audio benchmark datasets. Third, we implement our AQA model on-device\nlocally and investigate its CPU inference for edge applications."
                },
                "authors": [
                    {
                        "name": "Arvind Krishna Sridhar"
                    },
                    {
                        "name": "Yinyi Guo"
                    },
                    {
                        "name": "Erik Visser"
                    }
                ],
                "author_detail": {
                    "name": "Erik Visser"
                },
                "author": "Erik Visser",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06223v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06223v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10281v1",
                "updated": "2024-12-13T17:03:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    3,
                    56,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:03:56Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    3,
                    56,
                    4,
                    348,
                    0
                ],
                "title": "One world, one opinion? The superstar effect in LLM responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One world, one opinion? The superstar effect in LLM responses"
                },
                "summary": "As large language models (LLMs) are shaping the way information is shared and\naccessed online, their opinions have the potential to influence a wide\naudience. This study examines who the LLMs view as the most prominent figures\nacross various fields, using prompts in ten different languages to explore the\ninfluence of linguistic diversity. Our findings reveal low diversity in\nresponses, with a small number of figures dominating recognition across\nlanguages (also known as the \"superstar effect\"). These results highlight the\nrisk of narrowing global knowledge representation when LLMs retrieve subjective\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are shaping the way information is shared and\naccessed online, their opinions have the potential to influence a wide\naudience. This study examines who the LLMs view as the most prominent figures\nacross various fields, using prompts in ten different languages to explore the\ninfluence of linguistic diversity. Our findings reveal low diversity in\nresponses, with a small number of figures dominating recognition across\nlanguages (also known as the \"superstar effect\"). These results highlight the\nrisk of narrowing global knowledge representation when LLMs retrieve subjective\ninformation."
                },
                "authors": [
                    {
                        "name": "Sofie Goethals"
                    },
                    {
                        "name": "Lauren Rhue"
                    }
                ],
                "author_detail": {
                    "name": "Lauren Rhue"
                },
                "author": "Lauren Rhue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10271v1",
                "updated": "2024-12-13T16:46:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    46,
                    3,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T16:46:03Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    46,
                    3,
                    4,
                    348,
                    0
                ],
                "title": "Benchmarking Linguistic Diversity of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Linguistic Diversity of Large Language Models"
                },
                "summary": "The development and evaluation of Large Language Models (LLMs) has primarily\nfocused on their task-solving capabilities, with recent models even surpassing\nhuman performance in some areas. However, this focus often neglects whether\nmachine-generated language matches the human level of diversity, in terms of\nvocabulary choice, syntactic construction, and expression of meaning, raising\nquestions about whether the fundamentals of language generation have been fully\naddressed. This paper emphasizes the importance of examining the preservation\nof human linguistic richness by language models, given the concerning surge in\nonline content produced or aided by LLMs. We propose a comprehensive framework\nfor evaluating LLMs from various linguistic diversity perspectives including\nlexical, syntactic, and semantic dimensions. Using this framework, we benchmark\nseveral state-of-the-art LLMs across all diversity dimensions, and conduct an\nin-depth case study for syntactic diversity. Finally, we analyze how different\ndevelopment and deployment choices impact the linguistic diversity of LLM\noutputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development and evaluation of Large Language Models (LLMs) has primarily\nfocused on their task-solving capabilities, with recent models even surpassing\nhuman performance in some areas. However, this focus often neglects whether\nmachine-generated language matches the human level of diversity, in terms of\nvocabulary choice, syntactic construction, and expression of meaning, raising\nquestions about whether the fundamentals of language generation have been fully\naddressed. This paper emphasizes the importance of examining the preservation\nof human linguistic richness by language models, given the concerning surge in\nonline content produced or aided by LLMs. We propose a comprehensive framework\nfor evaluating LLMs from various linguistic diversity perspectives including\nlexical, syntactic, and semantic dimensions. Using this framework, we benchmark\nseveral state-of-the-art LLMs across all diversity dimensions, and conduct an\nin-depth case study for syntactic diversity. Finally, we analyze how different\ndevelopment and deployment choices impact the linguistic diversity of LLM\noutputs."
                },
                "authors": [
                    {
                        "name": "Yanzhu Guo"
                    },
                    {
                        "name": "Guokan Shang"
                    },
                    {
                        "name": "Chloé Clavel"
                    }
                ],
                "author_detail": {
                    "name": "Chloé Clavel"
                },
                "author": "Chloé Clavel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10270v1",
                "updated": "2024-12-13T16:45:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    45,
                    49,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T16:45:49Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    45,
                    49,
                    4,
                    348,
                    0
                ],
                "title": "Cultural Evolution of Cooperation among LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural Evolution of Cooperation among LLM Agents"
                },
                "summary": "Large language models (LLMs) provide a compelling foundation for building\ngenerally-capable AI agents. These agents may soon be deployed at scale in the\nreal world, representing the interests of individual humans (e.g., AI\nassistants) or groups of humans (e.g., AI-accelerated corporations). At\npresent, relatively little is known about the dynamics of multiple LLM agents\ninteracting over many generations of iterative deployment. In this paper, we\nexamine whether a \"society\" of LLM agents can learn mutually beneficial social\nnorms in the face of incentives to defect, a distinctive feature of human\nsociality that is arguably crucial to the success of civilization. In\nparticular, we study the evolution of indirect reciprocity across generations\nof LLM agents playing a classic iterated Donor Game in which agents can observe\nthe recent behavior of their peers. We find that the evolution of cooperation\ndiffers markedly across base models, with societies of Claude 3.5 Sonnet agents\nachieving significantly higher average scores than Gemini 1.5 Flash, which, in\nturn, outperforms GPT-4o. Further, Claude 3.5 Sonnet can make use of an\nadditional mechanism for costly punishment to achieve yet higher scores, while\nGemini 1.5 Flash and GPT-4o fail to do so. For each model class, we also\nobserve variation in emergent behavior across random seeds, suggesting an\nunderstudied sensitive dependence on initial conditions. We suggest that our\nevaluation regime could inspire an inexpensive and informative new class of LLM\nbenchmarks, focussed on the implications of LLM agent deployment for the\ncooperative infrastructure of society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) provide a compelling foundation for building\ngenerally-capable AI agents. These agents may soon be deployed at scale in the\nreal world, representing the interests of individual humans (e.g., AI\nassistants) or groups of humans (e.g., AI-accelerated corporations). At\npresent, relatively little is known about the dynamics of multiple LLM agents\ninteracting over many generations of iterative deployment. In this paper, we\nexamine whether a \"society\" of LLM agents can learn mutually beneficial social\nnorms in the face of incentives to defect, a distinctive feature of human\nsociality that is arguably crucial to the success of civilization. In\nparticular, we study the evolution of indirect reciprocity across generations\nof LLM agents playing a classic iterated Donor Game in which agents can observe\nthe recent behavior of their peers. We find that the evolution of cooperation\ndiffers markedly across base models, with societies of Claude 3.5 Sonnet agents\nachieving significantly higher average scores than Gemini 1.5 Flash, which, in\nturn, outperforms GPT-4o. Further, Claude 3.5 Sonnet can make use of an\nadditional mechanism for costly punishment to achieve yet higher scores, while\nGemini 1.5 Flash and GPT-4o fail to do so. For each model class, we also\nobserve variation in emergent behavior across random seeds, suggesting an\nunderstudied sensitive dependence on initial conditions. We suggest that our\nevaluation regime could inspire an inexpensive and informative new class of LLM\nbenchmarks, focussed on the implications of LLM agent deployment for the\ncooperative infrastructure of society."
                },
                "authors": [
                    {
                        "name": "Aron Vallinder"
                    },
                    {
                        "name": "Edward Hughes"
                    }
                ],
                "author_detail": {
                    "name": "Edward Hughes"
                },
                "author": "Edward Hughes",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10267v1",
                "updated": "2024-12-13T16:37:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    37,
                    20,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T16:37:20Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    37,
                    20,
                    4,
                    348,
                    0
                ],
                "title": "Does Multiple Choice Have a Future in the Age of Generative AI? A\n  Posttest-only RCT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Multiple Choice Have a Future in the Age of Generative AI? A\n  Posttest-only RCT"
                },
                "summary": "The role of multiple-choice questions (MCQs) as effective learning tools has\nbeen debated in past research. While MCQs are widely used due to their ease in\ngrading, open response questions are increasingly used for instruction, given\nadvances in large language models (LLMs) for automated grading. This study\nevaluates MCQs effectiveness relative to open-response questions, both\nindividually and in combination, on learning. These activities are embedded\nwithin six tutor lessons on advocacy. Using a posttest-only randomized control\ndesign, we compare the performance of 234 tutors (790 lesson completions)\nacross three conditions: MCQ only, open response only, and a combination of\nboth. We find no significant learning differences across conditions at\nposttest, but tutors in the MCQ condition took significantly less time to\ncomplete instruction. These findings suggest that MCQs are as effective, and\nmore efficient, than open response tasks for learning when practice time is\nlimited. To further enhance efficiency, we autograded open responses using\nGPT-4o and GPT-4-turbo. GPT models demonstrate proficiency for purposes of\nlow-stakes assessment, though further research is needed for broader use. This\nstudy contributes a dataset of lesson log data, human annotation rubrics, and\nLLM prompts to promote transparency and reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The role of multiple-choice questions (MCQs) as effective learning tools has\nbeen debated in past research. While MCQs are widely used due to their ease in\ngrading, open response questions are increasingly used for instruction, given\nadvances in large language models (LLMs) for automated grading. This study\nevaluates MCQs effectiveness relative to open-response questions, both\nindividually and in combination, on learning. These activities are embedded\nwithin six tutor lessons on advocacy. Using a posttest-only randomized control\ndesign, we compare the performance of 234 tutors (790 lesson completions)\nacross three conditions: MCQ only, open response only, and a combination of\nboth. We find no significant learning differences across conditions at\nposttest, but tutors in the MCQ condition took significantly less time to\ncomplete instruction. These findings suggest that MCQs are as effective, and\nmore efficient, than open response tasks for learning when practice time is\nlimited. To further enhance efficiency, we autograded open responses using\nGPT-4o and GPT-4-turbo. GPT models demonstrate proficiency for purposes of\nlow-stakes assessment, though further research is needed for broader use. This\nstudy contributes a dataset of lesson log data, human annotation rubrics, and\nLLM prompts to promote transparency and reproducibility."
                },
                "authors": [
                    {
                        "name": "Danielle R. Thomas"
                    },
                    {
                        "name": "Conrad Borchers"
                    },
                    {
                        "name": "Sanjit Kakarla"
                    },
                    {
                        "name": "Jionghao Lin"
                    },
                    {
                        "name": "Shambhavi Bhushan"
                    },
                    {
                        "name": "Boyuan Guo"
                    },
                    {
                        "name": "Erin Gatz"
                    },
                    {
                        "name": "Kenneth R. Koedinger"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth R. Koedinger"
                },
                "author": "Kenneth R. Koedinger",
                "arxiv_doi": "10.1145/3706468.3706530",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706468.3706530",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.10267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Full research paper accepted to Learning Analytics and Knowledge (LAK\n  2025)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10266v1",
                "updated": "2024-12-13T16:34:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    34,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T16:34:39Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    34,
                    39,
                    4,
                    348,
                    0
                ],
                "title": "Reasoner Outperforms: Generative Stance Detection with Rationalization\n  for Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoner Outperforms: Generative Stance Detection with Rationalization\n  for Social Media"
                },
                "summary": "Stance detection is crucial for fostering a human-centric Web by analyzing\nuser-generated content to identify biases and harmful narratives that undermine\ntrust. With the development of Large Language Models (LLMs), existing\napproaches treat stance detection as a classification problem, providing robust\nmethodologies for modeling complex group interactions and advancing\ncapabilities in natural language tasks. However, these methods often lack\ninterpretability, limiting their ability to offer transparent and\nunderstandable justifications for predictions. This study adopts a generative\napproach, where stance predictions include explicit, interpretable rationales,\nand integrates them into smaller language models through single-task and\nmultitask learning. We find that incorporating reasoning into stance detection\nenables the smaller model (FlanT5) to outperform GPT-3.5's zero-shot\nperformance, achieving an improvement of up to 9.57%. Moreover, our results\nshow that reasoning capabilities enhance multitask learning performance but may\nreduce effectiveness in single-task settings. Crucially, we demonstrate that\nfaithful rationales improve rationale distillation into SLMs, advancing efforts\nto build interpretable, trustworthy systems for addressing discrimination,\nfostering trust, and promoting equitable engagement on social media.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stance detection is crucial for fostering a human-centric Web by analyzing\nuser-generated content to identify biases and harmful narratives that undermine\ntrust. With the development of Large Language Models (LLMs), existing\napproaches treat stance detection as a classification problem, providing robust\nmethodologies for modeling complex group interactions and advancing\ncapabilities in natural language tasks. However, these methods often lack\ninterpretability, limiting their ability to offer transparent and\nunderstandable justifications for predictions. This study adopts a generative\napproach, where stance predictions include explicit, interpretable rationales,\nand integrates them into smaller language models through single-task and\nmultitask learning. We find that incorporating reasoning into stance detection\nenables the smaller model (FlanT5) to outperform GPT-3.5's zero-shot\nperformance, achieving an improvement of up to 9.57%. Moreover, our results\nshow that reasoning capabilities enhance multitask learning performance but may\nreduce effectiveness in single-task settings. Crucially, we demonstrate that\nfaithful rationales improve rationale distillation into SLMs, advancing efforts\nto build interpretable, trustworthy systems for addressing discrimination,\nfostering trust, and promoting equitable engagement on social media."
                },
                "authors": [
                    {
                        "name": "Jiaqing Yuan"
                    },
                    {
                        "name": "Ruijie Xi"
                    },
                    {
                        "name": "Munindar P. Singh"
                    }
                ],
                "author_detail": {
                    "name": "Munindar P. Singh"
                },
                "author": "Munindar P. Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10257v1",
                "updated": "2024-12-13T16:26:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    26,
                    34,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T16:26:34Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    26,
                    34,
                    4,
                    348,
                    0
                ],
                "title": "Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in\n  Large Language Models"
                },
                "summary": "The sheer scale of data required to train modern large language models (LLMs)\nposes significant risks, as models are likely to gain knowledge of sensitive\ntopics such as bio-security, as well the ability to replicate copyrighted\nworks. Methods designed to remove such knowledge must do so from all prompt\ndirections, in a multi-lingual capacity and without degrading general model\nperformance. To this end, we introduce the targeted angular reversal (TARS)\nmethod of knowledge removal from LLMs. The TARS method firstly leverages the\nLLM in combination with a detailed prompt to aggregate information about a\nselected concept in the internal representation space of the LLM. It then\nrefines this approximate concept vector to trigger the concept token with high\nprobability, by perturbing the approximate concept vector with noise and\ntransforming it into token scores with the language model head. The feedforward\nweight vectors in the LLM which operate directly on the internal representation\nspace, and have the highest cosine similarity with this targeting vector, are\nthen replaced by a reversed targeting vector, thus limiting the ability of the\nconcept to propagate through the model. The modularity of the TARS method\nallows for a sequential removal of concepts from Llama 3.1 8B, such as the\nfamous literary detective Sherlock Holmes, and the planet Saturn. It is\ndemonstrated that the probability of triggering target concepts can be reduced\nto 0.00 with as few as 1 TARS edit, whilst simultaneously removing the\nknowledge bi-directionally. Moreover, knowledge is shown to be removed across\nall languages despite only being targeted in English. Importantly, TARS has\nminimal impact on the general model capabilities, as after removing 5 diverse\nconcepts in a modular fashion, there is minimal KL divergence in the next token\nprobabilities of the LLM on large corpora of Wikipedia text (median of 0.002).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sheer scale of data required to train modern large language models (LLMs)\nposes significant risks, as models are likely to gain knowledge of sensitive\ntopics such as bio-security, as well the ability to replicate copyrighted\nworks. Methods designed to remove such knowledge must do so from all prompt\ndirections, in a multi-lingual capacity and without degrading general model\nperformance. To this end, we introduce the targeted angular reversal (TARS)\nmethod of knowledge removal from LLMs. The TARS method firstly leverages the\nLLM in combination with a detailed prompt to aggregate information about a\nselected concept in the internal representation space of the LLM. It then\nrefines this approximate concept vector to trigger the concept token with high\nprobability, by perturbing the approximate concept vector with noise and\ntransforming it into token scores with the language model head. The feedforward\nweight vectors in the LLM which operate directly on the internal representation\nspace, and have the highest cosine similarity with this targeting vector, are\nthen replaced by a reversed targeting vector, thus limiting the ability of the\nconcept to propagate through the model. The modularity of the TARS method\nallows for a sequential removal of concepts from Llama 3.1 8B, such as the\nfamous literary detective Sherlock Holmes, and the planet Saturn. It is\ndemonstrated that the probability of triggering target concepts can be reduced\nto 0.00 with as few as 1 TARS edit, whilst simultaneously removing the\nknowledge bi-directionally. Moreover, knowledge is shown to be removed across\nall languages despite only being targeted in English. Importantly, TARS has\nminimal impact on the general model capabilities, as after removing 5 diverse\nconcepts in a modular fashion, there is minimal KL divergence in the next token\nprobabilities of the LLM on large corpora of Wikipedia text (median of 0.002)."
                },
                "authors": [
                    {
                        "name": "Harry J. Davies"
                    },
                    {
                        "name": "Giorgos Iacovides"
                    },
                    {
                        "name": "Danilo P. Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo P. Mandic"
                },
                "author": "Danilo P. Mandic",
                "arxiv_comment": "14 pages, 5 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10246v1",
                "updated": "2024-12-13T16:14:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    14,
                    49,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T16:14:49Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    14,
                    49,
                    4,
                    348,
                    0
                ],
                "title": "Detecting LLM Hallucination Through Layer-wise Information Deficiency:\n  Analysis of Unanswerable Questions and Ambiguous Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting LLM Hallucination Through Layer-wise Information Deficiency:\n  Analysis of Unanswerable Questions and Ambiguous Prompts"
                },
                "summary": "Large language models (LLMs) frequently generate confident yet inaccurate\nresponses, introducing significant risks for deployment in safety-critical\ndomains. We present a novel approach to detecting model hallucination through\nsystematic analysis of information flow across model layers when processing\ninputs with insufficient or ambiguous context. Our investigation reveals that\nhallucination manifests as usable information deficiencies in inter-layer\ntransmissions. While existing approaches primarily focus on final-layer output\nanalysis, we demonstrate that tracking cross-layer information dynamics\n($\\mathcal{L}$I) provides robust indicators of model reliability, accounting\nfor both information gain and loss during computation. $\\mathcal{L}$I improves\nmodel reliability by immediately integrating with universal LLMs without\nadditional training or architectural modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) frequently generate confident yet inaccurate\nresponses, introducing significant risks for deployment in safety-critical\ndomains. We present a novel approach to detecting model hallucination through\nsystematic analysis of information flow across model layers when processing\ninputs with insufficient or ambiguous context. Our investigation reveals that\nhallucination manifests as usable information deficiencies in inter-layer\ntransmissions. While existing approaches primarily focus on final-layer output\nanalysis, we demonstrate that tracking cross-layer information dynamics\n($\\mathcal{L}$I) provides robust indicators of model reliability, accounting\nfor both information gain and loss during computation. $\\mathcal{L}$I improves\nmodel reliability by immediately integrating with universal LLMs without\nadditional training or architectural modifications."
                },
                "authors": [
                    {
                        "name": "Hazel Kim"
                    },
                    {
                        "name": "Adel Bibi"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Yarin Gal"
                    }
                ],
                "author_detail": {
                    "name": "Yarin Gal"
                },
                "author": "Yarin Gal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10245v1",
                "updated": "2024-12-13T16:14:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    14,
                    27,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T16:14:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    14,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "Regression trees for nonparametric diagnostics of sequential positivity\n  violations in longitudinal causal inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression trees for nonparametric diagnostics of sequential positivity\n  violations in longitudinal causal inference"
                },
                "summary": "Sequential positivity is often a necessary assumption for drawing causal\ninferences, such as through marginal structural modeling. Unfortunately,\nverification of this assumption can be challenging because it usually relies on\nmultiple parametric propensity score models, unlikely all correctly specified.\nTherefore, we propose a new algorithm, called \"sequential Positivity Regression\nTree\" (sPoRT), to check this assumption with greater ease under either static\nor dynamic treatment strategies. This algorithm also identifies the subgroups\nfound to be violating this assumption, allowing for insights about the nature\nof the violations and potential solutions. We first present different versions\nof sPoRT based on either stratifying or pooling over time. Finally, we\nillustrate its use in a real-life application of HIV-positive children in\nSouthern Africa with and without pooling over time. An R notebook showing how\nto use sPoRT is available at github.com/ArthurChatton/sPoRT-notebook.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential positivity is often a necessary assumption for drawing causal\ninferences, such as through marginal structural modeling. Unfortunately,\nverification of this assumption can be challenging because it usually relies on\nmultiple parametric propensity score models, unlikely all correctly specified.\nTherefore, we propose a new algorithm, called \"sequential Positivity Regression\nTree\" (sPoRT), to check this assumption with greater ease under either static\nor dynamic treatment strategies. This algorithm also identifies the subgroups\nfound to be violating this assumption, allowing for insights about the nature\nof the violations and potential solutions. We first present different versions\nof sPoRT based on either stratifying or pooling over time. Finally, we\nillustrate its use in a real-life application of HIV-positive children in\nSouthern Africa with and without pooling over time. An R notebook showing how\nto use sPoRT is available at github.com/ArthurChatton/sPoRT-notebook."
                },
                "authors": [
                    {
                        "name": "Arthur Chatton"
                    },
                    {
                        "name": "Michael Schomaker"
                    },
                    {
                        "name": "Miguel-Angel Luque-Fernandez"
                    },
                    {
                        "name": "Robert W. Platt"
                    },
                    {
                        "name": "Mireille E. Schnitzer"
                    }
                ],
                "author_detail": {
                    "name": "Mireille E. Schnitzer"
                },
                "author": "Mireille E. Schnitzer",
                "arxiv_comment": "10 pages, 4 Tables, 2 Figures. Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10244v1",
                "updated": "2024-12-13T16:13:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    35,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T16:13:35Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    35,
                    4,
                    348,
                    0
                ],
                "title": "Efficient Continual Pre-training of LLMs for Low-resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Continual Pre-training of LLMs for Low-resource Languages"
                },
                "summary": "Open-source Large Language models (OsLLMs) propel the democratization of\nnatural language research by giving the flexibility to augment or update model\nparameters for performance improvement. Nevertheless, like proprietary LLMs,\nOs-LLMs offer poorer performance on low-resource languages (LRLs) than\nhigh-resource languages (HRLs), owing to smaller amounts of training data and\nunderrepresented vocabulary. On the other hand, continual pre-training (CPT)\nwith large amounts of language-specific data is a costly proposition in terms\nof data acquisition and computational resources. Our goal is to drastically\nreduce CPT cost. To that end, we first develop a new algorithm to select a\nsubset of texts from a larger corpus. We show the effectiveness of our\ntechnique using very little CPT data. In search of further improvement, we\ndesign a new algorithm to select tokens to include in the LLM vocabulary. We\nexperiment with the recent Llama-3 model and nine Indian languages with diverse\nscripts and extent of resource availability. For evaluation, we use\nIndicGenBench, a generation task benchmark dataset for Indic languages. We\nexperiment with various CPT corpora and augmented vocabulary size and offer\ninsights across language families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source Large Language models (OsLLMs) propel the democratization of\nnatural language research by giving the flexibility to augment or update model\nparameters for performance improvement. Nevertheless, like proprietary LLMs,\nOs-LLMs offer poorer performance on low-resource languages (LRLs) than\nhigh-resource languages (HRLs), owing to smaller amounts of training data and\nunderrepresented vocabulary. On the other hand, continual pre-training (CPT)\nwith large amounts of language-specific data is a costly proposition in terms\nof data acquisition and computational resources. Our goal is to drastically\nreduce CPT cost. To that end, we first develop a new algorithm to select a\nsubset of texts from a larger corpus. We show the effectiveness of our\ntechnique using very little CPT data. In search of further improvement, we\ndesign a new algorithm to select tokens to include in the LLM vocabulary. We\nexperiment with the recent Llama-3 model and nine Indian languages with diverse\nscripts and extent of resource availability. For evaluation, we use\nIndicGenBench, a generation task benchmark dataset for Indic languages. We\nexperiment with various CPT corpora and augmented vocabulary size and offer\ninsights across language families."
                },
                "authors": [
                    {
                        "name": "Arijit Nag"
                    },
                    {
                        "name": "Soumen Chakrabarti"
                    },
                    {
                        "name": "Animesh Mukherjee"
                    },
                    {
                        "name": "Niloy Ganguly"
                    }
                ],
                "author_detail": {
                    "name": "Niloy Ganguly"
                },
                "author": "Niloy Ganguly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10242v1",
                "updated": "2024-12-13T16:10:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    10,
                    54,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T16:10:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    10,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Equation of State Independent Determination on the Radius of a 1.4\n  $M_{\\odot}$ Neutron Star Using Mass-Radius Measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Equation of State Independent Determination on the Radius of a 1.4\n  $M_{\\odot}$ Neutron Star Using Mass-Radius Measurements"
                },
                "summary": "Traditional methods for determining the radius of a 1.4 $M_{\\odot}$ neutron\nstar ($R_{1.4}$) rely on specific equations of state (EOS) models that describe\nvarious types of dense nuclear matter. This dependence on EOS models can\nintroduce substantial systematic uncertainties, which may exceed the\nmeasurement uncertainties when constraining $R_{1.4}$. In this study, we\nexplore a novel approach to constraining $R_{1.4}$ using data from NICER\nobservations of PSR J0030+0451 (J0030) and PSR J0437-4715 (J0437). However,\nthis work presents a more data-driven analysis framework, substantially\ndecreasing the need for EOS assumptions. By analyzing the Mass-Radius\nmeasurements of these two neutron stars, we infer $R_{1.4}$ using statistical\nmethods based mostly on observational data. We examine various hotspot\nconfigurations for J0030, along with new J0437 observations, and their effects\non the inferred radius. Our results are consistent with X-ray timing,\ngravitational wave, and nuclear physics constraints, while avoiding EOS-related\nbiases. The same method has also been applied to a simulated mass-radius\ndataset, based on our knowledge of future X-ray telescopes, demonstrating the\nmodel's ability to recover the injected $R_{1.4}$ value in certain cases. This\nmethod provides a data-driven pathway for extracting neutron star properties\nand offers a new approach for future observational efforts in neutron star\nastrophysics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional methods for determining the radius of a 1.4 $M_{\\odot}$ neutron\nstar ($R_{1.4}$) rely on specific equations of state (EOS) models that describe\nvarious types of dense nuclear matter. This dependence on EOS models can\nintroduce substantial systematic uncertainties, which may exceed the\nmeasurement uncertainties when constraining $R_{1.4}$. In this study, we\nexplore a novel approach to constraining $R_{1.4}$ using data from NICER\nobservations of PSR J0030+0451 (J0030) and PSR J0437-4715 (J0437). However,\nthis work presents a more data-driven analysis framework, substantially\ndecreasing the need for EOS assumptions. By analyzing the Mass-Radius\nmeasurements of these two neutron stars, we infer $R_{1.4}$ using statistical\nmethods based mostly on observational data. We examine various hotspot\nconfigurations for J0030, along with new J0437 observations, and their effects\non the inferred radius. Our results are consistent with X-ray timing,\ngravitational wave, and nuclear physics constraints, while avoiding EOS-related\nbiases. The same method has also been applied to a simulated mass-radius\ndataset, based on our knowledge of future X-ray telescopes, demonstrating the\nmodel's ability to recover the injected $R_{1.4}$ value in certain cases. This\nmethod provides a data-driven pathway for extracting neutron star properties\nand offers a new approach for future observational efforts in neutron star\nastrophysics."
                },
                "authors": [
                    {
                        "name": "Chun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chun Huang"
                },
                "author": "Chun Huang",
                "arxiv_comment": "Accepted publication in ApJ Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10233v1",
                "updated": "2024-12-13T16:02:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    2,
                    38,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T16:02:38Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    2,
                    38,
                    4,
                    348,
                    0
                ],
                "title": "Nonequilibrium Fluctuation-Response Relations for State Observables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonequilibrium Fluctuation-Response Relations for State Observables"
                },
                "summary": "Time-integrated state observables, which quantify the fraction of time spent\nin a specific pool of states, are important in many fields, such as chemical\nsensing or theory of fluorescence spectroscopy. We derive exact identities,\ncalled Fluctuation-Response Relations (FRRs), that connect the fluctuations of\nsuch observables to their response to external perturbations in nonequilibrium\nsteady state of Markov jump processes. Using these results, we derive novel\nupper and lower bounds for fluctuations. We further demonstrate their\napplicability for simplifying calculations of fluctuations in large Markov\nnetworks, use them to explain the physical origin of positive and negative\ncorrelations of occupation times in a double quantum dot device, and discuss\ntheir relevance for model inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-integrated state observables, which quantify the fraction of time spent\nin a specific pool of states, are important in many fields, such as chemical\nsensing or theory of fluorescence spectroscopy. We derive exact identities,\ncalled Fluctuation-Response Relations (FRRs), that connect the fluctuations of\nsuch observables to their response to external perturbations in nonequilibrium\nsteady state of Markov jump processes. Using these results, we derive novel\nupper and lower bounds for fluctuations. We further demonstrate their\napplicability for simplifying calculations of fluctuations in large Markov\nnetworks, use them to explain the physical origin of positive and negative\ncorrelations of occupation times in a double quantum dot device, and discuss\ntheir relevance for model inference."
                },
                "authors": [
                    {
                        "name": "Krzysztof Ptaszynski"
                    },
                    {
                        "name": "Timur Aslyamov"
                    },
                    {
                        "name": "Massimiliano Esposito"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Esposito"
                },
                "author": "Massimiliano Esposito",
                "arxiv_comment": "8 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12435v2",
                "updated": "2024-12-13T15:50:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    50,
                    26,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-19T03:29:40Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    3,
                    29,
                    40,
                    3,
                    263,
                    0
                ],
                "title": "Linguistic Minimal Pairs Elicit Linguistic Similarity in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic Minimal Pairs Elicit Linguistic Similarity in Large Language\n  Models"
                },
                "summary": "We introduce a novel analysis that leverages linguistic minimal pairs to\nprobe the internal linguistic representations of Large Language Models (LLMs).\nBy measuring the similarity between LLM activation differences across minimal\npairs, we quantify the and gain insight into the linguistic knowledge captured\nby LLMs. Our large-scale experiments, spanning 100+ LLMs and 150k minimal pairs\nin three languages, reveal properties of linguistic similarity from four key\naspects: consistency across LLMs, relation to theoretical categorizations,\ndependency to semantic context, and cross-lingual alignment of relevant\nphenomena. Our findings suggest that 1) linguistic similarity is significantly\ninfluenced by training data exposure, leading to higher cross-LLM agreement in\nhigher-resource languages. 2) Linguistic similarity strongly aligns with\nfine-grained theoretical linguistic categories but weakly with broader ones. 3)\nLinguistic similarity shows a weak correlation with semantic similarity,\nshowing its context-dependent nature. 4) LLMs exhibit limited cross-lingual\nalignment in their understanding of relevant linguistic phenomena. This work\ndemonstrates the potential of minimal pairs as a window into the neural\nrepresentations of language in LLMs, shedding light on the relationship between\nLLMs and linguistic theory. Codes and data are available at\nhttps://github.com/ChenDelong1999/Linguistic-Similarity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel analysis that leverages linguistic minimal pairs to\nprobe the internal linguistic representations of Large Language Models (LLMs).\nBy measuring the similarity between LLM activation differences across minimal\npairs, we quantify the and gain insight into the linguistic knowledge captured\nby LLMs. Our large-scale experiments, spanning 100+ LLMs and 150k minimal pairs\nin three languages, reveal properties of linguistic similarity from four key\naspects: consistency across LLMs, relation to theoretical categorizations,\ndependency to semantic context, and cross-lingual alignment of relevant\nphenomena. Our findings suggest that 1) linguistic similarity is significantly\ninfluenced by training data exposure, leading to higher cross-LLM agreement in\nhigher-resource languages. 2) Linguistic similarity strongly aligns with\nfine-grained theoretical linguistic categories but weakly with broader ones. 3)\nLinguistic similarity shows a weak correlation with semantic similarity,\nshowing its context-dependent nature. 4) LLMs exhibit limited cross-lingual\nalignment in their understanding of relevant linguistic phenomena. This work\ndemonstrates the potential of minimal pairs as a window into the neural\nrepresentations of language in LLMs, shedding light on the relationship between\nLLMs and linguistic theory. Codes and data are available at\nhttps://github.com/ChenDelong1999/Linguistic-Similarity"
                },
                "authors": [
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Delong Chen"
                    },
                    {
                        "name": "Samuel Cahyawijaya"
                    },
                    {
                        "name": "Xufeng Duan"
                    },
                    {
                        "name": "Zhenguang G. Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zhenguang G. Cai"
                },
                "author": "Zhenguang G. Cai",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12171v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12171v2",
                "updated": "2024-12-13T15:48:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    48,
                    53,
                    4,
                    348,
                    0
                ],
                "published": "2024-06-18T00:44:14Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    0,
                    44,
                    14,
                    1,
                    170,
                    0
                ],
                "title": "Model Selection for Causal Modeling in Missing Exposure Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Selection for Causal Modeling in Missing Exposure Problems"
                },
                "summary": "In causal inference, properly selecting the propensity score (PS) model is an\nimportant topic and has been widely investigated in observational studies.\nThere is also a large literature focusing on the missing data problem. However,\nthere are very few studies investigating the model selection issue for causal\ninference when the exposure is missing at random (MAR). In this paper, we\ndiscuss how to select both imputation and PS models, which can result in the\nsmallest root mean squared error (RMSE) of the estimated causal effect in our\nsimulation study. Then, we propose a new criterion, called ``rank score'' for\nevaluating the overall performance of both models. The simulation studies show\nthat the full imputation plus the outcome-related PS models lead to the\nsmallest RMSE and the rank score can help select the best models. An\napplication study is conducted to quantify the causal effect of cardiovascular\ndisease (CVD) on the mortality of COVID-19 patients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In causal inference, properly selecting the propensity score (PS) model is an\nimportant topic and has been widely investigated in observational studies.\nThere is also a large literature focusing on the missing data problem. However,\nthere are very few studies investigating the model selection issue for causal\ninference when the exposure is missing at random (MAR). In this paper, we\ndiscuss how to select both imputation and PS models, which can result in the\nsmallest root mean squared error (RMSE) of the estimated causal effect in our\nsimulation study. Then, we propose a new criterion, called ``rank score'' for\nevaluating the overall performance of both models. The simulation studies show\nthat the full imputation plus the outcome-related PS models lead to the\nsmallest RMSE and the rank score can help select the best models. An\napplication study is conducted to quantify the causal effect of cardiovascular\ndisease (CVD) on the mortality of COVID-19 patients."
                },
                "authors": [
                    {
                        "name": "Yuliang Shi"
                    },
                    {
                        "name": "Yeying Zhu"
                    },
                    {
                        "name": "Joel A. Dubin"
                    }
                ],
                "author_detail": {
                    "name": "Joel A. Dubin"
                },
                "author": "Joel A. Dubin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12171v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12171v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10220v1",
                "updated": "2024-12-13T15:45:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    45,
                    45,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    45,
                    45,
                    4,
                    348,
                    0
                ],
                "title": "How good is my story? Towards quantitative metrics for evaluating\n  LLM-generated XAI narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How good is my story? Towards quantitative metrics for evaluating\n  LLM-generated XAI narratives"
                },
                "summary": "A rapidly developing application of LLMs in XAI is to convert quantitative\nexplanations such as SHAP into user-friendly narratives to explain the\ndecisions made by smaller prediction models. Evaluating the narratives without\nrelying on human preference studies or surveys is becoming increasingly\nimportant in this field. In this work we propose a framework and explore\nseveral automated metrics to evaluate LLM-generated narratives for explanations\nof tabular classification tasks. We apply our approach to compare several\nstate-of-the-art LLMs across different datasets and prompt types. As a\ndemonstration of their utility, these metrics allow us to identify new\nchallenges related to LLM hallucinations for XAI narratives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A rapidly developing application of LLMs in XAI is to convert quantitative\nexplanations such as SHAP into user-friendly narratives to explain the\ndecisions made by smaller prediction models. Evaluating the narratives without\nrelying on human preference studies or surveys is becoming increasingly\nimportant in this field. In this work we propose a framework and explore\nseveral automated metrics to evaluate LLM-generated narratives for explanations\nof tabular classification tasks. We apply our approach to compare several\nstate-of-the-art LLMs across different datasets and prompt types. As a\ndemonstration of their utility, these metrics allow us to identify new\nchallenges related to LLM hallucinations for XAI narratives."
                },
                "authors": [
                    {
                        "name": "Timour Ichmoukhamedov"
                    },
                    {
                        "name": "James Hinns"
                    },
                    {
                        "name": "David Martens"
                    }
                ],
                "author_detail": {
                    "name": "David Martens"
                },
                "author": "David Martens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10219v1",
                "updated": "2024-12-13T15:41:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    41,
                    8,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T15:41:08Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    41,
                    8,
                    4,
                    348,
                    0
                ],
                "title": "Learning Complex Non-Rigid Image Edits from Multimodal Conditioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Complex Non-Rigid Image Edits from Multimodal Conditioning"
                },
                "summary": "In this paper we focus on inserting a given human (specifically, a single\nimage of a person) into a novel scene. Our method, which builds on top of\nStable Diffusion, yields natural looking images while being highly controllable\nwith text and pose. To accomplish this we need to train on pairs of images, the\nfirst a reference image with the person, the second a \"target image\" showing\nthe same person (with a different pose and possibly in a different background).\nAdditionally we require a text caption describing the new pose relative to that\nin the reference image. In this paper we present a novel dataset following this\ncriteria, which we create using pairs of frames from human-centric and\naction-rich videos and employing a multimodal LLM to automatically summarize\nthe difference in human pose for the text captions. We demonstrate that\nidentity preservation is a more challenging task in scenes \"in-the-wild\", and\nespecially scenes where there is an interaction between persons and objects.\nCombining the weak supervision from noisy captions, with robust 2D pose\nimproves the quality of person-object interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we focus on inserting a given human (specifically, a single\nimage of a person) into a novel scene. Our method, which builds on top of\nStable Diffusion, yields natural looking images while being highly controllable\nwith text and pose. To accomplish this we need to train on pairs of images, the\nfirst a reference image with the person, the second a \"target image\" showing\nthe same person (with a different pose and possibly in a different background).\nAdditionally we require a text caption describing the new pose relative to that\nin the reference image. In this paper we present a novel dataset following this\ncriteria, which we create using pairs of frames from human-centric and\naction-rich videos and employing a multimodal LLM to automatically summarize\nthe difference in human pose for the text captions. We demonstrate that\nidentity preservation is a more challenging task in scenes \"in-the-wild\", and\nespecially scenes where there is an interaction between persons and objects.\nCombining the weak supervision from noisy captions, with robust 2D pose\nimproves the quality of person-object interactions."
                },
                "authors": [
                    {
                        "name": "Nikolai Warner"
                    },
                    {
                        "name": "Jack Kolb"
                    },
                    {
                        "name": "Meera Hahn"
                    },
                    {
                        "name": "Vighnesh Birodkar"
                    },
                    {
                        "name": "Jonathan Huang"
                    },
                    {
                        "name": "Irfan Essa"
                    }
                ],
                "author_detail": {
                    "name": "Irfan Essa"
                },
                "author": "Irfan Essa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08701v2",
                "updated": "2024-12-13T15:39:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    39,
                    19,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-11T19:00:00Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    19,
                    0,
                    0,
                    2,
                    346,
                    0
                ],
                "title": "GalSBI: Phenomenological galaxy population model for cosmology using\n  simulation-based inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GalSBI: Phenomenological galaxy population model for cosmology using\n  simulation-based inference"
                },
                "summary": "We present GalSBI, a phenomenological model of the galaxy population for\ncosmological applications using simulation-based inference. The model is based\non analytical parametrizations of galaxy luminosity functions, morphologies and\nspectral energy distributions. Model constraints are derived through iterative\nApproximate Bayesian Computation, by comparing Hyper Suprime-Cam deep field\nimages with simulations which include a forward model of instrumental,\nobservational and source extraction effects. We developed an emulator trained\non image simulations using a normalizing flow. We use it to accelerate the\ninference by predicting detection probabilities, including blending effects and\nphotometric properties of each object, while accounting for background and PSF\nvariations. This enables robustness tests for all elements of the forward model\nand the inference. The model demonstrates excellent performance when comparing\nphotometric properties from simulations with observed imaging data for key\nparameters such as magnitudes, colors and sizes. The redshift distribution of\nsimulated galaxies agrees well with high-precision photometric redshifts in the\nCOSMOS field within $1.5\\sigma$ for all magnitude cuts. Additionally, we\ndemonstrate how GalSBI's redshifts can be utilized for splitting galaxy\ncatalogs into tomographic bins, highlighting its potential for current and\nupcoming surveys. GalSBI is fully open-source, with the accompanying Python\npackage, $\\texttt{galsbi}$, offering an easy interface to quickly generate\nrealistic, survey-independent galaxy catalogs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present GalSBI, a phenomenological model of the galaxy population for\ncosmological applications using simulation-based inference. The model is based\non analytical parametrizations of galaxy luminosity functions, morphologies and\nspectral energy distributions. Model constraints are derived through iterative\nApproximate Bayesian Computation, by comparing Hyper Suprime-Cam deep field\nimages with simulations which include a forward model of instrumental,\nobservational and source extraction effects. We developed an emulator trained\non image simulations using a normalizing flow. We use it to accelerate the\ninference by predicting detection probabilities, including blending effects and\nphotometric properties of each object, while accounting for background and PSF\nvariations. This enables robustness tests for all elements of the forward model\nand the inference. The model demonstrates excellent performance when comparing\nphotometric properties from simulations with observed imaging data for key\nparameters such as magnitudes, colors and sizes. The redshift distribution of\nsimulated galaxies agrees well with high-precision photometric redshifts in the\nCOSMOS field within $1.5\\sigma$ for all magnitude cuts. Additionally, we\ndemonstrate how GalSBI's redshifts can be utilized for splitting galaxy\ncatalogs into tomographic bins, highlighting its potential for current and\nupcoming surveys. GalSBI is fully open-source, with the accompanying Python\npackage, $\\texttt{galsbi}$, offering an easy interface to quickly generate\nrealistic, survey-independent galaxy catalogs."
                },
                "authors": [
                    {
                        "name": "Silvan Fischbacher"
                    },
                    {
                        "name": "Tomasz Kacprzak"
                    },
                    {
                        "name": "Luca Tortorelli"
                    },
                    {
                        "name": "Beatrice Moser"
                    },
                    {
                        "name": "Alexandre Refregier"
                    },
                    {
                        "name": "Patrick Gebhardt"
                    },
                    {
                        "name": "Daniel Gruen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Gruen"
                },
                "author": "Daniel Gruen",
                "arxiv_comment": "29 pages, 14 figures, submitted to Phys. Rev. D",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12741v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12741v3",
                "updated": "2024-12-13T15:37:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    37,
                    1,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-19T13:03:24Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    13,
                    3,
                    24,
                    3,
                    263,
                    0
                ],
                "title": "Fine Tuning Large Language Models for Medicine: The Role and Importance\n  of Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine Tuning Large Language Models for Medicine: The Role and Importance\n  of Direct Preference Optimization"
                },
                "summary": "Large Language Model (LLM) fine tuning is underutilized in the field of\nmedicine. Two of the most common methods of fine tuning are Supervised Fine\nTuning (SFT) and Direct Preference Optimization (DPO), but there is little\nguidance informing users when to use either technique. In this investigation,\nwe compare the performance of SFT and DPO for five common natural language\ntasks in medicine: Classification with text data, Classification with numeric\ndata, Clinical Reasoning, Summarization, and Clinical Triage. We find that SFT\nalone is sufficient for Classification with text data, whereas DPO improves\nperformance for the more complex tasks of Clinical Reasoning, Summarization and\nClinical Triage. Our results establish the role and importance of DPO fine\ntuning within medicine, and consequently call attention to current software\ngaps that prevent widespread deployment of this technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) fine tuning is underutilized in the field of\nmedicine. Two of the most common methods of fine tuning are Supervised Fine\nTuning (SFT) and Direct Preference Optimization (DPO), but there is little\nguidance informing users when to use either technique. In this investigation,\nwe compare the performance of SFT and DPO for five common natural language\ntasks in medicine: Classification with text data, Classification with numeric\ndata, Clinical Reasoning, Summarization, and Clinical Triage. We find that SFT\nalone is sufficient for Classification with text data, whereas DPO improves\nperformance for the more complex tasks of Clinical Reasoning, Summarization and\nClinical Triage. Our results establish the role and importance of DPO fine\ntuning within medicine, and consequently call attention to current software\ngaps that prevent widespread deployment of this technique."
                },
                "authors": [
                    {
                        "name": "Thomas Savage"
                    },
                    {
                        "name": "Stephen Ma"
                    },
                    {
                        "name": "Abdessalem Boukil"
                    },
                    {
                        "name": "Vishwesh Patel"
                    },
                    {
                        "name": "Ekanath Rangan"
                    },
                    {
                        "name": "Ivan Lopez"
                    },
                    {
                        "name": "Jonathan H Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan H Chen"
                },
                "author": "Jonathan H Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12741v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12741v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17569v2",
                "updated": "2024-12-13T15:36:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    36,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-26T16:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    31,
                    18,
                    1,
                    331,
                    0
                ],
                "title": "RTL-Breaker: Assessing the Security of LLMs against Backdoor Attacks on\n  HDL Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTL-Breaker: Assessing the Security of LLMs against Backdoor Attacks on\n  HDL Code Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable potential with code\ngeneration/completion tasks for hardware design. In fact, LLM-based hardware\ndescription language (HDL) code generation has enabled the industry to realize\ncomplex designs more quickly, reducing the time and effort required in the\ndevelopment cycle. However, the increased reliance on such automation\nintroduces critical security risks. Notably, given that LLMs have to be trained\non vast datasets of codes that are typically sourced from publicly available\nrepositories (often without thorough validation), LLMs are susceptible to\nso-called data poisoning or backdoor attacks. Here, attackers inject malicious\ncode for the training data, which can be carried over into the HDL code\ngenerated by LLMs. This threat vector can compromise the security and integrity\nof entire hardware systems. In this work, we propose RTL-Breaker, a novel\nbackdoor attack framework on LLM-based HDL code generation. RTL-Breaker\nprovides an in-depth analysis for essential aspects of this novel problem: 1)\nvarious trigger mechanisms versus their effectiveness for inserting malicious\nmodifications, and 2) side-effects by backdoor attacks on code generation in\ngeneral, i.e., impact on code quality. RTL-Breaker emphasizes the urgent need\nfor more robust measures to safeguard against such attacks. Toward that end, we\nopen-source our framework and all data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable potential with code\ngeneration/completion tasks for hardware design. In fact, LLM-based hardware\ndescription language (HDL) code generation has enabled the industry to realize\ncomplex designs more quickly, reducing the time and effort required in the\ndevelopment cycle. However, the increased reliance on such automation\nintroduces critical security risks. Notably, given that LLMs have to be trained\non vast datasets of codes that are typically sourced from publicly available\nrepositories (often without thorough validation), LLMs are susceptible to\nso-called data poisoning or backdoor attacks. Here, attackers inject malicious\ncode for the training data, which can be carried over into the HDL code\ngenerated by LLMs. This threat vector can compromise the security and integrity\nof entire hardware systems. In this work, we propose RTL-Breaker, a novel\nbackdoor attack framework on LLM-based HDL code generation. RTL-Breaker\nprovides an in-depth analysis for essential aspects of this novel problem: 1)\nvarious trigger mechanisms versus their effectiveness for inserting malicious\nmodifications, and 2) side-effects by backdoor attacks on code generation in\ngeneral, i.e., impact on code quality. RTL-Breaker emphasizes the urgent need\nfor more robust measures to safeguard against such attacks. Toward that end, we\nopen-source our framework and all data."
                },
                "authors": [
                    {
                        "name": "Lakshmi Likhitha Mankali"
                    },
                    {
                        "name": "Jitendra Bhandari"
                    },
                    {
                        "name": "Manaar Alam"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Michail Maniatakos"
                    },
                    {
                        "name": "Ozgur Sinanoglu"
                    },
                    {
                        "name": "Johann Knechtel"
                    }
                ],
                "author_detail": {
                    "name": "Johann Knechtel"
                },
                "author": "Johann Knechtel",
                "arxiv_comment": "Accepted at 2025 Design, Automation & Test in Europe (DATE)\n  Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10208v1",
                "updated": "2024-12-13T15:31:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    31,
                    17,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T15:31:17Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    31,
                    17,
                    4,
                    348,
                    0
                ],
                "title": "Efficient Generative Modeling with Residual Vector Quantization-Based\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Generative Modeling with Residual Vector Quantization-Based\n  Tokens"
                },
                "summary": "We explore the use of Residual Vector Quantization (RVQ) for high-fidelity\ngeneration in vector-quantized generative models. This quantization technique\nmaintains higher data fidelity by employing more in-depth tokens. However,\nincreasing the token number in generative models leads to slower inference\nspeeds. To this end, we introduce ResGen, an efficient RVQ-based discrete\ndiffusion model that generates high-fidelity samples without compromising\nsampling speed. Our key idea is a direct prediction of vector embedding of\ncollective tokens rather than individual ones. Moreover, we demonstrate that\nour proposed token masking and multi-token prediction method can be formulated\nwithin a principled probabilistic framework using a discrete diffusion process\nand variational inference. We validate the efficacy and generalizability of the\nproposed method on two challenging tasks across different modalities:\nconditional image generation} on ImageNet 256x256 and zero-shot text-to-speech\nsynthesis. Experimental results demonstrate that ResGen outperforms\nautoregressive counterparts in both tasks, delivering superior performance\nwithout compromising sampling speed. Furthermore, as we scale the depth of RVQ,\nour generative models exhibit enhanced generation fidelity or faster sampling\nspeeds compared to similarly sized baseline models. The project page can be\nfound at https://resgen-genai.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the use of Residual Vector Quantization (RVQ) for high-fidelity\ngeneration in vector-quantized generative models. This quantization technique\nmaintains higher data fidelity by employing more in-depth tokens. However,\nincreasing the token number in generative models leads to slower inference\nspeeds. To this end, we introduce ResGen, an efficient RVQ-based discrete\ndiffusion model that generates high-fidelity samples without compromising\nsampling speed. Our key idea is a direct prediction of vector embedding of\ncollective tokens rather than individual ones. Moreover, we demonstrate that\nour proposed token masking and multi-token prediction method can be formulated\nwithin a principled probabilistic framework using a discrete diffusion process\nand variational inference. We validate the efficacy and generalizability of the\nproposed method on two challenging tasks across different modalities:\nconditional image generation} on ImageNet 256x256 and zero-shot text-to-speech\nsynthesis. Experimental results demonstrate that ResGen outperforms\nautoregressive counterparts in both tasks, delivering superior performance\nwithout compromising sampling speed. Furthermore, as we scale the depth of RVQ,\nour generative models exhibit enhanced generation fidelity or faster sampling\nspeeds compared to similarly sized baseline models. The project page can be\nfound at https://resgen-genai.github.io"
                },
                "authors": [
                    {
                        "name": "Jaehyeon Kim"
                    },
                    {
                        "name": "Taehong Moon"
                    },
                    {
                        "name": "Keon Lee"
                    },
                    {
                        "name": "Jaewoong Cho"
                    }
                ],
                "author_detail": {
                    "name": "Jaewoong Cho"
                },
                "author": "Jaewoong Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10207v1",
                "updated": "2024-12-13T15:30:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    30,
                    20,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T15:30:20Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    30,
                    20,
                    4,
                    348,
                    0
                ],
                "title": "Retrieval-Augmented Semantic Parsing: Using Large Language Models to\n  Improve Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Semantic Parsing: Using Large Language Models to\n  Improve Generalization"
                },
                "summary": "Open-domain semantic parsing remains a challenging task, as models often rely\non heuristics and struggle to handle unseen concepts. In this paper, we\ninvestigate the potential of large language models (LLMs) for this task and\nintroduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effective\napproach that integrates external lexical knowledge into the parsing process.\nOur experiments not only show that LLMs outperform previous encoder-decoder\nbaselines for semantic parsing, but that RASP further enhances their ability to\npredict unseen concepts, nearly doubling the performance of previous models on\nout-of-distribution concepts. These findings highlight the promise of\nleveraging large language models and retrieval mechanisms for robust and\nopen-domain semantic parsing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-domain semantic parsing remains a challenging task, as models often rely\non heuristics and struggle to handle unseen concepts. In this paper, we\ninvestigate the potential of large language models (LLMs) for this task and\nintroduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effective\napproach that integrates external lexical knowledge into the parsing process.\nOur experiments not only show that LLMs outperform previous encoder-decoder\nbaselines for semantic parsing, but that RASP further enhances their ability to\npredict unseen concepts, nearly doubling the performance of previous models on\nout-of-distribution concepts. These findings highlight the promise of\nleveraging large language models and retrieval mechanisms for robust and\nopen-domain semantic parsing."
                },
                "authors": [
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Qianru Meng"
                    },
                    {
                        "name": "Johan Bos"
                    }
                ],
                "author_detail": {
                    "name": "Johan Bos"
                },
                "author": "Johan Bos",
                "arxiv_comment": "Submitted to ARR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18916v2",
                "updated": "2024-12-13T15:15:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    15,
                    46,
                    4,
                    348,
                    0
                ],
                "published": "2024-06-27T06:13:05Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    6,
                    13,
                    5,
                    3,
                    179,
                    0
                ],
                "title": "TrustUQA: A Trustful Framework for Unified Structured Data Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrustUQA: A Trustful Framework for Unified Structured Data Question\n  Answering"
                },
                "summary": "Natural language question answering (QA) over structured data sources such as\ntables and knowledge graphs have been widely investigated, especially with\nLarge Language Models (LLMs) in recent years. The main solutions include\nquestion to formal query parsing and retrieval-based answer generation.\nHowever, current methods of the former often suffer from weak generalization,\nfailing to dealing with multi-types of sources, while the later is limited in\ntrustfulness. In this paper, we propose TrustUQA, a trustful QA framework that\ncan simultaneously support multiple types of structured data in a unified way.\nTo this end, it adopts an LLM-friendly and unified knowledge representation\nmethod called Condition Graph(CG), and uses an LLM and demonstration-based\ntwo-level method for CG querying. For enhancement, it is also equipped with\ndynamic demonstration retrieval. We have evaluated TrustUQA with 5 benchmarks\ncovering 3 types of structured data. It outperforms 2 existing unified\nstructured data QA methods. In comparison with the baselines that are specific\nto one data type, it achieves state-of-the-art on 2 of the datasets. Further\nmore, we have demonstrated the potential of our method for more general QA\ntasks, QA over mixed structured data and QA across structured data. The code is\navailable at https://github.com/zjukg/TrustUQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language question answering (QA) over structured data sources such as\ntables and knowledge graphs have been widely investigated, especially with\nLarge Language Models (LLMs) in recent years. The main solutions include\nquestion to formal query parsing and retrieval-based answer generation.\nHowever, current methods of the former often suffer from weak generalization,\nfailing to dealing with multi-types of sources, while the later is limited in\ntrustfulness. In this paper, we propose TrustUQA, a trustful QA framework that\ncan simultaneously support multiple types of structured data in a unified way.\nTo this end, it adopts an LLM-friendly and unified knowledge representation\nmethod called Condition Graph(CG), and uses an LLM and demonstration-based\ntwo-level method for CG querying. For enhancement, it is also equipped with\ndynamic demonstration retrieval. We have evaluated TrustUQA with 5 benchmarks\ncovering 3 types of structured data. It outperforms 2 existing unified\nstructured data QA methods. In comparison with the baselines that are specific\nto one data type, it achieves state-of-the-art on 2 of the datasets. Further\nmore, we have demonstrated the potential of our method for more general QA\ntasks, QA over mixed structured data and QA across structured data. The code is\navailable at https://github.com/zjukg/TrustUQA."
                },
                "authors": [
                    {
                        "name": "Wen Zhang"
                    },
                    {
                        "name": "Long Jin"
                    },
                    {
                        "name": "Yushan Zhu"
                    },
                    {
                        "name": "Jiaoyan Chen"
                    },
                    {
                        "name": "Zhiwei Huang"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Yin Hua"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10198v1",
                "updated": "2024-12-13T15:15:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    15,
                    24,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T15:15:24Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    15,
                    24,
                    4,
                    348,
                    0
                ],
                "title": "From Allies to Adversaries: Manipulating LLM Tool-Calling through\n  Adversarial Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Allies to Adversaries: Manipulating LLM Tool-Calling through\n  Adversarial Injection"
                },
                "summary": "Tool-calling has changed Large Language Model (LLM) applications by\nintegrating external tools, significantly enhancing their functionality across\ndiverse tasks. However, this integration also introduces new security\nvulnerabilities, particularly in the tool scheduling mechanisms of LLM, which\nhave not been extensively studied. To fill this gap, we present ToolCommander,\na novel framework designed to exploit vulnerabilities in LLM tool-calling\nsystems through adversarial tool injection. Our framework employs a\nwell-designed two-stage attack strategy. Firstly, it injects malicious tools to\ncollect user queries, then dynamically updates the injected tools based on the\nstolen information to enhance subsequent attacks. These stages enable\nToolCommander to execute privacy theft, launch denial-of-service attacks, and\neven manipulate business competition by triggering unscheduled tool-calling.\nNotably, the ASR reaches 91.67% for privacy theft and hits 100% for\ndenial-of-service and unscheduled tool calling in certain cases. Our work\ndemonstrates that these vulnerabilities can lead to severe consequences beyond\nsimple misuse of tool-calling systems, underscoring the urgent need for robust\ndefensive strategies to secure LLM Tool-calling systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-calling has changed Large Language Model (LLM) applications by\nintegrating external tools, significantly enhancing their functionality across\ndiverse tasks. However, this integration also introduces new security\nvulnerabilities, particularly in the tool scheduling mechanisms of LLM, which\nhave not been extensively studied. To fill this gap, we present ToolCommander,\na novel framework designed to exploit vulnerabilities in LLM tool-calling\nsystems through adversarial tool injection. Our framework employs a\nwell-designed two-stage attack strategy. Firstly, it injects malicious tools to\ncollect user queries, then dynamically updates the injected tools based on the\nstolen information to enhance subsequent attacks. These stages enable\nToolCommander to execute privacy theft, launch denial-of-service attacks, and\neven manipulate business competition by triggering unscheduled tool-calling.\nNotably, the ASR reaches 91.67% for privacy theft and hits 100% for\ndenial-of-service and unscheduled tool calling in certain cases. Our work\ndemonstrates that these vulnerabilities can lead to severe consequences beyond\nsimple misuse of tool-calling systems, underscoring the urgent need for robust\ndefensive strategies to secure LLM Tool-calling systems."
                },
                "authors": [
                    {
                        "name": "Haowei Wang"
                    },
                    {
                        "name": "Rupeng Zhang"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Mingyang Li"
                    },
                    {
                        "name": "Yuekai Huang"
                    },
                    {
                        "name": "Dandan Wang"
                    },
                    {
                        "name": "Qing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qing Wang"
                },
                "author": "Qing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17461v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17461v2",
                "updated": "2024-12-13T15:08:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    8,
                    32,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-26T14:28:25Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    28,
                    25,
                    1,
                    331,
                    0
                ],
                "title": "SoK: Decentralized AI (DeAI)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Decentralized AI (DeAI)"
                },
                "summary": "The centralization of Artificial Intelligence (AI) poses significant\nchallenges, including single points of failure, inherent biases, data privacy\nconcerns, and scalability issues. These problems are especially prevalent in\nclosed-source large language models (LLMs), where user data is collected and\nused without transparency. To mitigate these issues, blockchain-based\ndecentralized AI (DeAI) has emerged as a promising solution. DeAI combines the\nstrengths of both blockchain and AI technologies to enhance the transparency,\nsecurity, decentralization, and trustworthiness of AI systems. However, a\ncomprehensive understanding of state-of-the-art DeAI development, particularly\nfor active industry solutions, is still lacking. In this work, we present a\nSystematization of Knowledge (SoK) for blockchain-based DeAI solutions. We\npropose a taxonomy to classify existing DeAI protocols based on the model\nlifecycle. Based on this taxonomy, we provide a structured way to clarify the\nlandscape of DeAI protocols and identify their similarities and differences. We\nanalyze the functionalities of blockchain in DeAI, investigating how blockchain\nfeatures contribute to enhancing the security, transparency, and\ntrustworthiness of AI processes, while also ensuring fair incentives for AI\ndata and model contributors. In addition, we identify key insights and research\ngaps in developing DeAI protocols, highlighting several critical avenues for\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The centralization of Artificial Intelligence (AI) poses significant\nchallenges, including single points of failure, inherent biases, data privacy\nconcerns, and scalability issues. These problems are especially prevalent in\nclosed-source large language models (LLMs), where user data is collected and\nused without transparency. To mitigate these issues, blockchain-based\ndecentralized AI (DeAI) has emerged as a promising solution. DeAI combines the\nstrengths of both blockchain and AI technologies to enhance the transparency,\nsecurity, decentralization, and trustworthiness of AI systems. However, a\ncomprehensive understanding of state-of-the-art DeAI development, particularly\nfor active industry solutions, is still lacking. In this work, we present a\nSystematization of Knowledge (SoK) for blockchain-based DeAI solutions. We\npropose a taxonomy to classify existing DeAI protocols based on the model\nlifecycle. Based on this taxonomy, we provide a structured way to clarify the\nlandscape of DeAI protocols and identify their similarities and differences. We\nanalyze the functionalities of blockchain in DeAI, investigating how blockchain\nfeatures contribute to enhancing the security, transparency, and\ntrustworthiness of AI processes, while also ensuring fair incentives for AI\ndata and model contributors. In addition, we identify key insights and research\ngaps in developing DeAI protocols, highlighting several critical avenues for\nfuture research."
                },
                "authors": [
                    {
                        "name": "Zhipeng Wang"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Elizabeth Lui"
                    },
                    {
                        "name": "Vatsal Shah"
                    },
                    {
                        "name": "Xihan Xiong"
                    },
                    {
                        "name": "Jiahao Sun"
                    },
                    {
                        "name": "Davide Crapis"
                    },
                    {
                        "name": "William Knottenbelt"
                    }
                ],
                "author_detail": {
                    "name": "William Knottenbelt"
                },
                "author": "William Knottenbelt",
                "arxiv_comment": "This is a Systematization of Knowledge (SoK) for the rapidly evolving\n  field of Decentralized AI (DeAI). We welcome valuable comments, suggestions,\n  and collaboration to further refine and enhance this work. We hope our\n  contribution will help accelerate the advancement of DeAI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17461v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17461v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10188v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10188v1",
                "updated": "2024-12-13T14:57:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    57,
                    21,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:57:21Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    57,
                    21,
                    4,
                    348,
                    0
                ],
                "title": "Electron Heating by Parallel Electric Fields in Magnetotail Reconnection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron Heating by Parallel Electric Fields in Magnetotail Reconnection"
                },
                "summary": "We investigate electron heating by magnetic-field-aligned electric fields\n($E_\\parallel$) during anti-parallel magnetic reconnection in the Earth's\nmagnetotail. Using a statistical sample of 140 reconnection outflows, we infer\nthe acceleration potential associated with $E_\\parallel$ from the shape of the\nelectron velocity distribution functions. We show that heating by $E_\\parallel$\nin the reconnection outflow can reach up to ten times the inflow electron\ntemperature. We demonstrate that the magnitude of the acceleration potential\nscales with the inflow Alfv\\'en and electron thermal speeds to maintain\nquasi-neutrality in the reconnection region. Our results suggest that\n$E_\\parallel$ plays a major role in the ion-to-electron energy partition\nassociated with magnetic reconnection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate electron heating by magnetic-field-aligned electric fields\n($E_\\parallel$) during anti-parallel magnetic reconnection in the Earth's\nmagnetotail. Using a statistical sample of 140 reconnection outflows, we infer\nthe acceleration potential associated with $E_\\parallel$ from the shape of the\nelectron velocity distribution functions. We show that heating by $E_\\parallel$\nin the reconnection outflow can reach up to ten times the inflow electron\ntemperature. We demonstrate that the magnitude of the acceleration potential\nscales with the inflow Alfv\\'en and electron thermal speeds to maintain\nquasi-neutrality in the reconnection region. Our results suggest that\n$E_\\parallel$ plays a major role in the ion-to-electron energy partition\nassociated with magnetic reconnection."
                },
                "authors": [
                    {
                        "name": "Louis Richard"
                    },
                    {
                        "name": "Yuri V. Khotyaintsev"
                    },
                    {
                        "name": "Cecilia Norgren"
                    },
                    {
                        "name": "Konrad Steinvall"
                    },
                    {
                        "name": "Daniel B. Graham"
                    },
                    {
                        "name": "Jan Egedal"
                    },
                    {
                        "name": "Andris Vaivads"
                    },
                    {
                        "name": "Rumi Nakamura"
                    }
                ],
                "author_detail": {
                    "name": "Rumi Nakamura"
                },
                "author": "Rumi Nakamura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10188v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.space-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10182v1",
                "updated": "2024-12-13T14:53:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    53,
                    47,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:53:47Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    53,
                    47,
                    4,
                    348,
                    0
                ],
                "title": "Multi-Head Encoding for Extreme Label Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Encoding for Extreme Label Classification"
                },
                "summary": "The number of categories of instances in the real world is normally huge, and\neach instance may contain multiple labels. To distinguish these massive labels\nutilizing machine learning, eXtreme Label Classification (XLC) has been\nestablished. However, as the number of categories increases, the number of\nparameters and nonlinear operations in the classifier also rises. This results\nin a Classifier Computational Overload Problem (CCOP). To address this, we\npropose a Multi-Head Encoding (MHE) mechanism, which replaces the vanilla\nclassifier with a multi-head classifier. During the training process, MHE\ndecomposes extreme labels into the product of multiple short local labels, with\neach head trained on these local labels. During testing, the predicted labels\ncan be directly calculated from the local predictions of each head. This\nreduces the computational load geometrically. Then, according to the\ncharacteristics of different XLC tasks, e.g., single-label, multi-label, and\nmodel pretraining tasks, three MHE-based implementations, i.e., Multi-Head\nProduct, Multi-Head Cascade, and Multi-Head Sampling, are proposed to more\neffectively cope with CCOP. Moreover, we theoretically demonstrate that MHE can\nachieve performance approximately equivalent to that of the vanilla classifier\nby generalizing the low-rank approximation problem from Frobenius-norm to\nCross-Entropy. Experimental results show that the proposed methods achieve\nstate-of-the-art performance while significantly streamlining the training and\ninference processes of XLC tasks. The source code has been made public at\nhttps://github.com/Anoise/MHE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The number of categories of instances in the real world is normally huge, and\neach instance may contain multiple labels. To distinguish these massive labels\nutilizing machine learning, eXtreme Label Classification (XLC) has been\nestablished. However, as the number of categories increases, the number of\nparameters and nonlinear operations in the classifier also rises. This results\nin a Classifier Computational Overload Problem (CCOP). To address this, we\npropose a Multi-Head Encoding (MHE) mechanism, which replaces the vanilla\nclassifier with a multi-head classifier. During the training process, MHE\ndecomposes extreme labels into the product of multiple short local labels, with\neach head trained on these local labels. During testing, the predicted labels\ncan be directly calculated from the local predictions of each head. This\nreduces the computational load geometrically. Then, according to the\ncharacteristics of different XLC tasks, e.g., single-label, multi-label, and\nmodel pretraining tasks, three MHE-based implementations, i.e., Multi-Head\nProduct, Multi-Head Cascade, and Multi-Head Sampling, are proposed to more\neffectively cope with CCOP. Moreover, we theoretically demonstrate that MHE can\nachieve performance approximately equivalent to that of the vanilla classifier\nby generalizing the low-rank approximation problem from Frobenius-norm to\nCross-Entropy. Experimental results show that the proposed methods achieve\nstate-of-the-art performance while significantly streamlining the training and\ninference processes of XLC tasks. The source code has been made public at\nhttps://github.com/Anoise/MHE."
                },
                "authors": [
                    {
                        "name": "Daojun Liang"
                    },
                    {
                        "name": "Haixia Zhang"
                    },
                    {
                        "name": "Dongfeng Yuan"
                    },
                    {
                        "name": "Minggao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minggao Zhang"
                },
                "author": "Minggao Zhang",
                "arxiv_comment": "20 pages, 12 figs, Published in TPAMI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10178v1",
                "updated": "2024-12-13T14:50:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    50,
                    26,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:50:26Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    50,
                    26,
                    4,
                    348,
                    0
                ],
                "title": "SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftTry: Fast and Consistent Video Virtual Try-On with Diffusion Models"
                },
                "summary": "Given an input video of a person and a new garment, the objective of this\npaper is to synthesize a new video where the person is wearing the specified\ngarment while maintaining spatiotemporal consistency. While significant\nadvances have been made in image-based virtual try-ons, extending these\nsuccesses to video often results in frame-to-frame inconsistencies. Some\napproaches have attempted to address this by increasing the overlap of frames\nacross multiple video chunks, but this comes at a steep computational cost due\nto the repeated processing of the same frames, especially for long video\nsequence. To address these challenges, we reconceptualize video virtual try-on\nas a conditional video inpainting task, with garments serving as input\nconditions. Specifically, our approach enhances image diffusion models by\nincorporating temporal attention layers to improve temporal coherence. To\nreduce computational overhead, we introduce ShiftCaching, a novel technique\nthat maintains temporal consistency while minimizing redundant computations.\nFurthermore, we introduce the \\dataname~dataset, a new video try-on dataset\nfeaturing more complex backgrounds, challenging movements, and higher\nresolution compared to existing public datasets. Extensive experiments show\nthat our approach outperforms current baselines, particularly in terms of video\nconsistency and inference speed. Data and code are available at\nhttps://github.com/VinAIResearch/swift-try",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given an input video of a person and a new garment, the objective of this\npaper is to synthesize a new video where the person is wearing the specified\ngarment while maintaining spatiotemporal consistency. While significant\nadvances have been made in image-based virtual try-ons, extending these\nsuccesses to video often results in frame-to-frame inconsistencies. Some\napproaches have attempted to address this by increasing the overlap of frames\nacross multiple video chunks, but this comes at a steep computational cost due\nto the repeated processing of the same frames, especially for long video\nsequence. To address these challenges, we reconceptualize video virtual try-on\nas a conditional video inpainting task, with garments serving as input\nconditions. Specifically, our approach enhances image diffusion models by\nincorporating temporal attention layers to improve temporal coherence. To\nreduce computational overhead, we introduce ShiftCaching, a novel technique\nthat maintains temporal consistency while minimizing redundant computations.\nFurthermore, we introduce the \\dataname~dataset, a new video try-on dataset\nfeaturing more complex backgrounds, challenging movements, and higher\nresolution compared to existing public datasets. Extensive experiments show\nthat our approach outperforms current baselines, particularly in terms of video\nconsistency and inference speed. Data and code are available at\nhttps://github.com/VinAIResearch/swift-try"
                },
                "authors": [
                    {
                        "name": "Hung Nguyen"
                    },
                    {
                        "name": "Quang Qui-Vinh Nguyen"
                    },
                    {
                        "name": "Khoi Nguyen"
                    },
                    {
                        "name": "Rang Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Rang Nguyen"
                },
                "author": "Rang Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10175v1",
                "updated": "2024-12-13T14:44:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    44,
                    32,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:44:32Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    44,
                    32,
                    4,
                    348,
                    0
                ],
                "title": "Uncertainties in Signal Recovery from Heterogeneous and Convoluted Time\n  Series with Principal Component Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainties in Signal Recovery from Heterogeneous and Convoluted Time\n  Series with Principal Component Analysis"
                },
                "summary": "Principal Component Analysis (PCA) is one of the most used tools for\nextracting low-dimensional representations of data, in particular for time\nseries. Performances are known to strongly depend on the quality (amount of\nnoise) and the quantity of data. We here investigate the impact of\nheterogeneities, often present in real data, on the reconstruction of\nlow-dimensional trajectories and of their associated modes. We focus in\nparticular on the effects of sample-to-sample fluctuations and of\ncomponent-dependent temporal convolution and noise in the measurements. We\nderive analytical predictions for the error on the reconstructed trajectory and\nthe confusion between the modes using the replica method in a high-dimensional\nsetting, in which the number and the dimension of the data are comparable. We\nfind in particular that sample-to-sample variability, is deleterious for the\nreconstruction of the signal trajectory, but beneficial for the inference of\nthe modes, and that the fluctuations in the temporal convolution kernels\nprevent perfect recovery of the latent modes even for very weak measurement\nnoise. Our predictions are corroborated by simulations with synthetic data for\na variety of control parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Principal Component Analysis (PCA) is one of the most used tools for\nextracting low-dimensional representations of data, in particular for time\nseries. Performances are known to strongly depend on the quality (amount of\nnoise) and the quantity of data. We here investigate the impact of\nheterogeneities, often present in real data, on the reconstruction of\nlow-dimensional trajectories and of their associated modes. We focus in\nparticular on the effects of sample-to-sample fluctuations and of\ncomponent-dependent temporal convolution and noise in the measurements. We\nderive analytical predictions for the error on the reconstructed trajectory and\nthe confusion between the modes using the replica method in a high-dimensional\nsetting, in which the number and the dimension of the data are comparable. We\nfind in particular that sample-to-sample variability, is deleterious for the\nreconstruction of the signal trajectory, but beneficial for the inference of\nthe modes, and that the fluctuations in the temporal convolution kernels\nprevent perfect recovery of the latent modes even for very weak measurement\nnoise. Our predictions are corroborated by simulations with synthetic data for\na variety of control parameters."
                },
                "authors": [
                    {
                        "name": "Mariia Legenkaia"
                    },
                    {
                        "name": "Laurent Bourdieu"
                    },
                    {
                        "name": "Rémi Monasson"
                    }
                ],
                "author_detail": {
                    "name": "Rémi Monasson"
                },
                "author": "Rémi Monasson",
                "arxiv_comment": "18 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10173v1",
                "updated": "2024-12-13T14:39:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    39,
                    23,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:39:23Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    39,
                    23,
                    4,
                    348,
                    0
                ],
                "title": "Scalable magnetic resonance fingerprinting: Incremental inference of\n  high dimensional elliptical mixtures from large data volumes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable magnetic resonance fingerprinting: Incremental inference of\n  high dimensional elliptical mixtures from large data volumes"
                },
                "summary": "Magnetic Resonance Fingerprinting (MRF) is an emerging technology with the\npotential to revolutionize radiology and medical diagnostics. In comparison to\ntraditional magnetic resonance imaging (MRI), MRF enables the rapid,\nsimultaneous, non-invasive acquisition and reconstruction of multiple tissue\nparameters, paving the way for novel diagnostic techniques. In the original\nmatching approach, reconstruction is based on the search for the best matches\nbetween in vivo acquired signals and a dictionary of high-dimensional simulated\nsignals (fingerprints) with known tissue properties. A critical and limiting\nchallenge is that the size of the simulated dictionary increases exponentially\nwith the number of parameters, leading to an extremely costly subsequent\nmatching. In this work, we propose to address this scalability issue by\nconsidering probabilistic mixtures of high-dimensional elliptical\ndistributions, to learn more efficient dictionary representations. Mixture\ncomponents are modelled as flexible ellipitic shapes in low dimensional\nsubspaces. They are exploited to cluster similar signals and reduce their\ndimension locally cluster-wise to limit information loss. To estimate such a\nmixture model, we provide a new incremental algorithm capable of handling large\nnumbers of signals, allowing us to go far beyond the hardware limitations\nencountered by standard implementations. We demonstrate, on simulated and real\ndata, that our method effectively manages large volumes of MRF data with\nmaintained accuracy. It offers a more efficient solution for accurate tissue\ncharacterization and significantly reduces the computational burden, making the\nclinical application of MRF more practical and accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetic Resonance Fingerprinting (MRF) is an emerging technology with the\npotential to revolutionize radiology and medical diagnostics. In comparison to\ntraditional magnetic resonance imaging (MRI), MRF enables the rapid,\nsimultaneous, non-invasive acquisition and reconstruction of multiple tissue\nparameters, paving the way for novel diagnostic techniques. In the original\nmatching approach, reconstruction is based on the search for the best matches\nbetween in vivo acquired signals and a dictionary of high-dimensional simulated\nsignals (fingerprints) with known tissue properties. A critical and limiting\nchallenge is that the size of the simulated dictionary increases exponentially\nwith the number of parameters, leading to an extremely costly subsequent\nmatching. In this work, we propose to address this scalability issue by\nconsidering probabilistic mixtures of high-dimensional elliptical\ndistributions, to learn more efficient dictionary representations. Mixture\ncomponents are modelled as flexible ellipitic shapes in low dimensional\nsubspaces. They are exploited to cluster similar signals and reduce their\ndimension locally cluster-wise to limit information loss. To estimate such a\nmixture model, we provide a new incremental algorithm capable of handling large\nnumbers of signals, allowing us to go far beyond the hardware limitations\nencountered by standard implementations. We demonstrate, on simulated and real\ndata, that our method effectively manages large volumes of MRF data with\nmaintained accuracy. It offers a more efficient solution for accurate tissue\ncharacterization and significantly reduces the computational burden, making the\nclinical application of MRF more practical and accessible."
                },
                "authors": [
                    {
                        "name": "Geoffroy Oudoumanessah"
                    },
                    {
                        "name": "Thomas Coudert"
                    },
                    {
                        "name": "Carole Lartizien"
                    },
                    {
                        "name": "Michel Dojat"
                    },
                    {
                        "name": "Thomas Christen"
                    },
                    {
                        "name": "Florence Forbes"
                    }
                ],
                "author_detail": {
                    "name": "Florence Forbes"
                },
                "author": "Florence Forbes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10169v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10169v1",
                "updated": "2024-12-13T14:34:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    34,
                    9,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:34:09Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    34,
                    9,
                    4,
                    348,
                    0
                ],
                "title": "Recovering Pulsar Braking Index from a Population of Millisecond Pulsars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recovering Pulsar Braking Index from a Population of Millisecond Pulsars"
                },
                "summary": "The braking index, $n$, of a pulsar is a measure of its angular momentum loss\nand the value it takes corresponds to different spin-down mechanisms. For a\npulsar spinning down due to gravitational wave emission from the principal mass\nquadrupole mode alone, the braking index would equal exactly 5. Unfortunately,\nfor millisecond pulsars, it can be hard to measure observationally due to the\nextremely small second time derivative of the rotation frequency, $\\ddot{f}$.\nThis paper aims to examine whether it could be possible to extract the\ndistribution of $n$ for a whole population of pulsars rather than measuring the\nvalues individually. We use simulated data with an injected $n=5$ signal for 47\nmillisecond pulsars and extract the distribution using hierarchical Bayesian\ninference methods. We find that while possible, observation times of over 20\nyears and RMS noise of the order of $10^{-5}$ ms are needed, which can be\ncompared to the mean noise value of $3\\times10^{-4}$ ms for the recent wideband\n12.5-year NANOGrav sample, which provided the pulsar timing data used in this\npaper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The braking index, $n$, of a pulsar is a measure of its angular momentum loss\nand the value it takes corresponds to different spin-down mechanisms. For a\npulsar spinning down due to gravitational wave emission from the principal mass\nquadrupole mode alone, the braking index would equal exactly 5. Unfortunately,\nfor millisecond pulsars, it can be hard to measure observationally due to the\nextremely small second time derivative of the rotation frequency, $\\ddot{f}$.\nThis paper aims to examine whether it could be possible to extract the\ndistribution of $n$ for a whole population of pulsars rather than measuring the\nvalues individually. We use simulated data with an injected $n=5$ signal for 47\nmillisecond pulsars and extract the distribution using hierarchical Bayesian\ninference methods. We find that while possible, observation times of over 20\nyears and RMS noise of the order of $10^{-5}$ ms are needed, which can be\ncompared to the mean noise value of $3\\times10^{-4}$ ms for the recent wideband\n12.5-year NANOGrav sample, which provided the pulsar timing data used in this\npaper."
                },
                "authors": [
                    {
                        "name": "A. L. Hewitt"
                    },
                    {
                        "name": "M. Pitkin"
                    },
                    {
                        "name": "I. M. Hook"
                    }
                ],
                "author_detail": {
                    "name": "I. M. Hook"
                },
                "author": "I. M. Hook",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10169v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.03551v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.03551v3",
                "updated": "2024-12-13T14:11:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    35,
                    4,
                    348,
                    0
                ],
                "published": "2024-03-06T08:51:09Z",
                "published_parsed": [
                    2024,
                    3,
                    6,
                    8,
                    51,
                    9,
                    2,
                    66,
                    0
                ],
                "title": "Enhanced Low-Dose CT Image Reconstruction by Domain and Task Shifting\n  Gaussian Denoisers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Low-Dose CT Image Reconstruction by Domain and Task Shifting\n  Gaussian Denoisers"
                },
                "summary": "Computed tomography from a low radiation dose (LDCT) is challenging due to\nhigh noise in the projection data. Popular approaches for LDCT image\nreconstruction are two-stage methods, typically consisting of the filtered\nbackprojection (FBP) algorithm followed by a neural network for LDCT image\nenhancement. Two-stage methods are attractive for their simplicity and\npotential for computational efficiency, typically requiring only a single FBP\nand a neural network forward pass for inference. However, the best\nreconstruction quality is currently achieved by unrolled iterative methods\n(Learned Primal-Dual and ItNet), which are more complex and thus have a higher\ncomputational cost for training and inference. We propose a method combining\nthe simplicity and efficiency of two-stage methods with state-of-the-art\nreconstruction quality. Our strategy utilizes a neural network pretrained for\nGaussian noise removal from natural grayscale images, fine-tuned for LDCT image\nenhancement. We call this method FBP-DTSGD (Domain and Task Shifted Gaussian\nDenoisers) as the fine-tuning is a task shift from Gaussian denoising to\nenhancing LDCT images and a domain shift from natural grayscale to LDCT images.\nAn ablation study with three different pretrained Gaussian denoisers indicates\nthat the performance of FBP-DTSGD does not depend on a specific denoising\narchitecture, suggesting future advancements in Gaussian denoising could\nbenefit the method. The study also shows that pretraining on natural images\nenhances LDCT reconstruction quality, especially with limited training data.\nNotably, pretraining involves no additional cost, as existing pretrained models\nare used. The proposed method currently holds the top mean position in the\nLoDoPaB-CT challenge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computed tomography from a low radiation dose (LDCT) is challenging due to\nhigh noise in the projection data. Popular approaches for LDCT image\nreconstruction are two-stage methods, typically consisting of the filtered\nbackprojection (FBP) algorithm followed by a neural network for LDCT image\nenhancement. Two-stage methods are attractive for their simplicity and\npotential for computational efficiency, typically requiring only a single FBP\nand a neural network forward pass for inference. However, the best\nreconstruction quality is currently achieved by unrolled iterative methods\n(Learned Primal-Dual and ItNet), which are more complex and thus have a higher\ncomputational cost for training and inference. We propose a method combining\nthe simplicity and efficiency of two-stage methods with state-of-the-art\nreconstruction quality. Our strategy utilizes a neural network pretrained for\nGaussian noise removal from natural grayscale images, fine-tuned for LDCT image\nenhancement. We call this method FBP-DTSGD (Domain and Task Shifted Gaussian\nDenoisers) as the fine-tuning is a task shift from Gaussian denoising to\nenhancing LDCT images and a domain shift from natural grayscale to LDCT images.\nAn ablation study with three different pretrained Gaussian denoisers indicates\nthat the performance of FBP-DTSGD does not depend on a specific denoising\narchitecture, suggesting future advancements in Gaussian denoising could\nbenefit the method. The study also shows that pretraining on natural images\nenhances LDCT reconstruction quality, especially with limited training data.\nNotably, pretraining involves no additional cost, as existing pretrained models\nare used. The proposed method currently holds the top mean position in the\nLoDoPaB-CT challenge."
                },
                "authors": [
                    {
                        "name": "Tim Selig"
                    },
                    {
                        "name": "Thomas März"
                    },
                    {
                        "name": "Martin Storath"
                    },
                    {
                        "name": "Andreas Weinmann"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Weinmann"
                },
                "author": "Andreas Weinmann",
                "arxiv_comment": "13 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.03551v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.03551v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06969v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06969v3",
                "updated": "2024-12-13T13:56:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    56,
                    4,
                    4,
                    348,
                    0
                ],
                "published": "2024-04-10T12:29:05Z",
                "published_parsed": [
                    2024,
                    4,
                    10,
                    12,
                    29,
                    5,
                    2,
                    101,
                    0
                ],
                "title": "A Fixed-Point Approach for Causal Generative Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fixed-Point Approach for Causal Generative Modeling"
                },
                "summary": "We propose a novel formalism for describing Structural Causal Models (SCMs)\nas fixed-point problems on causally ordered variables, eliminating the need for\nDirected Acyclic Graphs (DAGs), and establish the weakest known conditions for\ntheir unique recovery given the topological ordering (TO). Based on this, we\ndesign a two-stage causal generative model that first infers in a zero-shot\nmanner a valid TO from observations, and then learns the generative SCM on the\nordered variables. To infer TOs, we propose to amortize the learning of TOs on\nsynthetically generated datasets by sequentially predicting the leaves of\ngraphs seen during training. To learn SCMs, we design a transformer-based\narchitecture that exploits a new attention mechanism enabling the modeling of\ncausal structures, and show that this parameterization is consistent with our\nformalism. Finally, we conduct an extensive evaluation of each method\nindividually, and show that when combined, our model outperforms various\nbaselines on generated out-of-distribution problems. The code is available on\n\\href{https://github.com/microsoft/causica/tree/main/research_experiments/fip}{Github}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel formalism for describing Structural Causal Models (SCMs)\nas fixed-point problems on causally ordered variables, eliminating the need for\nDirected Acyclic Graphs (DAGs), and establish the weakest known conditions for\ntheir unique recovery given the topological ordering (TO). Based on this, we\ndesign a two-stage causal generative model that first infers in a zero-shot\nmanner a valid TO from observations, and then learns the generative SCM on the\nordered variables. To infer TOs, we propose to amortize the learning of TOs on\nsynthetically generated datasets by sequentially predicting the leaves of\ngraphs seen during training. To learn SCMs, we design a transformer-based\narchitecture that exploits a new attention mechanism enabling the modeling of\ncausal structures, and show that this parameterization is consistent with our\nformalism. Finally, we conduct an extensive evaluation of each method\nindividually, and show that when combined, our model outperforms various\nbaselines on generated out-of-distribution problems. The code is available on\n\\href{https://github.com/microsoft/causica/tree/main/research_experiments/fip}{Github}."
                },
                "authors": [
                    {
                        "name": "Meyer Scetbon"
                    },
                    {
                        "name": "Joel Jennings"
                    },
                    {
                        "name": "Agrin Hilmkil"
                    },
                    {
                        "name": "Cheng Zhang"
                    },
                    {
                        "name": "Chao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chao Ma"
                },
                "author": "Chao Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06969v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06969v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08918v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08918v2",
                "updated": "2024-12-13T13:43:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    43,
                    11,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-12T04:01:28Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    4,
                    1,
                    28,
                    3,
                    347,
                    0
                ],
                "title": "CSSinger: End-to-End Chunkwise Streaming Singing Voice Synthesis System\n  Based on Conditional Variational Autoencoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSSinger: End-to-End Chunkwise Streaming Singing Voice Synthesis System\n  Based on Conditional Variational Autoencoder"
                },
                "summary": "Singing Voice Synthesis (SVS) aims to generate singing voices of high\nfidelity and expressiveness. Conventional SVS systems usually utilize an\nacoustic model to transform a music score into acoustic features, followed by a\nvocoder to reconstruct the singing voice. It was recently shown that end-to-end\nmodeling is effective in the fields of SVS and Text to Speech (TTS). In this\nwork, we thus present a fully end-to-end SVS method together with a chunkwise\nstreaming inference to address the latency issue for practical usages. Note\nthat this is the first attempt to fully implement end-to-end streaming audio\nsynthesis using latent representations in VAE. We have made specific\nimprovements to enhance the performance of streaming SVS using latent\nrepresentations. Experimental results demonstrate that the proposed method\nachieves synthesized audio with high expressiveness and pitch accuracy in both\nstreaming SVS and TTS tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Singing Voice Synthesis (SVS) aims to generate singing voices of high\nfidelity and expressiveness. Conventional SVS systems usually utilize an\nacoustic model to transform a music score into acoustic features, followed by a\nvocoder to reconstruct the singing voice. It was recently shown that end-to-end\nmodeling is effective in the fields of SVS and Text to Speech (TTS). In this\nwork, we thus present a fully end-to-end SVS method together with a chunkwise\nstreaming inference to address the latency issue for practical usages. Note\nthat this is the first attempt to fully implement end-to-end streaming audio\nsynthesis using latent representations in VAE. We have made specific\nimprovements to enhance the performance of streaming SVS using latent\nrepresentations. Experimental results demonstrate that the proposed method\nachieves synthesized audio with high expressiveness and pitch accuracy in both\nstreaming SVS and TTS tasks."
                },
                "authors": [
                    {
                        "name": "Jianwei Cui"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Shihao Chen"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Liping Chen"
                    },
                    {
                        "name": "Lirong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Lirong Dai"
                },
                "author": "Lirong Dai",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08918v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08918v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10139v1",
                "updated": "2024-12-13T13:41:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    41,
                    24,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T13:41:24Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    41,
                    24,
                    4,
                    348,
                    0
                ],
                "title": "TACOMORE: Leveraging the Potential of LLMs in Corpus-based Discourse\n  Analysis with Prompt Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TACOMORE: Leveraging the Potential of LLMs in Corpus-based Discourse\n  Analysis with Prompt Engineering"
                },
                "summary": "The capacity of LLMs to carry out automated qualitative analysis has been\nquestioned by corpus linguists, and it has been argued that corpus-based\ndiscourse analysis incorporating LLMs is hindered by issues of unsatisfying\nperformance, hallucination, and irreproducibility. Our proposed method,\nTACOMORE, aims to address these concerns by serving as an effective prompting\nframework in this domain. The framework consists of four principles, i.e.,\nTask, Context, Model and Reproducibility, and specifies five fundamental\nelements of a good prompt, i.e., Role Description, Task Definition, Task\nProcedures, Contextual Information and Output Format. We conduct experiments on\nthree LLMs, i.e., GPT-4o, Gemini-1.5-Pro and Gemini-1.5.Flash, and find that\nTACOMORE helps improve LLM performance in three representative discourse\nanalysis tasks, i.e., the analysis of keywords, collocates and concordances,\nbased on an open corpus of COVID-19 research articles. Our findings show the\nefficacy of the proposed prompting framework TACOMORE in corpus-based discourse\nanalysis in terms of Accuracy, Ethicality, Reasoning, and Reproducibility, and\nprovide novel insights into the application and evaluation of LLMs in automated\nqualitative studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capacity of LLMs to carry out automated qualitative analysis has been\nquestioned by corpus linguists, and it has been argued that corpus-based\ndiscourse analysis incorporating LLMs is hindered by issues of unsatisfying\nperformance, hallucination, and irreproducibility. Our proposed method,\nTACOMORE, aims to address these concerns by serving as an effective prompting\nframework in this domain. The framework consists of four principles, i.e.,\nTask, Context, Model and Reproducibility, and specifies five fundamental\nelements of a good prompt, i.e., Role Description, Task Definition, Task\nProcedures, Contextual Information and Output Format. We conduct experiments on\nthree LLMs, i.e., GPT-4o, Gemini-1.5-Pro and Gemini-1.5.Flash, and find that\nTACOMORE helps improve LLM performance in three representative discourse\nanalysis tasks, i.e., the analysis of keywords, collocates and concordances,\nbased on an open corpus of COVID-19 research articles. Our findings show the\nefficacy of the proposed prompting framework TACOMORE in corpus-based discourse\nanalysis in terms of Accuracy, Ethicality, Reasoning, and Reproducibility, and\nprovide novel insights into the application and evaluation of LLMs in automated\nqualitative studies."
                },
                "authors": [
                    {
                        "name": "Bingru Li"
                    },
                    {
                        "name": "Han Wang"
                    }
                ],
                "author_detail": {
                    "name": "Han Wang"
                },
                "author": "Han Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10138v1",
                "updated": "2024-12-13T13:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    41,
                    18,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T13:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    41,
                    18,
                    4,
                    348,
                    0
                ],
                "title": "ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL"
                },
                "summary": "Despite the significant advancements in Text-to-SQL (Text2SQL) facilitated by\nlarge language models (LLMs), the latest state-of-the-art techniques are still\ntrapped in the in-context learning of closed-source LLMs (e.g., GPT-4), which\nlimits their applicability in open scenarios. To address this challenge, we\npropose a novel RObust mUltitask Tuning and collaboration mEthod (ROUTE) to\nimprove the comprehensive capabilities of open-source LLMs for Text2SQL,\nthereby providing a more practical solution. Our approach begins with\nmulti-task supervised fine-tuning (SFT) using various synthetic training data\nrelated to SQL generation. Unlike existing SFT-based Text2SQL methods, we\nintroduced several additional SFT tasks, including schema linking, noise\ncorrection, and continuation writing. Engaging in a variety of SQL generation\ntasks enhances the model's understanding of SQL syntax and improves its ability\nto generate high-quality SQL queries. Additionally, inspired by the\ncollaborative modes of LLM agents, we introduce a Multitask Collaboration\nPrompting (MCP) strategy. This strategy leverages collaboration across several\nSQL-related tasks to reduce hallucinations during SQL generation, thereby\nmaximizing the potential of enhancing Text2SQL performance through explicit\nmultitask capabilities. Extensive experiments and in-depth analyses have been\nperformed on eight open-source LLMs and five widely-used benchmarks. The\nresults demonstrate that our proposal outperforms the latest Text2SQL methods\nand yields leading performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the significant advancements in Text-to-SQL (Text2SQL) facilitated by\nlarge language models (LLMs), the latest state-of-the-art techniques are still\ntrapped in the in-context learning of closed-source LLMs (e.g., GPT-4), which\nlimits their applicability in open scenarios. To address this challenge, we\npropose a novel RObust mUltitask Tuning and collaboration mEthod (ROUTE) to\nimprove the comprehensive capabilities of open-source LLMs for Text2SQL,\nthereby providing a more practical solution. Our approach begins with\nmulti-task supervised fine-tuning (SFT) using various synthetic training data\nrelated to SQL generation. Unlike existing SFT-based Text2SQL methods, we\nintroduced several additional SFT tasks, including schema linking, noise\ncorrection, and continuation writing. Engaging in a variety of SQL generation\ntasks enhances the model's understanding of SQL syntax and improves its ability\nto generate high-quality SQL queries. Additionally, inspired by the\ncollaborative modes of LLM agents, we introduce a Multitask Collaboration\nPrompting (MCP) strategy. This strategy leverages collaboration across several\nSQL-related tasks to reduce hallucinations during SQL generation, thereby\nmaximizing the potential of enhancing Text2SQL performance through explicit\nmultitask capabilities. Extensive experiments and in-depth analyses have been\nperformed on eight open-source LLMs and five widely-used benchmarks. The\nresults demonstrate that our proposal outperforms the latest Text2SQL methods\nand yields leading performance."
                },
                "authors": [
                    {
                        "name": "Yang Qin"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Zhihang Fu"
                    },
                    {
                        "name": "Ze Chen"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Peng Hu"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10136v1",
                "updated": "2024-12-13T13:32:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    32,
                    59,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T13:32:59Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    32,
                    59,
                    4,
                    348,
                    0
                ],
                "title": "Can LLMs Convert Graphs to Text-Attributed Graphs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Convert Graphs to Text-Attributed Graphs?"
                },
                "summary": "Graphs are ubiquitous data structures found in numerous real-world\napplications, such as drug discovery, recommender systems, and social network\nanalysis. Graph neural networks (GNNs) have become a popular tool to learn node\nembeddings through message passing on these structures. However, a significant\nchallenge arises when applying GNNs to multiple graphs with different feature\nspaces, as existing GNN architectures are not designed for cross-graph feature\nalignment. To address this, recent approaches introduce text-attributed graphs,\nwhere each node is associated with a textual description, enabling the use of a\nshared textual encoder to project nodes from different graphs into a unified\nfeature space. While promising, this method relies heavily on the availability\nof text-attributed data, which can be difficult to obtain in practice. To\nbridge this gap, we propose a novel method named Topology-Aware Node\ndescription Synthesis (TANS), which leverages large language models (LLMs) to\nautomatically convert existing graphs into text-attributed graphs. The key idea\nis to integrate topological information with each node's properties, enhancing\nthe LLMs' ability to explain how graph topology influences node semantics. We\nevaluate our TANS on text-rich, text-limited, and text-free graphs,\ndemonstrating that it enables a single GNN to operate across diverse graphs.\nNotably, on text-free graphs, our method significantly outperforms existing\napproaches that manually design node features, showcasing the potential of LLMs\nfor preprocessing graph-structured data, even in the absence of textual\ninformation. The code and data are available at\nhttps://github.com/Zehong-Wang/TANS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are ubiquitous data structures found in numerous real-world\napplications, such as drug discovery, recommender systems, and social network\nanalysis. Graph neural networks (GNNs) have become a popular tool to learn node\nembeddings through message passing on these structures. However, a significant\nchallenge arises when applying GNNs to multiple graphs with different feature\nspaces, as existing GNN architectures are not designed for cross-graph feature\nalignment. To address this, recent approaches introduce text-attributed graphs,\nwhere each node is associated with a textual description, enabling the use of a\nshared textual encoder to project nodes from different graphs into a unified\nfeature space. While promising, this method relies heavily on the availability\nof text-attributed data, which can be difficult to obtain in practice. To\nbridge this gap, we propose a novel method named Topology-Aware Node\ndescription Synthesis (TANS), which leverages large language models (LLMs) to\nautomatically convert existing graphs into text-attributed graphs. The key idea\nis to integrate topological information with each node's properties, enhancing\nthe LLMs' ability to explain how graph topology influences node semantics. We\nevaluate our TANS on text-rich, text-limited, and text-free graphs,\ndemonstrating that it enables a single GNN to operate across diverse graphs.\nNotably, on text-free graphs, our method significantly outperforms existing\napproaches that manually design node features, showcasing the potential of LLMs\nfor preprocessing graph-structured data, even in the absence of textual\ninformation. The code and data are available at\nhttps://github.com/Zehong-Wang/TANS."
                },
                "authors": [
                    {
                        "name": "Zehong Wang"
                    },
                    {
                        "name": "Sidney Liu"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Tianyi Ma"
                    },
                    {
                        "name": "Chuxu Zhang"
                    },
                    {
                        "name": "Yanfang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Yanfang Ye"
                },
                "author": "Yanfang Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10135v1",
                "updated": "2024-12-13T13:32:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    32,
                    13,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T13:32:13Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    32,
                    13,
                    4,
                    348,
                    0
                ],
                "title": "ASLoRA: Adaptive Sharing Low-Rank Adaptation Across Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASLoRA: Adaptive Sharing Low-Rank Adaptation Across Layers"
                },
                "summary": "As large language models (LLMs) grow in size, traditional full fine-tuning\nbecomes increasingly impractical due to its high computational and storage\ncosts. Although popular parameter-efficient fine-tuning methods, such as LoRA,\nhave significantly reduced the number of tunable parameters, there is still\nroom for further optimization. In this work, we propose ASLoRA, a cross-layer\nparameter-sharing strategy combining global sharing with partial adaptive\nsharing. Specifically, we share the low-rank matrix A across all layers and\nadaptively merge matrix B during training. This sharing mechanism not only\nmitigates overfitting effectively but also captures inter-layer dependencies,\nsignificantly enhancing the model's representational capability. We conduct\nextensive experiments on various NLP tasks, showing that ASLoRA outperforms\nLoRA while using less than 25% of the parameters, highlighting its flexibility\nand superior parameter efficiency. Furthermore, in-depth analyses of the\nadaptive sharing strategy confirm its significant advantages in enhancing both\nmodel flexibility and task adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) grow in size, traditional full fine-tuning\nbecomes increasingly impractical due to its high computational and storage\ncosts. Although popular parameter-efficient fine-tuning methods, such as LoRA,\nhave significantly reduced the number of tunable parameters, there is still\nroom for further optimization. In this work, we propose ASLoRA, a cross-layer\nparameter-sharing strategy combining global sharing with partial adaptive\nsharing. Specifically, we share the low-rank matrix A across all layers and\nadaptively merge matrix B during training. This sharing mechanism not only\nmitigates overfitting effectively but also captures inter-layer dependencies,\nsignificantly enhancing the model's representational capability. We conduct\nextensive experiments on various NLP tasks, showing that ASLoRA outperforms\nLoRA while using less than 25% of the parameters, highlighting its flexibility\nand superior parameter efficiency. Furthermore, in-depth analyses of the\nadaptive sharing strategy confirm its significant advantages in enhancing both\nmodel flexibility and task adaptability."
                },
                "authors": [
                    {
                        "name": "Junyan Hu"
                    },
                    {
                        "name": "Xue Xiao"
                    },
                    {
                        "name": "Mengqi Zhang"
                    },
                    {
                        "name": "Xiao Chen"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Zhumin Chen"
                    },
                    {
                        "name": "Pengjie Ren"
                    }
                ],
                "author_detail": {
                    "name": "Pengjie Ren"
                },
                "author": "Pengjie Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10133v1",
                "updated": "2024-12-13T13:30:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    30,
                    51,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T13:30:51Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    30,
                    51,
                    4,
                    348,
                    0
                ],
                "title": "You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary\n  Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary\n  Projects"
                },
                "summary": "The ability to execute the test suite of a project is essential in many\nscenarios, e.g., to assess code quality and code coverage, to validate code\nchanges made by developers or automated tools, and to ensure compatibility with\ndependencies. Despite its importance, executing the test suite of a project can\nbe challenging in practice because different projects use different programming\nlanguages, software ecosystems, build systems, testing frameworks, and other\ntools. These challenges make it difficult to create a reliable, universal test\nexecution method that works across different projects. This paper presents\nExecutionAgent, an automated technique that installs arbitrary projects,\nconfigures them to run test cases, and produces project-specific scripts to\nreproduce the setup. Inspired by the way a human developer would address this\ntask, our approach is a large language model-based agent that autonomously\nexecutes commands and interacts with the host system. The agent uses\nmeta-prompting to gather guidelines on the latest technologies related to the\ngiven project, and it iteratively refines its process based on feedback from\nthe previous steps. Our evaluation applies ExecutionAgent to 50 open-source\nprojects that use 14 different programming languages and many different build\nand testing tools. The approach successfully executes the test suites of 33/55\nprojects, while matching the test results of ground truth test suite executions\nwith a deviation of only 7.5\\%. These results improve over the best previously\navailable technique by 6.6x. The costs imposed by the approach are reasonable,\nwith an execution time of 74 minutes and LLM costs of 0.16 dollars, on average\nper project. We envision ExecutionAgent to serve as a valuable tool for\ndevelopers, automated programming tools, and researchers that need to execute\ntests across a wide variety of projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to execute the test suite of a project is essential in many\nscenarios, e.g., to assess code quality and code coverage, to validate code\nchanges made by developers or automated tools, and to ensure compatibility with\ndependencies. Despite its importance, executing the test suite of a project can\nbe challenging in practice because different projects use different programming\nlanguages, software ecosystems, build systems, testing frameworks, and other\ntools. These challenges make it difficult to create a reliable, universal test\nexecution method that works across different projects. This paper presents\nExecutionAgent, an automated technique that installs arbitrary projects,\nconfigures them to run test cases, and produces project-specific scripts to\nreproduce the setup. Inspired by the way a human developer would address this\ntask, our approach is a large language model-based agent that autonomously\nexecutes commands and interacts with the host system. The agent uses\nmeta-prompting to gather guidelines on the latest technologies related to the\ngiven project, and it iteratively refines its process based on feedback from\nthe previous steps. Our evaluation applies ExecutionAgent to 50 open-source\nprojects that use 14 different programming languages and many different build\nand testing tools. The approach successfully executes the test suites of 33/55\nprojects, while matching the test results of ground truth test suite executions\nwith a deviation of only 7.5\\%. These results improve over the best previously\navailable technique by 6.6x. The costs imposed by the approach are reasonable,\nwith an execution time of 74 minutes and LLM costs of 0.16 dollars, on average\nper project. We envision ExecutionAgent to serve as a valuable tool for\ndevelopers, automated programming tools, and researchers that need to execute\ntests across a wide variety of projects."
                },
                "authors": [
                    {
                        "name": "Islem Bouzenia"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10131v1",
                "updated": "2024-12-13T13:27:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    27,
                    8,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T13:27:08Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    27,
                    8,
                    4,
                    348,
                    0
                ],
                "title": "RTFAST-Spectra: Emulation of X-ray reverberation mapping for active\n  galactic nuclei",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTFAST-Spectra: Emulation of X-ray reverberation mapping for active\n  galactic nuclei"
                },
                "summary": "Bayesian analysis has begun to be more widely adopted in X-ray spectroscopy,\nbut it has largely been constrained to relatively simple physical models due to\nlimitations in X-ray modelling software and computation time. As a result,\nBayesian analysis of numerical models with high physics complexity have\nremained out of reach. This is a challenge, for example when modelling the\nX-ray emission of accreting black hole X-ray binaries, where the slow model\ncomputations severely limit explorations of parameter space and may bias the\ninference of astrophysical parameters. Here, we present RTFAST-Spectra: a\nneural network emulator that acts as a drop in replacement for the spectral\nportion of the black hole X-ray reverberation model RTDIST. This is the first\nemulator for the reltrans model suite and the first emulator for a\nstate-of-the-art x-ray reflection model incorporating relativistic effects with\n17 physically meaningful model parameters. We use Principal Component Analysis\nto create a light-weight neural network that is able to preserve correlations\nbetween complex atomic lines and simple continuum, enabling consistent\nmodelling of key parameters of scientific interest. We achieve a\n$\\mathcal{O}(10^2)$ times speed up over the original model in the most\nconservative conditions with $\\mathcal{O}(1\\%)$ precision over all 17 free\nparameters in the original numerical model, taking full posterior fits from\nmonths to hours. We employ Markov Chain Monte Carlo sampling to show how we can\nbetter explore the posteriors of model parameters in simulated data and discuss\nthe complexities in interpreting the model when fitting real data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian analysis has begun to be more widely adopted in X-ray spectroscopy,\nbut it has largely been constrained to relatively simple physical models due to\nlimitations in X-ray modelling software and computation time. As a result,\nBayesian analysis of numerical models with high physics complexity have\nremained out of reach. This is a challenge, for example when modelling the\nX-ray emission of accreting black hole X-ray binaries, where the slow model\ncomputations severely limit explorations of parameter space and may bias the\ninference of astrophysical parameters. Here, we present RTFAST-Spectra: a\nneural network emulator that acts as a drop in replacement for the spectral\nportion of the black hole X-ray reverberation model RTDIST. This is the first\nemulator for the reltrans model suite and the first emulator for a\nstate-of-the-art x-ray reflection model incorporating relativistic effects with\n17 physically meaningful model parameters. We use Principal Component Analysis\nto create a light-weight neural network that is able to preserve correlations\nbetween complex atomic lines and simple continuum, enabling consistent\nmodelling of key parameters of scientific interest. We achieve a\n$\\mathcal{O}(10^2)$ times speed up over the original model in the most\nconservative conditions with $\\mathcal{O}(1\\%)$ precision over all 17 free\nparameters in the original numerical model, taking full posterior fits from\nmonths to hours. We employ Markov Chain Monte Carlo sampling to show how we can\nbetter explore the posteriors of model parameters in simulated data and discuss\nthe complexities in interpreting the model when fitting real data."
                },
                "authors": [
                    {
                        "name": "Benjamin Ricketts"
                    },
                    {
                        "name": "Daniela Huppenkothen"
                    },
                    {
                        "name": "Matteo Lucchini"
                    },
                    {
                        "name": "Adam Ingram"
                    },
                    {
                        "name": "Guglielmo Mastroserio"
                    },
                    {
                        "name": "Matthew Ho"
                    },
                    {
                        "name": "Benjamin Wandelt"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Wandelt"
                },
                "author": "Benjamin Wandelt",
                "arxiv_comment": "22 pages, 35 figures. Submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10117v1",
                "updated": "2024-12-13T12:59:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    12,
                    59,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T12:59:39Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    12,
                    59,
                    39,
                    4,
                    348,
                    0
                ],
                "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language\n  Models"
                },
                "summary": "In our previous work, we introduced CosyVoice, a multilingual speech\nsynthesis model based on supervised discrete speech tokens. By employing\nprogressive semantic decoding with two popular generative models, language\nmodels (LMs) and Flow Matching, CosyVoice demonstrated high prosody\nnaturalness, content consistency, and speaker similarity in speech in-context\nlearning. Recently, significant progress has been made in multi-modal large\nlanguage models (LLMs), where the response latency and real-time factor of\nspeech synthesis play a crucial role in the interactive experience. Therefore,\nin this report, we present an improved streaming speech synthesis model,\nCosyVoice 2, which incorporates comprehensive and systematic optimizations.\nSpecifically, we introduce finite-scalar quantization to improve the codebook\nutilization of speech tokens. For the text-speech LM, we streamline the model\narchitecture to allow direct use of a pre-trained LLM as the backbone. In\naddition, we develop a chunk-aware causal flow matching model to support\nvarious synthesis scenarios, enabling both streaming and non-streaming\nsynthesis within a single model. By training on a large-scale multilingual\ndataset, CosyVoice 2 achieves human-parity naturalness, minimal response\nlatency, and virtually lossless synthesis quality in the streaming mode. We\ninvite readers to listen to the demos at\nhttps://funaudiollm.github.io/cosyvoice2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our previous work, we introduced CosyVoice, a multilingual speech\nsynthesis model based on supervised discrete speech tokens. By employing\nprogressive semantic decoding with two popular generative models, language\nmodels (LMs) and Flow Matching, CosyVoice demonstrated high prosody\nnaturalness, content consistency, and speaker similarity in speech in-context\nlearning. Recently, significant progress has been made in multi-modal large\nlanguage models (LLMs), where the response latency and real-time factor of\nspeech synthesis play a crucial role in the interactive experience. Therefore,\nin this report, we present an improved streaming speech synthesis model,\nCosyVoice 2, which incorporates comprehensive and systematic optimizations.\nSpecifically, we introduce finite-scalar quantization to improve the codebook\nutilization of speech tokens. For the text-speech LM, we streamline the model\narchitecture to allow direct use of a pre-trained LLM as the backbone. In\naddition, we develop a chunk-aware causal flow matching model to support\nvarious synthesis scenarios, enabling both streaming and non-streaming\nsynthesis within a single model. By training on a large-scale multilingual\ndataset, CosyVoice 2 achieves human-parity naturalness, minimal response\nlatency, and virtually lossless synthesis quality in the streaming mode. We\ninvite readers to listen to the demos at\nhttps://funaudiollm.github.io/cosyvoice2."
                },
                "authors": [
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xian Shi"
                    },
                    {
                        "name": "Xiang Lv"
                    },
                    {
                        "name": "Tianyu Zhao"
                    },
                    {
                        "name": "Zhifu Gao"
                    },
                    {
                        "name": "Yexin Yang"
                    },
                    {
                        "name": "Changfeng Gao"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Huadai Liu"
                    },
                    {
                        "name": "Zhengyan Sheng"
                    },
                    {
                        "name": "Yue Gu"
                    },
                    {
                        "name": "Chong Deng"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Zhijie Yan"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "Tech report, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10107v1",
                "updated": "2024-12-13T12:48:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    12,
                    48,
                    15,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T12:48:15Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    12,
                    48,
                    15,
                    4,
                    348,
                    0
                ],
                "title": "NetOrchLLM: Mastering Wireless Network Orchestration with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetOrchLLM: Mastering Wireless Network Orchestration with Large Language\n  Models"
                },
                "summary": "The transition to 6G networks promises unprecedented advancements in wireless\ncommunication, with increased data rates, ultra-low latency, and enhanced\ncapacity. However, the complexity of managing and optimizing these\nnext-generation networks presents significant challenges. The advent of large\nlanguage models (LLMs) has revolutionized various domains by leveraging their\nsophisticated natural language understanding capabilities. However, the\npractical application of LLMs in wireless network orchestration and management\nremains largely unexplored. Existing literature predominantly offers visionary\nperspectives without concrete implementations, leaving a significant gap in the\nfield. To address this gap, this paper presents NETORCHLLM, a wireless NETwork\nORCHestrator LLM framework that uses LLMs to seamlessly orchestrate diverse\nwireless-specific models from wireless communication communities using their\nlanguage understanding and generation capabilities. A comprehensive framework\nis introduced, demonstrating the practical viability of our approach and\nshowcasing how LLMs can be effectively harnessed to optimize dense network\noperations, manage dynamic environments, and improve overall network\nperformance. NETORCHLLM bridges the theoretical aspirations of prior research\nwith practical, actionable solutions, paving the way for future advancements in\nintegrating generative AI technologies within the wireless communications\nsector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition to 6G networks promises unprecedented advancements in wireless\ncommunication, with increased data rates, ultra-low latency, and enhanced\ncapacity. However, the complexity of managing and optimizing these\nnext-generation networks presents significant challenges. The advent of large\nlanguage models (LLMs) has revolutionized various domains by leveraging their\nsophisticated natural language understanding capabilities. However, the\npractical application of LLMs in wireless network orchestration and management\nremains largely unexplored. Existing literature predominantly offers visionary\nperspectives without concrete implementations, leaving a significant gap in the\nfield. To address this gap, this paper presents NETORCHLLM, a wireless NETwork\nORCHestrator LLM framework that uses LLMs to seamlessly orchestrate diverse\nwireless-specific models from wireless communication communities using their\nlanguage understanding and generation capabilities. A comprehensive framework\nis introduced, demonstrating the practical viability of our approach and\nshowcasing how LLMs can be effectively harnessed to optimize dense network\noperations, manage dynamic environments, and improve overall network\nperformance. NETORCHLLM bridges the theoretical aspirations of prior research\nwith practical, actionable solutions, paving the way for future advancements in\nintegrating generative AI technologies within the wireless communications\nsector."
                },
                "authors": [
                    {
                        "name": "Asmaa Abdallah"
                    },
                    {
                        "name": "Abdullatif Albaseer"
                    },
                    {
                        "name": "Abdulkadir Celik"
                    },
                    {
                        "name": "Mohamed Abdallah"
                    },
                    {
                        "name": "Ahmed M. Eltawil"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed M. Eltawil"
                },
                "author": "Ahmed M. Eltawil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07205v2",
                "updated": "2024-12-13T12:38:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    12,
                    38,
                    4,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-10T05:50:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    5,
                    50,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "Crack-EdgeSAM Self-Prompting Crack Segmentation System for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Crack-EdgeSAM Self-Prompting Crack Segmentation System for Edge Devices"
                },
                "summary": "Structural health monitoring (SHM) is essential for the early detection of\ninfrastructure defects, such as cracks in concrete bridge pier. but often faces\nchallenges in efficiency and accuracy in complex environments. Although the\nSegment Anything Model (SAM) achieves excellent segmentation performance, its\ncomputational demands limit its suitability for real-time applications on edge\ndevices. To address these challenges, this paper proposes Crack-EdgeSAM, a\nself-prompting crack segmentation system that integrates YOLOv8 for generating\nprompt boxes and a fine-tuned EdgeSAM model for crack segmentation. To ensure\ncomputational efficiency, the method employs ConvLoRA, a Parameter-Efficient\nFine-Tuning (PEFT) technique, along with DiceFocalLoss to fine-tune the EdgeSAM\nmodel. Our experimental results on public datasets and the climbing robot\nautomatic inspections demonstrate that the system achieves high segmentation\naccuracy and significantly enhanced inference speed compared to the most recent\nmethods. Notably, the system processes 1024 x 1024 pixels images at 46 FPS on\nour PC and 8 FPS on Jetson Orin Nano.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structural health monitoring (SHM) is essential for the early detection of\ninfrastructure defects, such as cracks in concrete bridge pier. but often faces\nchallenges in efficiency and accuracy in complex environments. Although the\nSegment Anything Model (SAM) achieves excellent segmentation performance, its\ncomputational demands limit its suitability for real-time applications on edge\ndevices. To address these challenges, this paper proposes Crack-EdgeSAM, a\nself-prompting crack segmentation system that integrates YOLOv8 for generating\nprompt boxes and a fine-tuned EdgeSAM model for crack segmentation. To ensure\ncomputational efficiency, the method employs ConvLoRA, a Parameter-Efficient\nFine-Tuning (PEFT) technique, along with DiceFocalLoss to fine-tune the EdgeSAM\nmodel. Our experimental results on public datasets and the climbing robot\nautomatic inspections demonstrate that the system achieves high segmentation\naccuracy and significantly enhanced inference speed compared to the most recent\nmethods. Notably, the system processes 1024 x 1024 pixels images at 46 FPS on\nour PC and 8 FPS on Jetson Orin Nano."
                },
                "authors": [
                    {
                        "name": "Yingchu Wang"
                    },
                    {
                        "name": "Ji He"
                    },
                    {
                        "name": "Shijie Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shijie Yu"
                },
                "author": "Shijie Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07646v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07646v3",
                "updated": "2024-12-13T12:35:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    12,
                    35,
                    21,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-10T16:32:19Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    32,
                    19,
                    1,
                    345,
                    0
                ],
                "title": "Searching for Structure: Investigating Emergent Communication with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Searching for Structure: Investigating Emergent Communication with Large\n  Language Models"
                },
                "summary": "Human languages have evolved to be structured through repeated language\nlearning and use. These processes introduce biases that operate during language\nacquisition and shape linguistic systems toward communicative efficiency. In\nthis paper, we investigate whether the same happens if artificial languages are\noptimised for implicit biases of Large Language Models (LLMs). To this end, we\nsimulate a classical referential game in which LLMs learn and use artificial\nlanguages. Our results show that initially unstructured holistic languages are\nindeed shaped to have some structural properties that allow two LLM agents to\ncommunicate successfully. Similar to observations in human experiments,\ngenerational transmission increases the learnability of languages, but can at\nthe same time result in non-humanlike degenerate vocabularies. Taken together,\nthis work extends experimental findings, shows that LLMs can be used as tools\nin simulations of language evolution, and opens possibilities for future\nhuman-machine experiments in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human languages have evolved to be structured through repeated language\nlearning and use. These processes introduce biases that operate during language\nacquisition and shape linguistic systems toward communicative efficiency. In\nthis paper, we investigate whether the same happens if artificial languages are\noptimised for implicit biases of Large Language Models (LLMs). To this end, we\nsimulate a classical referential game in which LLMs learn and use artificial\nlanguages. Our results show that initially unstructured holistic languages are\nindeed shaped to have some structural properties that allow two LLM agents to\ncommunicate successfully. Similar to observations in human experiments,\ngenerational transmission increases the learnability of languages, but can at\nthe same time result in non-humanlike degenerate vocabularies. Taken together,\nthis work extends experimental findings, shows that LLMs can be used as tools\nin simulations of language evolution, and opens possibilities for future\nhuman-machine experiments in this field."
                },
                "authors": [
                    {
                        "name": "Tom Kouwenhoven"
                    },
                    {
                        "name": "Max Peeperkorn"
                    },
                    {
                        "name": "Tessa Verhoef"
                    }
                ],
                "author_detail": {
                    "name": "Tessa Verhoef"
                },
                "author": "Tessa Verhoef",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07646v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07646v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10096v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10096v1",
                "updated": "2024-12-13T12:32:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    12,
                    32,
                    53,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T12:32:53Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    12,
                    32,
                    53,
                    4,
                    348,
                    0
                ],
                "title": "Reward Machine Inference for Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward Machine Inference for Robotic Manipulation"
                },
                "summary": "Learning from Demonstrations (LfD) and Reinforcement Learning (RL) have\nenabled robot agents to accomplish complex tasks. Reward Machines (RMs) enhance\nRL's capability to train policies over extended time horizons by structuring\nhigh-level task information. In this work, we introduce a novel LfD approach\nfor learning RMs directly from visual demonstrations of robotic manipulation\ntasks. Unlike previous methods, our approach requires no predefined\npropositions or prior knowledge of the underlying sparse reward signals.\nInstead, it jointly learns the RM structure and identifies key high-level\nevents that drive transitions between RM states. We validate our method on\nvision-based manipulation tasks, showing that the inferred RM accurately\ncaptures task structure and enables an RL agent to effectively learn an optimal\npolicy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning from Demonstrations (LfD) and Reinforcement Learning (RL) have\nenabled robot agents to accomplish complex tasks. Reward Machines (RMs) enhance\nRL's capability to train policies over extended time horizons by structuring\nhigh-level task information. In this work, we introduce a novel LfD approach\nfor learning RMs directly from visual demonstrations of robotic manipulation\ntasks. Unlike previous methods, our approach requires no predefined\npropositions or prior knowledge of the underlying sparse reward signals.\nInstead, it jointly learns the RM structure and identifies key high-level\nevents that drive transitions between RM states. We validate our method on\nvision-based manipulation tasks, showing that the inferred RM accurately\ncaptures task structure and enables an RL agent to effectively learn an optimal\npolicy."
                },
                "authors": [
                    {
                        "name": "Mattijs Baert"
                    },
                    {
                        "name": "Sam Leroux"
                    },
                    {
                        "name": "Pieter Simoens"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Simoens"
                },
                "author": "Pieter Simoens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10096v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10064v1",
                "updated": "2024-12-13T11:50:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    50,
                    51,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T11:50:51Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    50,
                    51,
                    4,
                    348,
                    0
                ],
                "title": "Text2Cypher: Bridging Natural Language and Graph Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Cypher: Bridging Natural Language and Graph Databases"
                },
                "summary": "Knowledge graphs use nodes, relationships, and properties to represent\narbitrarily complex data. When stored in a graph database, the Cypher query\nlanguage enables efficient modeling and querying of knowledge graphs. However,\nusing Cypher requires specialized knowledge, which can present a challenge for\nnon-expert users. Our work Text2Cypher aims to bridge this gap by translating\nnatural language queries into Cypher query language and extending the utility\nof knowledge graphs to non-technical expert users.\n  While large language models (LLMs) can be used for this purpose, they often\nstruggle to capture complex nuances, resulting in incomplete or incorrect\noutputs. Fine-tuning LLMs on domain-specific datasets has proven to be a more\npromising approach, but the limited availability of high-quality, publicly\navailable Text2Cypher datasets makes this challenging. In this work, we show\nhow we combined, cleaned and organized several publicly available datasets into\na total of 44,387 instances, enabling effective fine-tuning and evaluation.\nModels fine-tuned on this dataset showed significant performance gains, with\nimprovements in Google-BLEU and Exact Match scores over baseline models,\nhighlighting the importance of high-quality datasets and fine-tuning in\nimproving Text2Cypher performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs use nodes, relationships, and properties to represent\narbitrarily complex data. When stored in a graph database, the Cypher query\nlanguage enables efficient modeling and querying of knowledge graphs. However,\nusing Cypher requires specialized knowledge, which can present a challenge for\nnon-expert users. Our work Text2Cypher aims to bridge this gap by translating\nnatural language queries into Cypher query language and extending the utility\nof knowledge graphs to non-technical expert users.\n  While large language models (LLMs) can be used for this purpose, they often\nstruggle to capture complex nuances, resulting in incomplete or incorrect\noutputs. Fine-tuning LLMs on domain-specific datasets has proven to be a more\npromising approach, but the limited availability of high-quality, publicly\navailable Text2Cypher datasets makes this challenging. In this work, we show\nhow we combined, cleaned and organized several publicly available datasets into\na total of 44,387 instances, enabling effective fine-tuning and evaluation.\nModels fine-tuned on this dataset showed significant performance gains, with\nimprovements in Google-BLEU and Exact Match scores over baseline models,\nhighlighting the importance of high-quality datasets and fine-tuning in\nimproving Text2Cypher performance."
                },
                "authors": [
                    {
                        "name": "Makbule Gulcin Ozsoy"
                    },
                    {
                        "name": "Leila Messallem"
                    },
                    {
                        "name": "Jon Besga"
                    },
                    {
                        "name": "Gianandrea Minneci"
                    }
                ],
                "author_detail": {
                    "name": "Gianandrea Minneci"
                },
                "author": "Gianandrea Minneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16667v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16667v3",
                "updated": "2024-12-13T11:50:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    50,
                    50,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-25T06:54:29Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    6,
                    54,
                    29,
                    2,
                    269,
                    0
                ],
                "title": "A Character-Centric Creative Story Generation via Imagination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Character-Centric Creative Story Generation via Imagination"
                },
                "summary": "Creative story generation has long been a goal of NLP research. While\nexisting methodologies have aimed to generate long and coherent stories, they\nfall significantly short of human capabilities in terms of diversity and\ncharacter depth. To address this, we introduce a novel story generation\nframework called CCI (Character-centric Creative story generation via\nImagination). CCI features two modules for creative story generation: IG\n(Image-Guided Imagination) and MW (Multi-Writer model). In the IG module, we\nutilize a text-to-image model to create visual representations of key story\nelements, such as characters, backgrounds, and main plots, in a more novel and\nconcrete manner than text-only approaches. The MW module uses these story\nelements to generate multiple persona-description candidates and selects the\nbest one to insert into the story, thereby enhancing the richness and depth of\nthe narrative. We compared the stories generated by CCI and baseline models\nthrough statistical analysis, as well as human and LLM evaluations. The results\nshowed that the IG and MW modules significantly improve various aspects of the\nstories' creativity. Furthermore, our framework enables interactive multi-modal\nstory generation with users, opening up new possibilities for human-LLM\nintegration in cultural development. Project page : https://www.2024cci.p-e.kr/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative story generation has long been a goal of NLP research. While\nexisting methodologies have aimed to generate long and coherent stories, they\nfall significantly short of human capabilities in terms of diversity and\ncharacter depth. To address this, we introduce a novel story generation\nframework called CCI (Character-centric Creative story generation via\nImagination). CCI features two modules for creative story generation: IG\n(Image-Guided Imagination) and MW (Multi-Writer model). In the IG module, we\nutilize a text-to-image model to create visual representations of key story\nelements, such as characters, backgrounds, and main plots, in a more novel and\nconcrete manner than text-only approaches. The MW module uses these story\nelements to generate multiple persona-description candidates and selects the\nbest one to insert into the story, thereby enhancing the richness and depth of\nthe narrative. We compared the stories generated by CCI and baseline models\nthrough statistical analysis, as well as human and LLM evaluations. The results\nshowed that the IG and MW modules significantly improve various aspects of the\nstories' creativity. Furthermore, our framework enables interactive multi-modal\nstory generation with users, opening up new possibilities for human-LLM\nintegration in cultural development. Project page : https://www.2024cci.p-e.kr/"
                },
                "authors": [
                    {
                        "name": "Kyeongman Park"
                    },
                    {
                        "name": "Minbeom Kim"
                    },
                    {
                        "name": "Kyomin Jung"
                    }
                ],
                "author_detail": {
                    "name": "Kyomin Jung"
                },
                "author": "Kyomin Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16667v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16667v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10061v1",
                "updated": "2024-12-13T11:44:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    44,
                    56,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T11:44:56Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    44,
                    56,
                    4,
                    348,
                    0
                ],
                "title": "Quaffure: Real-Time Quasi-Static Neural Hair Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quaffure: Real-Time Quasi-Static Neural Hair Simulation"
                },
                "summary": "Realistic hair motion is crucial for high-quality avatars, but it is often\nlimited by the computational resources available for real-time applications. To\naddress this challenge, we propose a novel neural approach to predict\nphysically plausible hair deformations that generalizes to various body poses,\nshapes, and hairstyles. Our model is trained using a self-supervised loss,\neliminating the need for expensive data generation and storage. We demonstrate\nour method's effectiveness through numerous results across a wide range of pose\nand shape variations, showcasing its robust generalization capabilities and\ntemporally smooth results. Our approach is highly suitable for real-time\napplications with an inference time of only a few milliseconds on consumer\nhardware and its ability to scale to predicting the drape of 1000 grooms in 0.3\nseconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realistic hair motion is crucial for high-quality avatars, but it is often\nlimited by the computational resources available for real-time applications. To\naddress this challenge, we propose a novel neural approach to predict\nphysically plausible hair deformations that generalizes to various body poses,\nshapes, and hairstyles. Our model is trained using a self-supervised loss,\neliminating the need for expensive data generation and storage. We demonstrate\nour method's effectiveness through numerous results across a wide range of pose\nand shape variations, showcasing its robust generalization capabilities and\ntemporally smooth results. Our approach is highly suitable for real-time\napplications with an inference time of only a few milliseconds on consumer\nhardware and its ability to scale to predicting the drape of 1000 grooms in 0.3\nseconds."
                },
                "authors": [
                    {
                        "name": "Tuur Stuyck"
                    },
                    {
                        "name": "Gene Wei-Chin Lin"
                    },
                    {
                        "name": "Egor Larionov"
                    },
                    {
                        "name": "Hsiao-yu Chen"
                    },
                    {
                        "name": "Aljaz Bozic"
                    },
                    {
                        "name": "Nikolaos Sarafianos"
                    },
                    {
                        "name": "Doug Roble"
                    }
                ],
                "author_detail": {
                    "name": "Doug Roble"
                },
                "author": "Doug Roble",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10059v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10059v1",
                "updated": "2024-12-13T11:44:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    44,
                    9,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T11:44:09Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    44,
                    9,
                    4,
                    348,
                    0
                ],
                "title": "Panacea: Novel DNN Accelerator using Accuracy-Preserving Asymmetric\n  Quantization and Energy-Saving Bit-Slice Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panacea: Novel DNN Accelerator using Accuracy-Preserving Asymmetric\n  Quantization and Energy-Saving Bit-Slice Sparsity"
                },
                "summary": "Low bit-precisions and their bit-slice sparsity have recently been studied to\naccelerate general matrix-multiplications (GEMM) during large-scale deep neural\nnetwork (DNN) inferences. While the conventional symmetric quantization\nfacilitates low-resolution processing with bit-slice sparsity for both weight\nand activation, its accuracy loss caused by the activation's asymmetric\ndistributions cannot be acceptable, especially for large-scale DNNs. In efforts\nto mitigate this accuracy loss, recent studies have actively utilized\nasymmetric quantization for activations without requiring additional\noperations. However, the cutting-edge asymmetric quantization produces numerous\nnonzero slices that cannot be compressed and skipped by recent bit-slice GEMM\naccelerators, naturally consuming more processing energy to handle the\nquantized DNN models.\n  To simultaneously achieve high accuracy and hardware efficiency for\nlarge-scale DNN inferences, this paper proposes an Asymmetrically-Quantized\nbit-Slice GEMM (AQS-GEMM) for the first time. In contrast to the previous\nbit-slice computing, which only skips operations of zero slices, the AQS-GEMM\ncompresses frequent nonzero slices, generated by asymmetric quantization, and\nskips their operations. To increase the slice-level sparsity of activations, we\nalso introduce two algorithm-hardware co-optimization methods: a zero-point\nmanipulation and a distribution-based bit-slicing. To support the proposed\nAQS-GEMM and optimizations at the hardware-level, we newly introduce a DNN\naccelerator, Panacea, which efficiently handles sparse/dense workloads of the\ntiled AQS-GEMM to increase data reuse and utilization. Panacea supports a\nspecialized dataflow and run-length encoding to maximize data reuse and\nminimize external memory accesses, significantly improving its hardware\nefficiency. Our benchmark evaluations show Panacea outperforms existing DNN\naccelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low bit-precisions and their bit-slice sparsity have recently been studied to\naccelerate general matrix-multiplications (GEMM) during large-scale deep neural\nnetwork (DNN) inferences. While the conventional symmetric quantization\nfacilitates low-resolution processing with bit-slice sparsity for both weight\nand activation, its accuracy loss caused by the activation's asymmetric\ndistributions cannot be acceptable, especially for large-scale DNNs. In efforts\nto mitigate this accuracy loss, recent studies have actively utilized\nasymmetric quantization for activations without requiring additional\noperations. However, the cutting-edge asymmetric quantization produces numerous\nnonzero slices that cannot be compressed and skipped by recent bit-slice GEMM\naccelerators, naturally consuming more processing energy to handle the\nquantized DNN models.\n  To simultaneously achieve high accuracy and hardware efficiency for\nlarge-scale DNN inferences, this paper proposes an Asymmetrically-Quantized\nbit-Slice GEMM (AQS-GEMM) for the first time. In contrast to the previous\nbit-slice computing, which only skips operations of zero slices, the AQS-GEMM\ncompresses frequent nonzero slices, generated by asymmetric quantization, and\nskips their operations. To increase the slice-level sparsity of activations, we\nalso introduce two algorithm-hardware co-optimization methods: a zero-point\nmanipulation and a distribution-based bit-slicing. To support the proposed\nAQS-GEMM and optimizations at the hardware-level, we newly introduce a DNN\naccelerator, Panacea, which efficiently handles sparse/dense workloads of the\ntiled AQS-GEMM to increase data reuse and utilization. Panacea supports a\nspecialized dataflow and run-length encoding to maximize data reuse and\nminimize external memory accesses, significantly improving its hardware\nefficiency. Our benchmark evaluations show Panacea outperforms existing DNN\naccelerators."
                },
                "authors": [
                    {
                        "name": "Dongyun Kam"
                    },
                    {
                        "name": "Myeongji Yun"
                    },
                    {
                        "name": "Sunwoo Yoo"
                    },
                    {
                        "name": "Seungwoo Hong"
                    },
                    {
                        "name": "Zhengya Zhang"
                    },
                    {
                        "name": "Youngjoo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Youngjoo Lee"
                },
                "author": "Youngjoo Lee",
                "arxiv_comment": "15 pages, 20 figures, Accepted to HPCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10059v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10059v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10056v1",
                "updated": "2024-12-13T11:38:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    38,
                    10,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T11:38:10Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    38,
                    10,
                    4,
                    348,
                    0
                ],
                "title": "GAOKAO-Eval: Does high scores truly reflect strong capabilities in LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GAOKAO-Eval: Does high scores truly reflect strong capabilities in LLMs?"
                },
                "summary": "Large Language Models (LLMs) are commonly evaluated using human-crafted\nbenchmarks, under the premise that higher scores implicitly reflect stronger\nhuman-like performance. However, there is growing concern that LLMs may ``game\"\nthese benchmarks due to data leakage, achieving high scores while struggling\nwith tasks simple for humans. To substantively address the problem, we create\nGAOKAO-Eval, a comprehensive benchmark based on China's National College\nEntrance Examination (Gaokao), and conduct ``closed-book\" evaluations for\nrepresentative models released prior to Gaokao. Contrary to prevailing\nconsensus, even after addressing data leakage and comprehensiveness,\nGAOKAO-Eval reveals that high scores still fail to truly reflect human-aligned\ncapabilities. To better understand this mismatch, We introduce the Rasch model\nfrom cognitive psychology to analyze LLM scoring patterns and identify two key\ndiscrepancies: 1) anomalous consistent performance across various question\ndifficulties, and 2) high variance in performance on questions of similar\ndifficulty. In addition, We identified inconsistent grading of LLM-generated\nanswers among teachers and recurring mistake patterns. we find that the\nphenomenons are well-grounded in the motivations behind OpenAI o1, and o1's\nreasoning-as-difficulties can mitigate the mismatch. These results show that\nGAOKAO-Eval can reveal limitations in LLM capabilities not captured by current\nbenchmarks and highlight the need for more LLM-aligned difficulty analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are commonly evaluated using human-crafted\nbenchmarks, under the premise that higher scores implicitly reflect stronger\nhuman-like performance. However, there is growing concern that LLMs may ``game\"\nthese benchmarks due to data leakage, achieving high scores while struggling\nwith tasks simple for humans. To substantively address the problem, we create\nGAOKAO-Eval, a comprehensive benchmark based on China's National College\nEntrance Examination (Gaokao), and conduct ``closed-book\" evaluations for\nrepresentative models released prior to Gaokao. Contrary to prevailing\nconsensus, even after addressing data leakage and comprehensiveness,\nGAOKAO-Eval reveals that high scores still fail to truly reflect human-aligned\ncapabilities. To better understand this mismatch, We introduce the Rasch model\nfrom cognitive psychology to analyze LLM scoring patterns and identify two key\ndiscrepancies: 1) anomalous consistent performance across various question\ndifficulties, and 2) high variance in performance on questions of similar\ndifficulty. In addition, We identified inconsistent grading of LLM-generated\nanswers among teachers and recurring mistake patterns. we find that the\nphenomenons are well-grounded in the motivations behind OpenAI o1, and o1's\nreasoning-as-difficulties can mitigate the mismatch. These results show that\nGAOKAO-Eval can reveal limitations in LLM capabilities not captured by current\nbenchmarks and highlight the need for more LLM-aligned difficulty analysis."
                },
                "authors": [
                    {
                        "name": "Zhikai Lei"
                    },
                    {
                        "name": "Tianyi Liang"
                    },
                    {
                        "name": "Hanglei Hu"
                    },
                    {
                        "name": "Jin Zhang"
                    },
                    {
                        "name": "Yunhua Zhou"
                    },
                    {
                        "name": "Yunfan Shao"
                    },
                    {
                        "name": "Linyang Li"
                    },
                    {
                        "name": "Chenchui Li"
                    },
                    {
                        "name": "Changbo Wang"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Qipeng Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qipeng Guo"
                },
                "author": "Qipeng Guo",
                "arxiv_comment": "10 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10047v1",
                "updated": "2024-12-13T11:19:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    19,
                    56,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T11:19:56Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    19,
                    56,
                    4,
                    348,
                    0
                ],
                "title": "Large Action Models: From Inception to Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Action Models: From Inception to Implementation"
                },
                "summary": "As AI continues to advance, there is a growing demand for systems that go\nbeyond language-based assistance and move toward intelligent agents capable of\nperforming real-world actions. This evolution requires the transition from\ntraditional Large Language Models (LLMs), which excel at generating textual\nresponses, to Large Action Models (LAMs), designed for action generation and\nexecution within dynamic environments. Enabled by agent systems, LAMs hold the\npotential to transform AI from passive language understanding to active task\ncompletion, marking a significant milestone in the progression toward\nartificial general intelligence.\n  In this paper, we present a comprehensive framework for developing LAMs,\noffering a systematic approach to their creation, from inception to deployment.\nWe begin with an overview of LAMs, highlighting their unique characteristics\nand delineating their differences from LLMs. Using a Windows OS-based agent as\na case study, we provide a detailed, step-by-step guide on the key stages of\nLAM development, including data collection, model training, environment\nintegration, grounding, and evaluation. This generalizable workflow can serve\nas a blueprint for creating functional LAMs in various application domains. We\nconclude by identifying the current limitations of LAMs and discussing\ndirections for future research and industrial deployment, emphasizing the\nchallenges and opportunities that lie ahead in realizing the full potential of\nLAMs in real-world applications.\n  The code for the data collection process utilized in this paper is publicly\navailable at: https://github.com/microsoft/UFO/tree/main/dataflow, and\ncomprehensive documentation can be found at\nhttps://microsoft.github.io/UFO/dataflow/overview/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI continues to advance, there is a growing demand for systems that go\nbeyond language-based assistance and move toward intelligent agents capable of\nperforming real-world actions. This evolution requires the transition from\ntraditional Large Language Models (LLMs), which excel at generating textual\nresponses, to Large Action Models (LAMs), designed for action generation and\nexecution within dynamic environments. Enabled by agent systems, LAMs hold the\npotential to transform AI from passive language understanding to active task\ncompletion, marking a significant milestone in the progression toward\nartificial general intelligence.\n  In this paper, we present a comprehensive framework for developing LAMs,\noffering a systematic approach to their creation, from inception to deployment.\nWe begin with an overview of LAMs, highlighting their unique characteristics\nand delineating their differences from LLMs. Using a Windows OS-based agent as\na case study, we provide a detailed, step-by-step guide on the key stages of\nLAM development, including data collection, model training, environment\nintegration, grounding, and evaluation. This generalizable workflow can serve\nas a blueprint for creating functional LAMs in various application domains. We\nconclude by identifying the current limitations of LAMs and discussing\ndirections for future research and industrial deployment, emphasizing the\nchallenges and opportunities that lie ahead in realizing the full potential of\nLAMs in real-world applications.\n  The code for the data collection process utilized in this paper is publicly\navailable at: https://github.com/microsoft/UFO/tree/main/dataflow, and\ncomprehensive documentation can be found at\nhttps://microsoft.github.io/UFO/dataflow/overview/."
                },
                "authors": [
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Junting Lu"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Bo Qiao"
                    },
                    {
                        "name": "Ray Huang"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Qisheng Su"
                    },
                    {
                        "name": "Jiayi Ye"
                    },
                    {
                        "name": "Yudi Zhang"
                    },
                    {
                        "name": "Jian-Guang Lou"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "25pages,12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10038v1",
                "updated": "2024-12-13T10:59:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    10,
                    59,
                    28,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T10:59:28Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    10,
                    59,
                    28,
                    4,
                    348,
                    0
                ],
                "title": "Stochastic Variational Inference for Structured Additive Distributional\n  Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic Variational Inference for Structured Additive Distributional\n  Regression"
                },
                "summary": "In structured additive distributional regression, the conditional\ndistribution of the response variables given the covariate information and the\nvector of model parameters is modelled using a P-parametric probability density\nfunction where each parameter is modelled through a linear predictor and a\nbijective response function that maps the domain of the predictor into the\ndomain of the parameter. We present a method to perform inference in structured\nadditive distributional regression using stochastic variational inference. We\npropose two strategies for constructing a multivariate Gaussian variational\ndistribution to estimate the posterior distribution of the regression\ncoefficients. The first strategy leverages covariate information and\nhyperparameters to learn both the location vector and the precision matrix. The\nsecond strategy tackles the complexity challenges of the first by initially\nassuming independence among all smooth terms and then introducing correlations\nthrough an additional set of variational parameters. Furthermore, we present\ntwo approaches for estimating the smoothing parameters. The first treats them\nas free parameters and provides point estimates, while the second accounts for\nuncertainty by applying a variational approximation to the posterior\ndistribution. Our model was benchmarked against state-of-the-art competitors in\nlogistic and gamma regression simulation studies. Finally, we validated our\napproach by comparing its posterior estimates to those obtained using Markov\nChain Monte Carlo on a dataset of patents from the biotechnology/pharmaceutics\nand semiconductor/computer sectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In structured additive distributional regression, the conditional\ndistribution of the response variables given the covariate information and the\nvector of model parameters is modelled using a P-parametric probability density\nfunction where each parameter is modelled through a linear predictor and a\nbijective response function that maps the domain of the predictor into the\ndomain of the parameter. We present a method to perform inference in structured\nadditive distributional regression using stochastic variational inference. We\npropose two strategies for constructing a multivariate Gaussian variational\ndistribution to estimate the posterior distribution of the regression\ncoefficients. The first strategy leverages covariate information and\nhyperparameters to learn both the location vector and the precision matrix. The\nsecond strategy tackles the complexity challenges of the first by initially\nassuming independence among all smooth terms and then introducing correlations\nthrough an additional set of variational parameters. Furthermore, we present\ntwo approaches for estimating the smoothing parameters. The first treats them\nas free parameters and provides point estimates, while the second accounts for\nuncertainty by applying a variational approximation to the posterior\ndistribution. Our model was benchmarked against state-of-the-art competitors in\nlogistic and gamma regression simulation studies. Finally, we validated our\napproach by comparing its posterior estimates to those obtained using Markov\nChain Monte Carlo on a dataset of patents from the biotechnology/pharmaceutics\nand semiconductor/computer sectors."
                },
                "authors": [
                    {
                        "name": "Gianmarco Callegher"
                    },
                    {
                        "name": "Thomas Kneib"
                    },
                    {
                        "name": "Johannes Söding"
                    },
                    {
                        "name": "Paul Wiemann"
                    }
                ],
                "author_detail": {
                    "name": "Paul Wiemann"
                },
                "author": "Paul Wiemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03536v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03536v3",
                "updated": "2024-12-13T10:55:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    10,
                    55,
                    37,
                    4,
                    348,
                    0
                ],
                "published": "2024-07-03T22:45:36Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    22,
                    45,
                    36,
                    2,
                    185,
                    0
                ],
                "title": "Social Bias in Large Language Models For Bangla: An Empirical Study on\n  Gender and Religious Bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social Bias in Large Language Models For Bangla: An Empirical Study on\n  Gender and Religious Bias"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) has put forward the study of\nbiases as a crucial field. It is important to assess the influence of different\ntypes of biases embedded in LLMs to ensure fair use in sensitive fields.\nAlthough there have been extensive works on bias assessment in English, such\nefforts are rare and scarce for a major language like Bangla. In this work, we\nexamine two types of social biases in LLM generated outputs for Bangla\nlanguage. Our main contributions in this work are: (1) bias studies on two\ndifferent social biases for Bangla, (2) a curated dataset for bias measurement\nbenchmarking and (3) testing two different probing techniques for bias\ndetection in the context of Bangla. This is the first work of such kind\ninvolving bias assessment of LLMs for Bangla to the best of our knowledge. All\nour code and resources are publicly available for the progress of bias related\nresearch in Bangla NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) has put forward the study of\nbiases as a crucial field. It is important to assess the influence of different\ntypes of biases embedded in LLMs to ensure fair use in sensitive fields.\nAlthough there have been extensive works on bias assessment in English, such\nefforts are rare and scarce for a major language like Bangla. In this work, we\nexamine two types of social biases in LLM generated outputs for Bangla\nlanguage. Our main contributions in this work are: (1) bias studies on two\ndifferent social biases for Bangla, (2) a curated dataset for bias measurement\nbenchmarking and (3) testing two different probing techniques for bias\ndetection in the context of Bangla. This is the first work of such kind\ninvolving bias assessment of LLMs for Bangla to the best of our knowledge. All\nour code and resources are publicly available for the progress of bias related\nresearch in Bangla NLP."
                },
                "authors": [
                    {
                        "name": "Jayanta Sadhu"
                    },
                    {
                        "name": "Maneesha Rani Saha"
                    },
                    {
                        "name": "Rifat Shahriyar"
                    }
                ],
                "author_detail": {
                    "name": "Rifat Shahriyar"
                },
                "author": "Rifat Shahriyar",
                "arxiv_comment": "Accepted at The First Workshop on Language Models for Low-Resource\n  Languages (LoResLM) at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03536v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03536v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13125v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13125v2",
                "updated": "2024-12-13T10:48:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    10,
                    48,
                    13,
                    4,
                    348,
                    0
                ],
                "published": "2024-02-20T16:38:33Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    16,
                    38,
                    33,
                    1,
                    51,
                    0
                ],
                "title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through\n  Tree Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeEval: Benchmark-Free Evaluation of Large Language Models through\n  Tree Planning"
                },
                "summary": "Recently, numerous new benchmarks have been established to evaluate the\nperformance of large language models (LLMs) via either computing a holistic\nscore or employing another LLM as a judge. However, these approaches suffer\nfrom data leakage due to the open access of the benchmark and inflexible\nevaluation process. To address this issue, we introduce $\\textbf{TreeEval}$, a\nbenchmark-free evaluation method for LLMs that let a high-performance LLM host\nan irreproducible evaluation session and essentially avoids the data leakage.\nMoreover, this LLM performs as an examiner to raise up a series of questions\nunder a topic with a tree planing strategy, which considers the current\nevaluation status to decide the next question generation and ensures the\ncompleteness and efficiency of the evaluation process. We evaluate $6$ models\nof different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately\nachieved the highest correlation coefficient with AlpacaEval2.0 using only\naround $45$ questions. We also conduct more analysis to show the robustness and\nreliability of TreeEval. Our code can be accessed via the provided\nhttps://github.com/Ashura5/TreeEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, numerous new benchmarks have been established to evaluate the\nperformance of large language models (LLMs) via either computing a holistic\nscore or employing another LLM as a judge. However, these approaches suffer\nfrom data leakage due to the open access of the benchmark and inflexible\nevaluation process. To address this issue, we introduce $\\textbf{TreeEval}$, a\nbenchmark-free evaluation method for LLMs that let a high-performance LLM host\nan irreproducible evaluation session and essentially avoids the data leakage.\nMoreover, this LLM performs as an examiner to raise up a series of questions\nunder a topic with a tree planing strategy, which considers the current\nevaluation status to decide the next question generation and ensures the\ncompleteness and efficiency of the evaluation process. We evaluate $6$ models\nof different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately\nachieved the highest correlation coefficient with AlpacaEval2.0 using only\naround $45$ questions. We also conduct more analysis to show the robustness and\nreliability of TreeEval. Our code can be accessed via the provided\nhttps://github.com/Ashura5/TreeEval."
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yunshi Lan"
                    },
                    {
                        "name": "Chao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Yang"
                },
                "author": "Chao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13125v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13125v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.03067v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.03067v2",
                "updated": "2024-12-13T10:47:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    10,
                    47,
                    35,
                    4,
                    348,
                    0
                ],
                "published": "2023-10-04T18:00:01Z",
                "published_parsed": [
                    2023,
                    10,
                    4,
                    18,
                    0,
                    1,
                    2,
                    277,
                    0
                ],
                "title": "GA-NIFS: A high number of dual AGN at z~3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GA-NIFS: A high number of dual AGN at z~3"
                },
                "summary": "Merger events can trigger gas accretion onto supermassive black holes (SMBHs)\nlocated at the centre of galaxies, and form close pairs of AGN. The fraction of\nAGN in pairs offers critical insights into the dynamics of galaxy interactions,\nSMBH growth, and their co-evolution with host galaxies. However, the\nidentification of dual AGN is difficult, as it requires high-quality spatial\nand spectral data; hence, only very few pairs have been found in the distant\nUniverse so far. This study aims to provide a first observational estimate of\nthe fraction of dual AGN at 2<z<6 by analysing a sample of 16 AGN observed with\nJWST/NIRSpec IFS, as part of the GA-NIFS survey. For two AGN in our sample, we\nalso incorporate archival VLT/MUSE data to expand the search area. We searched\nfor nearby companion galaxies and emission-line sources within the ~20x20 kpc\nfield of view of the NIRSpec data cubes, extending up to ~50 kpc using the MUSE\ndata cubes. We analysed the spectra of such emitters to determine their\nphysical and kinematic properties. We report the serendipitous discovery of a\ntriple AGN and four dual AGN (two of which considered as candidates), with\nprojected separations in the range 3-28 kpc. Their AGN classification is mainly\nbased on standard optical emission line flux ratios, as observed with\nJWST/NIRSpec, and is complemented with additional multi-wavelength diagnostics.\nThe identification of these 3-5 multiple AGN out of the 16 AGN systems in\nGA-NIFS (i.e. ~20-30%) suggests they might be more common than previously\nthought based on other observational campaigns. Moreover, our inferred fraction\nof dual AGN moderately exceeds predictions from cosmological simulations that\nmimic our observational criteria (~10%). This work highlights the exceptional\ncapabilities of NIRSpec for detecting distant dual AGN, and prompts new\ninvestigations to constrain their fraction across cosmic time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merger events can trigger gas accretion onto supermassive black holes (SMBHs)\nlocated at the centre of galaxies, and form close pairs of AGN. The fraction of\nAGN in pairs offers critical insights into the dynamics of galaxy interactions,\nSMBH growth, and their co-evolution with host galaxies. However, the\nidentification of dual AGN is difficult, as it requires high-quality spatial\nand spectral data; hence, only very few pairs have been found in the distant\nUniverse so far. This study aims to provide a first observational estimate of\nthe fraction of dual AGN at 2<z<6 by analysing a sample of 16 AGN observed with\nJWST/NIRSpec IFS, as part of the GA-NIFS survey. For two AGN in our sample, we\nalso incorporate archival VLT/MUSE data to expand the search area. We searched\nfor nearby companion galaxies and emission-line sources within the ~20x20 kpc\nfield of view of the NIRSpec data cubes, extending up to ~50 kpc using the MUSE\ndata cubes. We analysed the spectra of such emitters to determine their\nphysical and kinematic properties. We report the serendipitous discovery of a\ntriple AGN and four dual AGN (two of which considered as candidates), with\nprojected separations in the range 3-28 kpc. Their AGN classification is mainly\nbased on standard optical emission line flux ratios, as observed with\nJWST/NIRSpec, and is complemented with additional multi-wavelength diagnostics.\nThe identification of these 3-5 multiple AGN out of the 16 AGN systems in\nGA-NIFS (i.e. ~20-30%) suggests they might be more common than previously\nthought based on other observational campaigns. Moreover, our inferred fraction\nof dual AGN moderately exceeds predictions from cosmological simulations that\nmimic our observational criteria (~10%). This work highlights the exceptional\ncapabilities of NIRSpec for detecting distant dual AGN, and prompts new\ninvestigations to constrain their fraction across cosmic time."
                },
                "authors": [
                    {
                        "name": "Michele Perna"
                    },
                    {
                        "name": "Santiago Arribas"
                    },
                    {
                        "name": "Isabella Lamperti"
                    },
                    {
                        "name": "Chiara Circosta"
                    },
                    {
                        "name": "Elena Bertola"
                    },
                    {
                        "name": "Pablo G. Pérez-González"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Hannah Übler"
                    },
                    {
                        "name": "Giovanni Cresci"
                    },
                    {
                        "name": "Marta Volonteri"
                    },
                    {
                        "name": "Filippo Mannucci"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Bruno Rodríguez Del Pino"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Stéphane Charlot"
                    },
                    {
                        "name": "Chris J. Willott"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Torsten Böker"
                    },
                    {
                        "name": "Mirko Curti"
                    },
                    {
                        "name": "Gareth Jones"
                    },
                    {
                        "name": "Nimisha Kumari"
                    },
                    {
                        "name": "Madeline A. Marshall"
                    },
                    {
                        "name": "Aayush Saxena"
                    },
                    {
                        "name": "Jan Scholtz"
                    },
                    {
                        "name": "Giacomo Venturi"
                    },
                    {
                        "name": "Joris Witstok"
                    }
                ],
                "author_detail": {
                    "name": "Joris Witstok"
                },
                "author": "Joris Witstok",
                "arxiv_comment": "25 pages, 7 figures, submitted to A&A, comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.03067v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.03067v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10028v1",
                "updated": "2024-12-13T10:39:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    10,
                    39,
                    27,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T10:39:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    10,
                    39,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "Mr. DETR: Instructive Multi-Route Training for Detection Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mr. DETR: Instructive Multi-Route Training for Detection Transformers"
                },
                "summary": "Existing methods enhance the training of detection transformers by\nincorporating an auxiliary one-to-many assignment. In this work, we treat the\nmodel as a multi-task framework, simultaneously performing one-to-one and\none-to-many predictions. We investigate the roles of each component in the\ntransformer decoder across these two training targets, including\nself-attention, cross-attention, and feed-forward network. Our empirical\nresults demonstrate that any independent component in the decoder can\neffectively learn both targets simultaneously, even when other components are\nshared. This finding leads us to propose a multi-route training mechanism,\nfeaturing a primary route for one-to-one prediction and two auxiliary training\nroutes for one-to-many prediction. We enhance the training mechanism with a\nnovel instructive self-attention that dynamically and flexibly guides object\nqueries for one-to-many prediction. The auxiliary routes are removed during\ninference, ensuring no impact on model architecture or inference cost. We\nconduct extensive experiments on various baselines, achieving consistent\nimprovements as shown in Figure 1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing methods enhance the training of detection transformers by\nincorporating an auxiliary one-to-many assignment. In this work, we treat the\nmodel as a multi-task framework, simultaneously performing one-to-one and\none-to-many predictions. We investigate the roles of each component in the\ntransformer decoder across these two training targets, including\nself-attention, cross-attention, and feed-forward network. Our empirical\nresults demonstrate that any independent component in the decoder can\neffectively learn both targets simultaneously, even when other components are\nshared. This finding leads us to propose a multi-route training mechanism,\nfeaturing a primary route for one-to-one prediction and two auxiliary training\nroutes for one-to-many prediction. We enhance the training mechanism with a\nnovel instructive self-attention that dynamically and flexibly guides object\nqueries for one-to-many prediction. The auxiliary routes are removed during\ninference, ensuring no impact on model architecture or inference cost. We\nconduct extensive experiments on various baselines, achieving consistent\nimprovements as shown in Figure 1."
                },
                "authors": [
                    {
                        "name": "Chang-Bin Zhang"
                    },
                    {
                        "name": "Yujie Zhong"
                    },
                    {
                        "name": "Kai Han"
                    }
                ],
                "author_detail": {
                    "name": "Kai Han"
                },
                "author": "Kai Han",
                "arxiv_comment": "Tech. report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06175v2",
                "updated": "2024-12-13T10:11:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    10,
                    11,
                    31,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-09T13:17:39Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    13,
                    17,
                    39,
                    5,
                    314,
                    0
                ],
                "title": "Clustering Algorithms and RAG Enhancing Semi-Supervised Text\n  Classification with Large LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clustering Algorithms and RAG Enhancing Semi-Supervised Text\n  Classification with Large LLMs"
                },
                "summary": "This paper introduces a novel semi-supervised learning framework specifically\ndesigned for text classification tasks, effectively addressing the challenge of\nvast datasets with limited labeled examples. By integrating multi-level\nsimilarity based data augmentation techniques from Retrieval-Augmented\nGeneration (RAG) to Large Language Model (LLM) rewriting and traditional word\nsubstitution-we constructed an intelligent augmentation pipeline. This\nframework innovatively employs the selection of representative landmarks\nthrough clustering, which serve as intermediaries in the retrieval and\nrewriting processes, ensuring that the augmented data maintains a distribution\nsimilar to the original dataset. Empirical results show that even in complex\ntext document classification scenarios with over 100 categories, our method\nachieves state-of-the-art accuracies of 95.41% and 82.43% on the Reuters and\nWeb of Science datasets, respectively. These findings highlight the\neffectiveness and broad applicability of our semi-supervised learning approach\nfor text classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel semi-supervised learning framework specifically\ndesigned for text classification tasks, effectively addressing the challenge of\nvast datasets with limited labeled examples. By integrating multi-level\nsimilarity based data augmentation techniques from Retrieval-Augmented\nGeneration (RAG) to Large Language Model (LLM) rewriting and traditional word\nsubstitution-we constructed an intelligent augmentation pipeline. This\nframework innovatively employs the selection of representative landmarks\nthrough clustering, which serve as intermediaries in the retrieval and\nrewriting processes, ensuring that the augmented data maintains a distribution\nsimilar to the original dataset. Empirical results show that even in complex\ntext document classification scenarios with over 100 categories, our method\nachieves state-of-the-art accuracies of 95.41% and 82.43% on the Reuters and\nWeb of Science datasets, respectively. These findings highlight the\neffectiveness and broad applicability of our semi-supervised learning approach\nfor text classification tasks."
                },
                "authors": [
                    {
                        "name": "Shan Zhong"
                    },
                    {
                        "name": "Jiahao Zeng"
                    },
                    {
                        "name": "Yongxin Yu"
                    },
                    {
                        "name": "Bohong Lin"
                    }
                ],
                "author_detail": {
                    "name": "Bohong Lin"
                },
                "author": "Bohong Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01272v2",
                "updated": "2024-12-13T09:50:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    50,
                    35,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-02T08:38:20Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    38,
                    20,
                    0,
                    337,
                    0
                ],
                "title": "Uncertainty-Aware Artificial Intelligence for Gear Fault Diagnosis in\n  Motor Drives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Aware Artificial Intelligence for Gear Fault Diagnosis in\n  Motor Drives"
                },
                "summary": "This paper introduces a novel approach to quantify the uncertainties in fault\ndiagnosis of motor drives using Bayesian neural networks (BNN). Conventional\ndata-driven approaches used for fault diagnosis often rely on point-estimate\nneural networks, which merely provide deterministic outputs and fail to capture\nthe uncertainty associated with the inference process. In contrast, BNNs offer\na principled framework to model uncertainty by treating network weights as\nprobability distributions rather than fixed values. It offers several\nadvantages: (a) improved robustness to noisy data, (b) enhanced\ninterpretability of model predictions, and (c) the ability to quantify\nuncertainty in the decision-making processes. To test the robustness of the\nproposed BNN, it has been tested under a conservative dataset of gear fault\ndata from an experimental prototype of three fault types at first, and is then\nincrementally trained on new fault classes and datasets to explore its\nuncertainty quantification features and model interpretability under noisy data\nand unseen fault scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach to quantify the uncertainties in fault\ndiagnosis of motor drives using Bayesian neural networks (BNN). Conventional\ndata-driven approaches used for fault diagnosis often rely on point-estimate\nneural networks, which merely provide deterministic outputs and fail to capture\nthe uncertainty associated with the inference process. In contrast, BNNs offer\na principled framework to model uncertainty by treating network weights as\nprobability distributions rather than fixed values. It offers several\nadvantages: (a) improved robustness to noisy data, (b) enhanced\ninterpretability of model predictions, and (c) the ability to quantify\nuncertainty in the decision-making processes. To test the robustness of the\nproposed BNN, it has been tested under a conservative dataset of gear fault\ndata from an experimental prototype of three fault types at first, and is then\nincrementally trained on new fault classes and datasets to explore its\nuncertainty quantification features and model interpretability under noisy data\nand unseen fault scenarios."
                },
                "authors": [
                    {
                        "name": "Subham Sahoo"
                    },
                    {
                        "name": "Huai Wang"
                    },
                    {
                        "name": "Frede Blaabjerg"
                    }
                ],
                "author_detail": {
                    "name": "Frede Blaabjerg"
                },
                "author": "Frede Blaabjerg",
                "arxiv_comment": "The manuscript has been accepted for publication in 2025 IEEE Applied\n  Power Electronics Conference and Exposition (APEC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10008v1",
                "updated": "2024-12-13T09:47:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    47,
                    26,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T09:47:26Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    47,
                    26,
                    4,
                    348,
                    0
                ],
                "title": "Automated Collection of Evaluation Dataset for Semantic Search in\n  Low-Resource Domain Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Collection of Evaluation Dataset for Semantic Search in\n  Low-Resource Domain Language"
                },
                "summary": "Domain-specific languages that use a lot of specific terminology often fall\ninto the category of low-resource languages. Collecting test datasets in a\nnarrow domain is time-consuming and requires skilled human resources with\ndomain knowledge and training for the annotation task. This study addresses the\nchallenge of automated collecting test datasets to evaluate semantic search in\nlow-resource domain-specific German language of the process industry. Our\napproach proposes an end-to-end annotation pipeline for automated query\ngeneration to the score reassessment of query-document pairs. To overcome the\nlack of text encoders trained in the German chemistry domain, we explore a\nprinciple of an ensemble of \"weak\" text encoders trained on common knowledge\ndatasets. We combine individual relevance scores from diverse models to\nretrieve document candidates and relevance scores generated by an LLM, aiming\nto achieve consensus on query-document alignment. Evaluation results\ndemonstrate that the ensemble method significantly improves alignment with\nhuman-assigned relevance scores, outperforming individual models in both\ninter-coder agreement and accuracy metrics. These findings suggest that\nensemble learning can effectively adapt semantic search systems for\nspecialized, low-resource languages, offering a practical solution to resource\nlimitations in domain-specific contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-specific languages that use a lot of specific terminology often fall\ninto the category of low-resource languages. Collecting test datasets in a\nnarrow domain is time-consuming and requires skilled human resources with\ndomain knowledge and training for the annotation task. This study addresses the\nchallenge of automated collecting test datasets to evaluate semantic search in\nlow-resource domain-specific German language of the process industry. Our\napproach proposes an end-to-end annotation pipeline for automated query\ngeneration to the score reassessment of query-document pairs. To overcome the\nlack of text encoders trained in the German chemistry domain, we explore a\nprinciple of an ensemble of \"weak\" text encoders trained on common knowledge\ndatasets. We combine individual relevance scores from diverse models to\nretrieve document candidates and relevance scores generated by an LLM, aiming\nto achieve consensus on query-document alignment. Evaluation results\ndemonstrate that the ensemble method significantly improves alignment with\nhuman-assigned relevance scores, outperforming individual models in both\ninter-coder agreement and accuracy metrics. These findings suggest that\nensemble learning can effectively adapt semantic search systems for\nspecialized, low-resource languages, offering a practical solution to resource\nlimitations in domain-specific contexts."
                },
                "authors": [
                    {
                        "name": "Anastasia Zhukova"
                    },
                    {
                        "name": "Christian E. Matt"
                    },
                    {
                        "name": "Bela Gipp"
                    }
                ],
                "author_detail": {
                    "name": "Bela Gipp"
                },
                "author": "Bela Gipp",
                "arxiv_comment": "accepted in the First Workshop on Language Models for Low-Resource\n  Languages (LoResLM) co-located with the 31st International Conference on\n  Computational Linguistics (COLING 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09998v1",
                "updated": "2024-12-13T09:35:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    35,
                    34,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T09:35:34Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    35,
                    34,
                    4,
                    348,
                    0
                ],
                "title": "Cycle-Consistent Bridge Diffusion Model for Accelerated MRI\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cycle-Consistent Bridge Diffusion Model for Accelerated MRI\n  Reconstruction"
                },
                "summary": "Accelerated MRI reconstruction techniques aim to reduce examination time\nwhile maintaining high image fidelity, which is highly desirable in clinical\nsettings for improving patient comfort and hospital efficiency. Existing deep\nlearning methods typically reconstruct images from under-sampled data with\ntraditional reconstruction approaches, but they still struggle to provide\nhigh-fidelity results. Diffusion models show great potential to improve\nfidelity of generated images in recent years. However, their inference process\nstarting with a random Gaussian noise introduces instability into the results\nand usually requires thousands of sampling steps, resulting in sub-optimal\nreconstruction quality and low efficiency. To address these challenges, we\npropose Cycle-Consistent Bridge Diffusion Model (CBDM). CBDM employs two bridge\ndiffusion models to construct a cycle-consistent diffusion process with a\nconsistency loss, enhancing the fine-grained details of reconstructed images\nand reducing the number of diffusion steps. Moreover, CBDM incorporates a\nContourlet Decomposition Embedding Module (CDEM) which captures multi-scale\nstructural texture knowledge in images through frequency domain decomposition\npyramids and directional filter banks to improve structural fidelity. Extensive\nexperiments demonstrate the superiority of our model by higher reconstruction\nquality and fewer training iterations, achieving a new state of the art for\naccelerated MRI reconstruction in both fastMRI and IXI datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerated MRI reconstruction techniques aim to reduce examination time\nwhile maintaining high image fidelity, which is highly desirable in clinical\nsettings for improving patient comfort and hospital efficiency. Existing deep\nlearning methods typically reconstruct images from under-sampled data with\ntraditional reconstruction approaches, but they still struggle to provide\nhigh-fidelity results. Diffusion models show great potential to improve\nfidelity of generated images in recent years. However, their inference process\nstarting with a random Gaussian noise introduces instability into the results\nand usually requires thousands of sampling steps, resulting in sub-optimal\nreconstruction quality and low efficiency. To address these challenges, we\npropose Cycle-Consistent Bridge Diffusion Model (CBDM). CBDM employs two bridge\ndiffusion models to construct a cycle-consistent diffusion process with a\nconsistency loss, enhancing the fine-grained details of reconstructed images\nand reducing the number of diffusion steps. Moreover, CBDM incorporates a\nContourlet Decomposition Embedding Module (CDEM) which captures multi-scale\nstructural texture knowledge in images through frequency domain decomposition\npyramids and directional filter banks to improve structural fidelity. Extensive\nexperiments demonstrate the superiority of our model by higher reconstruction\nquality and fewer training iterations, achieving a new state of the art for\naccelerated MRI reconstruction in both fastMRI and IXI datasets."
                },
                "authors": [
                    {
                        "name": "Tao Song"
                    },
                    {
                        "name": "Yicheng Wu"
                    },
                    {
                        "name": "Minhao Hu"
                    },
                    {
                        "name": "Xiangde Luo"
                    },
                    {
                        "name": "Guoting Luo"
                    },
                    {
                        "name": "Guotai Wang"
                    },
                    {
                        "name": "Yi Guo"
                    },
                    {
                        "name": "Feng Xu"
                    },
                    {
                        "name": "Shaoting Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shaoting Zhang"
                },
                "author": "Shaoting Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09318v2",
                "updated": "2024-12-13T09:30:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    30,
                    36,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-12T14:43:03Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    3,
                    3,
                    347,
                    0
                ],
                "title": "Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction"
                },
                "summary": "LLMs can generate human-like dialogues, yet their ability to simulate early\nchild-adult interactions remains largely unexplored. In this paper, we examined\nhow effectively LLMs can capture the distinctive features of child-caregiver\nlanguage in interaction, using both static and interactive benchmarking\nmethods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can\napproximate child-caregiver dialogues at the word and utterance level, but they\nstruggle to reproduce the child and caregiver's discursive patterns, exaggerate\nalignment, and fail to reach the level of diversity shown by humans. The\nbroader goal of this work is to initiate the development of a comprehensive\nbenchmark for LLMs in child-oriented applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can generate human-like dialogues, yet their ability to simulate early\nchild-adult interactions remains largely unexplored. In this paper, we examined\nhow effectively LLMs can capture the distinctive features of child-caregiver\nlanguage in interaction, using both static and interactive benchmarking\nmethods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can\napproximate child-caregiver dialogues at the word and utterance level, but they\nstruggle to reproduce the child and caregiver's discursive patterns, exaggerate\nalignment, and fail to reach the level of diversity shown by humans. The\nbroader goal of this work is to initiate the development of a comprehensive\nbenchmark for LLMs in child-oriented applications."
                },
                "authors": [
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Abdellah Fourtassi"
                    }
                ],
                "author_detail": {
                    "name": "Abdellah Fourtassi"
                },
                "author": "Abdellah Fourtassi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09993v1",
                "updated": "2024-12-13T09:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    29,
                    27,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T09:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    29,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "A Comparative Study of LLMs, NMT Models, and Their Combination in\n  Persian-English Idiom Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Study of LLMs, NMT Models, and Their Combination in\n  Persian-English Idiom Translation"
                },
                "summary": "Large language models (LLMs) have shown superior capabilities in translating\nfigurative language compared to neural machine translation (NMT) systems.\nHowever, the impact of different prompting methods and LLM-NMT combinations on\nidiom translation has yet to be thoroughly investigated. This paper introduces\ntwo parallel datasets of sentences containing idiomatic expressions for\nPersian$\\rightarrow$English and English$\\rightarrow$Persian translations, with\nPersian idioms sampled from our PersianIdioms resource, a collection of 2,200\nidioms and their meanings. Using these datasets, we evaluate various open- and\nclosed-source LLMs, NMT models, and their combinations. Translation quality is\nassessed through idiom translation accuracy and fluency. We also find that\nautomatic evaluation methods like LLM-as-a-judge, BLEU and BERTScore are\neffective for comparing different aspects of model performance. Our experiments\nreveal that Claude-3.5-Sonnet delivers outstanding results in both translation\ndirections. For English$\\rightarrow$Persian, combining weaker LLMs with Google\nTranslate improves results, while Persian$\\rightarrow$English translations\nbenefit from single prompts for simpler models and complex prompts for advanced\nones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown superior capabilities in translating\nfigurative language compared to neural machine translation (NMT) systems.\nHowever, the impact of different prompting methods and LLM-NMT combinations on\nidiom translation has yet to be thoroughly investigated. This paper introduces\ntwo parallel datasets of sentences containing idiomatic expressions for\nPersian$\\rightarrow$English and English$\\rightarrow$Persian translations, with\nPersian idioms sampled from our PersianIdioms resource, a collection of 2,200\nidioms and their meanings. Using these datasets, we evaluate various open- and\nclosed-source LLMs, NMT models, and their combinations. Translation quality is\nassessed through idiom translation accuracy and fluency. We also find that\nautomatic evaluation methods like LLM-as-a-judge, BLEU and BERTScore are\neffective for comparing different aspects of model performance. Our experiments\nreveal that Claude-3.5-Sonnet delivers outstanding results in both translation\ndirections. For English$\\rightarrow$Persian, combining weaker LLMs with Google\nTranslate improves results, while Persian$\\rightarrow$English translations\nbenefit from single prompts for simpler models and complex prompts for advanced\nones."
                },
                "authors": [
                    {
                        "name": "Sara Rezaeimanesh"
                    },
                    {
                        "name": "Faezeh Hosseini"
                    },
                    {
                        "name": "Yadollah Yaghoobzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Yadollah Yaghoobzadeh"
                },
                "author": "Yadollah Yaghoobzadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09990v1",
                "updated": "2024-12-13T09:23:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    23,
                    58,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T09:23:58Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    23,
                    58,
                    4,
                    348,
                    0
                ],
                "title": "Small Language Model as Data Prospector for Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Model as Data Prospector for Large Language Model"
                },
                "summary": "The quality of instruction data directly affects the performance of\nfine-tuned Large Language Models (LLMs). Previously, \\cite{li2023one} proposed\n\\texttt{NUGGETS}, which identifies and selects high-quality quality data from a\nlarge dataset by identifying those individual instruction examples that can\nsignificantly improve the performance of different tasks after being learnt as\none-shot instances. In this work, we propose \\texttt{SuperNUGGETS}, an improved\nvariant of \\texttt{NUGGETS} optimised for efficiency and performance. Our\n\\texttt{SuperNUGGETS} uses a small language model (SLM) instead of a large\nlanguage model (LLM) to filter the data for outstanding one-shot instances and\nrefines the predefined set of tests. The experimental results show that the\nperformance of \\texttt{SuperNUGGETS} only decreases by 1-2% compared to\n\\texttt{NUGGETS}, but the efficiency can be increased by a factor of 58.\nCompared to the original \\texttt{NUGGETS}, our \\texttt{SuperNUGGETS} has a\nhigher utility value due to the significantly lower resource consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quality of instruction data directly affects the performance of\nfine-tuned Large Language Models (LLMs). Previously, \\cite{li2023one} proposed\n\\texttt{NUGGETS}, which identifies and selects high-quality quality data from a\nlarge dataset by identifying those individual instruction examples that can\nsignificantly improve the performance of different tasks after being learnt as\none-shot instances. In this work, we propose \\texttt{SuperNUGGETS}, an improved\nvariant of \\texttt{NUGGETS} optimised for efficiency and performance. Our\n\\texttt{SuperNUGGETS} uses a small language model (SLM) instead of a large\nlanguage model (LLM) to filter the data for outstanding one-shot instances and\nrefines the predefined set of tests. The experimental results show that the\nperformance of \\texttt{SuperNUGGETS} only decreases by 1-2% compared to\n\\texttt{NUGGETS}, but the efficiency can be increased by a factor of 58.\nCompared to the original \\texttt{NUGGETS}, our \\texttt{SuperNUGGETS} has a\nhigher utility value due to the significantly lower resource consumption."
                },
                "authors": [
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Haihong Wu"
                    },
                    {
                        "name": "Di Yang"
                    },
                    {
                        "name": "Qiang Qu"
                    },
                    {
                        "name": "Hamid Alinejad-Rokny"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09988v1",
                "updated": "2024-12-13T09:15:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    15,
                    20,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T09:15:20Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    15,
                    20,
                    4,
                    348,
                    0
                ],
                "title": "AI and the Future of Digital Public Squares",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI and the Future of Digital Public Squares"
                },
                "summary": "Two substantial technological advances have reshaped the public square in\nrecent decades: first with the advent of the internet and second with the\nrecent introduction of large language models (LLMs). LLMs offer opportunities\nfor a paradigm shift towards more decentralized, participatory online spaces\nthat can be used to facilitate deliberative dialogues at scale, but also create\nrisks of exacerbating societal schisms. Here, we explore four applications of\nLLMs to improve digital public squares: collective dialogue systems, bridging\nsystems, community moderation, and proof-of-humanity systems. Building on the\ninput from over 70 civil society experts and technologists, we argue that LLMs\nboth afford promising opportunities to shift the paradigm for conversations at\nscale and pose distinct risks for digital public squares. We lay out an agenda\nfor future research and investments in AI that will strengthen digital public\nsquares and safeguard against potential misuses of AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two substantial technological advances have reshaped the public square in\nrecent decades: first with the advent of the internet and second with the\nrecent introduction of large language models (LLMs). LLMs offer opportunities\nfor a paradigm shift towards more decentralized, participatory online spaces\nthat can be used to facilitate deliberative dialogues at scale, but also create\nrisks of exacerbating societal schisms. Here, we explore four applications of\nLLMs to improve digital public squares: collective dialogue systems, bridging\nsystems, community moderation, and proof-of-humanity systems. Building on the\ninput from over 70 civil society experts and technologists, we argue that LLMs\nboth afford promising opportunities to shift the paradigm for conversations at\nscale and pose distinct risks for digital public squares. We lay out an agenda\nfor future research and investments in AI that will strengthen digital public\nsquares and safeguard against potential misuses of AI."
                },
                "authors": [
                    {
                        "name": "Beth Goldberg"
                    },
                    {
                        "name": "Diana Acosta-Navas"
                    },
                    {
                        "name": "Michiel Bakker"
                    },
                    {
                        "name": "Ian Beacock"
                    },
                    {
                        "name": "Matt Botvinick"
                    },
                    {
                        "name": "Prateek Buch"
                    },
                    {
                        "name": "Renée DiResta"
                    },
                    {
                        "name": "Nandika Donthi"
                    },
                    {
                        "name": "Nathanael Fast"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Zaria Jalan"
                    },
                    {
                        "name": "Andrew Konya"
                    },
                    {
                        "name": "Grace Kwak Danciu"
                    },
                    {
                        "name": "Hélène Landemore"
                    },
                    {
                        "name": "Alice Marwick"
                    },
                    {
                        "name": "Carl Miller"
                    },
                    {
                        "name": "Aviv Ovadya"
                    },
                    {
                        "name": "Emily Saltz"
                    },
                    {
                        "name": "Lisa Schirch"
                    },
                    {
                        "name": "Dalit Shalom"
                    },
                    {
                        "name": "Divya Siddarth"
                    },
                    {
                        "name": "Felix Sieker"
                    },
                    {
                        "name": "Christopher Small"
                    },
                    {
                        "name": "Jonathan Stray"
                    },
                    {
                        "name": "Audrey Tang"
                    },
                    {
                        "name": "Michael Henry Tessler"
                    },
                    {
                        "name": "Amy Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Amy Zhang"
                },
                "author": "Amy Zhang",
                "arxiv_comment": "40 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09986v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09986v1",
                "updated": "2024-12-13T09:11:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    11,
                    45,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T09:11:45Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    11,
                    45,
                    4,
                    348,
                    0
                ],
                "title": "On the Geometry of the Near-Core Magnetic Field in Massive Stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Geometry of the Near-Core Magnetic Field in Massive Stars"
                },
                "summary": "It is well-known that the cores of massive stars sustain a stellar dynamo\nwith a complex magnetic field configuration. However, the same cannot be said\nfor the field's strength and geometry at the convective-radiative boundary,\nwhich are crucial when performing asteroseismic inference. In this Letter, we\npresent three-dimensional (3D) magnetohydrodynamic (MHD) simulations of a 7\nsolar mass mid-main sequence star, with particular attention given to the\nconvective-radiative boundary in the near-core region. Our simulations reveal\nthat the toroidal magnetic field is significantly stronger than the poloidal\nfield in this region, contrary to recent assumptions. Moreover, the rotational\nshear layer, also important for asteroseismic inference, is specifically\nconfined within the extent of the buoyancy frequency peak. These results, which\nare based on the inferred properties of HD 43317, have widespread implications\nfor asteroseismic studies of rotation, mixing and magnetism in stars. While we\nexpect our results to be broadly applicable across stars with similar buoyancy\nfrequency profiles and stellar masses, we also expect the MHD parameters and\nthe initial stellar rotation rate to impact the geometry of the field and\ndifferential rotation at the convective-radiative interface.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is well-known that the cores of massive stars sustain a stellar dynamo\nwith a complex magnetic field configuration. However, the same cannot be said\nfor the field's strength and geometry at the convective-radiative boundary,\nwhich are crucial when performing asteroseismic inference. In this Letter, we\npresent three-dimensional (3D) magnetohydrodynamic (MHD) simulations of a 7\nsolar mass mid-main sequence star, with particular attention given to the\nconvective-radiative boundary in the near-core region. Our simulations reveal\nthat the toroidal magnetic field is significantly stronger than the poloidal\nfield in this region, contrary to recent assumptions. Moreover, the rotational\nshear layer, also important for asteroseismic inference, is specifically\nconfined within the extent of the buoyancy frequency peak. These results, which\nare based on the inferred properties of HD 43317, have widespread implications\nfor asteroseismic studies of rotation, mixing and magnetism in stars. While we\nexpect our results to be broadly applicable across stars with similar buoyancy\nfrequency profiles and stellar masses, we also expect the MHD parameters and\nthe initial stellar rotation rate to impact the geometry of the field and\ndifferential rotation at the convective-radiative interface."
                },
                "authors": [
                    {
                        "name": "Rathish P. Ratnasingam"
                    },
                    {
                        "name": "Philipp V. F. Edelmann"
                    },
                    {
                        "name": "Dominic M. Bowman"
                    },
                    {
                        "name": "Tamara M. Rogers"
                    }
                ],
                "author_detail": {
                    "name": "Tamara M. Rogers"
                },
                "author": "Tamara M. Rogers",
                "arxiv_doi": "10.3847/2041-8213/ad95f8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/2041-8213/ad95f8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.09986v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09986v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09971v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09971v1",
                "updated": "2024-12-13T08:58:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    58,
                    42,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T08:58:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    58,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "Limitations of emittance and source size measurement of\n  laser-accelerated electron beams using the pepper-pot mask method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limitations of emittance and source size measurement of\n  laser-accelerated electron beams using the pepper-pot mask method"
                },
                "summary": "The pepper-pot method is a widely used technique originally proposed for\nmeasuring the emittance of space-charge-dominated electron beams from\nradio-frequency photoinjectors. With recent advances in producing\nhigh-brightness electron beams via laser wakefield acceleration (LWFA), the\nmethod has also been applied to evaluate emittance in this new regime [1-3]. In\nthis work, we explore the limitations of the method in inferring the emittance\nand beam waist of LWFA electron beams, showing that the technique becomes\ninaccurate for small emittance values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The pepper-pot method is a widely used technique originally proposed for\nmeasuring the emittance of space-charge-dominated electron beams from\nradio-frequency photoinjectors. With recent advances in producing\nhigh-brightness electron beams via laser wakefield acceleration (LWFA), the\nmethod has also been applied to evaluate emittance in this new regime [1-3]. In\nthis work, we explore the limitations of the method in inferring the emittance\nand beam waist of LWFA electron beams, showing that the technique becomes\ninaccurate for small emittance values."
                },
                "authors": [
                    {
                        "name": "F. C. Salgado"
                    },
                    {
                        "name": "A. Kozan"
                    },
                    {
                        "name": "D. Seipt"
                    },
                    {
                        "name": "D. Hollatz"
                    },
                    {
                        "name": "P. Hilz"
                    },
                    {
                        "name": "M. Kaluza"
                    },
                    {
                        "name": "A. Sävert"
                    },
                    {
                        "name": "A. Seidel"
                    },
                    {
                        "name": "D. Ullmann"
                    },
                    {
                        "name": "Y. Zhao"
                    },
                    {
                        "name": "M. Zepf"
                    }
                ],
                "author_detail": {
                    "name": "M. Zepf"
                },
                "author": "M. Zepf",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09971v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09971v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04680v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04680v2",
                "updated": "2024-12-13T08:44:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    44,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-08-08T04:49:21Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    4,
                    49,
                    21,
                    3,
                    221,
                    0
                ],
                "title": "Dynamic Fog Computing for Enhanced LLM Execution in Medical Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Fog Computing for Enhanced LLM Execution in Medical Applications"
                },
                "summary": "The ability of large language models (LLMs) to transform, interpret, and\ncomprehend vast quantities of heterogeneous data presents a significant\nopportunity to enhance data-driven care delivery. However, the sensitive nature\nof protected health information (PHI) raises valid concerns about data privacy\nand trust in remote LLM platforms. In addition, the cost associated with\ncloud-based artificial intelligence (AI) services continues to impede\nwidespread adoption. To address these challenges, we propose a shift in the LLM\nexecution environment from opaque, centralized cloud providers to a\ndecentralized and dynamic fog computing architecture. By executing open-weight\nLLMs in more trusted environments, such as the user's edge device or a fog\nlayer within a local network, we aim to mitigate the privacy, trust, and\nfinancial challenges associated with cloud-based LLMs. We further present\nSpeziLLM, an open-source framework designed to facilitate rapid and seamless\nleveraging of different LLM execution layers and lowering barriers to LLM\nintegration in digital health applications. We demonstrate SpeziLLM's broad\napplicability across six digital health applications, showcasing its\nversatility in various healthcare settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of large language models (LLMs) to transform, interpret, and\ncomprehend vast quantities of heterogeneous data presents a significant\nopportunity to enhance data-driven care delivery. However, the sensitive nature\nof protected health information (PHI) raises valid concerns about data privacy\nand trust in remote LLM platforms. In addition, the cost associated with\ncloud-based artificial intelligence (AI) services continues to impede\nwidespread adoption. To address these challenges, we propose a shift in the LLM\nexecution environment from opaque, centralized cloud providers to a\ndecentralized and dynamic fog computing architecture. By executing open-weight\nLLMs in more trusted environments, such as the user's edge device or a fog\nlayer within a local network, we aim to mitigate the privacy, trust, and\nfinancial challenges associated with cloud-based LLMs. We further present\nSpeziLLM, an open-source framework designed to facilitate rapid and seamless\nleveraging of different LLM execution layers and lowering barriers to LLM\nintegration in digital health applications. We demonstrate SpeziLLM's broad\napplicability across six digital health applications, showcasing its\nversatility in various healthcare settings."
                },
                "authors": [
                    {
                        "name": "Philipp Zagar"
                    },
                    {
                        "name": "Vishnu Ravi"
                    },
                    {
                        "name": "Lauren Aalami"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Oliver Aalami"
                    },
                    {
                        "name": "Paul Schmiedmayer"
                    }
                ],
                "author_detail": {
                    "name": "Paul Schmiedmayer"
                },
                "author": "Paul Schmiedmayer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04680v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04680v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09265v2",
                "updated": "2024-12-13T08:44:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    44,
                    16,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-12T13:22:02Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    13,
                    22,
                    2,
                    3,
                    347,
                    0
                ],
                "title": "Score and Distribution Matching Policy: Advanced Accelerated Visuomotor\n  Policies via Matched Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Score and Distribution Matching Policy: Advanced Accelerated Visuomotor\n  Policies via Matched Distillation"
                },
                "summary": "Visual-motor policy learning has advanced with architectures like\ndiffusion-based policies, known for modeling complex robotic trajectories.\nHowever, their prolonged inference times hinder high-frequency control tasks\nrequiring real-time feedback. While consistency distillation (CD) accelerates\ninference, it introduces errors that compromise action quality. To address\nthese limitations, we propose the Score and Distribution Matching Policy (SDM\nPolicy), which transforms diffusion-based policies into single-step generators\nthrough a two-stage optimization process: score matching ensures alignment with\ntrue action distributions, and distribution matching minimizes KL divergence\nfor consistency. A dual-teacher mechanism integrates a frozen teacher for\nstability and an unfrozen teacher for adversarial training, enhancing\nrobustness and alignment with target distributions. Evaluated on a 57-task\nsimulation benchmark, SDM Policy achieves a 6x inference speedup while having\nstate-of-the-art action quality, providing an efficient and reliable framework\nfor high-frequency robotic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-motor policy learning has advanced with architectures like\ndiffusion-based policies, known for modeling complex robotic trajectories.\nHowever, their prolonged inference times hinder high-frequency control tasks\nrequiring real-time feedback. While consistency distillation (CD) accelerates\ninference, it introduces errors that compromise action quality. To address\nthese limitations, we propose the Score and Distribution Matching Policy (SDM\nPolicy), which transforms diffusion-based policies into single-step generators\nthrough a two-stage optimization process: score matching ensures alignment with\ntrue action distributions, and distribution matching minimizes KL divergence\nfor consistency. A dual-teacher mechanism integrates a frozen teacher for\nstability and an unfrozen teacher for adversarial training, enhancing\nrobustness and alignment with target distributions. Evaluated on a 57-task\nsimulation benchmark, SDM Policy achieves a 6x inference speedup while having\nstate-of-the-art action quality, providing an efficient and reliable framework\nfor high-frequency robotic tasks."
                },
                "authors": [
                    {
                        "name": "Bofang Jia"
                    },
                    {
                        "name": "Pengxiang Ding"
                    },
                    {
                        "name": "Can Cui"
                    },
                    {
                        "name": "Mingyang Sun"
                    },
                    {
                        "name": "Pengfang Qian"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Zhaoxin Fan"
                    },
                    {
                        "name": "Donglin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Donglin Wang"
                },
                "author": "Donglin Wang",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09962v1",
                "updated": "2024-12-13T08:43:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    43,
                    15,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T08:43:15Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    43,
                    15,
                    4,
                    348,
                    0
                ],
                "title": "Generating 3D Pseudo-Healthy Knee MR Images to Support Trochleoplasty\n  Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating 3D Pseudo-Healthy Knee MR Images to Support Trochleoplasty\n  Planning"
                },
                "summary": "Purpose: Trochlear Dysplasia (TD) is a common malformation in adolescents,\nleading to anterior knee pain and instability. Surgical interventions such as\ntrochleoplasty require precise planning to correct the trochlear groove.\nHowever, no standardized preoperative plan exists to guide surgeons in\nreshaping the femur. This study aims to generate patient-specific,\npseudo-healthy MR images of the trochlear region that should theoretically\nalign with the respective patient's patella, potentially supporting the\npre-operative planning of trochleoplasty.\n  Methods: We employ a Wavelet Diffusion Model (WDM) to generate personalized\npseudo-healthy, anatomically plausible MR scans of the trochlear region. We\ntrain our model using knee MR scans of healthy subjects. During inference, we\nmask out pathological regions around the patella in scans of patients affected\nby TD, and replace them with their pseudo-healthy counterpart. An orthopedic\nsurgeon measured the sulcus angle (SA), trochlear groove depth (TGD) and\nD\\'ejour classification in MR scans before and after inpainting. The code is\navailable at https://github.com/wehrlimi/Generate-Pseudo-Healthy-Knee-MRI .\n  Results: The inpainting by our model significantly improves the SA, TGD and\nD\\'ejour classification in a study with 49 knee MR scans.\n  Conclusion: This study demonstrates the potential of WDMs in providing\nsurgeons with patient-specific guidance. By offering anatomically plausible MR\nscans, the method could potentially enhance the precision and preoperative\nplanning of trochleoplasty, and pave the way to more minimally invasive\nsurgeries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: Trochlear Dysplasia (TD) is a common malformation in adolescents,\nleading to anterior knee pain and instability. Surgical interventions such as\ntrochleoplasty require precise planning to correct the trochlear groove.\nHowever, no standardized preoperative plan exists to guide surgeons in\nreshaping the femur. This study aims to generate patient-specific,\npseudo-healthy MR images of the trochlear region that should theoretically\nalign with the respective patient's patella, potentially supporting the\npre-operative planning of trochleoplasty.\n  Methods: We employ a Wavelet Diffusion Model (WDM) to generate personalized\npseudo-healthy, anatomically plausible MR scans of the trochlear region. We\ntrain our model using knee MR scans of healthy subjects. During inference, we\nmask out pathological regions around the patella in scans of patients affected\nby TD, and replace them with their pseudo-healthy counterpart. An orthopedic\nsurgeon measured the sulcus angle (SA), trochlear groove depth (TGD) and\nD\\'ejour classification in MR scans before and after inpainting. The code is\navailable at https://github.com/wehrlimi/Generate-Pseudo-Healthy-Knee-MRI .\n  Results: The inpainting by our model significantly improves the SA, TGD and\nD\\'ejour classification in a study with 49 knee MR scans.\n  Conclusion: This study demonstrates the potential of WDMs in providing\nsurgeons with patient-specific guidance. By offering anatomically plausible MR\nscans, the method could potentially enhance the precision and preoperative\nplanning of trochleoplasty, and pave the way to more minimally invasive\nsurgeries."
                },
                "authors": [
                    {
                        "name": "Michael Wehrli"
                    },
                    {
                        "name": "Alicia Durrer"
                    },
                    {
                        "name": "Paul Friedrich"
                    },
                    {
                        "name": "Volodimir Buchakchiyskiy"
                    },
                    {
                        "name": "Marcus Mumme"
                    },
                    {
                        "name": "Edwin Li"
                    },
                    {
                        "name": "Gyozo Lehoczky"
                    },
                    {
                        "name": "Carol C. Hasler"
                    },
                    {
                        "name": "Philippe C. Cattin"
                    }
                ],
                "author_detail": {
                    "name": "Philippe C. Cattin"
                },
                "author": "Philippe C. Cattin",
                "arxiv_comment": "Early accepted at IPCAI 2025. Code:\n  https://github.com/wehrlimi/Generate-Pseudo-Healthy-Knee-MRI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09952v1",
                "updated": "2024-12-13T08:22:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    22,
                    19,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T08:22:19Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    22,
                    19,
                    4,
                    348,
                    0
                ],
                "title": "Llama 3 Meets MoE: Efficient Upcycling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llama 3 Meets MoE: Efficient Upcycling"
                },
                "summary": "Scaling large language models (LLMs) significantly improves performance but\ncomes with prohibitive computational costs. Mixture-of-Experts (MoE) models\noffer an efficient alternative, increasing capacity without a proportional rise\nin compute requirements. However, training MoE models from scratch poses\nchallenges like overfitting and routing instability. We present an efficient\ntraining recipe leveraging pre-trained dense checkpoints, training an 8-Expert\nTop-2 MoE model from Llama 3-8B with less than $1\\%$ of typical pre-training\ncompute. Our approach enhances downstream performance on academic benchmarks,\nachieving a $\\textbf{2%}$ improvement in 0-shot accuracy on MMLU, while\nreaching a Model FLOPs Utilization (MFU) of $\\textbf{46.8%}$ during training\nusing our framework. We also integrate online upcycling in NeMo for seamless\nuse of pre-trained weights, enabling cost-effective development of\nhigh-capacity MoE models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling large language models (LLMs) significantly improves performance but\ncomes with prohibitive computational costs. Mixture-of-Experts (MoE) models\noffer an efficient alternative, increasing capacity without a proportional rise\nin compute requirements. However, training MoE models from scratch poses\nchallenges like overfitting and routing instability. We present an efficient\ntraining recipe leveraging pre-trained dense checkpoints, training an 8-Expert\nTop-2 MoE model from Llama 3-8B with less than $1\\%$ of typical pre-training\ncompute. Our approach enhances downstream performance on academic benchmarks,\nachieving a $\\textbf{2%}$ improvement in 0-shot accuracy on MMLU, while\nreaching a Model FLOPs Utilization (MFU) of $\\textbf{46.8%}$ during training\nusing our framework. We also integrate online upcycling in NeMo for seamless\nuse of pre-trained weights, enabling cost-effective development of\nhigh-capacity MoE models."
                },
                "authors": [
                    {
                        "name": "Aditya Vavre"
                    },
                    {
                        "name": "Ethan He"
                    },
                    {
                        "name": "Dennis Liu"
                    },
                    {
                        "name": "Zijie Yan"
                    },
                    {
                        "name": "June Yang"
                    },
                    {
                        "name": "Nima Tajbakhsh"
                    },
                    {
                        "name": "Ashwath Aithal"
                    }
                ],
                "author_detail": {
                    "name": "Ashwath Aithal"
                },
                "author": "Ashwath Aithal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05803v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05803v2",
                "updated": "2024-12-13T08:18:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    18,
                    7,
                    4,
                    348,
                    0
                ],
                "published": "2024-03-09T05:42:02Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    5,
                    42,
                    2,
                    5,
                    69,
                    0
                ],
                "title": "Semiparametric Inference for Regression-Discontinuity Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semiparametric Inference for Regression-Discontinuity Designs"
                },
                "summary": "Treatment effects in regression discontinuity designs (RDDs) are often\nestimated using local regression methods. \\cite{Hahn:01} demonstrated that the\nidentification of the average treatment effect at the cutoff in RDDs relies on\nthe unconfoundedness assumption and that, without this assumption, only the\nlocal average treatment effect at the cutoff can be identified. In this paper,\nwe propose a semiparametric framework tailored for identifying the average\ntreatment effect in RDDs, eliminating the need for the unconfoundedness\nassumption. Our approach globally conceptualizes the identification as a\npartially linear modeling problem, with the coefficient of a specified\npolynomial function of propensity score in the linear component capturing the\naverage treatment effect. This identification result underpins our\nsemiparametric inference for RDDs, employing the $P$-spline method to\napproximate the nonparametric function and establishing a procedure for\nconducting inference within this framework. Through theoretical analysis, we\ndemonstrate that our global approach achieves a faster convergence rate\ncompared to the local method. Monte Carlo simulations further confirm that the\nproposed method consistently outperforms alternatives across various scenarios.\nFurthermore, applications to real-world datasets illustrate that our global\napproach can provide more reliable inference for practical problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Treatment effects in regression discontinuity designs (RDDs) are often\nestimated using local regression methods. \\cite{Hahn:01} demonstrated that the\nidentification of the average treatment effect at the cutoff in RDDs relies on\nthe unconfoundedness assumption and that, without this assumption, only the\nlocal average treatment effect at the cutoff can be identified. In this paper,\nwe propose a semiparametric framework tailored for identifying the average\ntreatment effect in RDDs, eliminating the need for the unconfoundedness\nassumption. Our approach globally conceptualizes the identification as a\npartially linear modeling problem, with the coefficient of a specified\npolynomial function of propensity score in the linear component capturing the\naverage treatment effect. This identification result underpins our\nsemiparametric inference for RDDs, employing the $P$-spline method to\napproximate the nonparametric function and establishing a procedure for\nconducting inference within this framework. Through theoretical analysis, we\ndemonstrate that our global approach achieves a faster convergence rate\ncompared to the local method. Monte Carlo simulations further confirm that the\nproposed method consistently outperforms alternatives across various scenarios.\nFurthermore, applications to real-world datasets illustrate that our global\napproach can provide more reliable inference for practical problems."
                },
                "authors": [
                    {
                        "name": "Weiwei Jiang"
                    },
                    {
                        "name": "Rong J. B. Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Rong J. B. Zhu"
                },
                "author": "Rong J. B. Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05803v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05803v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09946v1",
                "updated": "2024-12-13T08:10:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    10,
                    56,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T08:10:56Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    10,
                    56,
                    4,
                    348,
                    0
                ],
                "title": "Enhancing Nursing and Elderly Care with Large Language Models: An\n  AI-Driven Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Nursing and Elderly Care with Large Language Models: An\n  AI-Driven Framework"
                },
                "summary": "This paper explores the application of large language models (LLMs) in\nnursing and elderly care, focusing on AI-driven patient monitoring and\ninteraction. We introduce a novel Chinese nursing dataset and implement\nincremental pre-training (IPT) and supervised fine-tuning (SFT) techniques to\nenhance LLM performance in specialized tasks. Using LangChain, we develop a\ndynamic nursing assistant capable of real-time care and personalized\ninterventions. Experimental results demonstrate significant improvements,\npaving the way for AI-driven solutions to meet the growing demands of\nhealthcare in aging populations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the application of large language models (LLMs) in\nnursing and elderly care, focusing on AI-driven patient monitoring and\ninteraction. We introduce a novel Chinese nursing dataset and implement\nincremental pre-training (IPT) and supervised fine-tuning (SFT) techniques to\nenhance LLM performance in specialized tasks. Using LangChain, we develop a\ndynamic nursing assistant capable of real-time care and personalized\ninterventions. Experimental results demonstrate significant improvements,\npaving the way for AI-driven solutions to meet the growing demands of\nhealthcare in aging populations."
                },
                "authors": [
                    {
                        "name": "Qiao Sun"
                    },
                    {
                        "name": "Jiexin Xie"
                    },
                    {
                        "name": "Nanyang Ye"
                    },
                    {
                        "name": "Qinying Gu"
                    },
                    {
                        "name": "Shijie Guo"
                    }
                ],
                "author_detail": {
                    "name": "Shijie Guo"
                },
                "author": "Shijie Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04617v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04617v2",
                "updated": "2024-12-13T08:05:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    5,
                    53,
                    4,
                    348,
                    0
                ],
                "published": "2024-10-06T20:34:03Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    20,
                    34,
                    3,
                    6,
                    280,
                    0
                ],
                "title": "Evaluation of Code LLMs on Geospatial Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of Code LLMs on Geospatial Code Generation"
                },
                "summary": "Software development support tools have been studied for a long time, with\nrecent approaches using Large Language Models (LLMs) for code generation. These\nmodels can generate Python code for data science and machine learning\napplications. LLMs are helpful for software engineers because they increase\nproductivity in daily work. An LLM can also serve as a \"mentor\" for\ninexperienced software developers, and be a viable learning support.\nHigh-quality code generation with LLMs can also be beneficial in geospatial\ndata science. However, this domain poses different challenges, and code\ngeneration LLMs are typically not evaluated on geospatial tasks. Here, we show\nhow we constructed an evaluation benchmark for code generation models, based on\na selection of geospatial tasks. We categorised geospatial tasks based on their\ncomplexity and required tools. Then, we created a dataset with tasks that test\nmodel capabilities in spatial reasoning, spatial data processing, and\ngeospatial tools usage. The dataset consists of specific coding problems that\nwere manually created for high quality. For every problem, we proposed a set of\ntest scenarios that make it possible to automatically check the generated code\nfor correctness. In addition, we tested a selection of existing code generation\nLLMs for code generation in the geospatial domain. We share our dataset and\nreproducible evaluation code on a public GitHub repository, arguing that this\ncan serve as an evaluation benchmark for new LLMs in the future. Our dataset\nwill hopefully contribute to the development new models capable of solving\ngeospatial coding tasks with high accuracy. These models will enable the\ncreation of coding assistants tailored for geospatial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software development support tools have been studied for a long time, with\nrecent approaches using Large Language Models (LLMs) for code generation. These\nmodels can generate Python code for data science and machine learning\napplications. LLMs are helpful for software engineers because they increase\nproductivity in daily work. An LLM can also serve as a \"mentor\" for\ninexperienced software developers, and be a viable learning support.\nHigh-quality code generation with LLMs can also be beneficial in geospatial\ndata science. However, this domain poses different challenges, and code\ngeneration LLMs are typically not evaluated on geospatial tasks. Here, we show\nhow we constructed an evaluation benchmark for code generation models, based on\na selection of geospatial tasks. We categorised geospatial tasks based on their\ncomplexity and required tools. Then, we created a dataset with tasks that test\nmodel capabilities in spatial reasoning, spatial data processing, and\ngeospatial tools usage. The dataset consists of specific coding problems that\nwere manually created for high quality. For every problem, we proposed a set of\ntest scenarios that make it possible to automatically check the generated code\nfor correctness. In addition, we tested a selection of existing code generation\nLLMs for code generation in the geospatial domain. We share our dataset and\nreproducible evaluation code on a public GitHub repository, arguing that this\ncan serve as an evaluation benchmark for new LLMs in the future. Our dataset\nwill hopefully contribute to the development new models capable of solving\ngeospatial coding tasks with high accuracy. These models will enable the\ncreation of coding assistants tailored for geospatial applications."
                },
                "authors": [
                    {
                        "name": "Piotr Gramacki"
                    },
                    {
                        "name": "Bruno Martins"
                    },
                    {
                        "name": "Piotr Szymański"
                    }
                ],
                "author_detail": {
                    "name": "Piotr Szymański"
                },
                "author": "Piotr Szymański",
                "arxiv_doi": "10.1145/3687123.3698286",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3687123.3698286",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.04617v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04617v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7th ACM SIGSPATIAL International Workshop on AI for Geographic\n  Knowledge Discovery (GeoAI'24)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09936v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09936v1",
                "updated": "2024-12-13T07:51:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    51,
                    32,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T07:51:32Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    51,
                    32,
                    4,
                    348,
                    0
                ],
                "title": "CaLoRAify: Calorie Estimation with Visual-Text Pairing and LoRA-Driven\n  Visual Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaLoRAify: Calorie Estimation with Visual-Text Pairing and LoRA-Driven\n  Visual Language Models"
                },
                "summary": "The obesity phenomenon, known as the heavy issue, is a leading cause of\npreventable chronic diseases worldwide. Traditional calorie estimation tools\noften rely on specific data formats or complex pipelines, limiting their\npracticality in real-world scenarios. Recently, vision-language models (VLMs)\nhave excelled in understanding real-world contexts and enabling conversational\ninteractions, making them ideal for downstream tasks such as ingredient\nanalysis. However, applying VLMs to calorie estimation requires domain-specific\ndata and alignment strategies. To this end, we curated CalData, a 330K\nimage-text pair dataset tailored for ingredient recognition and calorie\nestimation, combining a large-scale recipe dataset with detailed nutritional\ninstructions for robust vision-language training. Built upon this dataset, we\npresent CaLoRAify, a novel VLM framework aligning ingredient recognition and\ncalorie estimation via training with visual-text pairs. During inference, users\nonly need a single monocular food image to estimate calories while retaining\nthe flexibility of agent-based conversational interaction. With Low-rank\nAdaptation (LoRA) and Retrieve-augmented Generation (RAG) techniques, our\nsystem enhances the performance of foundational VLMs in the vertical domain of\ncalorie estimation. Our code and data are fully open-sourced at\nhttps://github.com/KennyYao2001/16824-CaLORAify.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The obesity phenomenon, known as the heavy issue, is a leading cause of\npreventable chronic diseases worldwide. Traditional calorie estimation tools\noften rely on specific data formats or complex pipelines, limiting their\npracticality in real-world scenarios. Recently, vision-language models (VLMs)\nhave excelled in understanding real-world contexts and enabling conversational\ninteractions, making them ideal for downstream tasks such as ingredient\nanalysis. However, applying VLMs to calorie estimation requires domain-specific\ndata and alignment strategies. To this end, we curated CalData, a 330K\nimage-text pair dataset tailored for ingredient recognition and calorie\nestimation, combining a large-scale recipe dataset with detailed nutritional\ninstructions for robust vision-language training. Built upon this dataset, we\npresent CaLoRAify, a novel VLM framework aligning ingredient recognition and\ncalorie estimation via training with visual-text pairs. During inference, users\nonly need a single monocular food image to estimate calories while retaining\nthe flexibility of agent-based conversational interaction. With Low-rank\nAdaptation (LoRA) and Retrieve-augmented Generation (RAG) techniques, our\nsystem enhances the performance of foundational VLMs in the vertical domain of\ncalorie estimation. Our code and data are fully open-sourced at\nhttps://github.com/KennyYao2001/16824-CaLORAify."
                },
                "authors": [
                    {
                        "name": "Dongyu Yao"
                    },
                    {
                        "name": "Keling Yao"
                    },
                    {
                        "name": "Junhong Zhou"
                    },
                    {
                        "name": "Yinghao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yinghao Zhang"
                },
                "author": "Yinghao Zhang",
                "arxiv_comment": "Disclaimer: This work is part of a course project and reflects\n  ongoing exploration in the field of vision-language models and calorie\n  estimation. Findings and conclusions are subject to further validation and\n  refinement",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09936v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09936v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68U35",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.2.6; I.5.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09934v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09934v1",
                "updated": "2024-12-13T07:40:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    40,
                    37,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T07:40:37Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    40,
                    37,
                    4,
                    348,
                    0
                ],
                "title": "Exploring the Dark Matter Disc Model in Dwarf Galaxies: Insights from\n  the LITTLE THINGS Sample",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Dark Matter Disc Model in Dwarf Galaxies: Insights from\n  the LITTLE THINGS Sample"
                },
                "summary": "We conducted an analysis of the velocity field of dwarf galaxies in the\nLITTLE THINGS sample, focusing on deriving 2D velocity maps that encompass both\nthe transverse and radial velocity fields. Within the range of radial distances\nwhere velocity anisotropies are sufficiently small for the disc to be\nconsidered rotationally supported, and where the warped geometry of the disc\ncan be neglected, we reconstructed the rotation curve while taking into account\nthe effect of the asymmetric drift. To fit the rotation curves, we employed the\nstandard halo model and the dark matter disc (DMD) model, which assumes that\ndark matter is primarily confined to the galactic discs and can be traced by\nthe distribution of \\HI{}. Interestingly, our analysis revealed that the fits\nfrom the DMD model are statistically comparable to those obtained using the\nstandard halo model, but the inferred masses of the galaxies in the DMD model\nare approximately 10 to 100 times smaller than the masses inferred in the\nstandard halo model. In the DMD model, the inner slope of the rotation curve is\ndirectly related to a linear combination of the surface density profiles of the\nstellar and gas components, which generally exhibit a flat core. Consequently,\nthe observation of a linear relationship between the rotation curve and the\nradius in the disc central regions is consistent with the framework of the DMD\nmodel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We conducted an analysis of the velocity field of dwarf galaxies in the\nLITTLE THINGS sample, focusing on deriving 2D velocity maps that encompass both\nthe transverse and radial velocity fields. Within the range of radial distances\nwhere velocity anisotropies are sufficiently small for the disc to be\nconsidered rotationally supported, and where the warped geometry of the disc\ncan be neglected, we reconstructed the rotation curve while taking into account\nthe effect of the asymmetric drift. To fit the rotation curves, we employed the\nstandard halo model and the dark matter disc (DMD) model, which assumes that\ndark matter is primarily confined to the galactic discs and can be traced by\nthe distribution of \\HI{}. Interestingly, our analysis revealed that the fits\nfrom the DMD model are statistically comparable to those obtained using the\nstandard halo model, but the inferred masses of the galaxies in the DMD model\nare approximately 10 to 100 times smaller than the masses inferred in the\nstandard halo model. In the DMD model, the inner slope of the rotation curve is\ndirectly related to a linear combination of the surface density profiles of the\nstellar and gas components, which generally exhibit a flat core. Consequently,\nthe observation of a linear relationship between the rotation curve and the\nradius in the disc central regions is consistent with the framework of the DMD\nmodel."
                },
                "authors": [
                    {
                        "name": "Francesco Sylos Labini"
                    },
                    {
                        "name": "Roberto Capuzzo-Dolcetta"
                    },
                    {
                        "name": "Giordano De Marzo"
                    },
                    {
                        "name": "Matteo Straccamore"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Straccamore"
                },
                "author": "Matteo Straccamore",
                "arxiv_comment": "27 pages, 52 figures, accepted for publication in Astronomy and\n  Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09934v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09934v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.17053v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.17053v2",
                "updated": "2024-12-13T07:21:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    21,
                    58,
                    4,
                    348,
                    0
                ],
                "published": "2024-08-30T07:23:59Z",
                "published_parsed": [
                    2024,
                    8,
                    30,
                    7,
                    23,
                    59,
                    4,
                    243,
                    0
                ],
                "title": "Estimating Conditional Average Treatment Effects via Sufficient\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating Conditional Average Treatment Effects via Sufficient\n  Representation Learning"
                },
                "summary": "Estimating the conditional average treatment effects (CATE) is very important\nin causal inference and has a wide range of applications across many fields. In\nthe estimation process of CATE, the unconfoundedness assumption is typically\nrequired to ensure the identifiability of the regression problems. When\nestimating CATE using high-dimensional data, there have been many variable\nselection methods and neural network approaches based on representation\nlearning, while these methods do not provide a way to verify whether the subset\nof variables after dimensionality reduction or the learned representations\nstill satisfy the unconfoundedness assumption during the estimation process,\nwhich can lead to ineffective estimates of the treatment effects. Additionally,\nthese methods typically use data from only the treatment or control group when\nestimating the regression functions for each group. This paper proposes a novel\nneural network approach named \\textbf{CrossNet} to learn a sufficient\nrepresentation for the features, based on which we then estimate the CATE,\nwhere cross indicates that in estimating the regression functions, we used data\nfrom their own group as well as cross-utilized data from another group.\nNumerical simulations and empirical results demonstrate that our method\noutperforms the competitive approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the conditional average treatment effects (CATE) is very important\nin causal inference and has a wide range of applications across many fields. In\nthe estimation process of CATE, the unconfoundedness assumption is typically\nrequired to ensure the identifiability of the regression problems. When\nestimating CATE using high-dimensional data, there have been many variable\nselection methods and neural network approaches based on representation\nlearning, while these methods do not provide a way to verify whether the subset\nof variables after dimensionality reduction or the learned representations\nstill satisfy the unconfoundedness assumption during the estimation process,\nwhich can lead to ineffective estimates of the treatment effects. Additionally,\nthese methods typically use data from only the treatment or control group when\nestimating the regression functions for each group. This paper proposes a novel\nneural network approach named \\textbf{CrossNet} to learn a sufficient\nrepresentation for the features, based on which we then estimate the CATE,\nwhere cross indicates that in estimating the regression functions, we used data\nfrom their own group as well as cross-utilized data from another group.\nNumerical simulations and empirical results demonstrate that our method\noutperforms the competitive approaches."
                },
                "authors": [
                    {
                        "name": "Pengfei Shi"
                    },
                    {
                        "name": "Wei Zhong"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Ningtao Wang"
                    },
                    {
                        "name": "Xing Fu"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Yin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Yin Jin"
                },
                "author": "Yin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.17053v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.17053v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07214v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07214v2",
                "updated": "2024-12-13T07:08:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    8,
                    54,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-10T06:11:23Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    6,
                    11,
                    23,
                    1,
                    345,
                    0
                ],
                "title": "Towards Automated Cross-domain Exploratory Data Analysis through Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Automated Cross-domain Exploratory Data Analysis through Large\n  Language Models"
                },
                "summary": "Exploratory data analysis (EDA), coupled with SQL, is essential for data\nanalysts involved in data exploration and analysis. However, data analysts\noften encounter two primary challenges: (1) the need to craft SQL queries\nskillfully, and (2) the requirement to generate suitable visualization types\nthat enhance the interpretation of query results. Due to its significance,\nsubstantial research efforts have been made to explore different approaches to\naddress these challenges, including leveraging large language models (LLMs).\nHowever, existing methods fail to meet real-world data exploration requirements\nprimarily due to (1) complex database schema; (2) unclear user intent; (3)\nlimited cross-domain generalization capability; and (4) insufficient end-to-end\ntext-to-visualization capability.\n  This paper presents TiInsight, an automated SQL-based cross-domain\nexploratory data analysis system. First, we propose hierarchical data context\n(i.e., HDC), which leverages LLMs to summarize the contexts related to the\ndatabase schema, which is crucial for open-world EDA systems to generalize\nacross data domains. Second, the EDA system is divided into four components\n(i.e., stages): HDC generation, question clarification and decomposition,\ntext-to-SQL generation (i.e., TiSQL), and data visualization (i.e., TiChart).\nFinally, we implemented an end-to-end EDA system with a user-friendly GUI\ninterface in the production environment at PingCAP. We have also open-sourced\nall APIs of TiInsight to facilitate research within the EDA community. Through\nextensive evaluations by a real-world user study, we demonstrate that TiInsight\noffers remarkable performance compared to human experts. Specifically, TiSQL\nachieves an execution accuracy of 86.3% on the Spider dataset using GPT-4. It\nalso demonstrates state-of-the-art performance on the Bird dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploratory data analysis (EDA), coupled with SQL, is essential for data\nanalysts involved in data exploration and analysis. However, data analysts\noften encounter two primary challenges: (1) the need to craft SQL queries\nskillfully, and (2) the requirement to generate suitable visualization types\nthat enhance the interpretation of query results. Due to its significance,\nsubstantial research efforts have been made to explore different approaches to\naddress these challenges, including leveraging large language models (LLMs).\nHowever, existing methods fail to meet real-world data exploration requirements\nprimarily due to (1) complex database schema; (2) unclear user intent; (3)\nlimited cross-domain generalization capability; and (4) insufficient end-to-end\ntext-to-visualization capability.\n  This paper presents TiInsight, an automated SQL-based cross-domain\nexploratory data analysis system. First, we propose hierarchical data context\n(i.e., HDC), which leverages LLMs to summarize the contexts related to the\ndatabase schema, which is crucial for open-world EDA systems to generalize\nacross data domains. Second, the EDA system is divided into four components\n(i.e., stages): HDC generation, question clarification and decomposition,\ntext-to-SQL generation (i.e., TiSQL), and data visualization (i.e., TiChart).\nFinally, we implemented an end-to-end EDA system with a user-friendly GUI\ninterface in the production environment at PingCAP. We have also open-sourced\nall APIs of TiInsight to facilitate research within the EDA community. Through\nextensive evaluations by a real-world user study, we demonstrate that TiInsight\noffers remarkable performance compared to human experts. Specifically, TiSQL\nachieves an execution accuracy of 86.3% on the Spider dataset using GPT-4. It\nalso demonstrates state-of-the-art performance on the Bird dataset."
                },
                "authors": [
                    {
                        "name": "Jun-Peng Zhu"
                    },
                    {
                        "name": "Boyan Niu"
                    },
                    {
                        "name": "Peng Cai"
                    },
                    {
                        "name": "Zheming Ni"
                    },
                    {
                        "name": "Jianwei Wan"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Jiajun Huang"
                    },
                    {
                        "name": "Shengbo Ma"
                    },
                    {
                        "name": "Bing Wang"
                    },
                    {
                        "name": "Xuan Zhou"
                    },
                    {
                        "name": "Guanglei Bao"
                    },
                    {
                        "name": "Donghui Zhang"
                    },
                    {
                        "name": "Liu Tang"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "arxiv_comment": "14 pages, 10 figures. Submitted to SIGMOD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07214v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07214v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09916v1",
                "updated": "2024-12-13T07:08:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    8,
                    13,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T07:08:13Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    8,
                    13,
                    4,
                    348,
                    0
                ],
                "title": "ProxyLLM : LLM-Driven Framework for Customer Support Through Text-Style\n  Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProxyLLM : LLM-Driven Framework for Customer Support Through Text-Style\n  Transfer"
                },
                "summary": "Chatbot-based customer support services have significantly advanced with the\nintroduction of large language models (LLMs), enabling enhanced response\nquality and broader application across industries. However, while these\nadvancements focus on reducing business costs and improving customer\nsatisfaction, limited attention has been given to the experiences of customer\nservice agents, who are critical to the service ecosystem. A major challenge\nfaced by agents is the stress caused by unnecessary emotional exhaustion from\nharmful texts, which not only impairs their efficiency but also negatively\naffects customer satisfaction and business outcomes. In this work, we propose\nan LLM-powered system designed to enhance the working conditions of customer\nservice agents by addressing emotionally intensive communications. Our proposed\nsystem leverages LLMs to transform the tone of customer messages, preserving\nactionable content while mitigating the emotional impact on human agents.\nFurthermore, the application is implemented as a Chrome extension, making it\nhighly adaptable and easy to integrate into existing systems. Our method aims\nto enhance the overall service experience for businesses, customers, and\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbot-based customer support services have significantly advanced with the\nintroduction of large language models (LLMs), enabling enhanced response\nquality and broader application across industries. However, while these\nadvancements focus on reducing business costs and improving customer\nsatisfaction, limited attention has been given to the experiences of customer\nservice agents, who are critical to the service ecosystem. A major challenge\nfaced by agents is the stress caused by unnecessary emotional exhaustion from\nharmful texts, which not only impairs their efficiency but also negatively\naffects customer satisfaction and business outcomes. In this work, we propose\nan LLM-powered system designed to enhance the working conditions of customer\nservice agents by addressing emotionally intensive communications. Our proposed\nsystem leverages LLMs to transform the tone of customer messages, preserving\nactionable content while mitigating the emotional impact on human agents.\nFurthermore, the application is implemented as a Chrome extension, making it\nhighly adaptable and easy to integrate into existing systems. Our method aims\nto enhance the overall service experience for businesses, customers, and\nagents."
                },
                "authors": [
                    {
                        "name": "Sehyeong Jo"
                    },
                    {
                        "name": "Jungwon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jungwon Seo"
                },
                "author": "Jungwon Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09914v1",
                "updated": "2024-12-13T07:05:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    5,
                    31,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T07:05:31Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    5,
                    31,
                    4,
                    348,
                    0
                ],
                "title": "Atomic Learning Objectives Labeling: A High-Resolution Approach for\n  Physics Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atomic Learning Objectives Labeling: A High-Resolution Approach for\n  Physics Education"
                },
                "summary": "This paper introduces a novel approach to create a high-resolution \"map\" for\nphysics learning: an \"atomic\" learning objectives (LOs) system designed to\ncapture detailed cognitive processes and concepts required for problem solving\nin a college-level introductory physics course. Our method leverages Large\nLanguage Models (LLMs) for automated labeling of physics questions and\nintroduces a comprehensive set of metrics to evaluate the quality of the\nlabeling outcomes. The atomic LO system, covering nine chapters of an\nintroductory physics course, uses a \"subject-verb-object'' structure to\nrepresent specific cognitive processes. We apply this system to 131 questions\nfrom expert-curated question banks and the OpenStax University Physics\ntextbook. Each question is labeled with 1-8 atomic LOs across three chapters.\nThrough extensive experiments using various prompting strategies and LLMs, we\ncompare automated LOs labeling results against human expert labeling. Our\nanalysis reveals both the strengths and limitations of LLMs, providing insight\ninto LLMs reasoning processes for labeling LOs and identifying areas for\nimprovement in LOs system design. Our work contributes to the field of learning\nanalytics by proposing a more granular approach to mapping learning objectives\nwith questions. Our findings have significant implications for the development\nof intelligent tutoring systems and personalized learning pathways in STEM\neducation, paving the way for more effective \"learning GPS'' systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach to create a high-resolution \"map\" for\nphysics learning: an \"atomic\" learning objectives (LOs) system designed to\ncapture detailed cognitive processes and concepts required for problem solving\nin a college-level introductory physics course. Our method leverages Large\nLanguage Models (LLMs) for automated labeling of physics questions and\nintroduces a comprehensive set of metrics to evaluate the quality of the\nlabeling outcomes. The atomic LO system, covering nine chapters of an\nintroductory physics course, uses a \"subject-verb-object'' structure to\nrepresent specific cognitive processes. We apply this system to 131 questions\nfrom expert-curated question banks and the OpenStax University Physics\ntextbook. Each question is labeled with 1-8 atomic LOs across three chapters.\nThrough extensive experiments using various prompting strategies and LLMs, we\ncompare automated LOs labeling results against human expert labeling. Our\nanalysis reveals both the strengths and limitations of LLMs, providing insight\ninto LLMs reasoning processes for labeling LOs and identifying areas for\nimprovement in LOs system design. Our work contributes to the field of learning\nanalytics by proposing a more granular approach to mapping learning objectives\nwith questions. Our findings have significant implications for the development\nof intelligent tutoring systems and personalized learning pathways in STEM\neducation, paving the way for more effective \"learning GPS'' systems."
                },
                "authors": [
                    {
                        "name": "Naiming Liu"
                    },
                    {
                        "name": "Shashank Sonkar"
                    },
                    {
                        "name": "Debshila Basu Mallick"
                    },
                    {
                        "name": "Richard Baraniuk"
                    },
                    {
                        "name": "Zhongzhou Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhongzhou Chen"
                },
                "author": "Zhongzhou Chen",
                "arxiv_comment": "The paper is accepted to LAK'2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16283v2",
                "updated": "2024-12-13T07:01:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    1,
                    16,
                    4,
                    348,
                    0
                ],
                "published": "2024-04-25T01:56:00Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    1,
                    56,
                    0,
                    3,
                    116,
                    0
                ],
                "title": "Andes: Defining and Enhancing Quality-of-Experience in LLM-Based Text\n  Streaming Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Andes: Defining and Enhancing Quality-of-Experience in LLM-Based Text\n  Streaming Services"
                },
                "summary": "Large language models (LLMs) are now at the core of conversational AI\nservices such as real-time translation and chatbots, which provide live user\ninteraction by incrementally streaming text to the user. However, existing LLM\nserving systems fail to provide good user experience because their optimization\nmetrics are not always aligned with user experience.\n  In this paper, we first introduce and define the notion of\nQuality-of-Experience (QoE) for text streaming services by considering each\nuser's end-to-end interaction timeline. Based on this, we propose Andes, a\nQoE-aware LLM serving system that enhances user experience by ensuring that\nusers receive the first token promptly and subsequent tokens at a smooth,\ndigestible pace, even during surge periods. This is enabled by Andes's\npreemptive request scheduler that dynamically prioritizes requests at the token\ngranularity based on each request's expected QoE gain and GPU resource usage.\nOur evaluations demonstrate that, compared to state-of-the-art LLM serving\nsystems, Andes improves the average QoE by up to $4.7\\times$ given the same GPU\nresource, or saves up to 61% GPU resources while maintaining the same high QoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are now at the core of conversational AI\nservices such as real-time translation and chatbots, which provide live user\ninteraction by incrementally streaming text to the user. However, existing LLM\nserving systems fail to provide good user experience because their optimization\nmetrics are not always aligned with user experience.\n  In this paper, we first introduce and define the notion of\nQuality-of-Experience (QoE) for text streaming services by considering each\nuser's end-to-end interaction timeline. Based on this, we propose Andes, a\nQoE-aware LLM serving system that enhances user experience by ensuring that\nusers receive the first token promptly and subsequent tokens at a smooth,\ndigestible pace, even during surge periods. This is enabled by Andes's\npreemptive request scheduler that dynamically prioritizes requests at the token\ngranularity based on each request's expected QoE gain and GPU resource usage.\nOur evaluations demonstrate that, compared to state-of-the-art LLM serving\nsystems, Andes improves the average QoE by up to $4.7\\times$ given the same GPU\nresource, or saves up to 61% GPU resources while maintaining the same high QoE."
                },
                "authors": [
                    {
                        "name": "Jiachen Liu"
                    },
                    {
                        "name": "Jae-Won Chung"
                    },
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Mosharaf Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Mosharaf Chowdhury"
                },
                "author": "Mosharaf Chowdhury",
                "arxiv_comment": "16 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.10372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10372v1",
                "updated": "2024-12-13T18:59:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    59,
                    40,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T18:59:40Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    59,
                    40,
                    4,
                    348,
                    0
                ],
                "title": "UniMed-CLIP: Towards a Unified Image-Text Pretraining Paradigm for\n  Diverse Medical Imaging Modalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniMed-CLIP: Towards a Unified Image-Text Pretraining Paradigm for\n  Diverse Medical Imaging Modalities"
                },
                "summary": "Vision-Language Models (VLMs) trained via contrastive learning have achieved\nnotable success in natural image tasks. However, their application in the\nmedical domain remains limited due to the scarcity of openly accessible,\nlarge-scale medical image-text datasets. Existing medical VLMs either train on\nclosed-source proprietary or relatively small open-source datasets that do not\ngeneralize well. Similarly, most models remain specific to a single or limited\nnumber of medical imaging domains, again restricting their applicability to\nother modalities. To address this gap, we introduce UniMed, a large-scale,\nopen-source multi-modal medical dataset comprising over 5.3 million image-text\npairs across six diverse imaging modalities: X-ray, CT, MRI, Ultrasound,\nPathology, and Fundus. UniMed is developed using a data-collection framework\nthat leverages Large Language Models (LLMs) to transform modality-specific\nclassification datasets into image-text formats while incorporating existing\nimage-text data from the medical domain, facilitating scalable VLM pretraining.\nUsing UniMed, we trained UniMed-CLIP, a unified VLM for six modalities that\nsignificantly outperforms existing generalist VLMs and matches\nmodality-specific medical VLMs, achieving notable gains in zero-shot\nevaluations. For instance, UniMed-CLIP improves over BiomedCLIP (trained on\nproprietary data) by an absolute gain of +12.61, averaged over 21 datasets,\nwhile using 3x less training data. To facilitate future research, we release\nUniMed dataset, training codes, and models at\nhttps://github.com/mbzuai-oryx/UniMed-CLIP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) trained via contrastive learning have achieved\nnotable success in natural image tasks. However, their application in the\nmedical domain remains limited due to the scarcity of openly accessible,\nlarge-scale medical image-text datasets. Existing medical VLMs either train on\nclosed-source proprietary or relatively small open-source datasets that do not\ngeneralize well. Similarly, most models remain specific to a single or limited\nnumber of medical imaging domains, again restricting their applicability to\nother modalities. To address this gap, we introduce UniMed, a large-scale,\nopen-source multi-modal medical dataset comprising over 5.3 million image-text\npairs across six diverse imaging modalities: X-ray, CT, MRI, Ultrasound,\nPathology, and Fundus. UniMed is developed using a data-collection framework\nthat leverages Large Language Models (LLMs) to transform modality-specific\nclassification datasets into image-text formats while incorporating existing\nimage-text data from the medical domain, facilitating scalable VLM pretraining.\nUsing UniMed, we trained UniMed-CLIP, a unified VLM for six modalities that\nsignificantly outperforms existing generalist VLMs and matches\nmodality-specific medical VLMs, achieving notable gains in zero-shot\nevaluations. For instance, UniMed-CLIP improves over BiomedCLIP (trained on\nproprietary data) by an absolute gain of +12.61, averaged over 21 datasets,\nwhile using 3x less training data. To facilitate future research, we release\nUniMed dataset, training codes, and models at\nhttps://github.com/mbzuai-oryx/UniMed-CLIP."
                },
                "authors": [
                    {
                        "name": "Muhammad Uzair Khattak"
                    },
                    {
                        "name": "Shahina Kunhimon"
                    },
                    {
                        "name": "Muzammal Naseer"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Shahbaz Khan"
                },
                "author": "Fahad Shahbaz Khan",
                "arxiv_comment": "Code, models and demo available at\n  https://github.com/mbzuai-oryx/UniMed-CLIP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10342v1",
                "updated": "2024-12-13T18:40:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    40,
                    10,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T18:40:10Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    40,
                    10,
                    4,
                    348,
                    0
                ],
                "title": "Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining"
                },
                "summary": "Digital agents are increasingly employed to automate tasks in interactive\ndigital environments such as web pages, software applications, and operating\nsystems. While text-based agents built on Large Language Models (LLMs) often\nrequire frequent updates due to platform-specific APIs, visual agents\nleveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability\nby interacting directly with Graphical User Interfaces (GUIs). However, these\nagents face significant challenges in visual perception, particularly when\nhandling high-resolution, visually complex digital environments. This paper\nintroduces Iris, a foundational visual agent that addresses these challenges\nthrough two key innovations: Information-Sensitive Cropping (ISC) and\nSelf-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes\nvisually dense regions using a edge detection algorithm, enabling efficient\nprocessing by allocating more computational resources to areas with higher\ninformation density. SRDL enhances the agent's ability to handle complex tasks\nby leveraging a dual-learning loop, where improvements in referring (describing\nUI elements) reinforce grounding (locating elements) and vice versa, all\nwithout requiring additional annotated data. Empirical evaluations demonstrate\nthat Iris achieves state-of-the-art performance across multiple benchmarks with\nonly 850K GUI annotations, outperforming methods using 10x more training data.\nThese improvements further translate to significant gains in both web and OS\nagent downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital agents are increasingly employed to automate tasks in interactive\ndigital environments such as web pages, software applications, and operating\nsystems. While text-based agents built on Large Language Models (LLMs) often\nrequire frequent updates due to platform-specific APIs, visual agents\nleveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability\nby interacting directly with Graphical User Interfaces (GUIs). However, these\nagents face significant challenges in visual perception, particularly when\nhandling high-resolution, visually complex digital environments. This paper\nintroduces Iris, a foundational visual agent that addresses these challenges\nthrough two key innovations: Information-Sensitive Cropping (ISC) and\nSelf-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes\nvisually dense regions using a edge detection algorithm, enabling efficient\nprocessing by allocating more computational resources to areas with higher\ninformation density. SRDL enhances the agent's ability to handle complex tasks\nby leveraging a dual-learning loop, where improvements in referring (describing\nUI elements) reinforce grounding (locating elements) and vice versa, all\nwithout requiring additional annotated data. Empirical evaluations demonstrate\nthat Iris achieves state-of-the-art performance across multiple benchmarks with\nonly 850K GUI annotations, outperforming methods using 10x more training data.\nThese improvements further translate to significant gains in both web and OS\nagent downstream tasks."
                },
                "authors": [
                    {
                        "name": "Zhiqi Ge"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Xinglei Pang"
                    },
                    {
                        "name": "Minghe Gao"
                    },
                    {
                        "name": "Kaihang Pan"
                    },
                    {
                        "name": "Wang Lin"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07029v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07029v2",
                "updated": "2024-12-13T18:32:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    32,
                    59,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-09T22:35:24Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    22,
                    35,
                    24,
                    0,
                    344,
                    0
                ],
                "title": "Key Focus Areas and Enabling Technologies for 6G",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key Focus Areas and Enabling Technologies for 6G"
                },
                "summary": "We provide a taxonomy of a dozen enabling network architectures, protocols,\nand technologies that will define the evolution from 5G to 6G. These\ntechnologies span the network protocol stack, different target deployment\nenvironments, and various perceived levels of technical maturity. We outline\nfour areas of societal focus that will be impacted by these technologies, and\noverview several research directions that hold the potential to address the\nproblems in these important focus areas.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We provide a taxonomy of a dozen enabling network architectures, protocols,\nand technologies that will define the evolution from 5G to 6G. These\ntechnologies span the network protocol stack, different target deployment\nenvironments, and various perceived levels of technical maturity. We outline\nfour areas of societal focus that will be impacted by these technologies, and\noverview several research directions that hold the potential to address the\nproblems in these important focus areas."
                },
                "authors": [
                    {
                        "name": "Christopher G. Brinton"
                    },
                    {
                        "name": "Mung Chiang"
                    },
                    {
                        "name": "Kwang Taik Kim"
                    },
                    {
                        "name": "David J. Love"
                    },
                    {
                        "name": "Michael Beesley"
                    },
                    {
                        "name": "Morris Repeta"
                    },
                    {
                        "name": "John Roese"
                    },
                    {
                        "name": "Per Beming"
                    },
                    {
                        "name": "Erik Ekudden"
                    },
                    {
                        "name": "Clara Li"
                    },
                    {
                        "name": "Geng Wu"
                    },
                    {
                        "name": "Nishant Batra"
                    },
                    {
                        "name": "Amitava Ghosh"
                    },
                    {
                        "name": "Volker Ziegler"
                    },
                    {
                        "name": "Tingfang Ji"
                    },
                    {
                        "name": "Rajat Prakash"
                    },
                    {
                        "name": "John Smee"
                    }
                ],
                "author_detail": {
                    "name": "John Smee"
                },
                "author": "John Smee",
                "arxiv_comment": "This paper has been accepted for publication in the IEEE\n  Communications Magazine. Portions were released online as a report 6G\n  Roadmap: A Global Taxonomy in November 2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07029v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07029v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10321v1",
                "updated": "2024-12-13T18:00:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    0,
                    57,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T18:00:57Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    0,
                    57,
                    4,
                    348,
                    0
                ],
                "title": "AdvPrefix: An Objective for Nuanced LLM Jailbreaks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdvPrefix: An Objective for Nuanced LLM Jailbreaks"
                },
                "summary": "Many jailbreak attacks on large language models (LLMs) rely on a common\nobjective: making the model respond with the prefix \"Sure, here is (harmful\nrequest)\". While straightforward, this objective has two limitations: limited\ncontrol over model behaviors, often resulting in incomplete or unrealistic\nresponses, and a rigid format that hinders optimization. To address these\nlimitations, we introduce AdvPrefix, a new prefix-forcing objective that\nenables more nuanced control over model behavior while being easy to optimize.\nOur objective leverages model-dependent prefixes, automatically selected based\non two criteria: high prefilling attack success rates and low negative\nlog-likelihood. It can further simplify optimization by using multiple prefixes\nfor a single user request. AdvPrefix can integrate seamlessly into existing\njailbreak attacks to improve their performance for free. For example, simply\nreplacing GCG attack's target prefixes with ours on Llama-3 improves nuanced\nattack success rates from 14% to 80%, suggesting that current alignment\nstruggles to generalize to unseen prefixes. Our work demonstrates the\nimportance of jailbreak objectives in achieving nuanced jailbreaks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many jailbreak attacks on large language models (LLMs) rely on a common\nobjective: making the model respond with the prefix \"Sure, here is (harmful\nrequest)\". While straightforward, this objective has two limitations: limited\ncontrol over model behaviors, often resulting in incomplete or unrealistic\nresponses, and a rigid format that hinders optimization. To address these\nlimitations, we introduce AdvPrefix, a new prefix-forcing objective that\nenables more nuanced control over model behavior while being easy to optimize.\nOur objective leverages model-dependent prefixes, automatically selected based\non two criteria: high prefilling attack success rates and low negative\nlog-likelihood. It can further simplify optimization by using multiple prefixes\nfor a single user request. AdvPrefix can integrate seamlessly into existing\njailbreak attacks to improve their performance for free. For example, simply\nreplacing GCG attack's target prefixes with ours on Llama-3 improves nuanced\nattack success rates from 14% to 80%, suggesting that current alignment\nstruggles to generalize to unseen prefixes. Our work demonstrates the\nimportance of jailbreak objectives in achieving nuanced jailbreaks."
                },
                "authors": [
                    {
                        "name": "Sicheng Zhu"
                    },
                    {
                        "name": "Brandon Amos"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Chuan Guo"
                    },
                    {
                        "name": "Ivan Evtimov"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Evtimov"
                },
                "author": "Ivan Evtimov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v2",
                "updated": "2024-12-13T17:53:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    53,
                    25,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Efficient Multi-LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06223v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06223v3",
                "updated": "2024-12-13T17:29:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    29,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-10T05:26:53Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    5,
                    26,
                    53,
                    1,
                    254,
                    0
                ],
                "title": "Enhancing Temporal Understanding in Audio Question Answering for Large\n  Audio Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Temporal Understanding in Audio Question Answering for Large\n  Audio Language Models"
                },
                "summary": "The Audio Question Answering (AQA) task includes audio event classification,\naudio captioning, and open-ended reasoning. Recently, AQA has garnered\nattention due to the advent of Large Audio Language Models (LALMs). Current\nliterature focuses on constructing LALMs by integrating audio encoders with\ntext-only Large Language Models (LLMs) through a projection module. While LALMs\nexcel in general audio understanding, they are limited in temporal reasoning,\nwhich may hinder their commercial applications and on-device deployment. This\npaper addresses these challenges and limitations in audio temporal reasoning.\nFirst, we introduce a data augmentation technique for generating reliable audio\ntemporal questions and answers using an LLM. Second, we perform a further\nfine-tuning of an existing baseline using curriculum learning strategy to\nspecialize in temporal reasoning without compromising performance on fine-tuned\ntasks. We demonstrate the performance of our model using state-of-the-art LALMs\non public audio benchmark datasets. Third, we implement our AQA model on-device\nlocally and investigate its CPU inference for edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Audio Question Answering (AQA) task includes audio event classification,\naudio captioning, and open-ended reasoning. Recently, AQA has garnered\nattention due to the advent of Large Audio Language Models (LALMs). Current\nliterature focuses on constructing LALMs by integrating audio encoders with\ntext-only Large Language Models (LLMs) through a projection module. While LALMs\nexcel in general audio understanding, they are limited in temporal reasoning,\nwhich may hinder their commercial applications and on-device deployment. This\npaper addresses these challenges and limitations in audio temporal reasoning.\nFirst, we introduce a data augmentation technique for generating reliable audio\ntemporal questions and answers using an LLM. Second, we perform a further\nfine-tuning of an existing baseline using curriculum learning strategy to\nspecialize in temporal reasoning without compromising performance on fine-tuned\ntasks. We demonstrate the performance of our model using state-of-the-art LALMs\non public audio benchmark datasets. Third, we implement our AQA model on-device\nlocally and investigate its CPU inference for edge applications."
                },
                "authors": [
                    {
                        "name": "Arvind Krishna Sridhar"
                    },
                    {
                        "name": "Yinyi Guo"
                    },
                    {
                        "name": "Erik Visser"
                    }
                ],
                "author_detail": {
                    "name": "Erik Visser"
                },
                "author": "Erik Visser",
                "arxiv_comment": "9 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06223v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06223v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10281v1",
                "updated": "2024-12-13T17:03:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    3,
                    56,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:03:56Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    3,
                    56,
                    4,
                    348,
                    0
                ],
                "title": "One world, one opinion? The superstar effect in LLM responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One world, one opinion? The superstar effect in LLM responses"
                },
                "summary": "As large language models (LLMs) are shaping the way information is shared and\naccessed online, their opinions have the potential to influence a wide\naudience. This study examines who the LLMs view as the most prominent figures\nacross various fields, using prompts in ten different languages to explore the\ninfluence of linguistic diversity. Our findings reveal low diversity in\nresponses, with a small number of figures dominating recognition across\nlanguages (also known as the \"superstar effect\"). These results highlight the\nrisk of narrowing global knowledge representation when LLMs retrieve subjective\ninformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are shaping the way information is shared and\naccessed online, their opinions have the potential to influence a wide\naudience. This study examines who the LLMs view as the most prominent figures\nacross various fields, using prompts in ten different languages to explore the\ninfluence of linguistic diversity. Our findings reveal low diversity in\nresponses, with a small number of figures dominating recognition across\nlanguages (also known as the \"superstar effect\"). These results highlight the\nrisk of narrowing global knowledge representation when LLMs retrieve subjective\ninformation."
                },
                "authors": [
                    {
                        "name": "Sofie Goethals"
                    },
                    {
                        "name": "Lauren Rhue"
                    }
                ],
                "author_detail": {
                    "name": "Lauren Rhue"
                },
                "author": "Lauren Rhue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10271v1",
                "updated": "2024-12-13T16:46:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    46,
                    3,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T16:46:03Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    46,
                    3,
                    4,
                    348,
                    0
                ],
                "title": "Benchmarking Linguistic Diversity of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Linguistic Diversity of Large Language Models"
                },
                "summary": "The development and evaluation of Large Language Models (LLMs) has primarily\nfocused on their task-solving capabilities, with recent models even surpassing\nhuman performance in some areas. However, this focus often neglects whether\nmachine-generated language matches the human level of diversity, in terms of\nvocabulary choice, syntactic construction, and expression of meaning, raising\nquestions about whether the fundamentals of language generation have been fully\naddressed. This paper emphasizes the importance of examining the preservation\nof human linguistic richness by language models, given the concerning surge in\nonline content produced or aided by LLMs. We propose a comprehensive framework\nfor evaluating LLMs from various linguistic diversity perspectives including\nlexical, syntactic, and semantic dimensions. Using this framework, we benchmark\nseveral state-of-the-art LLMs across all diversity dimensions, and conduct an\nin-depth case study for syntactic diversity. Finally, we analyze how different\ndevelopment and deployment choices impact the linguistic diversity of LLM\noutputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development and evaluation of Large Language Models (LLMs) has primarily\nfocused on their task-solving capabilities, with recent models even surpassing\nhuman performance in some areas. However, this focus often neglects whether\nmachine-generated language matches the human level of diversity, in terms of\nvocabulary choice, syntactic construction, and expression of meaning, raising\nquestions about whether the fundamentals of language generation have been fully\naddressed. This paper emphasizes the importance of examining the preservation\nof human linguistic richness by language models, given the concerning surge in\nonline content produced or aided by LLMs. We propose a comprehensive framework\nfor evaluating LLMs from various linguistic diversity perspectives including\nlexical, syntactic, and semantic dimensions. Using this framework, we benchmark\nseveral state-of-the-art LLMs across all diversity dimensions, and conduct an\nin-depth case study for syntactic diversity. Finally, we analyze how different\ndevelopment and deployment choices impact the linguistic diversity of LLM\noutputs."
                },
                "authors": [
                    {
                        "name": "Yanzhu Guo"
                    },
                    {
                        "name": "Guokan Shang"
                    },
                    {
                        "name": "Chloé Clavel"
                    }
                ],
                "author_detail": {
                    "name": "Chloé Clavel"
                },
                "author": "Chloé Clavel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10270v1",
                "updated": "2024-12-13T16:45:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    45,
                    49,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T16:45:49Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    45,
                    49,
                    4,
                    348,
                    0
                ],
                "title": "Cultural Evolution of Cooperation among LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cultural Evolution of Cooperation among LLM Agents"
                },
                "summary": "Large language models (LLMs) provide a compelling foundation for building\ngenerally-capable AI agents. These agents may soon be deployed at scale in the\nreal world, representing the interests of individual humans (e.g., AI\nassistants) or groups of humans (e.g., AI-accelerated corporations). At\npresent, relatively little is known about the dynamics of multiple LLM agents\ninteracting over many generations of iterative deployment. In this paper, we\nexamine whether a \"society\" of LLM agents can learn mutually beneficial social\nnorms in the face of incentives to defect, a distinctive feature of human\nsociality that is arguably crucial to the success of civilization. In\nparticular, we study the evolution of indirect reciprocity across generations\nof LLM agents playing a classic iterated Donor Game in which agents can observe\nthe recent behavior of their peers. We find that the evolution of cooperation\ndiffers markedly across base models, with societies of Claude 3.5 Sonnet agents\nachieving significantly higher average scores than Gemini 1.5 Flash, which, in\nturn, outperforms GPT-4o. Further, Claude 3.5 Sonnet can make use of an\nadditional mechanism for costly punishment to achieve yet higher scores, while\nGemini 1.5 Flash and GPT-4o fail to do so. For each model class, we also\nobserve variation in emergent behavior across random seeds, suggesting an\nunderstudied sensitive dependence on initial conditions. We suggest that our\nevaluation regime could inspire an inexpensive and informative new class of LLM\nbenchmarks, focussed on the implications of LLM agent deployment for the\ncooperative infrastructure of society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) provide a compelling foundation for building\ngenerally-capable AI agents. These agents may soon be deployed at scale in the\nreal world, representing the interests of individual humans (e.g., AI\nassistants) or groups of humans (e.g., AI-accelerated corporations). At\npresent, relatively little is known about the dynamics of multiple LLM agents\ninteracting over many generations of iterative deployment. In this paper, we\nexamine whether a \"society\" of LLM agents can learn mutually beneficial social\nnorms in the face of incentives to defect, a distinctive feature of human\nsociality that is arguably crucial to the success of civilization. In\nparticular, we study the evolution of indirect reciprocity across generations\nof LLM agents playing a classic iterated Donor Game in which agents can observe\nthe recent behavior of their peers. We find that the evolution of cooperation\ndiffers markedly across base models, with societies of Claude 3.5 Sonnet agents\nachieving significantly higher average scores than Gemini 1.5 Flash, which, in\nturn, outperforms GPT-4o. Further, Claude 3.5 Sonnet can make use of an\nadditional mechanism for costly punishment to achieve yet higher scores, while\nGemini 1.5 Flash and GPT-4o fail to do so. For each model class, we also\nobserve variation in emergent behavior across random seeds, suggesting an\nunderstudied sensitive dependence on initial conditions. We suggest that our\nevaluation regime could inspire an inexpensive and informative new class of LLM\nbenchmarks, focussed on the implications of LLM agent deployment for the\ncooperative infrastructure of society."
                },
                "authors": [
                    {
                        "name": "Aron Vallinder"
                    },
                    {
                        "name": "Edward Hughes"
                    }
                ],
                "author_detail": {
                    "name": "Edward Hughes"
                },
                "author": "Edward Hughes",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10267v1",
                "updated": "2024-12-13T16:37:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    37,
                    20,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T16:37:20Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    37,
                    20,
                    4,
                    348,
                    0
                ],
                "title": "Does Multiple Choice Have a Future in the Age of Generative AI? A\n  Posttest-only RCT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does Multiple Choice Have a Future in the Age of Generative AI? A\n  Posttest-only RCT"
                },
                "summary": "The role of multiple-choice questions (MCQs) as effective learning tools has\nbeen debated in past research. While MCQs are widely used due to their ease in\ngrading, open response questions are increasingly used for instruction, given\nadvances in large language models (LLMs) for automated grading. This study\nevaluates MCQs effectiveness relative to open-response questions, both\nindividually and in combination, on learning. These activities are embedded\nwithin six tutor lessons on advocacy. Using a posttest-only randomized control\ndesign, we compare the performance of 234 tutors (790 lesson completions)\nacross three conditions: MCQ only, open response only, and a combination of\nboth. We find no significant learning differences across conditions at\nposttest, but tutors in the MCQ condition took significantly less time to\ncomplete instruction. These findings suggest that MCQs are as effective, and\nmore efficient, than open response tasks for learning when practice time is\nlimited. To further enhance efficiency, we autograded open responses using\nGPT-4o and GPT-4-turbo. GPT models demonstrate proficiency for purposes of\nlow-stakes assessment, though further research is needed for broader use. This\nstudy contributes a dataset of lesson log data, human annotation rubrics, and\nLLM prompts to promote transparency and reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The role of multiple-choice questions (MCQs) as effective learning tools has\nbeen debated in past research. While MCQs are widely used due to their ease in\ngrading, open response questions are increasingly used for instruction, given\nadvances in large language models (LLMs) for automated grading. This study\nevaluates MCQs effectiveness relative to open-response questions, both\nindividually and in combination, on learning. These activities are embedded\nwithin six tutor lessons on advocacy. Using a posttest-only randomized control\ndesign, we compare the performance of 234 tutors (790 lesson completions)\nacross three conditions: MCQ only, open response only, and a combination of\nboth. We find no significant learning differences across conditions at\nposttest, but tutors in the MCQ condition took significantly less time to\ncomplete instruction. These findings suggest that MCQs are as effective, and\nmore efficient, than open response tasks for learning when practice time is\nlimited. To further enhance efficiency, we autograded open responses using\nGPT-4o and GPT-4-turbo. GPT models demonstrate proficiency for purposes of\nlow-stakes assessment, though further research is needed for broader use. This\nstudy contributes a dataset of lesson log data, human annotation rubrics, and\nLLM prompts to promote transparency and reproducibility."
                },
                "authors": [
                    {
                        "name": "Danielle R. Thomas"
                    },
                    {
                        "name": "Conrad Borchers"
                    },
                    {
                        "name": "Sanjit Kakarla"
                    },
                    {
                        "name": "Jionghao Lin"
                    },
                    {
                        "name": "Shambhavi Bhushan"
                    },
                    {
                        "name": "Boyuan Guo"
                    },
                    {
                        "name": "Erin Gatz"
                    },
                    {
                        "name": "Kenneth R. Koedinger"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth R. Koedinger"
                },
                "author": "Kenneth R. Koedinger",
                "arxiv_doi": "10.1145/3706468.3706530",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706468.3706530",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.10267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Full research paper accepted to Learning Analytics and Knowledge (LAK\n  2025)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10266v1",
                "updated": "2024-12-13T16:34:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    34,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T16:34:39Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    34,
                    39,
                    4,
                    348,
                    0
                ],
                "title": "Reasoner Outperforms: Generative Stance Detection with Rationalization\n  for Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoner Outperforms: Generative Stance Detection with Rationalization\n  for Social Media"
                },
                "summary": "Stance detection is crucial for fostering a human-centric Web by analyzing\nuser-generated content to identify biases and harmful narratives that undermine\ntrust. With the development of Large Language Models (LLMs), existing\napproaches treat stance detection as a classification problem, providing robust\nmethodologies for modeling complex group interactions and advancing\ncapabilities in natural language tasks. However, these methods often lack\ninterpretability, limiting their ability to offer transparent and\nunderstandable justifications for predictions. This study adopts a generative\napproach, where stance predictions include explicit, interpretable rationales,\nand integrates them into smaller language models through single-task and\nmultitask learning. We find that incorporating reasoning into stance detection\nenables the smaller model (FlanT5) to outperform GPT-3.5's zero-shot\nperformance, achieving an improvement of up to 9.57%. Moreover, our results\nshow that reasoning capabilities enhance multitask learning performance but may\nreduce effectiveness in single-task settings. Crucially, we demonstrate that\nfaithful rationales improve rationale distillation into SLMs, advancing efforts\nto build interpretable, trustworthy systems for addressing discrimination,\nfostering trust, and promoting equitable engagement on social media.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stance detection is crucial for fostering a human-centric Web by analyzing\nuser-generated content to identify biases and harmful narratives that undermine\ntrust. With the development of Large Language Models (LLMs), existing\napproaches treat stance detection as a classification problem, providing robust\nmethodologies for modeling complex group interactions and advancing\ncapabilities in natural language tasks. However, these methods often lack\ninterpretability, limiting their ability to offer transparent and\nunderstandable justifications for predictions. This study adopts a generative\napproach, where stance predictions include explicit, interpretable rationales,\nand integrates them into smaller language models through single-task and\nmultitask learning. We find that incorporating reasoning into stance detection\nenables the smaller model (FlanT5) to outperform GPT-3.5's zero-shot\nperformance, achieving an improvement of up to 9.57%. Moreover, our results\nshow that reasoning capabilities enhance multitask learning performance but may\nreduce effectiveness in single-task settings. Crucially, we demonstrate that\nfaithful rationales improve rationale distillation into SLMs, advancing efforts\nto build interpretable, trustworthy systems for addressing discrimination,\nfostering trust, and promoting equitable engagement on social media."
                },
                "authors": [
                    {
                        "name": "Jiaqing Yuan"
                    },
                    {
                        "name": "Ruijie Xi"
                    },
                    {
                        "name": "Munindar P. Singh"
                    }
                ],
                "author_detail": {
                    "name": "Munindar P. Singh"
                },
                "author": "Munindar P. Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10257v1",
                "updated": "2024-12-13T16:26:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    26,
                    34,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T16:26:34Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    26,
                    34,
                    4,
                    348,
                    0
                ],
                "title": "Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Targeted Angular Reversal of Weights (TARS) for Knowledge Removal in\n  Large Language Models"
                },
                "summary": "The sheer scale of data required to train modern large language models (LLMs)\nposes significant risks, as models are likely to gain knowledge of sensitive\ntopics such as bio-security, as well the ability to replicate copyrighted\nworks. Methods designed to remove such knowledge must do so from all prompt\ndirections, in a multi-lingual capacity and without degrading general model\nperformance. To this end, we introduce the targeted angular reversal (TARS)\nmethod of knowledge removal from LLMs. The TARS method firstly leverages the\nLLM in combination with a detailed prompt to aggregate information about a\nselected concept in the internal representation space of the LLM. It then\nrefines this approximate concept vector to trigger the concept token with high\nprobability, by perturbing the approximate concept vector with noise and\ntransforming it into token scores with the language model head. The feedforward\nweight vectors in the LLM which operate directly on the internal representation\nspace, and have the highest cosine similarity with this targeting vector, are\nthen replaced by a reversed targeting vector, thus limiting the ability of the\nconcept to propagate through the model. The modularity of the TARS method\nallows for a sequential removal of concepts from Llama 3.1 8B, such as the\nfamous literary detective Sherlock Holmes, and the planet Saturn. It is\ndemonstrated that the probability of triggering target concepts can be reduced\nto 0.00 with as few as 1 TARS edit, whilst simultaneously removing the\nknowledge bi-directionally. Moreover, knowledge is shown to be removed across\nall languages despite only being targeted in English. Importantly, TARS has\nminimal impact on the general model capabilities, as after removing 5 diverse\nconcepts in a modular fashion, there is minimal KL divergence in the next token\nprobabilities of the LLM on large corpora of Wikipedia text (median of 0.002).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The sheer scale of data required to train modern large language models (LLMs)\nposes significant risks, as models are likely to gain knowledge of sensitive\ntopics such as bio-security, as well the ability to replicate copyrighted\nworks. Methods designed to remove such knowledge must do so from all prompt\ndirections, in a multi-lingual capacity and without degrading general model\nperformance. To this end, we introduce the targeted angular reversal (TARS)\nmethod of knowledge removal from LLMs. The TARS method firstly leverages the\nLLM in combination with a detailed prompt to aggregate information about a\nselected concept in the internal representation space of the LLM. It then\nrefines this approximate concept vector to trigger the concept token with high\nprobability, by perturbing the approximate concept vector with noise and\ntransforming it into token scores with the language model head. The feedforward\nweight vectors in the LLM which operate directly on the internal representation\nspace, and have the highest cosine similarity with this targeting vector, are\nthen replaced by a reversed targeting vector, thus limiting the ability of the\nconcept to propagate through the model. The modularity of the TARS method\nallows for a sequential removal of concepts from Llama 3.1 8B, such as the\nfamous literary detective Sherlock Holmes, and the planet Saturn. It is\ndemonstrated that the probability of triggering target concepts can be reduced\nto 0.00 with as few as 1 TARS edit, whilst simultaneously removing the\nknowledge bi-directionally. Moreover, knowledge is shown to be removed across\nall languages despite only being targeted in English. Importantly, TARS has\nminimal impact on the general model capabilities, as after removing 5 diverse\nconcepts in a modular fashion, there is minimal KL divergence in the next token\nprobabilities of the LLM on large corpora of Wikipedia text (median of 0.002)."
                },
                "authors": [
                    {
                        "name": "Harry J. Davies"
                    },
                    {
                        "name": "Giorgos Iacovides"
                    },
                    {
                        "name": "Danilo P. Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo P. Mandic"
                },
                "author": "Danilo P. Mandic",
                "arxiv_comment": "14 pages, 5 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10246v1",
                "updated": "2024-12-13T16:14:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    14,
                    49,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T16:14:49Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    14,
                    49,
                    4,
                    348,
                    0
                ],
                "title": "Detecting LLM Hallucination Through Layer-wise Information Deficiency:\n  Analysis of Unanswerable Questions and Ambiguous Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting LLM Hallucination Through Layer-wise Information Deficiency:\n  Analysis of Unanswerable Questions and Ambiguous Prompts"
                },
                "summary": "Large language models (LLMs) frequently generate confident yet inaccurate\nresponses, introducing significant risks for deployment in safety-critical\ndomains. We present a novel approach to detecting model hallucination through\nsystematic analysis of information flow across model layers when processing\ninputs with insufficient or ambiguous context. Our investigation reveals that\nhallucination manifests as usable information deficiencies in inter-layer\ntransmissions. While existing approaches primarily focus on final-layer output\nanalysis, we demonstrate that tracking cross-layer information dynamics\n($\\mathcal{L}$I) provides robust indicators of model reliability, accounting\nfor both information gain and loss during computation. $\\mathcal{L}$I improves\nmodel reliability by immediately integrating with universal LLMs without\nadditional training or architectural modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) frequently generate confident yet inaccurate\nresponses, introducing significant risks for deployment in safety-critical\ndomains. We present a novel approach to detecting model hallucination through\nsystematic analysis of information flow across model layers when processing\ninputs with insufficient or ambiguous context. Our investigation reveals that\nhallucination manifests as usable information deficiencies in inter-layer\ntransmissions. While existing approaches primarily focus on final-layer output\nanalysis, we demonstrate that tracking cross-layer information dynamics\n($\\mathcal{L}$I) provides robust indicators of model reliability, accounting\nfor both information gain and loss during computation. $\\mathcal{L}$I improves\nmodel reliability by immediately integrating with universal LLMs without\nadditional training or architectural modifications."
                },
                "authors": [
                    {
                        "name": "Hazel Kim"
                    },
                    {
                        "name": "Adel Bibi"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Yarin Gal"
                    }
                ],
                "author_detail": {
                    "name": "Yarin Gal"
                },
                "author": "Yarin Gal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10244v1",
                "updated": "2024-12-13T16:13:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    35,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T16:13:35Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    35,
                    4,
                    348,
                    0
                ],
                "title": "Efficient Continual Pre-training of LLMs for Low-resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Continual Pre-training of LLMs for Low-resource Languages"
                },
                "summary": "Open-source Large Language models (OsLLMs) propel the democratization of\nnatural language research by giving the flexibility to augment or update model\nparameters for performance improvement. Nevertheless, like proprietary LLMs,\nOs-LLMs offer poorer performance on low-resource languages (LRLs) than\nhigh-resource languages (HRLs), owing to smaller amounts of training data and\nunderrepresented vocabulary. On the other hand, continual pre-training (CPT)\nwith large amounts of language-specific data is a costly proposition in terms\nof data acquisition and computational resources. Our goal is to drastically\nreduce CPT cost. To that end, we first develop a new algorithm to select a\nsubset of texts from a larger corpus. We show the effectiveness of our\ntechnique using very little CPT data. In search of further improvement, we\ndesign a new algorithm to select tokens to include in the LLM vocabulary. We\nexperiment with the recent Llama-3 model and nine Indian languages with diverse\nscripts and extent of resource availability. For evaluation, we use\nIndicGenBench, a generation task benchmark dataset for Indic languages. We\nexperiment with various CPT corpora and augmented vocabulary size and offer\ninsights across language families.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source Large Language models (OsLLMs) propel the democratization of\nnatural language research by giving the flexibility to augment or update model\nparameters for performance improvement. Nevertheless, like proprietary LLMs,\nOs-LLMs offer poorer performance on low-resource languages (LRLs) than\nhigh-resource languages (HRLs), owing to smaller amounts of training data and\nunderrepresented vocabulary. On the other hand, continual pre-training (CPT)\nwith large amounts of language-specific data is a costly proposition in terms\nof data acquisition and computational resources. Our goal is to drastically\nreduce CPT cost. To that end, we first develop a new algorithm to select a\nsubset of texts from a larger corpus. We show the effectiveness of our\ntechnique using very little CPT data. In search of further improvement, we\ndesign a new algorithm to select tokens to include in the LLM vocabulary. We\nexperiment with the recent Llama-3 model and nine Indian languages with diverse\nscripts and extent of resource availability. For evaluation, we use\nIndicGenBench, a generation task benchmark dataset for Indic languages. We\nexperiment with various CPT corpora and augmented vocabulary size and offer\ninsights across language families."
                },
                "authors": [
                    {
                        "name": "Arijit Nag"
                    },
                    {
                        "name": "Soumen Chakrabarti"
                    },
                    {
                        "name": "Animesh Mukherjee"
                    },
                    {
                        "name": "Niloy Ganguly"
                    }
                ],
                "author_detail": {
                    "name": "Niloy Ganguly"
                },
                "author": "Niloy Ganguly",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12435v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12435v2",
                "updated": "2024-12-13T15:50:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    50,
                    26,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-19T03:29:40Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    3,
                    29,
                    40,
                    3,
                    263,
                    0
                ],
                "title": "Linguistic Minimal Pairs Elicit Linguistic Similarity in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linguistic Minimal Pairs Elicit Linguistic Similarity in Large Language\n  Models"
                },
                "summary": "We introduce a novel analysis that leverages linguistic minimal pairs to\nprobe the internal linguistic representations of Large Language Models (LLMs).\nBy measuring the similarity between LLM activation differences across minimal\npairs, we quantify the and gain insight into the linguistic knowledge captured\nby LLMs. Our large-scale experiments, spanning 100+ LLMs and 150k minimal pairs\nin three languages, reveal properties of linguistic similarity from four key\naspects: consistency across LLMs, relation to theoretical categorizations,\ndependency to semantic context, and cross-lingual alignment of relevant\nphenomena. Our findings suggest that 1) linguistic similarity is significantly\ninfluenced by training data exposure, leading to higher cross-LLM agreement in\nhigher-resource languages. 2) Linguistic similarity strongly aligns with\nfine-grained theoretical linguistic categories but weakly with broader ones. 3)\nLinguistic similarity shows a weak correlation with semantic similarity,\nshowing its context-dependent nature. 4) LLMs exhibit limited cross-lingual\nalignment in their understanding of relevant linguistic phenomena. This work\ndemonstrates the potential of minimal pairs as a window into the neural\nrepresentations of language in LLMs, shedding light on the relationship between\nLLMs and linguistic theory. Codes and data are available at\nhttps://github.com/ChenDelong1999/Linguistic-Similarity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel analysis that leverages linguistic minimal pairs to\nprobe the internal linguistic representations of Large Language Models (LLMs).\nBy measuring the similarity between LLM activation differences across minimal\npairs, we quantify the and gain insight into the linguistic knowledge captured\nby LLMs. Our large-scale experiments, spanning 100+ LLMs and 150k minimal pairs\nin three languages, reveal properties of linguistic similarity from four key\naspects: consistency across LLMs, relation to theoretical categorizations,\ndependency to semantic context, and cross-lingual alignment of relevant\nphenomena. Our findings suggest that 1) linguistic similarity is significantly\ninfluenced by training data exposure, leading to higher cross-LLM agreement in\nhigher-resource languages. 2) Linguistic similarity strongly aligns with\nfine-grained theoretical linguistic categories but weakly with broader ones. 3)\nLinguistic similarity shows a weak correlation with semantic similarity,\nshowing its context-dependent nature. 4) LLMs exhibit limited cross-lingual\nalignment in their understanding of relevant linguistic phenomena. This work\ndemonstrates the potential of minimal pairs as a window into the neural\nrepresentations of language in LLMs, shedding light on the relationship between\nLLMs and linguistic theory. Codes and data are available at\nhttps://github.com/ChenDelong1999/Linguistic-Similarity"
                },
                "authors": [
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Delong Chen"
                    },
                    {
                        "name": "Samuel Cahyawijaya"
                    },
                    {
                        "name": "Xufeng Duan"
                    },
                    {
                        "name": "Zhenguang G. Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zhenguang G. Cai"
                },
                "author": "Zhenguang G. Cai",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12435v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12435v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10220v1",
                "updated": "2024-12-13T15:45:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    45,
                    45,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    45,
                    45,
                    4,
                    348,
                    0
                ],
                "title": "How good is my story? Towards quantitative metrics for evaluating\n  LLM-generated XAI narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How good is my story? Towards quantitative metrics for evaluating\n  LLM-generated XAI narratives"
                },
                "summary": "A rapidly developing application of LLMs in XAI is to convert quantitative\nexplanations such as SHAP into user-friendly narratives to explain the\ndecisions made by smaller prediction models. Evaluating the narratives without\nrelying on human preference studies or surveys is becoming increasingly\nimportant in this field. In this work we propose a framework and explore\nseveral automated metrics to evaluate LLM-generated narratives for explanations\nof tabular classification tasks. We apply our approach to compare several\nstate-of-the-art LLMs across different datasets and prompt types. As a\ndemonstration of their utility, these metrics allow us to identify new\nchallenges related to LLM hallucinations for XAI narratives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A rapidly developing application of LLMs in XAI is to convert quantitative\nexplanations such as SHAP into user-friendly narratives to explain the\ndecisions made by smaller prediction models. Evaluating the narratives without\nrelying on human preference studies or surveys is becoming increasingly\nimportant in this field. In this work we propose a framework and explore\nseveral automated metrics to evaluate LLM-generated narratives for explanations\nof tabular classification tasks. We apply our approach to compare several\nstate-of-the-art LLMs across different datasets and prompt types. As a\ndemonstration of their utility, these metrics allow us to identify new\nchallenges related to LLM hallucinations for XAI narratives."
                },
                "authors": [
                    {
                        "name": "Timour Ichmoukhamedov"
                    },
                    {
                        "name": "James Hinns"
                    },
                    {
                        "name": "David Martens"
                    }
                ],
                "author_detail": {
                    "name": "David Martens"
                },
                "author": "David Martens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10219v1",
                "updated": "2024-12-13T15:41:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    41,
                    8,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T15:41:08Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    41,
                    8,
                    4,
                    348,
                    0
                ],
                "title": "Learning Complex Non-Rigid Image Edits from Multimodal Conditioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Complex Non-Rigid Image Edits from Multimodal Conditioning"
                },
                "summary": "In this paper we focus on inserting a given human (specifically, a single\nimage of a person) into a novel scene. Our method, which builds on top of\nStable Diffusion, yields natural looking images while being highly controllable\nwith text and pose. To accomplish this we need to train on pairs of images, the\nfirst a reference image with the person, the second a \"target image\" showing\nthe same person (with a different pose and possibly in a different background).\nAdditionally we require a text caption describing the new pose relative to that\nin the reference image. In this paper we present a novel dataset following this\ncriteria, which we create using pairs of frames from human-centric and\naction-rich videos and employing a multimodal LLM to automatically summarize\nthe difference in human pose for the text captions. We demonstrate that\nidentity preservation is a more challenging task in scenes \"in-the-wild\", and\nespecially scenes where there is an interaction between persons and objects.\nCombining the weak supervision from noisy captions, with robust 2D pose\nimproves the quality of person-object interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we focus on inserting a given human (specifically, a single\nimage of a person) into a novel scene. Our method, which builds on top of\nStable Diffusion, yields natural looking images while being highly controllable\nwith text and pose. To accomplish this we need to train on pairs of images, the\nfirst a reference image with the person, the second a \"target image\" showing\nthe same person (with a different pose and possibly in a different background).\nAdditionally we require a text caption describing the new pose relative to that\nin the reference image. In this paper we present a novel dataset following this\ncriteria, which we create using pairs of frames from human-centric and\naction-rich videos and employing a multimodal LLM to automatically summarize\nthe difference in human pose for the text captions. We demonstrate that\nidentity preservation is a more challenging task in scenes \"in-the-wild\", and\nespecially scenes where there is an interaction between persons and objects.\nCombining the weak supervision from noisy captions, with robust 2D pose\nimproves the quality of person-object interactions."
                },
                "authors": [
                    {
                        "name": "Nikolai Warner"
                    },
                    {
                        "name": "Jack Kolb"
                    },
                    {
                        "name": "Meera Hahn"
                    },
                    {
                        "name": "Vighnesh Birodkar"
                    },
                    {
                        "name": "Jonathan Huang"
                    },
                    {
                        "name": "Irfan Essa"
                    }
                ],
                "author_detail": {
                    "name": "Irfan Essa"
                },
                "author": "Irfan Essa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07889v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07889v2",
                "updated": "2024-12-13T15:39:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    39,
                    27,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-10T19:48:57Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    19,
                    48,
                    57,
                    1,
                    345,
                    0
                ],
                "title": "Low-Latency Scalable Streaming for Event-Based Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Latency Scalable Streaming for Event-Based Vision"
                },
                "summary": "Recently, we have witnessed the rise of novel ``event-based'' camera sensors\nfor high-speed, low-power video capture. Rather than recording discrete image\nframes, these sensors output asynchronous ``event'' tuples with microsecond\nprecision, only when the brightness change of a given pixel exceeds a certain\nthreshold. Although these sensors have enabled compelling new computer vision\napplications, these applications often require expensive, power-hungry GPU\nsystems, rendering them incompatible for deployment on the low-power devices\nfor which event cameras are optimized. Whereas receiver-driven rate adaptation\nis a crucial feature of modern video streaming solutions, this topic is\nunderexplored in the realm of event-based vision systems. On a real-world event\ncamera dataset, we first demonstrate that a state-of-the-art object detection\napplication is resilient to dramatic data loss, and that this loss may be\nweighted towards the end of each temporal window. We then propose a scalable\nstreaming method for event-based data based on Media Over QUIC, prioritizing\nobject detection performance and low latency. The application server can\nreceive complementary event data across several streams simultaneously, and\ndrop streams as needed to maintain a certain latency. With a latency target of\n5 ms for end-to-end transmission across a small network, we observe an average\nreduction in detection mAP as low as 0.36. With a more relaxed latency target\nof 50 ms, we observe an average mAP reduction as low as 0.19.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, we have witnessed the rise of novel ``event-based'' camera sensors\nfor high-speed, low-power video capture. Rather than recording discrete image\nframes, these sensors output asynchronous ``event'' tuples with microsecond\nprecision, only when the brightness change of a given pixel exceeds a certain\nthreshold. Although these sensors have enabled compelling new computer vision\napplications, these applications often require expensive, power-hungry GPU\nsystems, rendering them incompatible for deployment on the low-power devices\nfor which event cameras are optimized. Whereas receiver-driven rate adaptation\nis a crucial feature of modern video streaming solutions, this topic is\nunderexplored in the realm of event-based vision systems. On a real-world event\ncamera dataset, we first demonstrate that a state-of-the-art object detection\napplication is resilient to dramatic data loss, and that this loss may be\nweighted towards the end of each temporal window. We then propose a scalable\nstreaming method for event-based data based on Media Over QUIC, prioritizing\nobject detection performance and low latency. The application server can\nreceive complementary event data across several streams simultaneously, and\ndrop streams as needed to maintain a certain latency. With a latency target of\n5 ms for end-to-end transmission across a small network, we observe an average\nreduction in detection mAP as low as 0.36. With a more relaxed latency target\nof 50 ms, we observe an average mAP reduction as low as 0.19."
                },
                "authors": [
                    {
                        "name": "Andrew Hamara"
                    },
                    {
                        "name": "Benjamin Kilpatrick"
                    },
                    {
                        "name": "Alex Baratta"
                    },
                    {
                        "name": "Brendon Kofink"
                    },
                    {
                        "name": "Andrew C. Freeman"
                    }
                ],
                "author_detail": {
                    "name": "Andrew C. Freeman"
                },
                "author": "Andrew C. Freeman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07889v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07889v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12741v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12741v3",
                "updated": "2024-12-13T15:37:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    37,
                    1,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-19T13:03:24Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    13,
                    3,
                    24,
                    3,
                    263,
                    0
                ],
                "title": "Fine Tuning Large Language Models for Medicine: The Role and Importance\n  of Direct Preference Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine Tuning Large Language Models for Medicine: The Role and Importance\n  of Direct Preference Optimization"
                },
                "summary": "Large Language Model (LLM) fine tuning is underutilized in the field of\nmedicine. Two of the most common methods of fine tuning are Supervised Fine\nTuning (SFT) and Direct Preference Optimization (DPO), but there is little\nguidance informing users when to use either technique. In this investigation,\nwe compare the performance of SFT and DPO for five common natural language\ntasks in medicine: Classification with text data, Classification with numeric\ndata, Clinical Reasoning, Summarization, and Clinical Triage. We find that SFT\nalone is sufficient for Classification with text data, whereas DPO improves\nperformance for the more complex tasks of Clinical Reasoning, Summarization and\nClinical Triage. Our results establish the role and importance of DPO fine\ntuning within medicine, and consequently call attention to current software\ngaps that prevent widespread deployment of this technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) fine tuning is underutilized in the field of\nmedicine. Two of the most common methods of fine tuning are Supervised Fine\nTuning (SFT) and Direct Preference Optimization (DPO), but there is little\nguidance informing users when to use either technique. In this investigation,\nwe compare the performance of SFT and DPO for five common natural language\ntasks in medicine: Classification with text data, Classification with numeric\ndata, Clinical Reasoning, Summarization, and Clinical Triage. We find that SFT\nalone is sufficient for Classification with text data, whereas DPO improves\nperformance for the more complex tasks of Clinical Reasoning, Summarization and\nClinical Triage. Our results establish the role and importance of DPO fine\ntuning within medicine, and consequently call attention to current software\ngaps that prevent widespread deployment of this technique."
                },
                "authors": [
                    {
                        "name": "Thomas Savage"
                    },
                    {
                        "name": "Stephen Ma"
                    },
                    {
                        "name": "Abdessalem Boukil"
                    },
                    {
                        "name": "Vishwesh Patel"
                    },
                    {
                        "name": "Ekanath Rangan"
                    },
                    {
                        "name": "Ivan Lopez"
                    },
                    {
                        "name": "Jonathan H Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan H Chen"
                },
                "author": "Jonathan H Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12741v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12741v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17569v2",
                "updated": "2024-12-13T15:36:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    36,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-26T16:31:18Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    31,
                    18,
                    1,
                    331,
                    0
                ],
                "title": "RTL-Breaker: Assessing the Security of LLMs against Backdoor Attacks on\n  HDL Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTL-Breaker: Assessing the Security of LLMs against Backdoor Attacks on\n  HDL Code Generation"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable potential with code\ngeneration/completion tasks for hardware design. In fact, LLM-based hardware\ndescription language (HDL) code generation has enabled the industry to realize\ncomplex designs more quickly, reducing the time and effort required in the\ndevelopment cycle. However, the increased reliance on such automation\nintroduces critical security risks. Notably, given that LLMs have to be trained\non vast datasets of codes that are typically sourced from publicly available\nrepositories (often without thorough validation), LLMs are susceptible to\nso-called data poisoning or backdoor attacks. Here, attackers inject malicious\ncode for the training data, which can be carried over into the HDL code\ngenerated by LLMs. This threat vector can compromise the security and integrity\nof entire hardware systems. In this work, we propose RTL-Breaker, a novel\nbackdoor attack framework on LLM-based HDL code generation. RTL-Breaker\nprovides an in-depth analysis for essential aspects of this novel problem: 1)\nvarious trigger mechanisms versus their effectiveness for inserting malicious\nmodifications, and 2) side-effects by backdoor attacks on code generation in\ngeneral, i.e., impact on code quality. RTL-Breaker emphasizes the urgent need\nfor more robust measures to safeguard against such attacks. Toward that end, we\nopen-source our framework and all data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable potential with code\ngeneration/completion tasks for hardware design. In fact, LLM-based hardware\ndescription language (HDL) code generation has enabled the industry to realize\ncomplex designs more quickly, reducing the time and effort required in the\ndevelopment cycle. However, the increased reliance on such automation\nintroduces critical security risks. Notably, given that LLMs have to be trained\non vast datasets of codes that are typically sourced from publicly available\nrepositories (often without thorough validation), LLMs are susceptible to\nso-called data poisoning or backdoor attacks. Here, attackers inject malicious\ncode for the training data, which can be carried over into the HDL code\ngenerated by LLMs. This threat vector can compromise the security and integrity\nof entire hardware systems. In this work, we propose RTL-Breaker, a novel\nbackdoor attack framework on LLM-based HDL code generation. RTL-Breaker\nprovides an in-depth analysis for essential aspects of this novel problem: 1)\nvarious trigger mechanisms versus their effectiveness for inserting malicious\nmodifications, and 2) side-effects by backdoor attacks on code generation in\ngeneral, i.e., impact on code quality. RTL-Breaker emphasizes the urgent need\nfor more robust measures to safeguard against such attacks. Toward that end, we\nopen-source our framework and all data."
                },
                "authors": [
                    {
                        "name": "Lakshmi Likhitha Mankali"
                    },
                    {
                        "name": "Jitendra Bhandari"
                    },
                    {
                        "name": "Manaar Alam"
                    },
                    {
                        "name": "Ramesh Karri"
                    },
                    {
                        "name": "Michail Maniatakos"
                    },
                    {
                        "name": "Ozgur Sinanoglu"
                    },
                    {
                        "name": "Johann Knechtel"
                    }
                ],
                "author_detail": {
                    "name": "Johann Knechtel"
                },
                "author": "Johann Knechtel",
                "arxiv_comment": "Accepted at 2025 Design, Automation & Test in Europe (DATE)\n  Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10207v1",
                "updated": "2024-12-13T15:30:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    30,
                    20,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T15:30:20Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    30,
                    20,
                    4,
                    348,
                    0
                ],
                "title": "Retrieval-Augmented Semantic Parsing: Using Large Language Models to\n  Improve Generalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Semantic Parsing: Using Large Language Models to\n  Improve Generalization"
                },
                "summary": "Open-domain semantic parsing remains a challenging task, as models often rely\non heuristics and struggle to handle unseen concepts. In this paper, we\ninvestigate the potential of large language models (LLMs) for this task and\nintroduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effective\napproach that integrates external lexical knowledge into the parsing process.\nOur experiments not only show that LLMs outperform previous encoder-decoder\nbaselines for semantic parsing, but that RASP further enhances their ability to\npredict unseen concepts, nearly doubling the performance of previous models on\nout-of-distribution concepts. These findings highlight the promise of\nleveraging large language models and retrieval mechanisms for robust and\nopen-domain semantic parsing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-domain semantic parsing remains a challenging task, as models often rely\non heuristics and struggle to handle unseen concepts. In this paper, we\ninvestigate the potential of large language models (LLMs) for this task and\nintroduce Retrieval-Augmented Semantic Parsing (RASP), a simple yet effective\napproach that integrates external lexical knowledge into the parsing process.\nOur experiments not only show that LLMs outperform previous encoder-decoder\nbaselines for semantic parsing, but that RASP further enhances their ability to\npredict unseen concepts, nearly doubling the performance of previous models on\nout-of-distribution concepts. These findings highlight the promise of\nleveraging large language models and retrieval mechanisms for robust and\nopen-domain semantic parsing."
                },
                "authors": [
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Qianru Meng"
                    },
                    {
                        "name": "Johan Bos"
                    }
                ],
                "author_detail": {
                    "name": "Johan Bos"
                },
                "author": "Johan Bos",
                "arxiv_comment": "Submitted to ARR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18916v2",
                "updated": "2024-12-13T15:15:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    15,
                    46,
                    4,
                    348,
                    0
                ],
                "published": "2024-06-27T06:13:05Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    6,
                    13,
                    5,
                    3,
                    179,
                    0
                ],
                "title": "TrustUQA: A Trustful Framework for Unified Structured Data Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrustUQA: A Trustful Framework for Unified Structured Data Question\n  Answering"
                },
                "summary": "Natural language question answering (QA) over structured data sources such as\ntables and knowledge graphs have been widely investigated, especially with\nLarge Language Models (LLMs) in recent years. The main solutions include\nquestion to formal query parsing and retrieval-based answer generation.\nHowever, current methods of the former often suffer from weak generalization,\nfailing to dealing with multi-types of sources, while the later is limited in\ntrustfulness. In this paper, we propose TrustUQA, a trustful QA framework that\ncan simultaneously support multiple types of structured data in a unified way.\nTo this end, it adopts an LLM-friendly and unified knowledge representation\nmethod called Condition Graph(CG), and uses an LLM and demonstration-based\ntwo-level method for CG querying. For enhancement, it is also equipped with\ndynamic demonstration retrieval. We have evaluated TrustUQA with 5 benchmarks\ncovering 3 types of structured data. It outperforms 2 existing unified\nstructured data QA methods. In comparison with the baselines that are specific\nto one data type, it achieves state-of-the-art on 2 of the datasets. Further\nmore, we have demonstrated the potential of our method for more general QA\ntasks, QA over mixed structured data and QA across structured data. The code is\navailable at https://github.com/zjukg/TrustUQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language question answering (QA) over structured data sources such as\ntables and knowledge graphs have been widely investigated, especially with\nLarge Language Models (LLMs) in recent years. The main solutions include\nquestion to formal query parsing and retrieval-based answer generation.\nHowever, current methods of the former often suffer from weak generalization,\nfailing to dealing with multi-types of sources, while the later is limited in\ntrustfulness. In this paper, we propose TrustUQA, a trustful QA framework that\ncan simultaneously support multiple types of structured data in a unified way.\nTo this end, it adopts an LLM-friendly and unified knowledge representation\nmethod called Condition Graph(CG), and uses an LLM and demonstration-based\ntwo-level method for CG querying. For enhancement, it is also equipped with\ndynamic demonstration retrieval. We have evaluated TrustUQA with 5 benchmarks\ncovering 3 types of structured data. It outperforms 2 existing unified\nstructured data QA methods. In comparison with the baselines that are specific\nto one data type, it achieves state-of-the-art on 2 of the datasets. Further\nmore, we have demonstrated the potential of our method for more general QA\ntasks, QA over mixed structured data and QA across structured data. The code is\navailable at https://github.com/zjukg/TrustUQA."
                },
                "authors": [
                    {
                        "name": "Wen Zhang"
                    },
                    {
                        "name": "Long Jin"
                    },
                    {
                        "name": "Yushan Zhu"
                    },
                    {
                        "name": "Jiaoyan Chen"
                    },
                    {
                        "name": "Zhiwei Huang"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Yin Hua"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10198v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10198v1",
                "updated": "2024-12-13T15:15:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    15,
                    24,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T15:15:24Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    15,
                    24,
                    4,
                    348,
                    0
                ],
                "title": "From Allies to Adversaries: Manipulating LLM Tool-Calling through\n  Adversarial Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Allies to Adversaries: Manipulating LLM Tool-Calling through\n  Adversarial Injection"
                },
                "summary": "Tool-calling has changed Large Language Model (LLM) applications by\nintegrating external tools, significantly enhancing their functionality across\ndiverse tasks. However, this integration also introduces new security\nvulnerabilities, particularly in the tool scheduling mechanisms of LLM, which\nhave not been extensively studied. To fill this gap, we present ToolCommander,\na novel framework designed to exploit vulnerabilities in LLM tool-calling\nsystems through adversarial tool injection. Our framework employs a\nwell-designed two-stage attack strategy. Firstly, it injects malicious tools to\ncollect user queries, then dynamically updates the injected tools based on the\nstolen information to enhance subsequent attacks. These stages enable\nToolCommander to execute privacy theft, launch denial-of-service attacks, and\neven manipulate business competition by triggering unscheduled tool-calling.\nNotably, the ASR reaches 91.67% for privacy theft and hits 100% for\ndenial-of-service and unscheduled tool calling in certain cases. Our work\ndemonstrates that these vulnerabilities can lead to severe consequences beyond\nsimple misuse of tool-calling systems, underscoring the urgent need for robust\ndefensive strategies to secure LLM Tool-calling systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-calling has changed Large Language Model (LLM) applications by\nintegrating external tools, significantly enhancing their functionality across\ndiverse tasks. However, this integration also introduces new security\nvulnerabilities, particularly in the tool scheduling mechanisms of LLM, which\nhave not been extensively studied. To fill this gap, we present ToolCommander,\na novel framework designed to exploit vulnerabilities in LLM tool-calling\nsystems through adversarial tool injection. Our framework employs a\nwell-designed two-stage attack strategy. Firstly, it injects malicious tools to\ncollect user queries, then dynamically updates the injected tools based on the\nstolen information to enhance subsequent attacks. These stages enable\nToolCommander to execute privacy theft, launch denial-of-service attacks, and\neven manipulate business competition by triggering unscheduled tool-calling.\nNotably, the ASR reaches 91.67% for privacy theft and hits 100% for\ndenial-of-service and unscheduled tool calling in certain cases. Our work\ndemonstrates that these vulnerabilities can lead to severe consequences beyond\nsimple misuse of tool-calling systems, underscoring the urgent need for robust\ndefensive strategies to secure LLM Tool-calling systems."
                },
                "authors": [
                    {
                        "name": "Haowei Wang"
                    },
                    {
                        "name": "Rupeng Zhang"
                    },
                    {
                        "name": "Junjie Wang"
                    },
                    {
                        "name": "Mingyang Li"
                    },
                    {
                        "name": "Yuekai Huang"
                    },
                    {
                        "name": "Dandan Wang"
                    },
                    {
                        "name": "Qing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qing Wang"
                },
                "author": "Qing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10198v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17461v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17461v2",
                "updated": "2024-12-13T15:08:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    15,
                    8,
                    32,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-26T14:28:25Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    28,
                    25,
                    1,
                    331,
                    0
                ],
                "title": "SoK: Decentralized AI (DeAI)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Decentralized AI (DeAI)"
                },
                "summary": "The centralization of Artificial Intelligence (AI) poses significant\nchallenges, including single points of failure, inherent biases, data privacy\nconcerns, and scalability issues. These problems are especially prevalent in\nclosed-source large language models (LLMs), where user data is collected and\nused without transparency. To mitigate these issues, blockchain-based\ndecentralized AI (DeAI) has emerged as a promising solution. DeAI combines the\nstrengths of both blockchain and AI technologies to enhance the transparency,\nsecurity, decentralization, and trustworthiness of AI systems. However, a\ncomprehensive understanding of state-of-the-art DeAI development, particularly\nfor active industry solutions, is still lacking. In this work, we present a\nSystematization of Knowledge (SoK) for blockchain-based DeAI solutions. We\npropose a taxonomy to classify existing DeAI protocols based on the model\nlifecycle. Based on this taxonomy, we provide a structured way to clarify the\nlandscape of DeAI protocols and identify their similarities and differences. We\nanalyze the functionalities of blockchain in DeAI, investigating how blockchain\nfeatures contribute to enhancing the security, transparency, and\ntrustworthiness of AI processes, while also ensuring fair incentives for AI\ndata and model contributors. In addition, we identify key insights and research\ngaps in developing DeAI protocols, highlighting several critical avenues for\nfuture research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The centralization of Artificial Intelligence (AI) poses significant\nchallenges, including single points of failure, inherent biases, data privacy\nconcerns, and scalability issues. These problems are especially prevalent in\nclosed-source large language models (LLMs), where user data is collected and\nused without transparency. To mitigate these issues, blockchain-based\ndecentralized AI (DeAI) has emerged as a promising solution. DeAI combines the\nstrengths of both blockchain and AI technologies to enhance the transparency,\nsecurity, decentralization, and trustworthiness of AI systems. However, a\ncomprehensive understanding of state-of-the-art DeAI development, particularly\nfor active industry solutions, is still lacking. In this work, we present a\nSystematization of Knowledge (SoK) for blockchain-based DeAI solutions. We\npropose a taxonomy to classify existing DeAI protocols based on the model\nlifecycle. Based on this taxonomy, we provide a structured way to clarify the\nlandscape of DeAI protocols and identify their similarities and differences. We\nanalyze the functionalities of blockchain in DeAI, investigating how blockchain\nfeatures contribute to enhancing the security, transparency, and\ntrustworthiness of AI processes, while also ensuring fair incentives for AI\ndata and model contributors. In addition, we identify key insights and research\ngaps in developing DeAI protocols, highlighting several critical avenues for\nfuture research."
                },
                "authors": [
                    {
                        "name": "Zhipeng Wang"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Elizabeth Lui"
                    },
                    {
                        "name": "Vatsal Shah"
                    },
                    {
                        "name": "Xihan Xiong"
                    },
                    {
                        "name": "Jiahao Sun"
                    },
                    {
                        "name": "Davide Crapis"
                    },
                    {
                        "name": "William Knottenbelt"
                    }
                ],
                "author_detail": {
                    "name": "William Knottenbelt"
                },
                "author": "William Knottenbelt",
                "arxiv_comment": "This is a Systematization of Knowledge (SoK) for the rapidly evolving\n  field of Decentralized AI (DeAI). We welcome valuable comments, suggestions,\n  and collaboration to further refine and enhance this work. We hope our\n  contribution will help accelerate the advancement of DeAI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17461v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17461v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10180v1",
                "updated": "2024-12-13T14:52:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    52,
                    41,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:52:41Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    52,
                    41,
                    4,
                    348,
                    0
                ],
                "title": "A General Safety Framework for Autonomous Manipulation in Human\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A General Safety Framework for Autonomous Manipulation in Human\n  Environments"
                },
                "summary": "Autonomous robots are projected to augment the manual workforce, especially\nin repetitive and hazardous tasks. For a successful deployment of such robots\nin human environments, it is crucial to guarantee human safety.\nState-of-the-art approaches to ensure human safety are either too restrictive\nto permit a natural human-robot collaboration or make strong assumptions that\ndo not hold when for autonomous robots, e.g., knowledge of a pre-defined\ntrajectory. Therefore, we propose SaRA-shield, a power and force limiting\nframework for AI-based manipulation in human environments that gives formal\nsafety guarantees while allowing for fast robot speeds. As recent studies have\nshown that unconstrained collisions allow for significantly higher contact\nforces than constrained collisions (clamping), we propose to classify contacts\nby their collision type using reachability analysis. We then verify that the\nkinetic energy of the robot is below pain and injury thresholds for the\ndetected collision type of the respective human body part in contact. Our\nreal-world experiments show that SaRA-shield can effectively reduce the speed\nof the robot to adhere to injury-preventing energy limits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous robots are projected to augment the manual workforce, especially\nin repetitive and hazardous tasks. For a successful deployment of such robots\nin human environments, it is crucial to guarantee human safety.\nState-of-the-art approaches to ensure human safety are either too restrictive\nto permit a natural human-robot collaboration or make strong assumptions that\ndo not hold when for autonomous robots, e.g., knowledge of a pre-defined\ntrajectory. Therefore, we propose SaRA-shield, a power and force limiting\nframework for AI-based manipulation in human environments that gives formal\nsafety guarantees while allowing for fast robot speeds. As recent studies have\nshown that unconstrained collisions allow for significantly higher contact\nforces than constrained collisions (clamping), we propose to classify contacts\nby their collision type using reachability analysis. We then verify that the\nkinetic energy of the robot is below pain and injury thresholds for the\ndetected collision type of the respective human body part in contact. Our\nreal-world experiments show that SaRA-shield can effectively reduce the speed\nof the robot to adhere to injury-preventing energy limits."
                },
                "authors": [
                    {
                        "name": "Jakob Thumm"
                    },
                    {
                        "name": "Julian Balletshofer"
                    },
                    {
                        "name": "Leonardo Maglanoc"
                    },
                    {
                        "name": "Luis Muschal"
                    },
                    {
                        "name": "Matthias Althoff"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Althoff"
                },
                "author": "Matthias Althoff",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10139v1",
                "updated": "2024-12-13T13:41:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    41,
                    24,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T13:41:24Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    41,
                    24,
                    4,
                    348,
                    0
                ],
                "title": "TACOMORE: Leveraging the Potential of LLMs in Corpus-based Discourse\n  Analysis with Prompt Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TACOMORE: Leveraging the Potential of LLMs in Corpus-based Discourse\n  Analysis with Prompt Engineering"
                },
                "summary": "The capacity of LLMs to carry out automated qualitative analysis has been\nquestioned by corpus linguists, and it has been argued that corpus-based\ndiscourse analysis incorporating LLMs is hindered by issues of unsatisfying\nperformance, hallucination, and irreproducibility. Our proposed method,\nTACOMORE, aims to address these concerns by serving as an effective prompting\nframework in this domain. The framework consists of four principles, i.e.,\nTask, Context, Model and Reproducibility, and specifies five fundamental\nelements of a good prompt, i.e., Role Description, Task Definition, Task\nProcedures, Contextual Information and Output Format. We conduct experiments on\nthree LLMs, i.e., GPT-4o, Gemini-1.5-Pro and Gemini-1.5.Flash, and find that\nTACOMORE helps improve LLM performance in three representative discourse\nanalysis tasks, i.e., the analysis of keywords, collocates and concordances,\nbased on an open corpus of COVID-19 research articles. Our findings show the\nefficacy of the proposed prompting framework TACOMORE in corpus-based discourse\nanalysis in terms of Accuracy, Ethicality, Reasoning, and Reproducibility, and\nprovide novel insights into the application and evaluation of LLMs in automated\nqualitative studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capacity of LLMs to carry out automated qualitative analysis has been\nquestioned by corpus linguists, and it has been argued that corpus-based\ndiscourse analysis incorporating LLMs is hindered by issues of unsatisfying\nperformance, hallucination, and irreproducibility. Our proposed method,\nTACOMORE, aims to address these concerns by serving as an effective prompting\nframework in this domain. The framework consists of four principles, i.e.,\nTask, Context, Model and Reproducibility, and specifies five fundamental\nelements of a good prompt, i.e., Role Description, Task Definition, Task\nProcedures, Contextual Information and Output Format. We conduct experiments on\nthree LLMs, i.e., GPT-4o, Gemini-1.5-Pro and Gemini-1.5.Flash, and find that\nTACOMORE helps improve LLM performance in three representative discourse\nanalysis tasks, i.e., the analysis of keywords, collocates and concordances,\nbased on an open corpus of COVID-19 research articles. Our findings show the\nefficacy of the proposed prompting framework TACOMORE in corpus-based discourse\nanalysis in terms of Accuracy, Ethicality, Reasoning, and Reproducibility, and\nprovide novel insights into the application and evaluation of LLMs in automated\nqualitative studies."
                },
                "authors": [
                    {
                        "name": "Bingru Li"
                    },
                    {
                        "name": "Han Wang"
                    }
                ],
                "author_detail": {
                    "name": "Han Wang"
                },
                "author": "Han Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10138v1",
                "updated": "2024-12-13T13:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    41,
                    18,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T13:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    41,
                    18,
                    4,
                    348,
                    0
                ],
                "title": "ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL"
                },
                "summary": "Despite the significant advancements in Text-to-SQL (Text2SQL) facilitated by\nlarge language models (LLMs), the latest state-of-the-art techniques are still\ntrapped in the in-context learning of closed-source LLMs (e.g., GPT-4), which\nlimits their applicability in open scenarios. To address this challenge, we\npropose a novel RObust mUltitask Tuning and collaboration mEthod (ROUTE) to\nimprove the comprehensive capabilities of open-source LLMs for Text2SQL,\nthereby providing a more practical solution. Our approach begins with\nmulti-task supervised fine-tuning (SFT) using various synthetic training data\nrelated to SQL generation. Unlike existing SFT-based Text2SQL methods, we\nintroduced several additional SFT tasks, including schema linking, noise\ncorrection, and continuation writing. Engaging in a variety of SQL generation\ntasks enhances the model's understanding of SQL syntax and improves its ability\nto generate high-quality SQL queries. Additionally, inspired by the\ncollaborative modes of LLM agents, we introduce a Multitask Collaboration\nPrompting (MCP) strategy. This strategy leverages collaboration across several\nSQL-related tasks to reduce hallucinations during SQL generation, thereby\nmaximizing the potential of enhancing Text2SQL performance through explicit\nmultitask capabilities. Extensive experiments and in-depth analyses have been\nperformed on eight open-source LLMs and five widely-used benchmarks. The\nresults demonstrate that our proposal outperforms the latest Text2SQL methods\nand yields leading performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the significant advancements in Text-to-SQL (Text2SQL) facilitated by\nlarge language models (LLMs), the latest state-of-the-art techniques are still\ntrapped in the in-context learning of closed-source LLMs (e.g., GPT-4), which\nlimits their applicability in open scenarios. To address this challenge, we\npropose a novel RObust mUltitask Tuning and collaboration mEthod (ROUTE) to\nimprove the comprehensive capabilities of open-source LLMs for Text2SQL,\nthereby providing a more practical solution. Our approach begins with\nmulti-task supervised fine-tuning (SFT) using various synthetic training data\nrelated to SQL generation. Unlike existing SFT-based Text2SQL methods, we\nintroduced several additional SFT tasks, including schema linking, noise\ncorrection, and continuation writing. Engaging in a variety of SQL generation\ntasks enhances the model's understanding of SQL syntax and improves its ability\nto generate high-quality SQL queries. Additionally, inspired by the\ncollaborative modes of LLM agents, we introduce a Multitask Collaboration\nPrompting (MCP) strategy. This strategy leverages collaboration across several\nSQL-related tasks to reduce hallucinations during SQL generation, thereby\nmaximizing the potential of enhancing Text2SQL performance through explicit\nmultitask capabilities. Extensive experiments and in-depth analyses have been\nperformed on eight open-source LLMs and five widely-used benchmarks. The\nresults demonstrate that our proposal outperforms the latest Text2SQL methods\nand yields leading performance."
                },
                "authors": [
                    {
                        "name": "Yang Qin"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Zhihang Fu"
                    },
                    {
                        "name": "Ze Chen"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Peng Hu"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02512v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02512v2",
                "updated": "2024-12-13T13:38:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    38,
                    42,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-18T11:25:28Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    25,
                    28,
                    0,
                    323,
                    0
                ],
                "title": "Pre-Deployment Information Sharing: A Zoning Taxonomy for Precursory\n  Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-Deployment Information Sharing: A Zoning Taxonomy for Precursory\n  Capabilities"
                },
                "summary": "High-impact and potentially dangerous capabilities can and should be broken\ndown into early warning shots long before reaching red lines. Each of these\nearly warning shots should correspond to a precursory capability. Each\nprecursory capability sits on a spectrum indicating its proximity to a final\nhigh-impact capability, corresponding to a red line. To meaningfully detect and\ntrack capability progress, we propose a taxonomy of dangerous capability zones\n(a zoning taxonomy) tied to a staggered information exchange framework that\nenables relevant bodies to take action accordingly. In the Frontier AI Safety\nCommitments, signatories commit to sharing more detailed information with\ntrusted actors, including an appointed body, as appropriate (Commitment VII).\nBuilding on our zoning taxonomy, this paper makes four recommendations for\nspecifying information sharing as detailed in Commitment VII. (1) Precursory\ncapabilities should be shared as soon as they become known through internal\nevaluations before deployment. (2) AI Safety Institutes (AISIs) should be the\ntrusted actors appointed to receive and coordinate information on precursory\ncomponents. (3) AISIs should establish adequate information protection\ninfrastructure and guarantee increased information security as precursory\ncapabilities move through the zones and towards red lines, including, if\nnecessary, by classifying the information on precursory capabilities or marking\nit as controlled. (4) High-impact capability progress in one geographical\nregion may translate to risk in other regions and necessitates more\ncomprehensive risk assessment internationally. As such, AISIs should exchange\ninformation on precursory capabilities with other AISIs, relying on the\nexisting frameworks on international classified exchanges and applying lessons\nlearned from other regulated high-risk sectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-impact and potentially dangerous capabilities can and should be broken\ndown into early warning shots long before reaching red lines. Each of these\nearly warning shots should correspond to a precursory capability. Each\nprecursory capability sits on a spectrum indicating its proximity to a final\nhigh-impact capability, corresponding to a red line. To meaningfully detect and\ntrack capability progress, we propose a taxonomy of dangerous capability zones\n(a zoning taxonomy) tied to a staggered information exchange framework that\nenables relevant bodies to take action accordingly. In the Frontier AI Safety\nCommitments, signatories commit to sharing more detailed information with\ntrusted actors, including an appointed body, as appropriate (Commitment VII).\nBuilding on our zoning taxonomy, this paper makes four recommendations for\nspecifying information sharing as detailed in Commitment VII. (1) Precursory\ncapabilities should be shared as soon as they become known through internal\nevaluations before deployment. (2) AI Safety Institutes (AISIs) should be the\ntrusted actors appointed to receive and coordinate information on precursory\ncomponents. (3) AISIs should establish adequate information protection\ninfrastructure and guarantee increased information security as precursory\ncapabilities move through the zones and towards red lines, including, if\nnecessary, by classifying the information on precursory capabilities or marking\nit as controlled. (4) High-impact capability progress in one geographical\nregion may translate to risk in other regions and necessitates more\ncomprehensive risk assessment internationally. As such, AISIs should exchange\ninformation on precursory capabilities with other AISIs, relying on the\nexisting frameworks on international classified exchanges and applying lessons\nlearned from other regulated high-risk sectors."
                },
                "authors": [
                    {
                        "name": "Matteo Pistillo"
                    },
                    {
                        "name": "Charlotte Stix"
                    }
                ],
                "author_detail": {
                    "name": "Charlotte Stix"
                },
                "author": "Charlotte Stix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02512v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02512v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10137v1",
                "updated": "2024-12-13T13:38:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    38,
                    41,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T13:38:41Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    38,
                    41,
                    4,
                    348,
                    0
                ],
                "title": "Constraint-Aware Zero-Shot Vision-Language Navigation in Continuous\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint-Aware Zero-Shot Vision-Language Navigation in Continuous\n  Environments"
                },
                "summary": "We address the task of Vision-Language Navigation in Continuous Environments\n(VLN-CE) under the zero-shot setting. Zero-shot VLN-CE is particularly\nchallenging due to the absence of expert demonstrations for training and\nminimal environment structural prior to guide navigation. To confront these\nchallenges, we propose a Constraint-Aware Navigator (CA-Nav), which reframes\nzero-shot VLN-CE as a sequential, constraint-aware sub-instruction completion\nprocess. CA-Nav continuously translates sub-instructions into navigation plans\nusing two core modules: the Constraint-Aware Sub-instruction Manager (CSM) and\nthe Constraint-Aware Value Mapper (CVM). CSM defines the completion criteria\nfor decomposed sub-instructions as constraints and tracks navigation progress\nby switching sub-instructions in a constraint-aware manner. CVM, guided by\nCSM's constraints, generates a value map on the fly and refines it using\nsuperpixel clustering to improve navigation stability. CA-Nav achieves the\nstate-of-the-art performance on two VLN-CE benchmarks, surpassing the previous\nbest method by 12 percent and 13 percent in Success Rate on the validation\nunseen splits of R2R-CE and RxR-CE, respectively. Moreover, CA-Nav demonstrates\nits effectiveness in real-world robot deployments across various indoor scenes\nand instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the task of Vision-Language Navigation in Continuous Environments\n(VLN-CE) under the zero-shot setting. Zero-shot VLN-CE is particularly\nchallenging due to the absence of expert demonstrations for training and\nminimal environment structural prior to guide navigation. To confront these\nchallenges, we propose a Constraint-Aware Navigator (CA-Nav), which reframes\nzero-shot VLN-CE as a sequential, constraint-aware sub-instruction completion\nprocess. CA-Nav continuously translates sub-instructions into navigation plans\nusing two core modules: the Constraint-Aware Sub-instruction Manager (CSM) and\nthe Constraint-Aware Value Mapper (CVM). CSM defines the completion criteria\nfor decomposed sub-instructions as constraints and tracks navigation progress\nby switching sub-instructions in a constraint-aware manner. CVM, guided by\nCSM's constraints, generates a value map on the fly and refines it using\nsuperpixel clustering to improve navigation stability. CA-Nav achieves the\nstate-of-the-art performance on two VLN-CE benchmarks, surpassing the previous\nbest method by 12 percent and 13 percent in Success Rate on the validation\nunseen splits of R2R-CE and RxR-CE, respectively. Moreover, CA-Nav demonstrates\nits effectiveness in real-world robot deployments across various indoor scenes\nand instructions."
                },
                "authors": [
                    {
                        "name": "Kehan Chen"
                    },
                    {
                        "name": "Dong An"
                    },
                    {
                        "name": "Yan Huang"
                    },
                    {
                        "name": "Rongtao Xu"
                    },
                    {
                        "name": "Yifei Su"
                    },
                    {
                        "name": "Yonggen Ling"
                    },
                    {
                        "name": "Ian Reid"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10136v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10136v1",
                "updated": "2024-12-13T13:32:59Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    32,
                    59,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T13:32:59Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    32,
                    59,
                    4,
                    348,
                    0
                ],
                "title": "Can LLMs Convert Graphs to Text-Attributed Graphs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Convert Graphs to Text-Attributed Graphs?"
                },
                "summary": "Graphs are ubiquitous data structures found in numerous real-world\napplications, such as drug discovery, recommender systems, and social network\nanalysis. Graph neural networks (GNNs) have become a popular tool to learn node\nembeddings through message passing on these structures. However, a significant\nchallenge arises when applying GNNs to multiple graphs with different feature\nspaces, as existing GNN architectures are not designed for cross-graph feature\nalignment. To address this, recent approaches introduce text-attributed graphs,\nwhere each node is associated with a textual description, enabling the use of a\nshared textual encoder to project nodes from different graphs into a unified\nfeature space. While promising, this method relies heavily on the availability\nof text-attributed data, which can be difficult to obtain in practice. To\nbridge this gap, we propose a novel method named Topology-Aware Node\ndescription Synthesis (TANS), which leverages large language models (LLMs) to\nautomatically convert existing graphs into text-attributed graphs. The key idea\nis to integrate topological information with each node's properties, enhancing\nthe LLMs' ability to explain how graph topology influences node semantics. We\nevaluate our TANS on text-rich, text-limited, and text-free graphs,\ndemonstrating that it enables a single GNN to operate across diverse graphs.\nNotably, on text-free graphs, our method significantly outperforms existing\napproaches that manually design node features, showcasing the potential of LLMs\nfor preprocessing graph-structured data, even in the absence of textual\ninformation. The code and data are available at\nhttps://github.com/Zehong-Wang/TANS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are ubiquitous data structures found in numerous real-world\napplications, such as drug discovery, recommender systems, and social network\nanalysis. Graph neural networks (GNNs) have become a popular tool to learn node\nembeddings through message passing on these structures. However, a significant\nchallenge arises when applying GNNs to multiple graphs with different feature\nspaces, as existing GNN architectures are not designed for cross-graph feature\nalignment. To address this, recent approaches introduce text-attributed graphs,\nwhere each node is associated with a textual description, enabling the use of a\nshared textual encoder to project nodes from different graphs into a unified\nfeature space. While promising, this method relies heavily on the availability\nof text-attributed data, which can be difficult to obtain in practice. To\nbridge this gap, we propose a novel method named Topology-Aware Node\ndescription Synthesis (TANS), which leverages large language models (LLMs) to\nautomatically convert existing graphs into text-attributed graphs. The key idea\nis to integrate topological information with each node's properties, enhancing\nthe LLMs' ability to explain how graph topology influences node semantics. We\nevaluate our TANS on text-rich, text-limited, and text-free graphs,\ndemonstrating that it enables a single GNN to operate across diverse graphs.\nNotably, on text-free graphs, our method significantly outperforms existing\napproaches that manually design node features, showcasing the potential of LLMs\nfor preprocessing graph-structured data, even in the absence of textual\ninformation. The code and data are available at\nhttps://github.com/Zehong-Wang/TANS."
                },
                "authors": [
                    {
                        "name": "Zehong Wang"
                    },
                    {
                        "name": "Sidney Liu"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Tianyi Ma"
                    },
                    {
                        "name": "Chuxu Zhang"
                    },
                    {
                        "name": "Yanfang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Yanfang Ye"
                },
                "author": "Yanfang Ye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10136v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10136v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10135v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10135v1",
                "updated": "2024-12-13T13:32:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    32,
                    13,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T13:32:13Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    32,
                    13,
                    4,
                    348,
                    0
                ],
                "title": "ASLoRA: Adaptive Sharing Low-Rank Adaptation Across Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASLoRA: Adaptive Sharing Low-Rank Adaptation Across Layers"
                },
                "summary": "As large language models (LLMs) grow in size, traditional full fine-tuning\nbecomes increasingly impractical due to its high computational and storage\ncosts. Although popular parameter-efficient fine-tuning methods, such as LoRA,\nhave significantly reduced the number of tunable parameters, there is still\nroom for further optimization. In this work, we propose ASLoRA, a cross-layer\nparameter-sharing strategy combining global sharing with partial adaptive\nsharing. Specifically, we share the low-rank matrix A across all layers and\nadaptively merge matrix B during training. This sharing mechanism not only\nmitigates overfitting effectively but also captures inter-layer dependencies,\nsignificantly enhancing the model's representational capability. We conduct\nextensive experiments on various NLP tasks, showing that ASLoRA outperforms\nLoRA while using less than 25% of the parameters, highlighting its flexibility\nand superior parameter efficiency. Furthermore, in-depth analyses of the\nadaptive sharing strategy confirm its significant advantages in enhancing both\nmodel flexibility and task adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) grow in size, traditional full fine-tuning\nbecomes increasingly impractical due to its high computational and storage\ncosts. Although popular parameter-efficient fine-tuning methods, such as LoRA,\nhave significantly reduced the number of tunable parameters, there is still\nroom for further optimization. In this work, we propose ASLoRA, a cross-layer\nparameter-sharing strategy combining global sharing with partial adaptive\nsharing. Specifically, we share the low-rank matrix A across all layers and\nadaptively merge matrix B during training. This sharing mechanism not only\nmitigates overfitting effectively but also captures inter-layer dependencies,\nsignificantly enhancing the model's representational capability. We conduct\nextensive experiments on various NLP tasks, showing that ASLoRA outperforms\nLoRA while using less than 25% of the parameters, highlighting its flexibility\nand superior parameter efficiency. Furthermore, in-depth analyses of the\nadaptive sharing strategy confirm its significant advantages in enhancing both\nmodel flexibility and task adaptability."
                },
                "authors": [
                    {
                        "name": "Junyan Hu"
                    },
                    {
                        "name": "Xue Xiao"
                    },
                    {
                        "name": "Mengqi Zhang"
                    },
                    {
                        "name": "Xiao Chen"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Zhumin Chen"
                    },
                    {
                        "name": "Pengjie Ren"
                    }
                ],
                "author_detail": {
                    "name": "Pengjie Ren"
                },
                "author": "Pengjie Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10135v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10133v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10133v1",
                "updated": "2024-12-13T13:30:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    30,
                    51,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T13:30:51Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    30,
                    51,
                    4,
                    348,
                    0
                ],
                "title": "You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary\n  Projects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Name It, I Run It: An LLM Agent to Execute Tests of Arbitrary\n  Projects"
                },
                "summary": "The ability to execute the test suite of a project is essential in many\nscenarios, e.g., to assess code quality and code coverage, to validate code\nchanges made by developers or automated tools, and to ensure compatibility with\ndependencies. Despite its importance, executing the test suite of a project can\nbe challenging in practice because different projects use different programming\nlanguages, software ecosystems, build systems, testing frameworks, and other\ntools. These challenges make it difficult to create a reliable, universal test\nexecution method that works across different projects. This paper presents\nExecutionAgent, an automated technique that installs arbitrary projects,\nconfigures them to run test cases, and produces project-specific scripts to\nreproduce the setup. Inspired by the way a human developer would address this\ntask, our approach is a large language model-based agent that autonomously\nexecutes commands and interacts with the host system. The agent uses\nmeta-prompting to gather guidelines on the latest technologies related to the\ngiven project, and it iteratively refines its process based on feedback from\nthe previous steps. Our evaluation applies ExecutionAgent to 50 open-source\nprojects that use 14 different programming languages and many different build\nand testing tools. The approach successfully executes the test suites of 33/55\nprojects, while matching the test results of ground truth test suite executions\nwith a deviation of only 7.5\\%. These results improve over the best previously\navailable technique by 6.6x. The costs imposed by the approach are reasonable,\nwith an execution time of 74 minutes and LLM costs of 0.16 dollars, on average\nper project. We envision ExecutionAgent to serve as a valuable tool for\ndevelopers, automated programming tools, and researchers that need to execute\ntests across a wide variety of projects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to execute the test suite of a project is essential in many\nscenarios, e.g., to assess code quality and code coverage, to validate code\nchanges made by developers or automated tools, and to ensure compatibility with\ndependencies. Despite its importance, executing the test suite of a project can\nbe challenging in practice because different projects use different programming\nlanguages, software ecosystems, build systems, testing frameworks, and other\ntools. These challenges make it difficult to create a reliable, universal test\nexecution method that works across different projects. This paper presents\nExecutionAgent, an automated technique that installs arbitrary projects,\nconfigures them to run test cases, and produces project-specific scripts to\nreproduce the setup. Inspired by the way a human developer would address this\ntask, our approach is a large language model-based agent that autonomously\nexecutes commands and interacts with the host system. The agent uses\nmeta-prompting to gather guidelines on the latest technologies related to the\ngiven project, and it iteratively refines its process based on feedback from\nthe previous steps. Our evaluation applies ExecutionAgent to 50 open-source\nprojects that use 14 different programming languages and many different build\nand testing tools. The approach successfully executes the test suites of 33/55\nprojects, while matching the test results of ground truth test suite executions\nwith a deviation of only 7.5\\%. These results improve over the best previously\navailable technique by 6.6x. The costs imposed by the approach are reasonable,\nwith an execution time of 74 minutes and LLM costs of 0.16 dollars, on average\nper project. We envision ExecutionAgent to serve as a valuable tool for\ndevelopers, automated programming tools, and researchers that need to execute\ntests across a wide variety of projects."
                },
                "authors": [
                    {
                        "name": "Islem Bouzenia"
                    },
                    {
                        "name": "Michael Pradel"
                    }
                ],
                "author_detail": {
                    "name": "Michael Pradel"
                },
                "author": "Michael Pradel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10133v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10133v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10117v1",
                "updated": "2024-12-13T12:59:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    12,
                    59,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T12:59:39Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    12,
                    59,
                    39,
                    4,
                    348,
                    0
                ],
                "title": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language\n  Models"
                },
                "summary": "In our previous work, we introduced CosyVoice, a multilingual speech\nsynthesis model based on supervised discrete speech tokens. By employing\nprogressive semantic decoding with two popular generative models, language\nmodels (LMs) and Flow Matching, CosyVoice demonstrated high prosody\nnaturalness, content consistency, and speaker similarity in speech in-context\nlearning. Recently, significant progress has been made in multi-modal large\nlanguage models (LLMs), where the response latency and real-time factor of\nspeech synthesis play a crucial role in the interactive experience. Therefore,\nin this report, we present an improved streaming speech synthesis model,\nCosyVoice 2, which incorporates comprehensive and systematic optimizations.\nSpecifically, we introduce finite-scalar quantization to improve the codebook\nutilization of speech tokens. For the text-speech LM, we streamline the model\narchitecture to allow direct use of a pre-trained LLM as the backbone. In\naddition, we develop a chunk-aware causal flow matching model to support\nvarious synthesis scenarios, enabling both streaming and non-streaming\nsynthesis within a single model. By training on a large-scale multilingual\ndataset, CosyVoice 2 achieves human-parity naturalness, minimal response\nlatency, and virtually lossless synthesis quality in the streaming mode. We\ninvite readers to listen to the demos at\nhttps://funaudiollm.github.io/cosyvoice2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our previous work, we introduced CosyVoice, a multilingual speech\nsynthesis model based on supervised discrete speech tokens. By employing\nprogressive semantic decoding with two popular generative models, language\nmodels (LMs) and Flow Matching, CosyVoice demonstrated high prosody\nnaturalness, content consistency, and speaker similarity in speech in-context\nlearning. Recently, significant progress has been made in multi-modal large\nlanguage models (LLMs), where the response latency and real-time factor of\nspeech synthesis play a crucial role in the interactive experience. Therefore,\nin this report, we present an improved streaming speech synthesis model,\nCosyVoice 2, which incorporates comprehensive and systematic optimizations.\nSpecifically, we introduce finite-scalar quantization to improve the codebook\nutilization of speech tokens. For the text-speech LM, we streamline the model\narchitecture to allow direct use of a pre-trained LLM as the backbone. In\naddition, we develop a chunk-aware causal flow matching model to support\nvarious synthesis scenarios, enabling both streaming and non-streaming\nsynthesis within a single model. By training on a large-scale multilingual\ndataset, CosyVoice 2 achieves human-parity naturalness, minimal response\nlatency, and virtually lossless synthesis quality in the streaming mode. We\ninvite readers to listen to the demos at\nhttps://funaudiollm.github.io/cosyvoice2."
                },
                "authors": [
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Xian Shi"
                    },
                    {
                        "name": "Xiang Lv"
                    },
                    {
                        "name": "Tianyu Zhao"
                    },
                    {
                        "name": "Zhifu Gao"
                    },
                    {
                        "name": "Yexin Yang"
                    },
                    {
                        "name": "Changfeng Gao"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Huadai Liu"
                    },
                    {
                        "name": "Zhengyan Sheng"
                    },
                    {
                        "name": "Yue Gu"
                    },
                    {
                        "name": "Chong Deng"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Zhijie Yan"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "Tech report, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10107v1",
                "updated": "2024-12-13T12:48:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    12,
                    48,
                    15,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T12:48:15Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    12,
                    48,
                    15,
                    4,
                    348,
                    0
                ],
                "title": "NetOrchLLM: Mastering Wireless Network Orchestration with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetOrchLLM: Mastering Wireless Network Orchestration with Large Language\n  Models"
                },
                "summary": "The transition to 6G networks promises unprecedented advancements in wireless\ncommunication, with increased data rates, ultra-low latency, and enhanced\ncapacity. However, the complexity of managing and optimizing these\nnext-generation networks presents significant challenges. The advent of large\nlanguage models (LLMs) has revolutionized various domains by leveraging their\nsophisticated natural language understanding capabilities. However, the\npractical application of LLMs in wireless network orchestration and management\nremains largely unexplored. Existing literature predominantly offers visionary\nperspectives without concrete implementations, leaving a significant gap in the\nfield. To address this gap, this paper presents NETORCHLLM, a wireless NETwork\nORCHestrator LLM framework that uses LLMs to seamlessly orchestrate diverse\nwireless-specific models from wireless communication communities using their\nlanguage understanding and generation capabilities. A comprehensive framework\nis introduced, demonstrating the practical viability of our approach and\nshowcasing how LLMs can be effectively harnessed to optimize dense network\noperations, manage dynamic environments, and improve overall network\nperformance. NETORCHLLM bridges the theoretical aspirations of prior research\nwith practical, actionable solutions, paving the way for future advancements in\nintegrating generative AI technologies within the wireless communications\nsector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition to 6G networks promises unprecedented advancements in wireless\ncommunication, with increased data rates, ultra-low latency, and enhanced\ncapacity. However, the complexity of managing and optimizing these\nnext-generation networks presents significant challenges. The advent of large\nlanguage models (LLMs) has revolutionized various domains by leveraging their\nsophisticated natural language understanding capabilities. However, the\npractical application of LLMs in wireless network orchestration and management\nremains largely unexplored. Existing literature predominantly offers visionary\nperspectives without concrete implementations, leaving a significant gap in the\nfield. To address this gap, this paper presents NETORCHLLM, a wireless NETwork\nORCHestrator LLM framework that uses LLMs to seamlessly orchestrate diverse\nwireless-specific models from wireless communication communities using their\nlanguage understanding and generation capabilities. A comprehensive framework\nis introduced, demonstrating the practical viability of our approach and\nshowcasing how LLMs can be effectively harnessed to optimize dense network\noperations, manage dynamic environments, and improve overall network\nperformance. NETORCHLLM bridges the theoretical aspirations of prior research\nwith practical, actionable solutions, paving the way for future advancements in\nintegrating generative AI technologies within the wireless communications\nsector."
                },
                "authors": [
                    {
                        "name": "Asmaa Abdallah"
                    },
                    {
                        "name": "Abdullatif Albaseer"
                    },
                    {
                        "name": "Abdulkadir Celik"
                    },
                    {
                        "name": "Mohamed Abdallah"
                    },
                    {
                        "name": "Ahmed M. Eltawil"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed M. Eltawil"
                },
                "author": "Ahmed M. Eltawil",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10106v1",
                "updated": "2024-12-13T12:47:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    12,
                    47,
                    30,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T12:47:30Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    12,
                    47,
                    30,
                    4,
                    348,
                    0
                ],
                "title": "A Cascaded Dilated Convolution Approach for Mpox Lesion Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Cascaded Dilated Convolution Approach for Mpox Lesion Classification"
                },
                "summary": "The global outbreak of Mpox virus, classified as a Public Health Emergency of\nInternational Concern by WHO, presents significant diagnostic challenges due to\nits visual similarity to other skin lesion diseases. Current clinical detection\ntechniques face limitations in accuracy and efficiency, necessitating improved\nautomated diagnostic solutions. This study introduces a novel Cascaded Atrous\nGroup Attention (CAGA) module, specifically designed to enhance multi-scale\nfeature representation while optimizing computational efficiency. By\nintegrating CAGA with EfficientViT-L1 as the backbone architecture, our\napproach achieves state-of-the-art performance with a score of 0.98% on the\nMCSI dataset, while reducing model parameters by 37.5% compared to the original\nEfficientViT-L1. This reduction in computational complexity maintains\ndiagnostic accuracy while enabling broader deployment across\nresource-constrained healthcare settings. Extensive validation across two other\nbenchmark datasets, including MSID and MSLD, demonstrate the model's\nrobustness, consistently outperforming existing approaches. Our findings\nsuggest that CAGA's efficient feature extraction mechanism could be adapted for\nother medical imaging tasks requiring fine-grained visual discrimination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The global outbreak of Mpox virus, classified as a Public Health Emergency of\nInternational Concern by WHO, presents significant diagnostic challenges due to\nits visual similarity to other skin lesion diseases. Current clinical detection\ntechniques face limitations in accuracy and efficiency, necessitating improved\nautomated diagnostic solutions. This study introduces a novel Cascaded Atrous\nGroup Attention (CAGA) module, specifically designed to enhance multi-scale\nfeature representation while optimizing computational efficiency. By\nintegrating CAGA with EfficientViT-L1 as the backbone architecture, our\napproach achieves state-of-the-art performance with a score of 0.98% on the\nMCSI dataset, while reducing model parameters by 37.5% compared to the original\nEfficientViT-L1. This reduction in computational complexity maintains\ndiagnostic accuracy while enabling broader deployment across\nresource-constrained healthcare settings. Extensive validation across two other\nbenchmark datasets, including MSID and MSLD, demonstrate the model's\nrobustness, consistently outperforming existing approaches. Our findings\nsuggest that CAGA's efficient feature extraction mechanism could be adapted for\nother medical imaging tasks requiring fine-grained visual discrimination."
                },
                "authors": [
                    {
                        "name": "Ayush Deshmukh"
                    }
                ],
                "author_detail": {
                    "name": "Ayush Deshmukh"
                },
                "author": "Ayush Deshmukh",
                "arxiv_comment": "(7 pages, 2 figures, 5 tables)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07646v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07646v3",
                "updated": "2024-12-13T12:35:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    12,
                    35,
                    21,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-10T16:32:19Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    32,
                    19,
                    1,
                    345,
                    0
                ],
                "title": "Searching for Structure: Investigating Emergent Communication with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Searching for Structure: Investigating Emergent Communication with Large\n  Language Models"
                },
                "summary": "Human languages have evolved to be structured through repeated language\nlearning and use. These processes introduce biases that operate during language\nacquisition and shape linguistic systems toward communicative efficiency. In\nthis paper, we investigate whether the same happens if artificial languages are\noptimised for implicit biases of Large Language Models (LLMs). To this end, we\nsimulate a classical referential game in which LLMs learn and use artificial\nlanguages. Our results show that initially unstructured holistic languages are\nindeed shaped to have some structural properties that allow two LLM agents to\ncommunicate successfully. Similar to observations in human experiments,\ngenerational transmission increases the learnability of languages, but can at\nthe same time result in non-humanlike degenerate vocabularies. Taken together,\nthis work extends experimental findings, shows that LLMs can be used as tools\nin simulations of language evolution, and opens possibilities for future\nhuman-machine experiments in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human languages have evolved to be structured through repeated language\nlearning and use. These processes introduce biases that operate during language\nacquisition and shape linguistic systems toward communicative efficiency. In\nthis paper, we investigate whether the same happens if artificial languages are\noptimised for implicit biases of Large Language Models (LLMs). To this end, we\nsimulate a classical referential game in which LLMs learn and use artificial\nlanguages. Our results show that initially unstructured holistic languages are\nindeed shaped to have some structural properties that allow two LLM agents to\ncommunicate successfully. Similar to observations in human experiments,\ngenerational transmission increases the learnability of languages, but can at\nthe same time result in non-humanlike degenerate vocabularies. Taken together,\nthis work extends experimental findings, shows that LLMs can be used as tools\nin simulations of language evolution, and opens possibilities for future\nhuman-machine experiments in this field."
                },
                "authors": [
                    {
                        "name": "Tom Kouwenhoven"
                    },
                    {
                        "name": "Max Peeperkorn"
                    },
                    {
                        "name": "Tessa Verhoef"
                    }
                ],
                "author_detail": {
                    "name": "Tessa Verhoef"
                },
                "author": "Tessa Verhoef",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07646v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07646v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10080v1",
                "updated": "2024-12-13T12:15:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    12,
                    15,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T12:15:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    12,
                    15,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "Africanus IV. The Stimela2 framework: scalable and reproducible\n  workflows, from local to cloud compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Africanus IV. The Stimela2 framework: scalable and reproducible\n  workflows, from local to cloud compute"
                },
                "summary": "Stimela2 is a new-generation framework for developing data reduction\nworkflows. It is designed for radio astronomy data but can be adapted for other\ndata processing applications. Stimela2 aims at the middle ground between ease\nof development, human readability, and enabling robust, scalable and\nreproducible workflows. It represents workflows by linear, concise and\nintuitive YAML-format \"recipes\". Atomic data reduction tasks (binary\nexecutables, Python functions and code, and CASA tasks) are described by\nYAML-format \"cab definitions\" detailing each task's \"schema\" (inputs and\noutputs). Stimela2 provides a rich syntax for chaining tasks together, and\nencourages a high degree of modularity: recipes may be nested into other\nrecipes, and configuration is cleanly separated from recipe logic. Tasks can be\nexecuted natively or in isolated environments using containerization\ntechnologies such as Apptainer. The container images are open-source and\nmaintained through a companion package called cult-cargo. This enables the\ndevelopment of system-agnostic and fully reproducible workflows. Stimela2\nfacilitates the deployment of scalable, distributed workflows by interfacing\nwith the Slurm scheduler and the Kubernetes API. The latter allows workflows to\nbe readily deployed in the cloud. Previous papers in this series used Stimela2\nas the underlying technology to run workflows on the AWS cloud.\n  This paper presents an overview of Stimela2's design, architecture and use in\nthe radio astronomy context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stimela2 is a new-generation framework for developing data reduction\nworkflows. It is designed for radio astronomy data but can be adapted for other\ndata processing applications. Stimela2 aims at the middle ground between ease\nof development, human readability, and enabling robust, scalable and\nreproducible workflows. It represents workflows by linear, concise and\nintuitive YAML-format \"recipes\". Atomic data reduction tasks (binary\nexecutables, Python functions and code, and CASA tasks) are described by\nYAML-format \"cab definitions\" detailing each task's \"schema\" (inputs and\noutputs). Stimela2 provides a rich syntax for chaining tasks together, and\nencourages a high degree of modularity: recipes may be nested into other\nrecipes, and configuration is cleanly separated from recipe logic. Tasks can be\nexecuted natively or in isolated environments using containerization\ntechnologies such as Apptainer. The container images are open-source and\nmaintained through a companion package called cult-cargo. This enables the\ndevelopment of system-agnostic and fully reproducible workflows. Stimela2\nfacilitates the deployment of scalable, distributed workflows by interfacing\nwith the Slurm scheduler and the Kubernetes API. The latter allows workflows to\nbe readily deployed in the cloud. Previous papers in this series used Stimela2\nas the underlying technology to run workflows on the AWS cloud.\n  This paper presents an overview of Stimela2's design, architecture and use in\nthe radio astronomy context."
                },
                "authors": [
                    {
                        "name": "Oleg M. Smirnov"
                    },
                    {
                        "name": "Sphesihle Makhathini"
                    },
                    {
                        "name": "Jonathan S. Kenyon"
                    },
                    {
                        "name": "Hertzog L. Bester"
                    },
                    {
                        "name": "Simon J. Perkins"
                    },
                    {
                        "name": "Athanaseus J. T. Ramaila"
                    },
                    {
                        "name": "Benjamin V. Hugo"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin V. Hugo"
                },
                "author": "Benjamin V. Hugo",
                "arxiv_comment": "26 pages, submitted to Astronomy & Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10064v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10064v1",
                "updated": "2024-12-13T11:50:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    50,
                    51,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T11:50:51Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    50,
                    51,
                    4,
                    348,
                    0
                ],
                "title": "Text2Cypher: Bridging Natural Language and Graph Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text2Cypher: Bridging Natural Language and Graph Databases"
                },
                "summary": "Knowledge graphs use nodes, relationships, and properties to represent\narbitrarily complex data. When stored in a graph database, the Cypher query\nlanguage enables efficient modeling and querying of knowledge graphs. However,\nusing Cypher requires specialized knowledge, which can present a challenge for\nnon-expert users. Our work Text2Cypher aims to bridge this gap by translating\nnatural language queries into Cypher query language and extending the utility\nof knowledge graphs to non-technical expert users.\n  While large language models (LLMs) can be used for this purpose, they often\nstruggle to capture complex nuances, resulting in incomplete or incorrect\noutputs. Fine-tuning LLMs on domain-specific datasets has proven to be a more\npromising approach, but the limited availability of high-quality, publicly\navailable Text2Cypher datasets makes this challenging. In this work, we show\nhow we combined, cleaned and organized several publicly available datasets into\na total of 44,387 instances, enabling effective fine-tuning and evaluation.\nModels fine-tuned on this dataset showed significant performance gains, with\nimprovements in Google-BLEU and Exact Match scores over baseline models,\nhighlighting the importance of high-quality datasets and fine-tuning in\nimproving Text2Cypher performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graphs use nodes, relationships, and properties to represent\narbitrarily complex data. When stored in a graph database, the Cypher query\nlanguage enables efficient modeling and querying of knowledge graphs. However,\nusing Cypher requires specialized knowledge, which can present a challenge for\nnon-expert users. Our work Text2Cypher aims to bridge this gap by translating\nnatural language queries into Cypher query language and extending the utility\nof knowledge graphs to non-technical expert users.\n  While large language models (LLMs) can be used for this purpose, they often\nstruggle to capture complex nuances, resulting in incomplete or incorrect\noutputs. Fine-tuning LLMs on domain-specific datasets has proven to be a more\npromising approach, but the limited availability of high-quality, publicly\navailable Text2Cypher datasets makes this challenging. In this work, we show\nhow we combined, cleaned and organized several publicly available datasets into\na total of 44,387 instances, enabling effective fine-tuning and evaluation.\nModels fine-tuned on this dataset showed significant performance gains, with\nimprovements in Google-BLEU and Exact Match scores over baseline models,\nhighlighting the importance of high-quality datasets and fine-tuning in\nimproving Text2Cypher performance."
                },
                "authors": [
                    {
                        "name": "Makbule Gulcin Ozsoy"
                    },
                    {
                        "name": "Leila Messallem"
                    },
                    {
                        "name": "Jon Besga"
                    },
                    {
                        "name": "Gianandrea Minneci"
                    }
                ],
                "author_detail": {
                    "name": "Gianandrea Minneci"
                },
                "author": "Gianandrea Minneci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10064v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10064v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16667v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16667v3",
                "updated": "2024-12-13T11:50:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    50,
                    50,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-25T06:54:29Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    6,
                    54,
                    29,
                    2,
                    269,
                    0
                ],
                "title": "A Character-Centric Creative Story Generation via Imagination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Character-Centric Creative Story Generation via Imagination"
                },
                "summary": "Creative story generation has long been a goal of NLP research. While\nexisting methodologies have aimed to generate long and coherent stories, they\nfall significantly short of human capabilities in terms of diversity and\ncharacter depth. To address this, we introduce a novel story generation\nframework called CCI (Character-centric Creative story generation via\nImagination). CCI features two modules for creative story generation: IG\n(Image-Guided Imagination) and MW (Multi-Writer model). In the IG module, we\nutilize a text-to-image model to create visual representations of key story\nelements, such as characters, backgrounds, and main plots, in a more novel and\nconcrete manner than text-only approaches. The MW module uses these story\nelements to generate multiple persona-description candidates and selects the\nbest one to insert into the story, thereby enhancing the richness and depth of\nthe narrative. We compared the stories generated by CCI and baseline models\nthrough statistical analysis, as well as human and LLM evaluations. The results\nshowed that the IG and MW modules significantly improve various aspects of the\nstories' creativity. Furthermore, our framework enables interactive multi-modal\nstory generation with users, opening up new possibilities for human-LLM\nintegration in cultural development. Project page : https://www.2024cci.p-e.kr/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creative story generation has long been a goal of NLP research. While\nexisting methodologies have aimed to generate long and coherent stories, they\nfall significantly short of human capabilities in terms of diversity and\ncharacter depth. To address this, we introduce a novel story generation\nframework called CCI (Character-centric Creative story generation via\nImagination). CCI features two modules for creative story generation: IG\n(Image-Guided Imagination) and MW (Multi-Writer model). In the IG module, we\nutilize a text-to-image model to create visual representations of key story\nelements, such as characters, backgrounds, and main plots, in a more novel and\nconcrete manner than text-only approaches. The MW module uses these story\nelements to generate multiple persona-description candidates and selects the\nbest one to insert into the story, thereby enhancing the richness and depth of\nthe narrative. We compared the stories generated by CCI and baseline models\nthrough statistical analysis, as well as human and LLM evaluations. The results\nshowed that the IG and MW modules significantly improve various aspects of the\nstories' creativity. Furthermore, our framework enables interactive multi-modal\nstory generation with users, opening up new possibilities for human-LLM\nintegration in cultural development. Project page : https://www.2024cci.p-e.kr/"
                },
                "authors": [
                    {
                        "name": "Kyeongman Park"
                    },
                    {
                        "name": "Minbeom Kim"
                    },
                    {
                        "name": "Kyomin Jung"
                    }
                ],
                "author_detail": {
                    "name": "Kyomin Jung"
                },
                "author": "Kyomin Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16667v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16667v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10063v1",
                "updated": "2024-12-13T11:49:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    49,
                    19,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T11:49:19Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    49,
                    19,
                    4,
                    348,
                    0
                ],
                "title": "Unveiling the Energy Vampires: A Methodology for Debugging Software\n  Energy Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling the Energy Vampires: A Methodology for Debugging Software\n  Energy Consumption"
                },
                "summary": "Energy consumption in software systems is becoming increasingly important,\nespecially in large-scale deployments. However, debugging energy-related issues\nremains challenging due to the lack of specialized tools. This paper presents\nan energy debugging methodology for identifying and isolating energy\nconsumption hotspots in software systems. We demonstrate the methodology's\neffectiveness through a case study of Redis, a popular in-memory database. Our\nanalysis reveals significant energy consumption differences between Alpine and\nUbuntu distributions, with Alpine consuming up to 20.2% more power in certain\noperations. We trace this difference to the implementation of the memcpy\nfunction in different C standard libraries (musl vs. glibc). By isolating and\nbenchmarking memcpy, we confirm it as the primary cause of the energy\ndiscrepancy. Our findings highlight the importance of considering energy\nefficiency in software dependencies and demonstrate the capability to assist\ndevelopers in identifying and addressing energy-related issues. This work\ncontributes to the growing field of sustainable software engineering by\nproviding a systematic approach to energy debugging and using it to unveil\nunexpected energy behaviors in Alpine.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy consumption in software systems is becoming increasingly important,\nespecially in large-scale deployments. However, debugging energy-related issues\nremains challenging due to the lack of specialized tools. This paper presents\nan energy debugging methodology for identifying and isolating energy\nconsumption hotspots in software systems. We demonstrate the methodology's\neffectiveness through a case study of Redis, a popular in-memory database. Our\nanalysis reveals significant energy consumption differences between Alpine and\nUbuntu distributions, with Alpine consuming up to 20.2% more power in certain\noperations. We trace this difference to the implementation of the memcpy\nfunction in different C standard libraries (musl vs. glibc). By isolating and\nbenchmarking memcpy, we confirm it as the primary cause of the energy\ndiscrepancy. Our findings highlight the importance of considering energy\nefficiency in software dependencies and demonstrate the capability to assist\ndevelopers in identifying and addressing energy-related issues. This work\ncontributes to the growing field of sustainable software engineering by\nproviding a systematic approach to energy debugging and using it to unveil\nunexpected energy behaviors in Alpine."
                },
                "authors": [
                    {
                        "name": "Enrique Barba Roque"
                    },
                    {
                        "name": "Luis Cruz"
                    },
                    {
                        "name": "Thomas Durieux"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Durieux"
                },
                "author": "Thomas Durieux",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10056v1",
                "updated": "2024-12-13T11:38:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    38,
                    10,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T11:38:10Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    38,
                    10,
                    4,
                    348,
                    0
                ],
                "title": "GAOKAO-Eval: Does high scores truly reflect strong capabilities in LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GAOKAO-Eval: Does high scores truly reflect strong capabilities in LLMs?"
                },
                "summary": "Large Language Models (LLMs) are commonly evaluated using human-crafted\nbenchmarks, under the premise that higher scores implicitly reflect stronger\nhuman-like performance. However, there is growing concern that LLMs may ``game\"\nthese benchmarks due to data leakage, achieving high scores while struggling\nwith tasks simple for humans. To substantively address the problem, we create\nGAOKAO-Eval, a comprehensive benchmark based on China's National College\nEntrance Examination (Gaokao), and conduct ``closed-book\" evaluations for\nrepresentative models released prior to Gaokao. Contrary to prevailing\nconsensus, even after addressing data leakage and comprehensiveness,\nGAOKAO-Eval reveals that high scores still fail to truly reflect human-aligned\ncapabilities. To better understand this mismatch, We introduce the Rasch model\nfrom cognitive psychology to analyze LLM scoring patterns and identify two key\ndiscrepancies: 1) anomalous consistent performance across various question\ndifficulties, and 2) high variance in performance on questions of similar\ndifficulty. In addition, We identified inconsistent grading of LLM-generated\nanswers among teachers and recurring mistake patterns. we find that the\nphenomenons are well-grounded in the motivations behind OpenAI o1, and o1's\nreasoning-as-difficulties can mitigate the mismatch. These results show that\nGAOKAO-Eval can reveal limitations in LLM capabilities not captured by current\nbenchmarks and highlight the need for more LLM-aligned difficulty analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are commonly evaluated using human-crafted\nbenchmarks, under the premise that higher scores implicitly reflect stronger\nhuman-like performance. However, there is growing concern that LLMs may ``game\"\nthese benchmarks due to data leakage, achieving high scores while struggling\nwith tasks simple for humans. To substantively address the problem, we create\nGAOKAO-Eval, a comprehensive benchmark based on China's National College\nEntrance Examination (Gaokao), and conduct ``closed-book\" evaluations for\nrepresentative models released prior to Gaokao. Contrary to prevailing\nconsensus, even after addressing data leakage and comprehensiveness,\nGAOKAO-Eval reveals that high scores still fail to truly reflect human-aligned\ncapabilities. To better understand this mismatch, We introduce the Rasch model\nfrom cognitive psychology to analyze LLM scoring patterns and identify two key\ndiscrepancies: 1) anomalous consistent performance across various question\ndifficulties, and 2) high variance in performance on questions of similar\ndifficulty. In addition, We identified inconsistent grading of LLM-generated\nanswers among teachers and recurring mistake patterns. we find that the\nphenomenons are well-grounded in the motivations behind OpenAI o1, and o1's\nreasoning-as-difficulties can mitigate the mismatch. These results show that\nGAOKAO-Eval can reveal limitations in LLM capabilities not captured by current\nbenchmarks and highlight the need for more LLM-aligned difficulty analysis."
                },
                "authors": [
                    {
                        "name": "Zhikai Lei"
                    },
                    {
                        "name": "Tianyi Liang"
                    },
                    {
                        "name": "Hanglei Hu"
                    },
                    {
                        "name": "Jin Zhang"
                    },
                    {
                        "name": "Yunhua Zhou"
                    },
                    {
                        "name": "Yunfan Shao"
                    },
                    {
                        "name": "Linyang Li"
                    },
                    {
                        "name": "Chenchui Li"
                    },
                    {
                        "name": "Changbo Wang"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Qipeng Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qipeng Guo"
                },
                "author": "Qipeng Guo",
                "arxiv_comment": "10 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10047v1",
                "updated": "2024-12-13T11:19:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    19,
                    56,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T11:19:56Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    19,
                    56,
                    4,
                    348,
                    0
                ],
                "title": "Large Action Models: From Inception to Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Action Models: From Inception to Implementation"
                },
                "summary": "As AI continues to advance, there is a growing demand for systems that go\nbeyond language-based assistance and move toward intelligent agents capable of\nperforming real-world actions. This evolution requires the transition from\ntraditional Large Language Models (LLMs), which excel at generating textual\nresponses, to Large Action Models (LAMs), designed for action generation and\nexecution within dynamic environments. Enabled by agent systems, LAMs hold the\npotential to transform AI from passive language understanding to active task\ncompletion, marking a significant milestone in the progression toward\nartificial general intelligence.\n  In this paper, we present a comprehensive framework for developing LAMs,\noffering a systematic approach to their creation, from inception to deployment.\nWe begin with an overview of LAMs, highlighting their unique characteristics\nand delineating their differences from LLMs. Using a Windows OS-based agent as\na case study, we provide a detailed, step-by-step guide on the key stages of\nLAM development, including data collection, model training, environment\nintegration, grounding, and evaluation. This generalizable workflow can serve\nas a blueprint for creating functional LAMs in various application domains. We\nconclude by identifying the current limitations of LAMs and discussing\ndirections for future research and industrial deployment, emphasizing the\nchallenges and opportunities that lie ahead in realizing the full potential of\nLAMs in real-world applications.\n  The code for the data collection process utilized in this paper is publicly\navailable at: https://github.com/microsoft/UFO/tree/main/dataflow, and\ncomprehensive documentation can be found at\nhttps://microsoft.github.io/UFO/dataflow/overview/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI continues to advance, there is a growing demand for systems that go\nbeyond language-based assistance and move toward intelligent agents capable of\nperforming real-world actions. This evolution requires the transition from\ntraditional Large Language Models (LLMs), which excel at generating textual\nresponses, to Large Action Models (LAMs), designed for action generation and\nexecution within dynamic environments. Enabled by agent systems, LAMs hold the\npotential to transform AI from passive language understanding to active task\ncompletion, marking a significant milestone in the progression toward\nartificial general intelligence.\n  In this paper, we present a comprehensive framework for developing LAMs,\noffering a systematic approach to their creation, from inception to deployment.\nWe begin with an overview of LAMs, highlighting their unique characteristics\nand delineating their differences from LLMs. Using a Windows OS-based agent as\na case study, we provide a detailed, step-by-step guide on the key stages of\nLAM development, including data collection, model training, environment\nintegration, grounding, and evaluation. This generalizable workflow can serve\nas a blueprint for creating functional LAMs in various application domains. We\nconclude by identifying the current limitations of LAMs and discussing\ndirections for future research and industrial deployment, emphasizing the\nchallenges and opportunities that lie ahead in realizing the full potential of\nLAMs in real-world applications.\n  The code for the data collection process utilized in this paper is publicly\navailable at: https://github.com/microsoft/UFO/tree/main/dataflow, and\ncomprehensive documentation can be found at\nhttps://microsoft.github.io/UFO/dataflow/overview/."
                },
                "authors": [
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Junting Lu"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Bo Qiao"
                    },
                    {
                        "name": "Ray Huang"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Qisheng Su"
                    },
                    {
                        "name": "Jiayi Ye"
                    },
                    {
                        "name": "Yudi Zhang"
                    },
                    {
                        "name": "Jian-Guang Lou"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "25pages,12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10040v1",
                "updated": "2024-12-13T11:00:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    0,
                    57,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T11:00:57Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    0,
                    57,
                    4,
                    348,
                    0
                ],
                "title": "RemDet: Rethinking Efficient Model Design for UAV Object Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RemDet: Rethinking Efficient Model Design for UAV Object Detection"
                },
                "summary": "Object detection in Unmanned Aerial Vehicle (UAV) images has emerged as a\nfocal area of research, which presents two significant challenges: i) objects\nare typically small and dense within vast images; ii) computational resource\nconstraints render most models unsuitable for real-time deployment. Current\nreal-time object detectors are not optimized for UAV images, and complex\nmethods designed for small object detection often lack real-time capabilities.\nTo address these challenges, we propose a novel detector, RemDet (Reparameter\nefficient multiplication Detector). Our contributions are as follows: 1)\nRethinking the challenges of existing detectors for small and dense UAV images,\nand proposing information loss as a design guideline for efficient models. 2)\nWe introduce the ChannelC2f module to enhance small object detection\nperformance, demonstrating that high-dimensional representations can\neffectively mitigate information loss. 3) We design the GatedFFN module to\nprovide not only strong performance but also low latency, effectively\naddressing the challenges of real-time detection. Our research reveals that\nGatedFFN, through the use of multiplication, is more cost-effective than\nfeed-forward networks for high-dimensional representation. 4) We propose the\nCED module, which combines the advantages of ViT and CNN downsampling to\neffectively reduce information loss. It specifically enhances context\ninformation for small and dense objects. Extensive experiments on large UAV\ndatasets, Visdrone and UAVDT, validate the real-time efficiency and superior\nperformance of our methods. On the challenging UAV dataset VisDrone, our\nmethods not only provided state-of-the-art results, improving detection by more\nthan 3.4%, but also achieve 110 FPS on a single 4090.Codes are available at\n(this URL)(https://github.com/HZAI-ZJNU/RemDet).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object detection in Unmanned Aerial Vehicle (UAV) images has emerged as a\nfocal area of research, which presents two significant challenges: i) objects\nare typically small and dense within vast images; ii) computational resource\nconstraints render most models unsuitable for real-time deployment. Current\nreal-time object detectors are not optimized for UAV images, and complex\nmethods designed for small object detection often lack real-time capabilities.\nTo address these challenges, we propose a novel detector, RemDet (Reparameter\nefficient multiplication Detector). Our contributions are as follows: 1)\nRethinking the challenges of existing detectors for small and dense UAV images,\nand proposing information loss as a design guideline for efficient models. 2)\nWe introduce the ChannelC2f module to enhance small object detection\nperformance, demonstrating that high-dimensional representations can\neffectively mitigate information loss. 3) We design the GatedFFN module to\nprovide not only strong performance but also low latency, effectively\naddressing the challenges of real-time detection. Our research reveals that\nGatedFFN, through the use of multiplication, is more cost-effective than\nfeed-forward networks for high-dimensional representation. 4) We propose the\nCED module, which combines the advantages of ViT and CNN downsampling to\neffectively reduce information loss. It specifically enhances context\ninformation for small and dense objects. Extensive experiments on large UAV\ndatasets, Visdrone and UAVDT, validate the real-time efficiency and superior\nperformance of our methods. On the challenging UAV dataset VisDrone, our\nmethods not only provided state-of-the-art results, improving detection by more\nthan 3.4%, but also achieve 110 FPS on a single 4090.Codes are available at\n(this URL)(https://github.com/HZAI-ZJNU/RemDet)."
                },
                "authors": [
                    {
                        "name": "Chen Li"
                    },
                    {
                        "name": "Rui Zhao"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Huiying Xu"
                    },
                    {
                        "name": "Xinzhong Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Xinzhong Zhu"
                },
                "author": "Xinzhong Zhu",
                "arxiv_comment": "Accepted to AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.03536v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.03536v3",
                "updated": "2024-12-13T10:55:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    10,
                    55,
                    37,
                    4,
                    348,
                    0
                ],
                "published": "2024-07-03T22:45:36Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    22,
                    45,
                    36,
                    2,
                    185,
                    0
                ],
                "title": "Social Bias in Large Language Models For Bangla: An Empirical Study on\n  Gender and Religious Bias",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social Bias in Large Language Models For Bangla: An Empirical Study on\n  Gender and Religious Bias"
                },
                "summary": "The rapid growth of Large Language Models (LLMs) has put forward the study of\nbiases as a crucial field. It is important to assess the influence of different\ntypes of biases embedded in LLMs to ensure fair use in sensitive fields.\nAlthough there have been extensive works on bias assessment in English, such\nefforts are rare and scarce for a major language like Bangla. In this work, we\nexamine two types of social biases in LLM generated outputs for Bangla\nlanguage. Our main contributions in this work are: (1) bias studies on two\ndifferent social biases for Bangla, (2) a curated dataset for bias measurement\nbenchmarking and (3) testing two different probing techniques for bias\ndetection in the context of Bangla. This is the first work of such kind\ninvolving bias assessment of LLMs for Bangla to the best of our knowledge. All\nour code and resources are publicly available for the progress of bias related\nresearch in Bangla NLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of Large Language Models (LLMs) has put forward the study of\nbiases as a crucial field. It is important to assess the influence of different\ntypes of biases embedded in LLMs to ensure fair use in sensitive fields.\nAlthough there have been extensive works on bias assessment in English, such\nefforts are rare and scarce for a major language like Bangla. In this work, we\nexamine two types of social biases in LLM generated outputs for Bangla\nlanguage. Our main contributions in this work are: (1) bias studies on two\ndifferent social biases for Bangla, (2) a curated dataset for bias measurement\nbenchmarking and (3) testing two different probing techniques for bias\ndetection in the context of Bangla. This is the first work of such kind\ninvolving bias assessment of LLMs for Bangla to the best of our knowledge. All\nour code and resources are publicly available for the progress of bias related\nresearch in Bangla NLP."
                },
                "authors": [
                    {
                        "name": "Jayanta Sadhu"
                    },
                    {
                        "name": "Maneesha Rani Saha"
                    },
                    {
                        "name": "Rifat Shahriyar"
                    }
                ],
                "author_detail": {
                    "name": "Rifat Shahriyar"
                },
                "author": "Rifat Shahriyar",
                "arxiv_comment": "Accepted at The First Workshop on Language Models for Low-Resource\n  Languages (LoResLM) at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.03536v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.03536v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13125v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13125v2",
                "updated": "2024-12-13T10:48:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    10,
                    48,
                    13,
                    4,
                    348,
                    0
                ],
                "published": "2024-02-20T16:38:33Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    16,
                    38,
                    33,
                    1,
                    51,
                    0
                ],
                "title": "TreeEval: Benchmark-Free Evaluation of Large Language Models through\n  Tree Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeEval: Benchmark-Free Evaluation of Large Language Models through\n  Tree Planning"
                },
                "summary": "Recently, numerous new benchmarks have been established to evaluate the\nperformance of large language models (LLMs) via either computing a holistic\nscore or employing another LLM as a judge. However, these approaches suffer\nfrom data leakage due to the open access of the benchmark and inflexible\nevaluation process. To address this issue, we introduce $\\textbf{TreeEval}$, a\nbenchmark-free evaluation method for LLMs that let a high-performance LLM host\nan irreproducible evaluation session and essentially avoids the data leakage.\nMoreover, this LLM performs as an examiner to raise up a series of questions\nunder a topic with a tree planing strategy, which considers the current\nevaluation status to decide the next question generation and ensures the\ncompleteness and efficiency of the evaluation process. We evaluate $6$ models\nof different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately\nachieved the highest correlation coefficient with AlpacaEval2.0 using only\naround $45$ questions. We also conduct more analysis to show the robustness and\nreliability of TreeEval. Our code can be accessed via the provided\nhttps://github.com/Ashura5/TreeEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, numerous new benchmarks have been established to evaluate the\nperformance of large language models (LLMs) via either computing a holistic\nscore or employing another LLM as a judge. However, these approaches suffer\nfrom data leakage due to the open access of the benchmark and inflexible\nevaluation process. To address this issue, we introduce $\\textbf{TreeEval}$, a\nbenchmark-free evaluation method for LLMs that let a high-performance LLM host\nan irreproducible evaluation session and essentially avoids the data leakage.\nMoreover, this LLM performs as an examiner to raise up a series of questions\nunder a topic with a tree planing strategy, which considers the current\nevaluation status to decide the next question generation and ensures the\ncompleteness and efficiency of the evaluation process. We evaluate $6$ models\nof different parameter sizes, including $7$B, $13$B, and $33$B, and ultimately\nachieved the highest correlation coefficient with AlpacaEval2.0 using only\naround $45$ questions. We also conduct more analysis to show the robustness and\nreliability of TreeEval. Our code can be accessed via the provided\nhttps://github.com/Ashura5/TreeEval."
                },
                "authors": [
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yunshi Lan"
                    },
                    {
                        "name": "Chao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Yang"
                },
                "author": "Chao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13125v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13125v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06175v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06175v2",
                "updated": "2024-12-13T10:11:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    10,
                    11,
                    31,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-09T13:17:39Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    13,
                    17,
                    39,
                    5,
                    314,
                    0
                ],
                "title": "Clustering Algorithms and RAG Enhancing Semi-Supervised Text\n  Classification with Large LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clustering Algorithms and RAG Enhancing Semi-Supervised Text\n  Classification with Large LLMs"
                },
                "summary": "This paper introduces a novel semi-supervised learning framework specifically\ndesigned for text classification tasks, effectively addressing the challenge of\nvast datasets with limited labeled examples. By integrating multi-level\nsimilarity based data augmentation techniques from Retrieval-Augmented\nGeneration (RAG) to Large Language Model (LLM) rewriting and traditional word\nsubstitution-we constructed an intelligent augmentation pipeline. This\nframework innovatively employs the selection of representative landmarks\nthrough clustering, which serve as intermediaries in the retrieval and\nrewriting processes, ensuring that the augmented data maintains a distribution\nsimilar to the original dataset. Empirical results show that even in complex\ntext document classification scenarios with over 100 categories, our method\nachieves state-of-the-art accuracies of 95.41% and 82.43% on the Reuters and\nWeb of Science datasets, respectively. These findings highlight the\neffectiveness and broad applicability of our semi-supervised learning approach\nfor text classification tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel semi-supervised learning framework specifically\ndesigned for text classification tasks, effectively addressing the challenge of\nvast datasets with limited labeled examples. By integrating multi-level\nsimilarity based data augmentation techniques from Retrieval-Augmented\nGeneration (RAG) to Large Language Model (LLM) rewriting and traditional word\nsubstitution-we constructed an intelligent augmentation pipeline. This\nframework innovatively employs the selection of representative landmarks\nthrough clustering, which serve as intermediaries in the retrieval and\nrewriting processes, ensuring that the augmented data maintains a distribution\nsimilar to the original dataset. Empirical results show that even in complex\ntext document classification scenarios with over 100 categories, our method\nachieves state-of-the-art accuracies of 95.41% and 82.43% on the Reuters and\nWeb of Science datasets, respectively. These findings highlight the\neffectiveness and broad applicability of our semi-supervised learning approach\nfor text classification tasks."
                },
                "authors": [
                    {
                        "name": "Shan Zhong"
                    },
                    {
                        "name": "Jiahao Zeng"
                    },
                    {
                        "name": "Yongxin Yu"
                    },
                    {
                        "name": "Bohong Lin"
                    }
                ],
                "author_detail": {
                    "name": "Bohong Lin"
                },
                "author": "Bohong Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06175v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06175v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10008v1",
                "updated": "2024-12-13T09:47:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    47,
                    26,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T09:47:26Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    47,
                    26,
                    4,
                    348,
                    0
                ],
                "title": "Automated Collection of Evaluation Dataset for Semantic Search in\n  Low-Resource Domain Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Collection of Evaluation Dataset for Semantic Search in\n  Low-Resource Domain Language"
                },
                "summary": "Domain-specific languages that use a lot of specific terminology often fall\ninto the category of low-resource languages. Collecting test datasets in a\nnarrow domain is time-consuming and requires skilled human resources with\ndomain knowledge and training for the annotation task. This study addresses the\nchallenge of automated collecting test datasets to evaluate semantic search in\nlow-resource domain-specific German language of the process industry. Our\napproach proposes an end-to-end annotation pipeline for automated query\ngeneration to the score reassessment of query-document pairs. To overcome the\nlack of text encoders trained in the German chemistry domain, we explore a\nprinciple of an ensemble of \"weak\" text encoders trained on common knowledge\ndatasets. We combine individual relevance scores from diverse models to\nretrieve document candidates and relevance scores generated by an LLM, aiming\nto achieve consensus on query-document alignment. Evaluation results\ndemonstrate that the ensemble method significantly improves alignment with\nhuman-assigned relevance scores, outperforming individual models in both\ninter-coder agreement and accuracy metrics. These findings suggest that\nensemble learning can effectively adapt semantic search systems for\nspecialized, low-resource languages, offering a practical solution to resource\nlimitations in domain-specific contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-specific languages that use a lot of specific terminology often fall\ninto the category of low-resource languages. Collecting test datasets in a\nnarrow domain is time-consuming and requires skilled human resources with\ndomain knowledge and training for the annotation task. This study addresses the\nchallenge of automated collecting test datasets to evaluate semantic search in\nlow-resource domain-specific German language of the process industry. Our\napproach proposes an end-to-end annotation pipeline for automated query\ngeneration to the score reassessment of query-document pairs. To overcome the\nlack of text encoders trained in the German chemistry domain, we explore a\nprinciple of an ensemble of \"weak\" text encoders trained on common knowledge\ndatasets. We combine individual relevance scores from diverse models to\nretrieve document candidates and relevance scores generated by an LLM, aiming\nto achieve consensus on query-document alignment. Evaluation results\ndemonstrate that the ensemble method significantly improves alignment with\nhuman-assigned relevance scores, outperforming individual models in both\ninter-coder agreement and accuracy metrics. These findings suggest that\nensemble learning can effectively adapt semantic search systems for\nspecialized, low-resource languages, offering a practical solution to resource\nlimitations in domain-specific contexts."
                },
                "authors": [
                    {
                        "name": "Anastasia Zhukova"
                    },
                    {
                        "name": "Christian E. Matt"
                    },
                    {
                        "name": "Bela Gipp"
                    }
                ],
                "author_detail": {
                    "name": "Bela Gipp"
                },
                "author": "Bela Gipp",
                "arxiv_comment": "accepted in the First Workshop on Language Models for Low-Resource\n  Languages (LoResLM) co-located with the 31st International Conference on\n  Computational Linguistics (COLING 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09318v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09318v2",
                "updated": "2024-12-13T09:30:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    30,
                    36,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-12T14:43:03Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    3,
                    3,
                    347,
                    0
                ],
                "title": "Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs for Mimicking Child-Caregiver Language in Interaction"
                },
                "summary": "LLMs can generate human-like dialogues, yet their ability to simulate early\nchild-adult interactions remains largely unexplored. In this paper, we examined\nhow effectively LLMs can capture the distinctive features of child-caregiver\nlanguage in interaction, using both static and interactive benchmarking\nmethods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can\napproximate child-caregiver dialogues at the word and utterance level, but they\nstruggle to reproduce the child and caregiver's discursive patterns, exaggerate\nalignment, and fail to reach the level of diversity shown by humans. The\nbroader goal of this work is to initiate the development of a comprehensive\nbenchmark for LLMs in child-oriented applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs can generate human-like dialogues, yet their ability to simulate early\nchild-adult interactions remains largely unexplored. In this paper, we examined\nhow effectively LLMs can capture the distinctive features of child-caregiver\nlanguage in interaction, using both static and interactive benchmarking\nmethods. We found that state-of-the-art LLMs like Llama 3 and GPT-4o can\napproximate child-caregiver dialogues at the word and utterance level, but they\nstruggle to reproduce the child and caregiver's discursive patterns, exaggerate\nalignment, and fail to reach the level of diversity shown by humans. The\nbroader goal of this work is to initiate the development of a comprehensive\nbenchmark for LLMs in child-oriented applications."
                },
                "authors": [
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Abdellah Fourtassi"
                    }
                ],
                "author_detail": {
                    "name": "Abdellah Fourtassi"
                },
                "author": "Abdellah Fourtassi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09318v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09993v1",
                "updated": "2024-12-13T09:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    29,
                    27,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T09:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    29,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "A Comparative Study of LLMs, NMT Models, and Their Combination in\n  Persian-English Idiom Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comparative Study of LLMs, NMT Models, and Their Combination in\n  Persian-English Idiom Translation"
                },
                "summary": "Large language models (LLMs) have shown superior capabilities in translating\nfigurative language compared to neural machine translation (NMT) systems.\nHowever, the impact of different prompting methods and LLM-NMT combinations on\nidiom translation has yet to be thoroughly investigated. This paper introduces\ntwo parallel datasets of sentences containing idiomatic expressions for\nPersian$\\rightarrow$English and English$\\rightarrow$Persian translations, with\nPersian idioms sampled from our PersianIdioms resource, a collection of 2,200\nidioms and their meanings. Using these datasets, we evaluate various open- and\nclosed-source LLMs, NMT models, and their combinations. Translation quality is\nassessed through idiom translation accuracy and fluency. We also find that\nautomatic evaluation methods like LLM-as-a-judge, BLEU and BERTScore are\neffective for comparing different aspects of model performance. Our experiments\nreveal that Claude-3.5-Sonnet delivers outstanding results in both translation\ndirections. For English$\\rightarrow$Persian, combining weaker LLMs with Google\nTranslate improves results, while Persian$\\rightarrow$English translations\nbenefit from single prompts for simpler models and complex prompts for advanced\nones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown superior capabilities in translating\nfigurative language compared to neural machine translation (NMT) systems.\nHowever, the impact of different prompting methods and LLM-NMT combinations on\nidiom translation has yet to be thoroughly investigated. This paper introduces\ntwo parallel datasets of sentences containing idiomatic expressions for\nPersian$\\rightarrow$English and English$\\rightarrow$Persian translations, with\nPersian idioms sampled from our PersianIdioms resource, a collection of 2,200\nidioms and their meanings. Using these datasets, we evaluate various open- and\nclosed-source LLMs, NMT models, and their combinations. Translation quality is\nassessed through idiom translation accuracy and fluency. We also find that\nautomatic evaluation methods like LLM-as-a-judge, BLEU and BERTScore are\neffective for comparing different aspects of model performance. Our experiments\nreveal that Claude-3.5-Sonnet delivers outstanding results in both translation\ndirections. For English$\\rightarrow$Persian, combining weaker LLMs with Google\nTranslate improves results, while Persian$\\rightarrow$English translations\nbenefit from single prompts for simpler models and complex prompts for advanced\nones."
                },
                "authors": [
                    {
                        "name": "Sara Rezaeimanesh"
                    },
                    {
                        "name": "Faezeh Hosseini"
                    },
                    {
                        "name": "Yadollah Yaghoobzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Yadollah Yaghoobzadeh"
                },
                "author": "Yadollah Yaghoobzadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09990v1",
                "updated": "2024-12-13T09:23:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    23,
                    58,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T09:23:58Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    23,
                    58,
                    4,
                    348,
                    0
                ],
                "title": "Small Language Model as Data Prospector for Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Model as Data Prospector for Large Language Model"
                },
                "summary": "The quality of instruction data directly affects the performance of\nfine-tuned Large Language Models (LLMs). Previously, \\cite{li2023one} proposed\n\\texttt{NUGGETS}, which identifies and selects high-quality quality data from a\nlarge dataset by identifying those individual instruction examples that can\nsignificantly improve the performance of different tasks after being learnt as\none-shot instances. In this work, we propose \\texttt{SuperNUGGETS}, an improved\nvariant of \\texttt{NUGGETS} optimised for efficiency and performance. Our\n\\texttt{SuperNUGGETS} uses a small language model (SLM) instead of a large\nlanguage model (LLM) to filter the data for outstanding one-shot instances and\nrefines the predefined set of tests. The experimental results show that the\nperformance of \\texttt{SuperNUGGETS} only decreases by 1-2% compared to\n\\texttt{NUGGETS}, but the efficiency can be increased by a factor of 58.\nCompared to the original \\texttt{NUGGETS}, our \\texttt{SuperNUGGETS} has a\nhigher utility value due to the significantly lower resource consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quality of instruction data directly affects the performance of\nfine-tuned Large Language Models (LLMs). Previously, \\cite{li2023one} proposed\n\\texttt{NUGGETS}, which identifies and selects high-quality quality data from a\nlarge dataset by identifying those individual instruction examples that can\nsignificantly improve the performance of different tasks after being learnt as\none-shot instances. In this work, we propose \\texttt{SuperNUGGETS}, an improved\nvariant of \\texttt{NUGGETS} optimised for efficiency and performance. Our\n\\texttt{SuperNUGGETS} uses a small language model (SLM) instead of a large\nlanguage model (LLM) to filter the data for outstanding one-shot instances and\nrefines the predefined set of tests. The experimental results show that the\nperformance of \\texttt{SuperNUGGETS} only decreases by 1-2% compared to\n\\texttt{NUGGETS}, but the efficiency can be increased by a factor of 58.\nCompared to the original \\texttt{NUGGETS}, our \\texttt{SuperNUGGETS} has a\nhigher utility value due to the significantly lower resource consumption."
                },
                "authors": [
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Haihong Wu"
                    },
                    {
                        "name": "Di Yang"
                    },
                    {
                        "name": "Qiang Qu"
                    },
                    {
                        "name": "Hamid Alinejad-Rokny"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09989v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09989v1",
                "updated": "2024-12-13T09:21:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    21,
                    2,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T09:21:02Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    21,
                    2,
                    4,
                    348,
                    0
                ],
                "title": "One Filter to Deploy Them All: Robust Safety for Quadrupedal Navigation\n  in Unknown Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Filter to Deploy Them All: Robust Safety for Quadrupedal Navigation\n  in Unknown Environments"
                },
                "summary": "As learning-based methods for legged robots rapidly grow in popularity, it is\nimportant that we can provide safety assurances efficiently across different\ncontrollers and environments. Existing works either rely on a priori knowledge\nof the environment and safety constraints to ensure system safety or provide\nassurances for a specific locomotion policy. To address these limitations, we\npropose an observation-conditioned reachability-based (OCR) safety-filter\nframework. Our key idea is to use an OCR value network (OCR-VN) that predicts\nthe optimal control-theoretic safety value function for new failure regions and\ndynamic uncertainty during deployment time. Specifically, the OCR-VN\nfacilitates rapid safety adaptation through two key components: a LiDAR-based\ninput that allows the dynamic construction of safe regions in light of new\nobstacles and a disturbance estimation module that accounts for dynamics\nuncertainty in the wild. The predicted safety value function is used to\nconstruct an adaptive safety filter that overrides the nominal quadruped\ncontroller when necessary to maintain safety. Through simulation studies and\nhardware experiments on a Unitree Go1 quadruped, we demonstrate that the\nproposed framework can automatically safeguard a wide range of hierarchical\nquadruped controllers, adapts to novel environments, and is robust to unmodeled\ndynamics without a priori access to the controllers or environments - hence,\n\"One Filter to Deploy Them All\". The experiment videos can be found on the\nproject website.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As learning-based methods for legged robots rapidly grow in popularity, it is\nimportant that we can provide safety assurances efficiently across different\ncontrollers and environments. Existing works either rely on a priori knowledge\nof the environment and safety constraints to ensure system safety or provide\nassurances for a specific locomotion policy. To address these limitations, we\npropose an observation-conditioned reachability-based (OCR) safety-filter\nframework. Our key idea is to use an OCR value network (OCR-VN) that predicts\nthe optimal control-theoretic safety value function for new failure regions and\ndynamic uncertainty during deployment time. Specifically, the OCR-VN\nfacilitates rapid safety adaptation through two key components: a LiDAR-based\ninput that allows the dynamic construction of safe regions in light of new\nobstacles and a disturbance estimation module that accounts for dynamics\nuncertainty in the wild. The predicted safety value function is used to\nconstruct an adaptive safety filter that overrides the nominal quadruped\ncontroller when necessary to maintain safety. Through simulation studies and\nhardware experiments on a Unitree Go1 quadruped, we demonstrate that the\nproposed framework can automatically safeguard a wide range of hierarchical\nquadruped controllers, adapts to novel environments, and is robust to unmodeled\ndynamics without a priori access to the controllers or environments - hence,\n\"One Filter to Deploy Them All\". The experiment videos can be found on the\nproject website."
                },
                "authors": [
                    {
                        "name": "Albert Lin"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Somil Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Somil Bansal"
                },
                "author": "Somil Bansal",
                "arxiv_comment": "Project website:\n  https://sia-lab-git.github.io/One_Filter_to_Deploy_Them_All/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09989v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09989v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09988v1",
                "updated": "2024-12-13T09:15:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    15,
                    20,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T09:15:20Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    9,
                    15,
                    20,
                    4,
                    348,
                    0
                ],
                "title": "AI and the Future of Digital Public Squares",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI and the Future of Digital Public Squares"
                },
                "summary": "Two substantial technological advances have reshaped the public square in\nrecent decades: first with the advent of the internet and second with the\nrecent introduction of large language models (LLMs). LLMs offer opportunities\nfor a paradigm shift towards more decentralized, participatory online spaces\nthat can be used to facilitate deliberative dialogues at scale, but also create\nrisks of exacerbating societal schisms. Here, we explore four applications of\nLLMs to improve digital public squares: collective dialogue systems, bridging\nsystems, community moderation, and proof-of-humanity systems. Building on the\ninput from over 70 civil society experts and technologists, we argue that LLMs\nboth afford promising opportunities to shift the paradigm for conversations at\nscale and pose distinct risks for digital public squares. We lay out an agenda\nfor future research and investments in AI that will strengthen digital public\nsquares and safeguard against potential misuses of AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two substantial technological advances have reshaped the public square in\nrecent decades: first with the advent of the internet and second with the\nrecent introduction of large language models (LLMs). LLMs offer opportunities\nfor a paradigm shift towards more decentralized, participatory online spaces\nthat can be used to facilitate deliberative dialogues at scale, but also create\nrisks of exacerbating societal schisms. Here, we explore four applications of\nLLMs to improve digital public squares: collective dialogue systems, bridging\nsystems, community moderation, and proof-of-humanity systems. Building on the\ninput from over 70 civil society experts and technologists, we argue that LLMs\nboth afford promising opportunities to shift the paradigm for conversations at\nscale and pose distinct risks for digital public squares. We lay out an agenda\nfor future research and investments in AI that will strengthen digital public\nsquares and safeguard against potential misuses of AI."
                },
                "authors": [
                    {
                        "name": "Beth Goldberg"
                    },
                    {
                        "name": "Diana Acosta-Navas"
                    },
                    {
                        "name": "Michiel Bakker"
                    },
                    {
                        "name": "Ian Beacock"
                    },
                    {
                        "name": "Matt Botvinick"
                    },
                    {
                        "name": "Prateek Buch"
                    },
                    {
                        "name": "Renée DiResta"
                    },
                    {
                        "name": "Nandika Donthi"
                    },
                    {
                        "name": "Nathanael Fast"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Zaria Jalan"
                    },
                    {
                        "name": "Andrew Konya"
                    },
                    {
                        "name": "Grace Kwak Danciu"
                    },
                    {
                        "name": "Hélène Landemore"
                    },
                    {
                        "name": "Alice Marwick"
                    },
                    {
                        "name": "Carl Miller"
                    },
                    {
                        "name": "Aviv Ovadya"
                    },
                    {
                        "name": "Emily Saltz"
                    },
                    {
                        "name": "Lisa Schirch"
                    },
                    {
                        "name": "Dalit Shalom"
                    },
                    {
                        "name": "Divya Siddarth"
                    },
                    {
                        "name": "Felix Sieker"
                    },
                    {
                        "name": "Christopher Small"
                    },
                    {
                        "name": "Jonathan Stray"
                    },
                    {
                        "name": "Audrey Tang"
                    },
                    {
                        "name": "Michael Henry Tessler"
                    },
                    {
                        "name": "Amy Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Amy Zhang"
                },
                "author": "Amy Zhang",
                "arxiv_comment": "40 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04680v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04680v2",
                "updated": "2024-12-13T08:44:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    44,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-08-08T04:49:21Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    4,
                    49,
                    21,
                    3,
                    221,
                    0
                ],
                "title": "Dynamic Fog Computing for Enhanced LLM Execution in Medical Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Fog Computing for Enhanced LLM Execution in Medical Applications"
                },
                "summary": "The ability of large language models (LLMs) to transform, interpret, and\ncomprehend vast quantities of heterogeneous data presents a significant\nopportunity to enhance data-driven care delivery. However, the sensitive nature\nof protected health information (PHI) raises valid concerns about data privacy\nand trust in remote LLM platforms. In addition, the cost associated with\ncloud-based artificial intelligence (AI) services continues to impede\nwidespread adoption. To address these challenges, we propose a shift in the LLM\nexecution environment from opaque, centralized cloud providers to a\ndecentralized and dynamic fog computing architecture. By executing open-weight\nLLMs in more trusted environments, such as the user's edge device or a fog\nlayer within a local network, we aim to mitigate the privacy, trust, and\nfinancial challenges associated with cloud-based LLMs. We further present\nSpeziLLM, an open-source framework designed to facilitate rapid and seamless\nleveraging of different LLM execution layers and lowering barriers to LLM\nintegration in digital health applications. We demonstrate SpeziLLM's broad\napplicability across six digital health applications, showcasing its\nversatility in various healthcare settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of large language models (LLMs) to transform, interpret, and\ncomprehend vast quantities of heterogeneous data presents a significant\nopportunity to enhance data-driven care delivery. However, the sensitive nature\nof protected health information (PHI) raises valid concerns about data privacy\nand trust in remote LLM platforms. In addition, the cost associated with\ncloud-based artificial intelligence (AI) services continues to impede\nwidespread adoption. To address these challenges, we propose a shift in the LLM\nexecution environment from opaque, centralized cloud providers to a\ndecentralized and dynamic fog computing architecture. By executing open-weight\nLLMs in more trusted environments, such as the user's edge device or a fog\nlayer within a local network, we aim to mitigate the privacy, trust, and\nfinancial challenges associated with cloud-based LLMs. We further present\nSpeziLLM, an open-source framework designed to facilitate rapid and seamless\nleveraging of different LLM execution layers and lowering barriers to LLM\nintegration in digital health applications. We demonstrate SpeziLLM's broad\napplicability across six digital health applications, showcasing its\nversatility in various healthcare settings."
                },
                "authors": [
                    {
                        "name": "Philipp Zagar"
                    },
                    {
                        "name": "Vishnu Ravi"
                    },
                    {
                        "name": "Lauren Aalami"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Oliver Aalami"
                    },
                    {
                        "name": "Paul Schmiedmayer"
                    }
                ],
                "author_detail": {
                    "name": "Paul Schmiedmayer"
                },
                "author": "Paul Schmiedmayer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04680v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04680v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09961v1",
                "updated": "2024-12-13T08:42:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    42,
                    19,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T08:42:19Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    42,
                    19,
                    4,
                    348,
                    0
                ],
                "title": "What constitutes a Deep Fake? The blurry line between legitimate\n  processing and manipulation under the EU AI Act",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What constitutes a Deep Fake? The blurry line between legitimate\n  processing and manipulation under the EU AI Act"
                },
                "summary": "When does a digital image resemble reality? The relevance of this question\nincreases as the generation of synthetic images -- so called deep fakes --\nbecomes increasingly popular. Deep fakes have gained much attention for a\nnumber of reasons -- among others, due to their potential to disrupt the\npolitical climate. In order to mitigate these threats, the EU AI Act implements\nspecific transparency regulations for generating synthetic content or\nmanipulating existing content. However, the distinction between real and\nsynthetic images is -- even from a computer vision perspective -- far from\ntrivial. We argue that the current definition of deep fakes in the AI act and\nthe corresponding obligations are not sufficiently specified to tackle the\nchallenges posed by deep fakes. By analyzing the life cycle of a digital photo\nfrom the camera sensor to the digital editing features, we find that: (1.) Deep\nfakes are ill-defined in the EU AI Act. The definition leaves too much scope\nfor what a deep fake is. (2.) It is unclear how editing functions like Google's\n``best take'' feature can be considered as an exception to transparency\nobligations. (3.) The exception for substantially edited images raises\nquestions about what constitutes substantial editing of content and whether or\nnot this editing must be perceptible by a natural person. Our results\ndemonstrate that complying with the current AI Act transparency obligations is\ndifficult for providers and deployers. As a consequence of the unclear\nprovisions, there is a risk that exceptions may be either too broad or too\nlimited. We intend our analysis to foster the discussion on what constitutes a\ndeep fake and to raise awareness about the pitfalls in the current AI Act\ntransparency obligations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When does a digital image resemble reality? The relevance of this question\nincreases as the generation of synthetic images -- so called deep fakes --\nbecomes increasingly popular. Deep fakes have gained much attention for a\nnumber of reasons -- among others, due to their potential to disrupt the\npolitical climate. In order to mitigate these threats, the EU AI Act implements\nspecific transparency regulations for generating synthetic content or\nmanipulating existing content. However, the distinction between real and\nsynthetic images is -- even from a computer vision perspective -- far from\ntrivial. We argue that the current definition of deep fakes in the AI act and\nthe corresponding obligations are not sufficiently specified to tackle the\nchallenges posed by deep fakes. By analyzing the life cycle of a digital photo\nfrom the camera sensor to the digital editing features, we find that: (1.) Deep\nfakes are ill-defined in the EU AI Act. The definition leaves too much scope\nfor what a deep fake is. (2.) It is unclear how editing functions like Google's\n``best take'' feature can be considered as an exception to transparency\nobligations. (3.) The exception for substantially edited images raises\nquestions about what constitutes substantial editing of content and whether or\nnot this editing must be perceptible by a natural person. Our results\ndemonstrate that complying with the current AI Act transparency obligations is\ndifficult for providers and deployers. As a consequence of the unclear\nprovisions, there is a risk that exceptions may be either too broad or too\nlimited. We intend our analysis to foster the discussion on what constitutes a\ndeep fake and to raise awareness about the pitfalls in the current AI Act\ntransparency obligations."
                },
                "authors": [
                    {
                        "name": "Kristof Meding"
                    },
                    {
                        "name": "Christoph Sorge"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Sorge"
                },
                "author": "Christoph Sorge",
                "arxiv_comment": "Preprint. Accepted at ACM CS&Law '25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09952v1",
                "updated": "2024-12-13T08:22:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    22,
                    19,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T08:22:19Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    22,
                    19,
                    4,
                    348,
                    0
                ],
                "title": "Llama 3 Meets MoE: Efficient Upcycling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Llama 3 Meets MoE: Efficient Upcycling"
                },
                "summary": "Scaling large language models (LLMs) significantly improves performance but\ncomes with prohibitive computational costs. Mixture-of-Experts (MoE) models\noffer an efficient alternative, increasing capacity without a proportional rise\nin compute requirements. However, training MoE models from scratch poses\nchallenges like overfitting and routing instability. We present an efficient\ntraining recipe leveraging pre-trained dense checkpoints, training an 8-Expert\nTop-2 MoE model from Llama 3-8B with less than $1\\%$ of typical pre-training\ncompute. Our approach enhances downstream performance on academic benchmarks,\nachieving a $\\textbf{2%}$ improvement in 0-shot accuracy on MMLU, while\nreaching a Model FLOPs Utilization (MFU) of $\\textbf{46.8%}$ during training\nusing our framework. We also integrate online upcycling in NeMo for seamless\nuse of pre-trained weights, enabling cost-effective development of\nhigh-capacity MoE models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling large language models (LLMs) significantly improves performance but\ncomes with prohibitive computational costs. Mixture-of-Experts (MoE) models\noffer an efficient alternative, increasing capacity without a proportional rise\nin compute requirements. However, training MoE models from scratch poses\nchallenges like overfitting and routing instability. We present an efficient\ntraining recipe leveraging pre-trained dense checkpoints, training an 8-Expert\nTop-2 MoE model from Llama 3-8B with less than $1\\%$ of typical pre-training\ncompute. Our approach enhances downstream performance on academic benchmarks,\nachieving a $\\textbf{2%}$ improvement in 0-shot accuracy on MMLU, while\nreaching a Model FLOPs Utilization (MFU) of $\\textbf{46.8%}$ during training\nusing our framework. We also integrate online upcycling in NeMo for seamless\nuse of pre-trained weights, enabling cost-effective development of\nhigh-capacity MoE models."
                },
                "authors": [
                    {
                        "name": "Aditya Vavre"
                    },
                    {
                        "name": "Ethan He"
                    },
                    {
                        "name": "Dennis Liu"
                    },
                    {
                        "name": "Zijie Yan"
                    },
                    {
                        "name": "June Yang"
                    },
                    {
                        "name": "Nima Tajbakhsh"
                    },
                    {
                        "name": "Ashwath Aithal"
                    }
                ],
                "author_detail": {
                    "name": "Ashwath Aithal"
                },
                "author": "Ashwath Aithal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09946v1",
                "updated": "2024-12-13T08:10:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    10,
                    56,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T08:10:56Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    10,
                    56,
                    4,
                    348,
                    0
                ],
                "title": "Enhancing Nursing and Elderly Care with Large Language Models: An\n  AI-Driven Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Nursing and Elderly Care with Large Language Models: An\n  AI-Driven Framework"
                },
                "summary": "This paper explores the application of large language models (LLMs) in\nnursing and elderly care, focusing on AI-driven patient monitoring and\ninteraction. We introduce a novel Chinese nursing dataset and implement\nincremental pre-training (IPT) and supervised fine-tuning (SFT) techniques to\nenhance LLM performance in specialized tasks. Using LangChain, we develop a\ndynamic nursing assistant capable of real-time care and personalized\ninterventions. Experimental results demonstrate significant improvements,\npaving the way for AI-driven solutions to meet the growing demands of\nhealthcare in aging populations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the application of large language models (LLMs) in\nnursing and elderly care, focusing on AI-driven patient monitoring and\ninteraction. We introduce a novel Chinese nursing dataset and implement\nincremental pre-training (IPT) and supervised fine-tuning (SFT) techniques to\nenhance LLM performance in specialized tasks. Using LangChain, we develop a\ndynamic nursing assistant capable of real-time care and personalized\ninterventions. Experimental results demonstrate significant improvements,\npaving the way for AI-driven solutions to meet the growing demands of\nhealthcare in aging populations."
                },
                "authors": [
                    {
                        "name": "Qiao Sun"
                    },
                    {
                        "name": "Jiexin Xie"
                    },
                    {
                        "name": "Nanyang Ye"
                    },
                    {
                        "name": "Qinying Gu"
                    },
                    {
                        "name": "Shijie Guo"
                    }
                ],
                "author_detail": {
                    "name": "Shijie Guo"
                },
                "author": "Shijie Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04617v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04617v2",
                "updated": "2024-12-13T08:05:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    8,
                    5,
                    53,
                    4,
                    348,
                    0
                ],
                "published": "2024-10-06T20:34:03Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    20,
                    34,
                    3,
                    6,
                    280,
                    0
                ],
                "title": "Evaluation of Code LLMs on Geospatial Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of Code LLMs on Geospatial Code Generation"
                },
                "summary": "Software development support tools have been studied for a long time, with\nrecent approaches using Large Language Models (LLMs) for code generation. These\nmodels can generate Python code for data science and machine learning\napplications. LLMs are helpful for software engineers because they increase\nproductivity in daily work. An LLM can also serve as a \"mentor\" for\ninexperienced software developers, and be a viable learning support.\nHigh-quality code generation with LLMs can also be beneficial in geospatial\ndata science. However, this domain poses different challenges, and code\ngeneration LLMs are typically not evaluated on geospatial tasks. Here, we show\nhow we constructed an evaluation benchmark for code generation models, based on\na selection of geospatial tasks. We categorised geospatial tasks based on their\ncomplexity and required tools. Then, we created a dataset with tasks that test\nmodel capabilities in spatial reasoning, spatial data processing, and\ngeospatial tools usage. The dataset consists of specific coding problems that\nwere manually created for high quality. For every problem, we proposed a set of\ntest scenarios that make it possible to automatically check the generated code\nfor correctness. In addition, we tested a selection of existing code generation\nLLMs for code generation in the geospatial domain. We share our dataset and\nreproducible evaluation code on a public GitHub repository, arguing that this\ncan serve as an evaluation benchmark for new LLMs in the future. Our dataset\nwill hopefully contribute to the development new models capable of solving\ngeospatial coding tasks with high accuracy. These models will enable the\ncreation of coding assistants tailored for geospatial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software development support tools have been studied for a long time, with\nrecent approaches using Large Language Models (LLMs) for code generation. These\nmodels can generate Python code for data science and machine learning\napplications. LLMs are helpful for software engineers because they increase\nproductivity in daily work. An LLM can also serve as a \"mentor\" for\ninexperienced software developers, and be a viable learning support.\nHigh-quality code generation with LLMs can also be beneficial in geospatial\ndata science. However, this domain poses different challenges, and code\ngeneration LLMs are typically not evaluated on geospatial tasks. Here, we show\nhow we constructed an evaluation benchmark for code generation models, based on\na selection of geospatial tasks. We categorised geospatial tasks based on their\ncomplexity and required tools. Then, we created a dataset with tasks that test\nmodel capabilities in spatial reasoning, spatial data processing, and\ngeospatial tools usage. The dataset consists of specific coding problems that\nwere manually created for high quality. For every problem, we proposed a set of\ntest scenarios that make it possible to automatically check the generated code\nfor correctness. In addition, we tested a selection of existing code generation\nLLMs for code generation in the geospatial domain. We share our dataset and\nreproducible evaluation code on a public GitHub repository, arguing that this\ncan serve as an evaluation benchmark for new LLMs in the future. Our dataset\nwill hopefully contribute to the development new models capable of solving\ngeospatial coding tasks with high accuracy. These models will enable the\ncreation of coding assistants tailored for geospatial applications."
                },
                "authors": [
                    {
                        "name": "Piotr Gramacki"
                    },
                    {
                        "name": "Bruno Martins"
                    },
                    {
                        "name": "Piotr Szymański"
                    }
                ],
                "author_detail": {
                    "name": "Piotr Szymański"
                },
                "author": "Piotr Szymański",
                "arxiv_doi": "10.1145/3687123.3698286",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3687123.3698286",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.04617v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04617v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "7th ACM SIGSPATIAL International Workshop on AI for Geographic\n  Knowledge Discovery (GeoAI'24)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07214v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07214v2",
                "updated": "2024-12-13T07:08:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    8,
                    54,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-10T06:11:23Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    6,
                    11,
                    23,
                    1,
                    345,
                    0
                ],
                "title": "Towards Automated Cross-domain Exploratory Data Analysis through Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Automated Cross-domain Exploratory Data Analysis through Large\n  Language Models"
                },
                "summary": "Exploratory data analysis (EDA), coupled with SQL, is essential for data\nanalysts involved in data exploration and analysis. However, data analysts\noften encounter two primary challenges: (1) the need to craft SQL queries\nskillfully, and (2) the requirement to generate suitable visualization types\nthat enhance the interpretation of query results. Due to its significance,\nsubstantial research efforts have been made to explore different approaches to\naddress these challenges, including leveraging large language models (LLMs).\nHowever, existing methods fail to meet real-world data exploration requirements\nprimarily due to (1) complex database schema; (2) unclear user intent; (3)\nlimited cross-domain generalization capability; and (4) insufficient end-to-end\ntext-to-visualization capability.\n  This paper presents TiInsight, an automated SQL-based cross-domain\nexploratory data analysis system. First, we propose hierarchical data context\n(i.e., HDC), which leverages LLMs to summarize the contexts related to the\ndatabase schema, which is crucial for open-world EDA systems to generalize\nacross data domains. Second, the EDA system is divided into four components\n(i.e., stages): HDC generation, question clarification and decomposition,\ntext-to-SQL generation (i.e., TiSQL), and data visualization (i.e., TiChart).\nFinally, we implemented an end-to-end EDA system with a user-friendly GUI\ninterface in the production environment at PingCAP. We have also open-sourced\nall APIs of TiInsight to facilitate research within the EDA community. Through\nextensive evaluations by a real-world user study, we demonstrate that TiInsight\noffers remarkable performance compared to human experts. Specifically, TiSQL\nachieves an execution accuracy of 86.3% on the Spider dataset using GPT-4. It\nalso demonstrates state-of-the-art performance on the Bird dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploratory data analysis (EDA), coupled with SQL, is essential for data\nanalysts involved in data exploration and analysis. However, data analysts\noften encounter two primary challenges: (1) the need to craft SQL queries\nskillfully, and (2) the requirement to generate suitable visualization types\nthat enhance the interpretation of query results. Due to its significance,\nsubstantial research efforts have been made to explore different approaches to\naddress these challenges, including leveraging large language models (LLMs).\nHowever, existing methods fail to meet real-world data exploration requirements\nprimarily due to (1) complex database schema; (2) unclear user intent; (3)\nlimited cross-domain generalization capability; and (4) insufficient end-to-end\ntext-to-visualization capability.\n  This paper presents TiInsight, an automated SQL-based cross-domain\nexploratory data analysis system. First, we propose hierarchical data context\n(i.e., HDC), which leverages LLMs to summarize the contexts related to the\ndatabase schema, which is crucial for open-world EDA systems to generalize\nacross data domains. Second, the EDA system is divided into four components\n(i.e., stages): HDC generation, question clarification and decomposition,\ntext-to-SQL generation (i.e., TiSQL), and data visualization (i.e., TiChart).\nFinally, we implemented an end-to-end EDA system with a user-friendly GUI\ninterface in the production environment at PingCAP. We have also open-sourced\nall APIs of TiInsight to facilitate research within the EDA community. Through\nextensive evaluations by a real-world user study, we demonstrate that TiInsight\noffers remarkable performance compared to human experts. Specifically, TiSQL\nachieves an execution accuracy of 86.3% on the Spider dataset using GPT-4. It\nalso demonstrates state-of-the-art performance on the Bird dataset."
                },
                "authors": [
                    {
                        "name": "Jun-Peng Zhu"
                    },
                    {
                        "name": "Boyan Niu"
                    },
                    {
                        "name": "Peng Cai"
                    },
                    {
                        "name": "Zheming Ni"
                    },
                    {
                        "name": "Jianwei Wan"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Jiajun Huang"
                    },
                    {
                        "name": "Shengbo Ma"
                    },
                    {
                        "name": "Bing Wang"
                    },
                    {
                        "name": "Xuan Zhou"
                    },
                    {
                        "name": "Guanglei Bao"
                    },
                    {
                        "name": "Donghui Zhang"
                    },
                    {
                        "name": "Liu Tang"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "arxiv_comment": "14 pages, 10 figures. Submitted to SIGMOD 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07214v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07214v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09916v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09916v1",
                "updated": "2024-12-13T07:08:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    8,
                    13,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T07:08:13Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    8,
                    13,
                    4,
                    348,
                    0
                ],
                "title": "ProxyLLM : LLM-Driven Framework for Customer Support Through Text-Style\n  Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProxyLLM : LLM-Driven Framework for Customer Support Through Text-Style\n  Transfer"
                },
                "summary": "Chatbot-based customer support services have significantly advanced with the\nintroduction of large language models (LLMs), enabling enhanced response\nquality and broader application across industries. However, while these\nadvancements focus on reducing business costs and improving customer\nsatisfaction, limited attention has been given to the experiences of customer\nservice agents, who are critical to the service ecosystem. A major challenge\nfaced by agents is the stress caused by unnecessary emotional exhaustion from\nharmful texts, which not only impairs their efficiency but also negatively\naffects customer satisfaction and business outcomes. In this work, we propose\nan LLM-powered system designed to enhance the working conditions of customer\nservice agents by addressing emotionally intensive communications. Our proposed\nsystem leverages LLMs to transform the tone of customer messages, preserving\nactionable content while mitigating the emotional impact on human agents.\nFurthermore, the application is implemented as a Chrome extension, making it\nhighly adaptable and easy to integrate into existing systems. Our method aims\nto enhance the overall service experience for businesses, customers, and\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chatbot-based customer support services have significantly advanced with the\nintroduction of large language models (LLMs), enabling enhanced response\nquality and broader application across industries. However, while these\nadvancements focus on reducing business costs and improving customer\nsatisfaction, limited attention has been given to the experiences of customer\nservice agents, who are critical to the service ecosystem. A major challenge\nfaced by agents is the stress caused by unnecessary emotional exhaustion from\nharmful texts, which not only impairs their efficiency but also negatively\naffects customer satisfaction and business outcomes. In this work, we propose\nan LLM-powered system designed to enhance the working conditions of customer\nservice agents by addressing emotionally intensive communications. Our proposed\nsystem leverages LLMs to transform the tone of customer messages, preserving\nactionable content while mitigating the emotional impact on human agents.\nFurthermore, the application is implemented as a Chrome extension, making it\nhighly adaptable and easy to integrate into existing systems. Our method aims\nto enhance the overall service experience for businesses, customers, and\nagents."
                },
                "authors": [
                    {
                        "name": "Sehyeong Jo"
                    },
                    {
                        "name": "Jungwon Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jungwon Seo"
                },
                "author": "Jungwon Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09916v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09916v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09914v1",
                "updated": "2024-12-13T07:05:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    5,
                    31,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T07:05:31Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    5,
                    31,
                    4,
                    348,
                    0
                ],
                "title": "Atomic Learning Objectives Labeling: A High-Resolution Approach for\n  Physics Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atomic Learning Objectives Labeling: A High-Resolution Approach for\n  Physics Education"
                },
                "summary": "This paper introduces a novel approach to create a high-resolution \"map\" for\nphysics learning: an \"atomic\" learning objectives (LOs) system designed to\ncapture detailed cognitive processes and concepts required for problem solving\nin a college-level introductory physics course. Our method leverages Large\nLanguage Models (LLMs) for automated labeling of physics questions and\nintroduces a comprehensive set of metrics to evaluate the quality of the\nlabeling outcomes. The atomic LO system, covering nine chapters of an\nintroductory physics course, uses a \"subject-verb-object'' structure to\nrepresent specific cognitive processes. We apply this system to 131 questions\nfrom expert-curated question banks and the OpenStax University Physics\ntextbook. Each question is labeled with 1-8 atomic LOs across three chapters.\nThrough extensive experiments using various prompting strategies and LLMs, we\ncompare automated LOs labeling results against human expert labeling. Our\nanalysis reveals both the strengths and limitations of LLMs, providing insight\ninto LLMs reasoning processes for labeling LOs and identifying areas for\nimprovement in LOs system design. Our work contributes to the field of learning\nanalytics by proposing a more granular approach to mapping learning objectives\nwith questions. Our findings have significant implications for the development\nof intelligent tutoring systems and personalized learning pathways in STEM\neducation, paving the way for more effective \"learning GPS'' systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach to create a high-resolution \"map\" for\nphysics learning: an \"atomic\" learning objectives (LOs) system designed to\ncapture detailed cognitive processes and concepts required for problem solving\nin a college-level introductory physics course. Our method leverages Large\nLanguage Models (LLMs) for automated labeling of physics questions and\nintroduces a comprehensive set of metrics to evaluate the quality of the\nlabeling outcomes. The atomic LO system, covering nine chapters of an\nintroductory physics course, uses a \"subject-verb-object'' structure to\nrepresent specific cognitive processes. We apply this system to 131 questions\nfrom expert-curated question banks and the OpenStax University Physics\ntextbook. Each question is labeled with 1-8 atomic LOs across three chapters.\nThrough extensive experiments using various prompting strategies and LLMs, we\ncompare automated LOs labeling results against human expert labeling. Our\nanalysis reveals both the strengths and limitations of LLMs, providing insight\ninto LLMs reasoning processes for labeling LOs and identifying areas for\nimprovement in LOs system design. Our work contributes to the field of learning\nanalytics by proposing a more granular approach to mapping learning objectives\nwith questions. Our findings have significant implications for the development\nof intelligent tutoring systems and personalized learning pathways in STEM\neducation, paving the way for more effective \"learning GPS'' systems."
                },
                "authors": [
                    {
                        "name": "Naiming Liu"
                    },
                    {
                        "name": "Shashank Sonkar"
                    },
                    {
                        "name": "Debshila Basu Mallick"
                    },
                    {
                        "name": "Richard Baraniuk"
                    },
                    {
                        "name": "Zhongzhou Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhongzhou Chen"
                },
                "author": "Zhongzhou Chen",
                "arxiv_comment": "The paper is accepted to LAK'2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09913v1",
                "updated": "2024-12-13T07:03:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    3,
                    5,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T07:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    3,
                    5,
                    4,
                    348,
                    0
                ],
                "title": "Digital Twin Enabled Runtime Verification for Autonomous Mobile Robots\n  under Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin Enabled Runtime Verification for Autonomous Mobile Robots\n  under Uncertainty"
                },
                "summary": "As autonomous robots increasingly navigate complex and unpredictable\nenvironments, ensuring their reliable behavior under uncertainty becomes a\ncritical challenge. This paper introduces a digital twin-based runtime\nverification for an autonomous mobile robot to mitigate the impact posed by\nuncertainty in the deployment environment. The safety and performance\nproperties are specified and synthesized as runtime monitors using TeSSLa. The\nintegration of the executable digital twin, via the MQTT protocol, enables\ncontinuous monitoring and validation of the robot's behavior in real-time. We\nexplore the sources of uncertainties, including sensor noise and environment\nvariations, and analyze their impact on the robot safety and performance.\nEquipped with high computation resources, the cloud-located digital twin serves\nas a watch-dog model to estimate the actual state, check the consistency of the\nrobot's actuations and intervene to override such actuations if a safety or\nperformance property is about to be violated. The experimental analysis\ndemonstrated high efficiency of the proposed approach in ensuring the\nreliability and robustness of the autonomous robot behavior in uncertain\nenvironments and securing high alignment between the actual and expected speeds\nwhere the difference is reduced by up to 41\\% compared to the default robot\nnavigation control.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As autonomous robots increasingly navigate complex and unpredictable\nenvironments, ensuring their reliable behavior under uncertainty becomes a\ncritical challenge. This paper introduces a digital twin-based runtime\nverification for an autonomous mobile robot to mitigate the impact posed by\nuncertainty in the deployment environment. The safety and performance\nproperties are specified and synthesized as runtime monitors using TeSSLa. The\nintegration of the executable digital twin, via the MQTT protocol, enables\ncontinuous monitoring and validation of the robot's behavior in real-time. We\nexplore the sources of uncertainties, including sensor noise and environment\nvariations, and analyze their impact on the robot safety and performance.\nEquipped with high computation resources, the cloud-located digital twin serves\nas a watch-dog model to estimate the actual state, check the consistency of the\nrobot's actuations and intervene to override such actuations if a safety or\nperformance property is about to be violated. The experimental analysis\ndemonstrated high efficiency of the proposed approach in ensuring the\nreliability and robustness of the autonomous robot behavior in uncertain\nenvironments and securing high alignment between the actual and expected speeds\nwhere the difference is reduced by up to 41\\% compared to the default robot\nnavigation control."
                },
                "authors": [
                    {
                        "name": "Joakim Schack Betzer"
                    },
                    {
                        "name": "Jalil Boudjadar"
                    },
                    {
                        "name": "Mirgita Frasheri"
                    },
                    {
                        "name": "Prasad Talasila"
                    }
                ],
                "author_detail": {
                    "name": "Prasad Talasila"
                },
                "author": "Prasad Talasila",
                "arxiv_comment": "10 pages, 10 figures, accepted at 2024 28th International Symposium\n  on Distributed Simulation and Real Time Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16283v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16283v2",
                "updated": "2024-12-13T07:01:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    7,
                    1,
                    16,
                    4,
                    348,
                    0
                ],
                "published": "2024-04-25T01:56:00Z",
                "published_parsed": [
                    2024,
                    4,
                    25,
                    1,
                    56,
                    0,
                    3,
                    116,
                    0
                ],
                "title": "Andes: Defining and Enhancing Quality-of-Experience in LLM-Based Text\n  Streaming Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Andes: Defining and Enhancing Quality-of-Experience in LLM-Based Text\n  Streaming Services"
                },
                "summary": "Large language models (LLMs) are now at the core of conversational AI\nservices such as real-time translation and chatbots, which provide live user\ninteraction by incrementally streaming text to the user. However, existing LLM\nserving systems fail to provide good user experience because their optimization\nmetrics are not always aligned with user experience.\n  In this paper, we first introduce and define the notion of\nQuality-of-Experience (QoE) for text streaming services by considering each\nuser's end-to-end interaction timeline. Based on this, we propose Andes, a\nQoE-aware LLM serving system that enhances user experience by ensuring that\nusers receive the first token promptly and subsequent tokens at a smooth,\ndigestible pace, even during surge periods. This is enabled by Andes's\npreemptive request scheduler that dynamically prioritizes requests at the token\ngranularity based on each request's expected QoE gain and GPU resource usage.\nOur evaluations demonstrate that, compared to state-of-the-art LLM serving\nsystems, Andes improves the average QoE by up to $4.7\\times$ given the same GPU\nresource, or saves up to 61% GPU resources while maintaining the same high QoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are now at the core of conversational AI\nservices such as real-time translation and chatbots, which provide live user\ninteraction by incrementally streaming text to the user. However, existing LLM\nserving systems fail to provide good user experience because their optimization\nmetrics are not always aligned with user experience.\n  In this paper, we first introduce and define the notion of\nQuality-of-Experience (QoE) for text streaming services by considering each\nuser's end-to-end interaction timeline. Based on this, we propose Andes, a\nQoE-aware LLM serving system that enhances user experience by ensuring that\nusers receive the first token promptly and subsequent tokens at a smooth,\ndigestible pace, even during surge periods. This is enabled by Andes's\npreemptive request scheduler that dynamically prioritizes requests at the token\ngranularity based on each request's expected QoE gain and GPU resource usage.\nOur evaluations demonstrate that, compared to state-of-the-art LLM serving\nsystems, Andes improves the average QoE by up to $4.7\\times$ given the same GPU\nresource, or saves up to 61% GPU resources while maintaining the same high QoE."
                },
                "authors": [
                    {
                        "name": "Jiachen Liu"
                    },
                    {
                        "name": "Jae-Won Chung"
                    },
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Mosharaf Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Mosharaf Chowdhury"
                },
                "author": "Mosharaf Chowdhury",
                "arxiv_comment": "16 pages, 21 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16283v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16283v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13748v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13748v2",
                "updated": "2024-12-13T06:55:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    55,
                    46,
                    4,
                    348,
                    0
                ],
                "published": "2024-06-19T18:01:08Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    18,
                    1,
                    8,
                    2,
                    171,
                    0
                ],
                "title": "Learn and Unlearn in Multilingual LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learn and Unlearn in Multilingual LLMs"
                },
                "summary": "This paper investigates the propagation of harmful information in\nmultilingual large language models (LLMs) and evaluates the efficacy of various\nunlearning methods. We demonstrate that fake information, regardless of the\nlanguage it is in, once introduced into these models through training data, can\nspread across different languages, compromising the integrity and reliability\nof the generated content. Our findings reveal that standard unlearning\ntechniques, which typically focus on English data, are insufficient in\nmitigating the spread of harmful content in multilingual contexts and could\ninadvertently reinforce harmful content across languages. We show that only by\naddressing harmful responses in both English and the original language of the\nharmful data can we effectively eliminate generations for all languages. This\nunderscores the critical need for comprehensive unlearning strategies that\nconsider the multilingual nature of modern LLMs to enhance their safety and\nreliability across diverse linguistic landscapes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the propagation of harmful information in\nmultilingual large language models (LLMs) and evaluates the efficacy of various\nunlearning methods. We demonstrate that fake information, regardless of the\nlanguage it is in, once introduced into these models through training data, can\nspread across different languages, compromising the integrity and reliability\nof the generated content. Our findings reveal that standard unlearning\ntechniques, which typically focus on English data, are insufficient in\nmitigating the spread of harmful content in multilingual contexts and could\ninadvertently reinforce harmful content across languages. We show that only by\naddressing harmful responses in both English and the original language of the\nharmful data can we effectively eliminate generations for all languages. This\nunderscores the critical need for comprehensive unlearning strategies that\nconsider the multilingual nature of modern LLMs to enhance their safety and\nreliability across diverse linguistic landscapes."
                },
                "authors": [
                    {
                        "name": "Taiming Lu"
                    },
                    {
                        "name": "Philipp Koehn"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Koehn"
                },
                "author": "Philipp Koehn",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13748v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09906v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09906v1",
                "updated": "2024-12-13T06:45:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    45,
                    26,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T06:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    45,
                    26,
                    4,
                    348,
                    0
                ],
                "title": "Enhancing the Reasoning Capabilities of Small Language Models via\n  Solution Guidance Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing the Reasoning Capabilities of Small Language Models via\n  Solution Guidance Fine-Tuning"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\na wide range of tasks. Advances in prompt engineering and fine-tuning\ntechniques have further enhanced their ability to address complex reasoning\nchallenges. However, these advanced capabilities are often exclusive to models\nexceeding 100 billion parameters. Although Chain-of-Thought (CoT) fine-tuning\nmethods have been explored for smaller models (under 10 billion parameters),\nthey typically depend on extensive CoT training data, which can introduce\ninconsistencies and limit effectiveness in low-data settings. To overcome these\nlimitations, this paper introduce a new reasoning strategy Solution Guidance\n(SG) and a plug-and-play training paradigm Solution-Guidance Fine-Tuning (SGFT)\nfor enhancing the reasoning capabilities of small language models. SG focuses\non problem understanding and decomposition at the semantic and logical levels,\nrather than specific computations, which can effectively improve the SLMs'\ngeneralization and reasoning abilities. With only a small amount of SG training\ndata, SGFT can fine-tune a SLM to produce accurate problem-solving guidances,\nwhich can then be flexibly fed to any SLM as prompts, enabling it to generate\ncorrect answers directly. Experimental results demonstrate that our method\nsignificantly improves the performance of SLMs on various reasoning tasks,\nenhancing both their practicality and efficiency within resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\na wide range of tasks. Advances in prompt engineering and fine-tuning\ntechniques have further enhanced their ability to address complex reasoning\nchallenges. However, these advanced capabilities are often exclusive to models\nexceeding 100 billion parameters. Although Chain-of-Thought (CoT) fine-tuning\nmethods have been explored for smaller models (under 10 billion parameters),\nthey typically depend on extensive CoT training data, which can introduce\ninconsistencies and limit effectiveness in low-data settings. To overcome these\nlimitations, this paper introduce a new reasoning strategy Solution Guidance\n(SG) and a plug-and-play training paradigm Solution-Guidance Fine-Tuning (SGFT)\nfor enhancing the reasoning capabilities of small language models. SG focuses\non problem understanding and decomposition at the semantic and logical levels,\nrather than specific computations, which can effectively improve the SLMs'\ngeneralization and reasoning abilities. With only a small amount of SG training\ndata, SGFT can fine-tune a SLM to produce accurate problem-solving guidances,\nwhich can then be flexibly fed to any SLM as prompts, enabling it to generate\ncorrect answers directly. Experimental results demonstrate that our method\nsignificantly improves the performance of SLMs on various reasoning tasks,\nenhancing both their practicality and efficiency within resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Jing Bi"
                    },
                    {
                        "name": "Yuting Wu"
                    },
                    {
                        "name": "Weiwei Xing"
                    },
                    {
                        "name": "Zhenjie Wei"
                    }
                ],
                "author_detail": {
                    "name": "Zhenjie Wei"
                },
                "author": "Zhenjie Wei",
                "arxiv_comment": "11 pages, 4 figures, to be published in The 31st International\n  Conference on Computational Linguistics (COLING 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09906v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09906v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08038v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08038v2",
                "updated": "2024-12-13T06:39:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    39,
                    0,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-11T02:37:32Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    2,
                    37,
                    32,
                    2,
                    346,
                    0
                ],
                "title": "Bootstrapping Heterogeneous Graph Representation Learning via Large\n  Language Models: A Generalized Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrapping Heterogeneous Graph Representation Learning via Large\n  Language Models: A Generalized Approach"
                },
                "summary": "Graph representation learning methods are highly effective in handling\ncomplex non-Euclidean data by capturing intricate relationships and features\nwithin graph structures. However, traditional methods face challenges when\ndealing with heterogeneous graphs that contain various types of nodes and edges\ndue to the diverse sources and complex nature of the data. Existing\nHeterogeneous Graph Neural Networks (HGNNs) have shown promising results but\nrequire prior knowledge of node and edge types and unified node feature\nformats, which limits their applicability. Recent advancements in graph\nrepresentation learning using Large Language Models (LLMs) offer new solutions\nby integrating LLMs' data processing capabilities, enabling the alignment of\nvarious graph representations. Nevertheless, these methods often overlook\nheterogeneous graph data and require extensive preprocessing. To address these\nlimitations, we propose a novel method that leverages the strengths of both LLM\nand GNN, allowing for the processing of graph data with any format and type of\nnodes and edges without the need for type information or special preprocessing.\nOur method employs LLM to automatically summarize and classify different data\nformats and types, aligns node features, and uses a specialized GNN for\ntargeted learning, thus obtaining effective graph representations for\ndownstream tasks. Theoretical analysis and experimental validation have\ndemonstrated the effectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph representation learning methods are highly effective in handling\ncomplex non-Euclidean data by capturing intricate relationships and features\nwithin graph structures. However, traditional methods face challenges when\ndealing with heterogeneous graphs that contain various types of nodes and edges\ndue to the diverse sources and complex nature of the data. Existing\nHeterogeneous Graph Neural Networks (HGNNs) have shown promising results but\nrequire prior knowledge of node and edge types and unified node feature\nformats, which limits their applicability. Recent advancements in graph\nrepresentation learning using Large Language Models (LLMs) offer new solutions\nby integrating LLMs' data processing capabilities, enabling the alignment of\nvarious graph representations. Nevertheless, these methods often overlook\nheterogeneous graph data and require extensive preprocessing. To address these\nlimitations, we propose a novel method that leverages the strengths of both LLM\nand GNN, allowing for the processing of graph data with any format and type of\nnodes and edges without the need for type information or special preprocessing.\nOur method employs LLM to automatically summarize and classify different data\nformats and types, aligns node features, and uses a specialized GNN for\ntargeted learning, thus obtaining effective graph representations for\ndownstream tasks. Theoretical analysis and experimental validation have\ndemonstrated the effectiveness of our method."
                },
                "authors": [
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Chenhao Zhang"
                    },
                    {
                        "name": "Fengge Wu"
                    },
                    {
                        "name": "Junsuo Zhao"
                    },
                    {
                        "name": "Changwen Zheng"
                    },
                    {
                        "name": "Huaping Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huaping Liu"
                },
                "author": "Huaping Liu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08038v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10952v2",
                "updated": "2024-12-13T06:24:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    24,
                    15,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-17T03:37:16Z",
                "published_parsed": [
                    2024,
                    11,
                    17,
                    3,
                    37,
                    16,
                    6,
                    322,
                    0
                ],
                "title": "To Adopt or Not to Adopt L4S-Compatible Congestion Control?\n  Understanding Performance in a Partial L4S Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To Adopt or Not to Adopt L4S-Compatible Congestion Control?\n  Understanding Performance in a Partial L4S Deployment"
                },
                "summary": "With few exceptions, the path to deployment for any Internet technology\nrequires that there be some benefit to unilateral adoption of the new\ntechnology. In an Internet where the technology is not fully deployed, is an\nindividual better off sticking to the status quo, or adopting the new\ntechnology? This question is especially relevant in the context of the Low\nLatency, Low Loss, Scalable Throughput (L4S) architecture, where the full\nbenefit is realized only when compatible protocols (scalable congestion\ncontrol, accurate ECN, and flow isolation at queues) are adopted at both\nendpoints of a connection and also at the bottleneck router. In this paper, we\nconsider the perspective of the sender of an L4S flow using scalable congestion\ncontrol, without knowing whether the bottleneck router uses an L4S queue, or\nwhether other flows sharing the bottleneck queue are also using scalable\ncongestion control. We show that whether the sender uses TCP Prague or BBRv2 as\nthe scalable congestion control, it cannot be assured that it will not harm or\nbe harmed by another flow sharing the bottleneck link. We further show that the\nharm is not necessarily mitigated when a scalable flow shares a bottleneck with\nmultiple classic flows. Finally, we evaluate the approach of BBRv3, where\nscalable congestion control is used only when the path delay is small, with ECN\nfeedback ignored otherwise, and show that it does not solve the coexistence\nproblem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With few exceptions, the path to deployment for any Internet technology\nrequires that there be some benefit to unilateral adoption of the new\ntechnology. In an Internet where the technology is not fully deployed, is an\nindividual better off sticking to the status quo, or adopting the new\ntechnology? This question is especially relevant in the context of the Low\nLatency, Low Loss, Scalable Throughput (L4S) architecture, where the full\nbenefit is realized only when compatible protocols (scalable congestion\ncontrol, accurate ECN, and flow isolation at queues) are adopted at both\nendpoints of a connection and also at the bottleneck router. In this paper, we\nconsider the perspective of the sender of an L4S flow using scalable congestion\ncontrol, without knowing whether the bottleneck router uses an L4S queue, or\nwhether other flows sharing the bottleneck queue are also using scalable\ncongestion control. We show that whether the sender uses TCP Prague or BBRv2 as\nthe scalable congestion control, it cannot be assured that it will not harm or\nbe harmed by another flow sharing the bottleneck link. We further show that the\nharm is not necessarily mitigated when a scalable flow shares a bottleneck with\nmultiple classic flows. Finally, we evaluate the approach of BBRv3, where\nscalable congestion control is used only when the path delay is small, with ECN\nfeedback ignored otherwise, and show that it does not solve the coexistence\nproblem."
                },
                "authors": [
                    {
                        "name": "Fatih Berkay Sarpkaya"
                    },
                    {
                        "name": "Fraida Fund"
                    },
                    {
                        "name": "Shivendra Panwar"
                    }
                ],
                "author_detail": {
                    "name": "Shivendra Panwar"
                },
                "author": "Shivendra Panwar",
                "arxiv_comment": "Accepted to Passive and Active Measurement (PAM) International\n  Conference, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01129v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01129v5",
                "updated": "2024-12-13T06:16:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    16,
                    6,
                    4,
                    348,
                    0
                ],
                "published": "2024-08-02T09:18:41Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    9,
                    18,
                    41,
                    4,
                    215,
                    0
                ],
                "title": "A Survey of Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Mamba"
                },
                "summary": "As one of the most representative DL techniques, Transformer architecture has\nempowered numerous advanced models, especially the large language models (LLMs)\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space models\n(SSMs), has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering three main aspects:\nthe advancements of Mamba-based models, the techniques of adapting Mamba to\ndiverse data, and the applications where Mamba can excel. Specifically, we\nfirst review the foundational knowledge of various representative deep learning\nmodels and the details of Mamba-1&2 as preliminaries. Then, to showcase the\nsignificance of Mamba for AI, we comprehensively review the related studies\nfocusing on Mamba models' architecture design, data adaptability, and\napplications. Finally, we present a discussion of current limitations and\nexplore various promising research directions to provide deeper insights for\nfuture investigations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As one of the most representative DL techniques, Transformer architecture has\nempowered numerous advanced models, especially the large language models (LLMs)\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space models\n(SSMs), has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering three main aspects:\nthe advancements of Mamba-based models, the techniques of adapting Mamba to\ndiverse data, and the applications where Mamba can excel. Specifically, we\nfirst review the foundational knowledge of various representative deep learning\nmodels and the details of Mamba-1&2 as preliminaries. Then, to showcase the\nsignificance of Mamba for AI, we comprehensively review the related studies\nfocusing on Mamba models' architecture design, data adaptability, and\napplications. Finally, we present a discussion of current limitations and\nexplore various promising research directions to provide deeper insights for\nfuture investigations."
                },
                "authors": [
                    {
                        "name": "Haohao Qu"
                    },
                    {
                        "name": "Liangbo Ning"
                    },
                    {
                        "name": "Rui An"
                    },
                    {
                        "name": "Wenqi Fan"
                    },
                    {
                        "name": "Tyler Derr"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01129v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01129v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03937v2",
                "updated": "2024-12-13T06:15:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    15,
                    54,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-05T07:35:19Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    7,
                    35,
                    19,
                    3,
                    340,
                    0
                ],
                "title": "AIpparel: A Large Multimodal Generative Model for Digital Garments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AIpparel: A Large Multimodal Generative Model for Digital Garments"
                },
                "summary": "Apparel is essential to human life, offering protection, mirroring cultural\nidentities, and showcasing personal style. Yet, the creation of garments\nremains a time-consuming process, largely due to the manual work involved in\ndesigning them. To simplify this process, we introduce AIpparel, a large\nmultimodal model for generating and editing sewing patterns. Our model\nfine-tunes state-of-the-art large multimodal models (LMMs) on a custom-curated\nlarge-scale dataset of over 120,000 unique garments, each with multimodal\nannotations including text, images, and sewing patterns. Additionally, we\npropose a novel tokenization scheme that concisely encodes these complex sewing\npatterns so that LLMs can learn to predict them efficiently. AIpparelachieves\nstate-of-the-art performance in single-modal tasks, including text-to-garment\nand image-to-garment prediction, and enables novel multimodal garment\ngeneration applications such as interactive garment editing. The project\nwebsite is at georgenakayama.github.io/AIpparel/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apparel is essential to human life, offering protection, mirroring cultural\nidentities, and showcasing personal style. Yet, the creation of garments\nremains a time-consuming process, largely due to the manual work involved in\ndesigning them. To simplify this process, we introduce AIpparel, a large\nmultimodal model for generating and editing sewing patterns. Our model\nfine-tunes state-of-the-art large multimodal models (LMMs) on a custom-curated\nlarge-scale dataset of over 120,000 unique garments, each with multimodal\nannotations including text, images, and sewing patterns. Additionally, we\npropose a novel tokenization scheme that concisely encodes these complex sewing\npatterns so that LLMs can learn to predict them efficiently. AIpparelachieves\nstate-of-the-art performance in single-modal tasks, including text-to-garment\nand image-to-garment prediction, and enables novel multimodal garment\ngeneration applications such as interactive garment editing. The project\nwebsite is at georgenakayama.github.io/AIpparel/."
                },
                "authors": [
                    {
                        "name": "Kiyohiro Nakayama"
                    },
                    {
                        "name": "Jan Ackermann"
                    },
                    {
                        "name": "Timur Levent Kesdogan"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Maria Korosteleva"
                    },
                    {
                        "name": "Olga Sorkine-Hornung"
                    },
                    {
                        "name": "Leonidas J. Guibas"
                    },
                    {
                        "name": "Guandao Yang"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    }
                ],
                "author_detail": {
                    "name": "Gordon Wetzstein"
                },
                "author": "Gordon Wetzstein",
                "arxiv_comment": "The project website is at georgenakayama.github.io/AIpparel/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00627v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00627v4",
                "updated": "2024-12-13T06:13:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    13,
                    8,
                    4,
                    348,
                    0
                ],
                "published": "2024-06-02T06:09:56Z",
                "published_parsed": [
                    2024,
                    6,
                    2,
                    6,
                    9,
                    56,
                    6,
                    154,
                    0
                ],
                "title": "Role-playing Prompt Framework: Generation and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-playing Prompt Framework: Generation and Evaluation"
                },
                "summary": "Large language models (LLMs) exhibit impressive proficiency in natural\nlanguage generation, understanding user instructions, and emulating human-like\nlanguage use, which has led to significant interest in their application to\nrole-playing scenarios. However, the manual collection of role-specific script\ndata and the evaluation of model performance are resource-intensive processes.\nThis paper introduces a prompt-based framework designed to leverage GPT's\ncapabilities for the generation of role-playing dialogue datasets and the\nevaluation of role-playing performance. To validate the effectiveness of the\nGPT-based generation and evaluation, we further incorporate the recall-oriented\nRouge-L metric, providing an additional quantitative measure of performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit impressive proficiency in natural\nlanguage generation, understanding user instructions, and emulating human-like\nlanguage use, which has led to significant interest in their application to\nrole-playing scenarios. However, the manual collection of role-specific script\ndata and the evaluation of model performance are resource-intensive processes.\nThis paper introduces a prompt-based framework designed to leverage GPT's\ncapabilities for the generation of role-playing dialogue datasets and the\nevaluation of role-playing performance. To validate the effectiveness of the\nGPT-based generation and evaluation, we further incorporate the recall-oriented\nRouge-L metric, providing an additional quantitative measure of performance."
                },
                "authors": [
                    {
                        "name": "Xun Liu"
                    },
                    {
                        "name": "Zhengwei Ni"
                    }
                ],
                "author_detail": {
                    "name": "Zhengwei Ni"
                },
                "author": "Zhengwei Ni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00627v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00627v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09884v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09884v1",
                "updated": "2024-12-13T05:52:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    5,
                    52,
                    37,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T05:52:37Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    5,
                    52,
                    37,
                    4,
                    348,
                    0
                ],
                "title": "Benchmarking Table Comprehension In The Wild",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Table Comprehension In The Wild"
                },
                "summary": "Large Language Models (LLMs), while being increasingly dominant on a myriad\nof knowledge-intensive activities, have only had limited success understanding\nlengthy table-text mixtures, such as academic papers and financial reports.\nRecent advances of long-context LLMs have opened up new possibilities for this\nfield. Nonetheless, we identify two roadblocks: (1) Prior benchmarks of table\nquestion answering (TableQA) have focused on isolated tables without context,\nmaking it hard to evaluate models in real-world scenarios. (2) Prior benchmarks\nhave focused on some narrow skill sets of table comprehension such as table\nrecognition, data manipulation/calculation, table summarization etc., while a\nskilled human employs those skills collectively. In this work, we introduce\nTableQuest, a new benchmark designed to evaluate the holistic table\ncomprehension capabilities of LLMs in the natural table-rich context of\nfinancial reports. We employ a rigorous data processing and filtering procedure\nto ensure that the question-answer pairs are logical, reasonable, and diverse.\nWe experiment with 7 state-of-the-art models, and find that despite reasonable\naccuracy in locating facts, they often falter when required to execute more\nsophisticated reasoning or multi-step calculations. We conclude with a\nqualitative study of the failure modes and discuss the challenges of\nconstructing a challenging benchmark. We make the evaluation data, judging\nprocedure and results of this study publicly available to facilitate research\nin this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), while being increasingly dominant on a myriad\nof knowledge-intensive activities, have only had limited success understanding\nlengthy table-text mixtures, such as academic papers and financial reports.\nRecent advances of long-context LLMs have opened up new possibilities for this\nfield. Nonetheless, we identify two roadblocks: (1) Prior benchmarks of table\nquestion answering (TableQA) have focused on isolated tables without context,\nmaking it hard to evaluate models in real-world scenarios. (2) Prior benchmarks\nhave focused on some narrow skill sets of table comprehension such as table\nrecognition, data manipulation/calculation, table summarization etc., while a\nskilled human employs those skills collectively. In this work, we introduce\nTableQuest, a new benchmark designed to evaluate the holistic table\ncomprehension capabilities of LLMs in the natural table-rich context of\nfinancial reports. We employ a rigorous data processing and filtering procedure\nto ensure that the question-answer pairs are logical, reasonable, and diverse.\nWe experiment with 7 state-of-the-art models, and find that despite reasonable\naccuracy in locating facts, they often falter when required to execute more\nsophisticated reasoning or multi-step calculations. We conclude with a\nqualitative study of the failure modes and discuss the challenges of\nconstructing a challenging benchmark. We make the evaluation data, judging\nprocedure and results of this study publicly available to facilitate research\nin this field."
                },
                "authors": [
                    {
                        "name": "Yikang Pan"
                    },
                    {
                        "name": "Yi Zhu"
                    },
                    {
                        "name": "Rand Xie"
                    },
                    {
                        "name": "Yizhi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yizhi Liu"
                },
                "author": "Yizhi Liu",
                "arxiv_comment": "Accepted at TRL Workshop@Neurips 2024. Link to data\n  https://github.com/boson-ai/Table_eval_public",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09884v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09884v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09879v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09879v1",
                "updated": "2024-12-13T05:50:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    5,
                    50,
                    22,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T05:50:22Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    5,
                    50,
                    22,
                    4,
                    348,
                    0
                ],
                "title": "On the Limit of Language Models as Planning Formalizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Limit of Language Models as Planning Formalizers"
                },
                "summary": "Large Language Models have been shown to fail to create executable and\nverifiable plans in grounded environments. An emerging line of work shows\nsuccess in using LLM as a formalizer to generate a formal representation (e.g.,\nPDDL) of the planning domain, which can be deterministically solved to find a\nplan. We systematically evaluate this methodology while bridging some major\ngaps. While previous work only generates a partial PDDL representation given\ntemplated and thus unrealistic environment descriptions, we generate the\ncomplete representation given descriptions of various naturalness levels. Among\nan array of observations critical to improve LLMs' formal planning ability, we\nnote that large enough models can effectively formalize descriptions as PDDL,\noutperforming those directly generating plans, while being robust to lexical\nperturbation. As the descriptions become more natural-sounding, we observe a\ndecrease in performance and provide detailed error analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have been shown to fail to create executable and\nverifiable plans in grounded environments. An emerging line of work shows\nsuccess in using LLM as a formalizer to generate a formal representation (e.g.,\nPDDL) of the planning domain, which can be deterministically solved to find a\nplan. We systematically evaluate this methodology while bridging some major\ngaps. While previous work only generates a partial PDDL representation given\ntemplated and thus unrealistic environment descriptions, we generate the\ncomplete representation given descriptions of various naturalness levels. Among\nan array of observations critical to improve LLMs' formal planning ability, we\nnote that large enough models can effectively formalize descriptions as PDDL,\noutperforming those directly generating plans, while being robust to lexical\nperturbation. As the descriptions become more natural-sounding, we observe a\ndecrease in performance and provide detailed error analysis."
                },
                "authors": [
                    {
                        "name": "Cassie Huang"
                    },
                    {
                        "name": "Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Li Zhang"
                },
                "author": "Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09879v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09879v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08299v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08299v3",
                "updated": "2024-12-13T05:48:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    5,
                    48,
                    45,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-13T02:41:02Z",
                "published_parsed": [
                    2024,
                    11,
                    13,
                    2,
                    41,
                    2,
                    2,
                    318,
                    0
                ],
                "title": "DNN Task Assignment in UAV Networks: A Generative AI Enhanced\n  Multi-Agent Reinforcement Learning Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNN Task Assignment in UAV Networks: A Generative AI Enhanced\n  Multi-Agent Reinforcement Learning Approach"
                },
                "summary": "Unmanned Aerial Vehicles (UAVs) possess high mobility and flexible deployment\ncapabilities, prompting the development of UAVs for various application\nscenarios within the Internet of Things (IoT). The unique capabilities of UAVs\ngive rise to increasingly critical and complex tasks in uncertain and\npotentially harsh environments. The substantial amount of data generated from\nthese applications necessitates processing and analysis through deep neural\nnetworks (DNNs). However, UAVs encounter challenges due to their limited\ncomputing resources when managing DNN models. This paper presents a joint\napproach that combines multiple-agent reinforcement learning (MARL) and\ngenerative diffusion models (GDM) for assigning DNN tasks to a UAV swarm, aimed\nat reducing latency from task capture to result output. To address these\nchallenges, we first consider the task size of the target area to be inspected\nand the shortest flying path as optimization constraints, employing a greedy\nalgorithm to resolve the subproblem with a focus on minimizing the UAV's flying\npath and the overall system cost. In the second stage, we introduce a novel DNN\ntask assignment algorithm, termed GDM-MADDPG, which utilizes the reverse\ndenoising process of GDM to replace the actor network in multi-agent deep\ndeterministic policy gradient (MADDPG). This approach generates specific DNN\ntask assignment actions based on agents' observations in a dynamic environment.\nSimulation results indicate that our algorithm performs favorably compared to\nbenchmarks in terms of path planning, Age of Information (AoI), energy\nconsumption, and task load balancing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Aerial Vehicles (UAVs) possess high mobility and flexible deployment\ncapabilities, prompting the development of UAVs for various application\nscenarios within the Internet of Things (IoT). The unique capabilities of UAVs\ngive rise to increasingly critical and complex tasks in uncertain and\npotentially harsh environments. The substantial amount of data generated from\nthese applications necessitates processing and analysis through deep neural\nnetworks (DNNs). However, UAVs encounter challenges due to their limited\ncomputing resources when managing DNN models. This paper presents a joint\napproach that combines multiple-agent reinforcement learning (MARL) and\ngenerative diffusion models (GDM) for assigning DNN tasks to a UAV swarm, aimed\nat reducing latency from task capture to result output. To address these\nchallenges, we first consider the task size of the target area to be inspected\nand the shortest flying path as optimization constraints, employing a greedy\nalgorithm to resolve the subproblem with a focus on minimizing the UAV's flying\npath and the overall system cost. In the second stage, we introduce a novel DNN\ntask assignment algorithm, termed GDM-MADDPG, which utilizes the reverse\ndenoising process of GDM to replace the actor network in multi-agent deep\ndeterministic policy gradient (MADDPG). This approach generates specific DNN\ntask assignment actions based on agents' observations in a dynamic environment.\nSimulation results indicate that our algorithm performs favorably compared to\nbenchmarks in terms of path planning, Age of Information (AoI), energy\nconsumption, and task load balancing."
                },
                "authors": [
                    {
                        "name": "Xin Tang"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Wenjie Weng"
                    },
                    {
                        "name": "Binhan Liao"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Xianbin Cao"
                    },
                    {
                        "name": "Xiaohuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohuan Li"
                },
                "author": "Xiaohuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08299v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08299v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09871v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09871v1",
                "updated": "2024-12-13T05:33:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    5,
                    33,
                    32,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T05:33:32Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    5,
                    33,
                    32,
                    4,
                    348,
                    0
                ],
                "title": "Byte Latent Transformer: Patches Scale Better Than Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Byte Latent Transformer: Patches Scale Better Than Tokens"
                },
                "summary": "We introduce the Byte Latent Transformer (BLT), a new byte-level LLM\narchitecture that, for the first time, matches tokenization-based LLM\nperformance at scale with significant improvements in inference efficiency and\nrobustness. BLT encodes bytes into dynamically sized patches, which serve as\nthe primary units of computation. Patches are segmented based on the entropy of\nthe next byte, allocating more compute and model capacity where increased data\ncomplexity demands it. We present the first FLOP controlled scaling study of\nbyte-level models up to 8B parameters and 4T training bytes. Our results\ndemonstrate the feasibility of scaling models trained on raw bytes without a\nfixed vocabulary. Both training and inference efficiency improve due to\ndynamically selecting long patches when data is predictable, along with\nqualitative improvements on reasoning and long tail generalization. Overall,\nfor fixed inference costs, BLT shows significantly better scaling than\ntokenization-based models, by simultaneously growing both patch and model size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Byte Latent Transformer (BLT), a new byte-level LLM\narchitecture that, for the first time, matches tokenization-based LLM\nperformance at scale with significant improvements in inference efficiency and\nrobustness. BLT encodes bytes into dynamically sized patches, which serve as\nthe primary units of computation. Patches are segmented based on the entropy of\nthe next byte, allocating more compute and model capacity where increased data\ncomplexity demands it. We present the first FLOP controlled scaling study of\nbyte-level models up to 8B parameters and 4T training bytes. Our results\ndemonstrate the feasibility of scaling models trained on raw bytes without a\nfixed vocabulary. Both training and inference efficiency improve due to\ndynamically selecting long patches when data is predictable, along with\nqualitative improvements on reasoning and long tail generalization. Overall,\nfor fixed inference costs, BLT shows significantly better scaling than\ntokenization-based models, by simultaneously growing both patch and model size."
                },
                "authors": [
                    {
                        "name": "Artidoro Pagnoni"
                    },
                    {
                        "name": "Ram Pasunuru"
                    },
                    {
                        "name": "Pedro Rodriguez"
                    },
                    {
                        "name": "John Nguyen"
                    },
                    {
                        "name": "Benjamin Muller"
                    },
                    {
                        "name": "Margaret Li"
                    },
                    {
                        "name": "Chunting Zhou"
                    },
                    {
                        "name": "Lili Yu"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Gargi Ghosh"
                    },
                    {
                        "name": "Mike Lewis"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Srinivasan Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Srinivasan Iyer"
                },
                "author": "Srinivasan Iyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09871v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09871v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08967v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08967v3",
                "updated": "2024-12-13T04:44:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    4,
                    44,
                    11,
                    4,
                    348,
                    0
                ],
                "published": "2024-01-17T04:43:21Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    4,
                    43,
                    21,
                    2,
                    17,
                    0
                ],
                "title": "ReFT: Reasoning with Reinforced Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFT: Reasoning with Reinforced Fine-Tuning"
                },
                "summary": "One way to enhance the reasoning capability of Large Language Models (LLMs)\nis to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)\nannotations. This approach does not show sufficiently strong generalization\nability, however, because the training only relies on the given CoT data. In\nmath problem-solving, for example, there is usually only one annotated\nreasoning path for each question in the training data. Intuitively, it would be\nbetter for the algorithm to learn from multiple annotated reasoning paths given\na question. To address this issue, we propose a simple yet effective approach\ncalled Reinforced Fine-Tuning (ReFT) to enhance the generalizability of\nlearning LLMs for reasoning, with math problem-solving as an example. ReFT\nfirst warmups the model with SFT, and then employs on-line reinforcement\nlearning, specifically the PPO algorithm in this paper, to further fine-tune\nthe model, where an abundance of reasoning paths are automatically sampled\ngiven the question and the rewards are naturally derived from the ground-truth\nanswers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that\nReFT significantly outperforms SFT, and the performance can be potentially\nfurther boosted by combining inference-time strategies such as majority voting\nand re-ranking. Note that ReFT obtains the improvement by learning from the\nsame training questions as SFT, without relying on extra or augmented training\nquestions. This indicates a superior generalization ability for ReFT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One way to enhance the reasoning capability of Large Language Models (LLMs)\nis to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)\nannotations. This approach does not show sufficiently strong generalization\nability, however, because the training only relies on the given CoT data. In\nmath problem-solving, for example, there is usually only one annotated\nreasoning path for each question in the training data. Intuitively, it would be\nbetter for the algorithm to learn from multiple annotated reasoning paths given\na question. To address this issue, we propose a simple yet effective approach\ncalled Reinforced Fine-Tuning (ReFT) to enhance the generalizability of\nlearning LLMs for reasoning, with math problem-solving as an example. ReFT\nfirst warmups the model with SFT, and then employs on-line reinforcement\nlearning, specifically the PPO algorithm in this paper, to further fine-tune\nthe model, where an abundance of reasoning paths are automatically sampled\ngiven the question and the rewards are naturally derived from the ground-truth\nanswers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that\nReFT significantly outperforms SFT, and the performance can be potentially\nfurther boosted by combining inference-time strategies such as majority voting\nand re-ranking. Note that ReFT obtains the improvement by learning from the\nsame training questions as SFT, without relying on extra or augmented training\nquestions. This indicates a superior generalization ability for ReFT."
                },
                "authors": [
                    {
                        "name": "Trung Quoc Luong"
                    },
                    {
                        "name": "Xinbo Zhang"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Xiaoran Jin"
                    },
                    {
                        "name": "Hang Li"
                    }
                ],
                "author_detail": {
                    "name": "Hang Li"
                },
                "author": "Hang Li",
                "arxiv_comment": "ACL 2024 main conference; adjust with reviewer comments; 13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08967v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08967v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12196v3",
                "updated": "2024-12-13T04:41:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    4,
                    41,
                    50,
                    4,
                    348,
                    0
                ],
                "published": "2024-03-18T19:10:12Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    19,
                    10,
                    12,
                    0,
                    78,
                    0
                ],
                "title": "Leveraging Large Language Models to Detect npm Malicious Packages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Large Language Models to Detect npm Malicious Packages"
                },
                "summary": "Existing malicious code detection techniques demand the integration of\nmultiple tools to detect different malware patterns, often suffering from high\nmisclassification rates. Therefore, malicious code detection techniques could\nbe enhanced by adopting advanced, more automated approaches to achieve high\naccuracy and a low misclassification rate. The goal of this study is to aid\nsecurity analysts in detecting malicious packages by empirically studying the\neffectiveness of Large Language Models (LLMs) in detecting malicious code. We\npresent SocketAI, a malicious code review workflow to detect malicious code. To\nevaluate the effectiveness of SocketAI, we leverage a benchmark dataset of\n5,115 npm packages, of which 2,180 packages have malicious code. We conducted a\nbaseline comparison of GPT-3 and GPT-4 models with the state-of-the-art CodeQL\nstatic analysis tool, using 39 custom CodeQL rules developed in prior research\nto detect malicious Javascript code. We also compare the effectiveness of\nstatic analysis as a pre-screener with SocketAI workflow, measuring the number\nof files that need to be analyzed. and the associated costs. Additionally, we\nperformed a qualitative study to understand the types of malicious activities\ndetected or missed by our workflow. Our baseline comparison demonstrates a 16%\nand 9% improvement over static analysis in precision and F1 scores,\nrespectively. GPT-4 achieves higher accuracy with 99% precision and 97% F1\nscores, while GPT-3 offers a more cost-effective balance at 91% precision and\n94% F1 scores. Pre-screening files with a static analyzer reduces the number of\nfiles requiring LLM analysis by 77.9% and decreases costs by 60.9% for GPT-3\nand 76.1% for GPT-4. Our qualitative analysis identified data theft, suspicious\ndomain connection, and arbitrary code execution as the top detected malicious\nactivities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing malicious code detection techniques demand the integration of\nmultiple tools to detect different malware patterns, often suffering from high\nmisclassification rates. Therefore, malicious code detection techniques could\nbe enhanced by adopting advanced, more automated approaches to achieve high\naccuracy and a low misclassification rate. The goal of this study is to aid\nsecurity analysts in detecting malicious packages by empirically studying the\neffectiveness of Large Language Models (LLMs) in detecting malicious code. We\npresent SocketAI, a malicious code review workflow to detect malicious code. To\nevaluate the effectiveness of SocketAI, we leverage a benchmark dataset of\n5,115 npm packages, of which 2,180 packages have malicious code. We conducted a\nbaseline comparison of GPT-3 and GPT-4 models with the state-of-the-art CodeQL\nstatic analysis tool, using 39 custom CodeQL rules developed in prior research\nto detect malicious Javascript code. We also compare the effectiveness of\nstatic analysis as a pre-screener with SocketAI workflow, measuring the number\nof files that need to be analyzed. and the associated costs. Additionally, we\nperformed a qualitative study to understand the types of malicious activities\ndetected or missed by our workflow. Our baseline comparison demonstrates a 16%\nand 9% improvement over static analysis in precision and F1 scores,\nrespectively. GPT-4 achieves higher accuracy with 99% precision and 97% F1\nscores, while GPT-3 offers a more cost-effective balance at 91% precision and\n94% F1 scores. Pre-screening files with a static analyzer reduces the number of\nfiles requiring LLM analysis by 77.9% and decreases costs by 60.9% for GPT-3\nand 76.1% for GPT-4. Our qualitative analysis identified data theft, suspicious\ndomain connection, and arbitrary code execution as the top detected malicious\nactivities."
                },
                "authors": [
                    {
                        "name": "Nusrat Zahan"
                    },
                    {
                        "name": "Philipp Burckhardt"
                    },
                    {
                        "name": "Mikola Lysenko"
                    },
                    {
                        "name": "Feross Aboukhadijeh"
                    },
                    {
                        "name": "Laurie Williams"
                    }
                ],
                "author_detail": {
                    "name": "Laurie Williams"
                },
                "author": "Laurie Williams",
                "arxiv_comment": "13 pages, 2 Figure, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09844v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09844v1",
                "updated": "2024-12-13T04:27:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    4,
                    27,
                    8,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T04:27:08Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    4,
                    27,
                    8,
                    4,
                    348,
                    0
                ],
                "title": "Real-time Identity Defenses against Malicious Personalization of\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Identity Defenses against Malicious Personalization of\n  Diffusion Models"
                },
                "summary": "Personalized diffusion models, capable of synthesizing highly realistic\nimages based on a few reference portraits, pose substantial social, ethical,\nand legal risks by enabling identity replication. Existing defense mechanisms\nrely on computationally intensive adversarial perturbations tailored to\nindividual images, rendering them impractical for real-world deployment. This\nstudy introduces Real-time Identity Defender (RID), a neural network designed\nto generate adversarial perturbations through a single forward pass, bypassing\nthe need for image-specific optimization. RID achieves unprecedented\nefficiency, with defense times as low as 0.12 seconds on a single GPU (4,400\ntimes faster than leading methods) and 1.1 seconds per image on a standard\nIntel i9 CPU, making it suitable for edge devices such as smartphones. Despite\nits efficiency, RID matches state-of-the-art performance across visual and\nquantitative benchmarks, effectively mitigating identity replication risks. Our\nanalysis reveals that RID's perturbations mimic the efficacy of traditional\ndefenses while exhibiting properties distinct from natural noise, such as\nGaussian perturbations. To enhance robustness, we extend RID into an ensemble\nframework that integrates multiple pre-trained text-to-image diffusion models,\nensuring resilience against black-box attacks and post-processing techniques,\nincluding JPEG compression and diffusion-based purification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized diffusion models, capable of synthesizing highly realistic\nimages based on a few reference portraits, pose substantial social, ethical,\nand legal risks by enabling identity replication. Existing defense mechanisms\nrely on computationally intensive adversarial perturbations tailored to\nindividual images, rendering them impractical for real-world deployment. This\nstudy introduces Real-time Identity Defender (RID), a neural network designed\nto generate adversarial perturbations through a single forward pass, bypassing\nthe need for image-specific optimization. RID achieves unprecedented\nefficiency, with defense times as low as 0.12 seconds on a single GPU (4,400\ntimes faster than leading methods) and 1.1 seconds per image on a standard\nIntel i9 CPU, making it suitable for edge devices such as smartphones. Despite\nits efficiency, RID matches state-of-the-art performance across visual and\nquantitative benchmarks, effectively mitigating identity replication risks. Our\nanalysis reveals that RID's perturbations mimic the efficacy of traditional\ndefenses while exhibiting properties distinct from natural noise, such as\nGaussian perturbations. To enhance robustness, we extend RID into an ensemble\nframework that integrates multiple pre-trained text-to-image diffusion models,\nensuring resilience against black-box attacks and post-processing techniques,\nincluding JPEG compression and diffusion-based purification."
                },
                "authors": [
                    {
                        "name": "Hanzhong Guo"
                    },
                    {
                        "name": "Shen Nie"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Tianyu Pang"
                    },
                    {
                        "name": "Hao Sun"
                    },
                    {
                        "name": "Chongxuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongxuan Li"
                },
                "author": "Chongxuan Li",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09844v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09844v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09840v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09840v1",
                "updated": "2024-12-13T04:19:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    4,
                    19,
                    28,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T04:19:28Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    4,
                    19,
                    28,
                    4,
                    348,
                    0
                ],
                "title": "LAVA: Lifetime-Aware VM Allocation with Learned Distributions and\n  Adaptation to Mispredictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAVA: Lifetime-Aware VM Allocation with Learned Distributions and\n  Adaptation to Mispredictions"
                },
                "summary": "Scheduling virtual machines (VMs) to hosts in cloud data centers dictates\nefficiency and is an NP-hard problem with incomplete information. Prior work\nimproved VM scheduling with predicted VM lifetimes. Our work further improves\nlifetime-aware scheduling using repredictions with lifetime distributions vs.\none-shot prediction. The approach repredicts and adjusts VM and host lifetimes\nwhen incorrect predictions emerge. We also present novel approaches for\ndefragmentation and regular system maintenance, which are essential to our data\ncenter reliability and optimizations, and are unexplored in prior work. We show\nthat repredictions deliver a fundamental advance in effectiveness over one-shot\nprediction.\n  We call our novel combination of distribution-based lifetime predictions and\nscheduling algorithms Lifetime Aware VM Allocation (LAVA). LAVA improves\nresource stranding and the number of empty hosts, which are critical for large\nVM scheduling, cloud system updates, and reducing dynamic energy consumption.\nOur approach runs in production within Google's hyperscale cloud data centers,\nwhere it improves efficiency by decreasing stranded compute and memory\nresources by ~3% and ~2% respectively, and increases availability for large VMs\nand cloud system updates by increasing empty hosts by 2.3-9.2 pp in production.\nWe also show a reduction in VM migrations for host defragmentation and\nmaintenance. In addition to our fleet-wide production deployment, we perform\nsimulation studies to characterize the design space and show that our algorithm\nsignificantly outperforms the state of the art lifetime-based scheduling\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scheduling virtual machines (VMs) to hosts in cloud data centers dictates\nefficiency and is an NP-hard problem with incomplete information. Prior work\nimproved VM scheduling with predicted VM lifetimes. Our work further improves\nlifetime-aware scheduling using repredictions with lifetime distributions vs.\none-shot prediction. The approach repredicts and adjusts VM and host lifetimes\nwhen incorrect predictions emerge. We also present novel approaches for\ndefragmentation and regular system maintenance, which are essential to our data\ncenter reliability and optimizations, and are unexplored in prior work. We show\nthat repredictions deliver a fundamental advance in effectiveness over one-shot\nprediction.\n  We call our novel combination of distribution-based lifetime predictions and\nscheduling algorithms Lifetime Aware VM Allocation (LAVA). LAVA improves\nresource stranding and the number of empty hosts, which are critical for large\nVM scheduling, cloud system updates, and reducing dynamic energy consumption.\nOur approach runs in production within Google's hyperscale cloud data centers,\nwhere it improves efficiency by decreasing stranded compute and memory\nresources by ~3% and ~2% respectively, and increases availability for large VMs\nand cloud system updates by increasing empty hosts by 2.3-9.2 pp in production.\nWe also show a reduction in VM migrations for host defragmentation and\nmaintenance. In addition to our fleet-wide production deployment, we perform\nsimulation studies to characterize the design space and show that our algorithm\nsignificantly outperforms the state of the art lifetime-based scheduling\napproach."
                },
                "authors": [
                    {
                        "name": "Jianheng Ling"
                    },
                    {
                        "name": "Pratik Worah"
                    },
                    {
                        "name": "Yawen Wang"
                    },
                    {
                        "name": "Yunchuan Kong"
                    },
                    {
                        "name": "Chunlei Wang"
                    },
                    {
                        "name": "Clifford Stein"
                    },
                    {
                        "name": "Diwakar Gupta"
                    },
                    {
                        "name": "Jason Behmer"
                    },
                    {
                        "name": "Logan A. Bush"
                    },
                    {
                        "name": "Prakash Ramanan"
                    },
                    {
                        "name": "Rajesh Kumar"
                    },
                    {
                        "name": "Thomas Chestna"
                    },
                    {
                        "name": "Yajing Liu"
                    },
                    {
                        "name": "Ying Liu"
                    },
                    {
                        "name": "Ye Zhao"
                    },
                    {
                        "name": "Kathryn S. McKinley"
                    },
                    {
                        "name": "Meeyoung Park"
                    },
                    {
                        "name": "Martin Maas"
                    }
                ],
                "author_detail": {
                    "name": "Martin Maas"
                },
                "author": "Martin Maas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09840v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09840v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12325v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12325v3",
                "updated": "2024-12-13T04:03:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    4,
                    3,
                    7,
                    4,
                    348,
                    0
                ],
                "published": "2024-08-22T12:00:31Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    12,
                    0,
                    31,
                    3,
                    235,
                    0
                ],
                "title": "Improving Factuality in Large Language Models via Decoding-Time\n  Hallucinatory and Truthful Comparators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Factuality in Large Language Models via Decoding-Time\n  Hallucinatory and Truthful Comparators"
                },
                "summary": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone\nto generate responses that contradict verifiable facts, i.e., unfaithful\nhallucination content. Existing efforts generally focus on optimizing model\nparameters or editing semantic representations, which compromise the internal\nfactual knowledge of target LLMs. In addition, hallucinations typically exhibit\nmultifaceted patterns in downstream tasks, limiting the model's holistic\nperformance across tasks. In this paper, we propose a Comparator-driven\nDecoding-Time (CDT) framework to alleviate the response hallucination. Firstly,\nwe construct hallucinatory and truthful comparators with multi-task fine-tuning\nsamples. In this case, we present an instruction prototype-guided mixture of\nexperts strategy to enhance the ability of the corresponding comparators to\ncapture different hallucination or truthfulness patterns in distinct task\ninstructions. CDT constrains next-token predictions to factuality-robust\ndistributions by contrasting the logit differences between the target LLMs and\nthese comparators. Systematic experiments on multiple downstream tasks show\nthat our framework can significantly improve the model performance and response\nfactuality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite their remarkable capabilities, Large Language Models (LLMs) are prone\nto generate responses that contradict verifiable facts, i.e., unfaithful\nhallucination content. Existing efforts generally focus on optimizing model\nparameters or editing semantic representations, which compromise the internal\nfactual knowledge of target LLMs. In addition, hallucinations typically exhibit\nmultifaceted patterns in downstream tasks, limiting the model's holistic\nperformance across tasks. In this paper, we propose a Comparator-driven\nDecoding-Time (CDT) framework to alleviate the response hallucination. Firstly,\nwe construct hallucinatory and truthful comparators with multi-task fine-tuning\nsamples. In this case, we present an instruction prototype-guided mixture of\nexperts strategy to enhance the ability of the corresponding comparators to\ncapture different hallucination or truthfulness patterns in distinct task\ninstructions. CDT constrains next-token predictions to factuality-robust\ndistributions by contrasting the logit differences between the target LLMs and\nthese comparators. Systematic experiments on multiple downstream tasks show\nthat our framework can significantly improve the model performance and response\nfactuality."
                },
                "authors": [
                    {
                        "name": "Dingkang Yang"
                    },
                    {
                        "name": "Dongling Xiao"
                    },
                    {
                        "name": "Jinjie Wei"
                    },
                    {
                        "name": "Mingcheng Li"
                    },
                    {
                        "name": "Zhaoyu Chen"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Lihua Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lihua Zhang"
                },
                "author": "Lihua Zhang",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12325v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12325v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06724v2",
                "updated": "2024-12-13T03:43:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    3,
                    43,
                    35,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-09T18:13:27Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    13,
                    27,
                    0,
                    344,
                    0
                ],
                "title": "AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoDCWorkflow: LLM-based Data Cleaning Workflow Auto-Generation and\n  Benchmark"
                },
                "summary": "We investigate the reasoning capabilities of large language models (LLMs) for\nautomatically generating data-cleaning workflows. To evaluate LLMs' ability to\ncomplete data-cleaning tasks, we implemented a pipeline for LLM-based Auto Data\nCleaning Workflow (AutoDCWorkflow), prompting LLMs on data cleaning operations\nto repair three types of data quality issues: duplicates, missing values, and\ninconsistent data formats. Given a dirty table and a purpose (expressed as a\nquery), this pipeline generates a minimal, clean table sufficient to address\nthe purpose and the data cleaning workflow used to produce the table. The\nplanning process involves three main LLM-driven components: (1) Select Target\nColumns: Identifies a set of target columns related to the purpose. (2) Inspect\nColumn Quality: Assesses the data quality for each target column and generates\na Data Quality Report as operation objectives. (3) Generate Operation &\nArguments: Predicts the next operation and arguments based on the data quality\nreport results. Additionally, we propose a data cleaning benchmark to evaluate\nthe capability of LLM agents to automatically generate workflows that address\ndata cleaning purposes of varying difficulty levels. The benchmark comprises\nthe annotated datasets as a collection of purpose, raw table, clean table, data\ncleaning workflow, and answer set. In our experiments, we evaluated three LLMs\nthat auto-generate purpose-driven data cleaning workflows. The results indicate\nthat LLMs perform well in planning and generating data-cleaning workflows\nwithout the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the reasoning capabilities of large language models (LLMs) for\nautomatically generating data-cleaning workflows. To evaluate LLMs' ability to\ncomplete data-cleaning tasks, we implemented a pipeline for LLM-based Auto Data\nCleaning Workflow (AutoDCWorkflow), prompting LLMs on data cleaning operations\nto repair three types of data quality issues: duplicates, missing values, and\ninconsistent data formats. Given a dirty table and a purpose (expressed as a\nquery), this pipeline generates a minimal, clean table sufficient to address\nthe purpose and the data cleaning workflow used to produce the table. The\nplanning process involves three main LLM-driven components: (1) Select Target\nColumns: Identifies a set of target columns related to the purpose. (2) Inspect\nColumn Quality: Assesses the data quality for each target column and generates\na Data Quality Report as operation objectives. (3) Generate Operation &\nArguments: Predicts the next operation and arguments based on the data quality\nreport results. Additionally, we propose a data cleaning benchmark to evaluate\nthe capability of LLM agents to automatically generate workflows that address\ndata cleaning purposes of varying difficulty levels. The benchmark comprises\nthe annotated datasets as a collection of purpose, raw table, clean table, data\ncleaning workflow, and answer set. In our experiments, we evaluated three LLMs\nthat auto-generate purpose-driven data cleaning workflows. The results indicate\nthat LLMs perform well in planning and generating data-cleaning workflows\nwithout the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Lan Li"
                    },
                    {
                        "name": "Liri Fang"
                    },
                    {
                        "name": "Vetle I. Torvik"
                    }
                ],
                "author_detail": {
                    "name": "Vetle I. Torvik"
                },
                "author": "Vetle I. Torvik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09237v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09237v2",
                "updated": "2024-12-13T03:33:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    3,
                    33,
                    38,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-12T12:47:09Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    47,
                    9,
                    3,
                    347,
                    0
                ],
                "title": "LMAgent: A Large-scale Multimodal Agents Society for Multi-user\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMAgent: A Large-scale Multimodal Agents Society for Multi-user\n  Simulation"
                },
                "summary": "The believable simulation of multi-user behavior is crucial for understanding\ncomplex social systems. Recently, large language models (LLMs)-based AI agents\nhave made significant progress, enabling them to achieve human-like\nintelligence across various tasks. However, real human societies are often\ndynamic and complex, involving numerous individuals engaging in multimodal\ninteractions. In this paper, taking e-commerce scenarios as an example, we\npresent LMAgent, a very large-scale and multimodal agents society based on\nmultimodal LLMs. In LMAgent, besides freely chatting with friends, the agents\ncan autonomously browse, purchase, and review products, even perform live\nstreaming e-commerce. To simulate this complex system, we introduce a\nself-consistency prompting mechanism to augment agents' multimodal\ncapabilities, resulting in significantly improved decision-making performance\nover the existing multi-agent system. Moreover, we propose a fast memory\nmechanism combined with the small-world model to enhance system efficiency,\nwhich supports more than 10,000 agent simulations in a society. Experiments on\nagents' behavior show that these agents achieve comparable performance to\nhumans in behavioral indicators. Furthermore, compared with the existing\nLLMs-based multi-agent system, more different and valuable phenomena are\nexhibited, such as herd behavior, which demonstrates the potential of LMAgent\nin credible large-scale social behavior simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The believable simulation of multi-user behavior is crucial for understanding\ncomplex social systems. Recently, large language models (LLMs)-based AI agents\nhave made significant progress, enabling them to achieve human-like\nintelligence across various tasks. However, real human societies are often\ndynamic and complex, involving numerous individuals engaging in multimodal\ninteractions. In this paper, taking e-commerce scenarios as an example, we\npresent LMAgent, a very large-scale and multimodal agents society based on\nmultimodal LLMs. In LMAgent, besides freely chatting with friends, the agents\ncan autonomously browse, purchase, and review products, even perform live\nstreaming e-commerce. To simulate this complex system, we introduce a\nself-consistency prompting mechanism to augment agents' multimodal\ncapabilities, resulting in significantly improved decision-making performance\nover the existing multi-agent system. Moreover, we propose a fast memory\nmechanism combined with the small-world model to enhance system efficiency,\nwhich supports more than 10,000 agent simulations in a society. Experiments on\nagents' behavior show that these agents achieve comparable performance to\nhumans in behavioral indicators. Furthermore, compared with the existing\nLLMs-based multi-agent system, more different and valuable phenomena are\nexhibited, such as herd behavior, which demonstrates the potential of LMAgent\nin credible large-scale social behavior simulations."
                },
                "authors": [
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Wu Liu"
                    },
                    {
                        "name": "Xiaoyan Gu"
                    },
                    {
                        "name": "Yong Rui"
                    },
                    {
                        "name": "Xiaodong He"
                    },
                    {
                        "name": "Yongdong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongdong Zhang"
                },
                "author": "Yongdong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09237v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09237v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03703v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03703v2",
                "updated": "2024-12-13T03:19:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    3,
                    19,
                    24,
                    4,
                    348,
                    0
                ],
                "published": "2024-08-07T11:33:46Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    11,
                    33,
                    46,
                    2,
                    220,
                    0
                ],
                "title": "CAS-ViT: Convolutional Additive Self-attention Vision Transformers for\n  Efficient Mobile Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAS-ViT: Convolutional Additive Self-attention Vision Transformers for\n  Efficient Mobile Applications"
                },
                "summary": "Vision Transformers (ViTs) mark a revolutionary advance in neural networks\nwith their token mixer's powerful global context capability. However, the\npairwise token affinity and complex matrix operations limit its deployment on\nresource-constrained scenarios and real-time applications, such as mobile\ndevices, although considerable efforts have been made in previous works. In\nthis paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision\nTransformers, to achieve a balance between efficiency and performance in mobile\napplications. Firstly, we argue that the capability of token mixers to obtain\nglobal contextual information hinges on multiple information interactions, such\nas spatial and channel domains. Subsequently, we propose Convolutional Additive\nToken Mixer (CATM) employing underlying spatial and channel attention as novel\ninteraction forms. This module eliminates troublesome complex operations such\nas matrix multiplication and Softmax. We introduce Convolutional Additive\nSelf-attention(CAS) block hybrid architecture and utilize CATM for each block.\nAnd further, we build a family of lightweight networks, which can be easily\nextended to various downstream tasks. Finally, we evaluate CAS-ViT across a\nvariety of vision tasks, including image classification, object detection,\ninstance segmentation, and semantic segmentation. Our M and T model achieves\n83.0\\%/84.1\\% top-1 with only 12M/21M parameters on ImageNet-1K. Meanwhile,\nthroughput evaluations on GPUs, ONNX, and iPhones also demonstrate superior\nresults compared to other state-of-the-art backbones. Extensive experiments\ndemonstrate that our approach achieves a better balance of performance,\nefficient inference and easy-to-deploy. Our code and model are available at:\n\\url{https://github.com/Tianfang-Zhang/CAS-ViT}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) mark a revolutionary advance in neural networks\nwith their token mixer's powerful global context capability. However, the\npairwise token affinity and complex matrix operations limit its deployment on\nresource-constrained scenarios and real-time applications, such as mobile\ndevices, although considerable efforts have been made in previous works. In\nthis paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision\nTransformers, to achieve a balance between efficiency and performance in mobile\napplications. Firstly, we argue that the capability of token mixers to obtain\nglobal contextual information hinges on multiple information interactions, such\nas spatial and channel domains. Subsequently, we propose Convolutional Additive\nToken Mixer (CATM) employing underlying spatial and channel attention as novel\ninteraction forms. This module eliminates troublesome complex operations such\nas matrix multiplication and Softmax. We introduce Convolutional Additive\nSelf-attention(CAS) block hybrid architecture and utilize CATM for each block.\nAnd further, we build a family of lightweight networks, which can be easily\nextended to various downstream tasks. Finally, we evaluate CAS-ViT across a\nvariety of vision tasks, including image classification, object detection,\ninstance segmentation, and semantic segmentation. Our M and T model achieves\n83.0\\%/84.1\\% top-1 with only 12M/21M parameters on ImageNet-1K. Meanwhile,\nthroughput evaluations on GPUs, ONNX, and iPhones also demonstrate superior\nresults compared to other state-of-the-art backbones. Extensive experiments\ndemonstrate that our approach achieves a better balance of performance,\nefficient inference and easy-to-deploy. Our code and model are available at:\n\\url{https://github.com/Tianfang-Zhang/CAS-ViT}"
                },
                "authors": [
                    {
                        "name": "Tianfang Zhang"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Wentao Liu"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Jenq-Neng Hwang"
                    },
                    {
                        "name": "Xiangyang Ji"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyang Ji"
                },
                "author": "Xiangyang Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03703v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03703v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09819v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09819v1",
                "updated": "2024-12-13T03:16:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    3,
                    16,
                    14,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T03:16:14Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    3,
                    16,
                    14,
                    4,
                    348,
                    0
                ],
                "title": "FDM-Bench: A Comprehensive Benchmark for Evaluating Large Language\n  Models in Additive Manufacturing Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FDM-Bench: A Comprehensive Benchmark for Evaluating Large Language\n  Models in Additive Manufacturing Tasks"
                },
                "summary": "Fused Deposition Modeling (FDM) is a widely used additive manufacturing (AM)\ntechnique valued for its flexibility and cost-efficiency, with applications in\na variety of industries including healthcare and aerospace. Recent developments\nhave made affordable FDM machines accessible and encouraged adoption among\ndiverse users. However, the design, planning, and production process in FDM\nrequire specialized interdisciplinary knowledge. Managing the complex\nparameters and resolving print defects in FDM remain challenging. These\ntechnical complexities form the most critical barrier preventing individuals\nwithout technical backgrounds and even professional engineers without training\nin other domains from participating in AM design and manufacturing. Large\nLanguage Models (LLMs), with their advanced capabilities in text and code\nprocessing, offer the potential for addressing these challenges in FDM.\nHowever, existing research on LLM applications in this field is limited,\ntypically focusing on specific use cases without providing comprehensive\nevaluations across multiple models and tasks. To this end, we introduce\nFDM-Bench, a benchmark dataset designed to evaluate LLMs on FDM-specific tasks.\nFDM-Bench enables a thorough assessment by including user queries across\nvarious experience levels and G-code samples that represent a range of\nanomalies. We evaluate two closed-source models (GPT-4o and Claude 3.5 Sonnet)\nand two open-source models (Llama-3.1-70B and Llama-3.1-405B) on FDM-Bench. A\npanel of FDM experts assess the models' responses to user queries in detail.\nResults indicate that closed-source models generally outperform open-source\nmodels in G-code anomaly detection, whereas Llama-3.1-405B demonstrates a\nslight advantage over other models in responding to user queries. These\nfindings underscore FDM-Bench's potential as a foundational tool for advancing\nresearch on LLM capabilities in FDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fused Deposition Modeling (FDM) is a widely used additive manufacturing (AM)\ntechnique valued for its flexibility and cost-efficiency, with applications in\na variety of industries including healthcare and aerospace. Recent developments\nhave made affordable FDM machines accessible and encouraged adoption among\ndiverse users. However, the design, planning, and production process in FDM\nrequire specialized interdisciplinary knowledge. Managing the complex\nparameters and resolving print defects in FDM remain challenging. These\ntechnical complexities form the most critical barrier preventing individuals\nwithout technical backgrounds and even professional engineers without training\nin other domains from participating in AM design and manufacturing. Large\nLanguage Models (LLMs), with their advanced capabilities in text and code\nprocessing, offer the potential for addressing these challenges in FDM.\nHowever, existing research on LLM applications in this field is limited,\ntypically focusing on specific use cases without providing comprehensive\nevaluations across multiple models and tasks. To this end, we introduce\nFDM-Bench, a benchmark dataset designed to evaluate LLMs on FDM-specific tasks.\nFDM-Bench enables a thorough assessment by including user queries across\nvarious experience levels and G-code samples that represent a range of\nanomalies. We evaluate two closed-source models (GPT-4o and Claude 3.5 Sonnet)\nand two open-source models (Llama-3.1-70B and Llama-3.1-405B) on FDM-Bench. A\npanel of FDM experts assess the models' responses to user queries in detail.\nResults indicate that closed-source models generally outperform open-source\nmodels in G-code anomaly detection, whereas Llama-3.1-405B demonstrates a\nslight advantage over other models in responding to user queries. These\nfindings underscore FDM-Bench's potential as a foundational tool for advancing\nresearch on LLM capabilities in FDM."
                },
                "authors": [
                    {
                        "name": "Ahmadreza Eslaminia"
                    },
                    {
                        "name": "Adrian Jackson"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Avi Stern"
                    },
                    {
                        "name": "Hallie Gordon"
                    },
                    {
                        "name": "Rajiv Malhotra"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    },
                    {
                        "name": "Chenhui Shao"
                    }
                ],
                "author_detail": {
                    "name": "Chenhui Shao"
                },
                "author": "Chenhui Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09819v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09819v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00416v2",
                "updated": "2024-12-13T03:15:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    3,
                    15,
                    51,
                    4,
                    348,
                    0
                ],
                "published": "2024-06-29T11:50:16Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    11,
                    50,
                    16,
                    5,
                    181,
                    0
                ],
                "title": "Too Late to Train, Too Early To Use? A Study on Necessity and Viability\n  of Low-Resource Bengali LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Too Late to Train, Too Early To Use? A Study on Necessity and Viability\n  of Low-Resource Bengali LLMs"
                },
                "summary": "Each new generation of English-oriented Large Language Models (LLMs) exhibits\nenhanced cross-lingual transfer capabilities and significantly outperforms\nolder LLMs on low-resource languages. This prompts the question: Is there a\nneed for LLMs dedicated to a particular low-resource language? We aim to\nexplore this question for Bengali, a low-to-moderate resource Indo-Aryan\nlanguage native to the Bengal region of South Asia.\n  We compare the performance of open-weight and closed-source LLMs such as\nLLaMA-3 and GPT-4 against fine-tuned encoder-decoder models across a diverse\nset of Bengali downstream tasks, including translation, summarization,\nparaphrasing, question-answering, and natural language inference. Our findings\nreveal that while LLMs generally excel in reasoning tasks, their performance in\ntasks requiring Bengali script generation is inconsistent. Key challenges\ninclude inefficient tokenization of Bengali script by existing LLMs, leading to\nincreased computational costs and potential performance degradation.\nAdditionally, we highlight biases in machine-translated datasets commonly used\nfor Bengali NLP tasks. We conclude that there is a significant need for a\nBengali-oriented LLM, but the field currently lacks the high-quality\npretraining and instruction-tuning datasets necessary to develop a highly\neffective model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Each new generation of English-oriented Large Language Models (LLMs) exhibits\nenhanced cross-lingual transfer capabilities and significantly outperforms\nolder LLMs on low-resource languages. This prompts the question: Is there a\nneed for LLMs dedicated to a particular low-resource language? We aim to\nexplore this question for Bengali, a low-to-moderate resource Indo-Aryan\nlanguage native to the Bengal region of South Asia.\n  We compare the performance of open-weight and closed-source LLMs such as\nLLaMA-3 and GPT-4 against fine-tuned encoder-decoder models across a diverse\nset of Bengali downstream tasks, including translation, summarization,\nparaphrasing, question-answering, and natural language inference. Our findings\nreveal that while LLMs generally excel in reasoning tasks, their performance in\ntasks requiring Bengali script generation is inconsistent. Key challenges\ninclude inefficient tokenization of Bengali script by existing LLMs, leading to\nincreased computational costs and potential performance degradation.\nAdditionally, we highlight biases in machine-translated datasets commonly used\nfor Bengali NLP tasks. We conclude that there is a significant need for a\nBengali-oriented LLM, but the field currently lacks the high-quality\npretraining and instruction-tuning datasets necessary to develop a highly\neffective model."
                },
                "authors": [
                    {
                        "name": "Tamzeed Mahfuz"
                    },
                    {
                        "name": "Satak Kumar Dey"
                    },
                    {
                        "name": "Ruwad Naswan"
                    },
                    {
                        "name": "Hasnaen Adil"
                    },
                    {
                        "name": "Khondker Salman Sayeed"
                    },
                    {
                        "name": "Haz Sameen Shahgir"
                    }
                ],
                "author_detail": {
                    "name": "Haz Sameen Shahgir"
                },
                "author": "Haz Sameen Shahgir",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09817v1",
                "updated": "2024-12-13T03:13:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    3,
                    13,
                    44,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T03:13:44Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    3,
                    13,
                    44,
                    4,
                    348,
                    0
                ],
                "title": "Enhancing Multimodal Large Language Models Complex Reason via Similarity\n  Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Multimodal Large Language Models Complex Reason via Similarity\n  Computation"
                },
                "summary": "Multimodal large language models have experienced rapid growth, and numerous\ndifferent models have emerged. The interpretability of LVLMs remains an\nunder-explored area. Especially when faced with more complex tasks such as\nchain-of-thought reasoning, its internal mechanisms still resemble a black box\nthat is difficult to decipher. By studying the interaction and information flow\nbetween images and text, we noticed that in models such as LLaVA1.5, image\ntokens that are semantically related to text are more likely to have\ninformation flow convergence in the LLM decoding layer, and these image tokens\nreceive higher attention scores. However, those image tokens that are less\nrelevant to the text do not have information flow convergence, and they only\nget very small attention scores. To efficiently utilize the image information,\nwe propose a new image token reduction method, Simignore, which aims to improve\nthe complex reasoning ability of LVLMs by computing the similarity between\nimage and text embeddings and ignoring image tokens that are irrelevant and\nunimportant to the text. Through extensive experiments, we demonstrate the\neffectiveness of our method for complex reasoning tasks. The paper's source\ncode can be accessed from \\url{https://github.com/FanshuoZeng/Simignore}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models have experienced rapid growth, and numerous\ndifferent models have emerged. The interpretability of LVLMs remains an\nunder-explored area. Especially when faced with more complex tasks such as\nchain-of-thought reasoning, its internal mechanisms still resemble a black box\nthat is difficult to decipher. By studying the interaction and information flow\nbetween images and text, we noticed that in models such as LLaVA1.5, image\ntokens that are semantically related to text are more likely to have\ninformation flow convergence in the LLM decoding layer, and these image tokens\nreceive higher attention scores. However, those image tokens that are less\nrelevant to the text do not have information flow convergence, and they only\nget very small attention scores. To efficiently utilize the image information,\nwe propose a new image token reduction method, Simignore, which aims to improve\nthe complex reasoning ability of LVLMs by computing the similarity between\nimage and text embeddings and ignoring image tokens that are irrelevant and\nunimportant to the text. Through extensive experiments, we demonstrate the\neffectiveness of our method for complex reasoning tasks. The paper's source\ncode can be accessed from \\url{https://github.com/FanshuoZeng/Simignore}."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Fanshuo Zeng"
                    },
                    {
                        "name": "Yihao Quan"
                    },
                    {
                        "name": "Zheng Hui"
                    },
                    {
                        "name": "Jiawei Yao"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Yao"
                },
                "author": "Jiawei Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10210v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10210v4",
                "updated": "2024-12-13T03:11:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    3,
                    11,
                    30,
                    4,
                    348,
                    0
                ],
                "published": "2024-04-16T01:41:22Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    1,
                    41,
                    22,
                    1,
                    107,
                    0
                ],
                "title": "MK-SGN: A Spiking Graph Convolutional Network with Multimodal Fusion and\n  Knowledge Distillation for Skeleton-based Action Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MK-SGN: A Spiking Graph Convolutional Network with Multimodal Fusion and\n  Knowledge Distillation for Skeleton-based Action Recognition"
                },
                "summary": "In recent years, multimodal Graph Convolutional Networks (GCNs) have achieved\nremarkable performance in skeleton-based action recognition. The reliance on\nhigh-energy-consuming continuous floating-point operations inherent in\nGCN-based methods poses significant challenges for deployment in\nenergy-constrained, battery-powered edge devices. To address these limitations,\nMK-SGN, a Spiking Graph Convolutional Network with Multimodal Fusion and\nKnowledge Distillation, is proposed to leverage the energy efficiency of\nSpiking Neural Networks (SNNs) for skeleton-based action recognition for the\nfirst time. By integrating the energy-saving properties of SNNs with the graph\nrepresentation capabilities of GCNs, MK-SGN achieves significant reductions in\nenergy consumption while maintaining competitive recognition accuracy. Firstly,\nwe formulate a Spiking Multimodal Fusion (SMF) module to effectively fuse\nmultimodal skeleton data represented as spike-form features. Secondly, we\npropose the Self-Attention Spiking Graph Convolution (SA-SGC) module and the\nSpiking Temporal Convolution (STC) module, to capture spatial relationships and\ntemporal dynamics of spike-form features. Finally, we propose an integrated\nknowledge distillation strategy to transfer information from the multimodal GCN\nto the SGN, incorporating both intermediate-layer distillation and soft-label\ndistillation to enhance the performance of the SGN. MK-SGN exhibits substantial\nadvantages, surpassing state-of-the-art GCN frameworks in energy efficiency and\noutperforming state-of-the-art SNN frameworks in recognition accuracy. The\nproposed method achieves a remarkable reduction in energy consumption,\nexceeding 98\\% compared to conventional GCN-based approaches. This research\nestablishes a robust baseline for developing high-performance, energy-efficient\nSNN-based models for skeleton-based action recognition",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, multimodal Graph Convolutional Networks (GCNs) have achieved\nremarkable performance in skeleton-based action recognition. The reliance on\nhigh-energy-consuming continuous floating-point operations inherent in\nGCN-based methods poses significant challenges for deployment in\nenergy-constrained, battery-powered edge devices. To address these limitations,\nMK-SGN, a Spiking Graph Convolutional Network with Multimodal Fusion and\nKnowledge Distillation, is proposed to leverage the energy efficiency of\nSpiking Neural Networks (SNNs) for skeleton-based action recognition for the\nfirst time. By integrating the energy-saving properties of SNNs with the graph\nrepresentation capabilities of GCNs, MK-SGN achieves significant reductions in\nenergy consumption while maintaining competitive recognition accuracy. Firstly,\nwe formulate a Spiking Multimodal Fusion (SMF) module to effectively fuse\nmultimodal skeleton data represented as spike-form features. Secondly, we\npropose the Self-Attention Spiking Graph Convolution (SA-SGC) module and the\nSpiking Temporal Convolution (STC) module, to capture spatial relationships and\ntemporal dynamics of spike-form features. Finally, we propose an integrated\nknowledge distillation strategy to transfer information from the multimodal GCN\nto the SGN, incorporating both intermediate-layer distillation and soft-label\ndistillation to enhance the performance of the SGN. MK-SGN exhibits substantial\nadvantages, surpassing state-of-the-art GCN frameworks in energy efficiency and\noutperforming state-of-the-art SNN frameworks in recognition accuracy. The\nproposed method achieves a remarkable reduction in energy consumption,\nexceeding 98\\% compared to conventional GCN-based approaches. This research\nestablishes a robust baseline for developing high-performance, energy-efficient\nSNN-based models for skeleton-based action recognition"
                },
                "authors": [
                    {
                        "name": "Naichuan Zheng"
                    },
                    {
                        "name": "Hailun Xia"
                    },
                    {
                        "name": "Zeyu Liang"
                    },
                    {
                        "name": "Yuchen Du"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Du"
                },
                "author": "Yuchen Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10210v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10210v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09812v1",
                "updated": "2024-12-13T03:00:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    3,
                    0,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T03:00:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    3,
                    0,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "ScaleOT: Privacy-utility-scalable Offsite-tuning with Dynamic\n  LayerReplace and Selective Rank Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScaleOT: Privacy-utility-scalable Offsite-tuning with Dynamic\n  LayerReplace and Selective Rank Compression"
                },
                "summary": "Offsite-tuning is a privacy-preserving method for tuning large language\nmodels (LLMs) by sharing a lossy compressed emulator from the LLM owners with\ndata owners for downstream task tuning. This approach protects the privacy of\nboth the model and data owners. However, current offsite tuning methods often\nsuffer from adaptation degradation, high computational costs, and limited\nprotection strength due to uniformly dropping LLM layers or relying on\nexpensive knowledge distillation. To address these issues, we propose ScaleOT,\na novel privacy-utility-scalable offsite-tuning framework that effectively\nbalances privacy and utility. ScaleOT introduces a novel layerwise lossy\ncompression algorithm that uses reinforcement learning to obtain the importance\nof each layer. It employs lightweight networks, termed harmonizers, to replace\nthe raw LLM layers. By combining important original LLM layers and harmonizers\nin different ratios, ScaleOT generates emulators tailored for optimal\nperformance with various model scales for enhanced privacy protection.\nAdditionally, we present a rank reduction method to further compress the\noriginal LLM layers, significantly enhancing privacy with negligible impact on\nutility. Comprehensive experiments show that ScaleOT can achieve nearly\nlossless offsite tuning performance compared with full fine-tuning while\nobtaining better model privacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offsite-tuning is a privacy-preserving method for tuning large language\nmodels (LLMs) by sharing a lossy compressed emulator from the LLM owners with\ndata owners for downstream task tuning. This approach protects the privacy of\nboth the model and data owners. However, current offsite tuning methods often\nsuffer from adaptation degradation, high computational costs, and limited\nprotection strength due to uniformly dropping LLM layers or relying on\nexpensive knowledge distillation. To address these issues, we propose ScaleOT,\na novel privacy-utility-scalable offsite-tuning framework that effectively\nbalances privacy and utility. ScaleOT introduces a novel layerwise lossy\ncompression algorithm that uses reinforcement learning to obtain the importance\nof each layer. It employs lightweight networks, termed harmonizers, to replace\nthe raw LLM layers. By combining important original LLM layers and harmonizers\nin different ratios, ScaleOT generates emulators tailored for optimal\nperformance with various model scales for enhanced privacy protection.\nAdditionally, we present a rank reduction method to further compress the\noriginal LLM layers, significantly enhancing privacy with negligible impact on\nutility. Comprehensive experiments show that ScaleOT can achieve nearly\nlossless offsite tuning performance compared with full fine-tuning while\nobtaining better model privacy."
                },
                "authors": [
                    {
                        "name": "Kai Yao"
                    },
                    {
                        "name": "Zhaorui Tan"
                    },
                    {
                        "name": "Tiandi Ye"
                    },
                    {
                        "name": "Lichun Li"
                    },
                    {
                        "name": "Yuan Zhao"
                    },
                    {
                        "name": "Wenyan Liu"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Jianke Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jianke Zhu"
                },
                "author": "Jianke Zhu",
                "arxiv_comment": "accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09688v2",
                "updated": "2024-12-13T02:48:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    48,
                    44,
                    4,
                    348,
                    0
                ],
                "published": "2024-08-19T03:53:48Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    3,
                    53,
                    48,
                    0,
                    232,
                    0
                ],
                "title": "Recording for Eyes, Not Echoing to Ears: Contextualized\n  Spoken-to-Written Conversion of ASR Transcripts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recording for Eyes, Not Echoing to Ears: Contextualized\n  Spoken-to-Written Conversion of ASR Transcripts"
                },
                "summary": "Automatic Speech Recognition (ASR) transcripts exhibit recognition errors and\nvarious spoken language phenomena such as disfluencies, ungrammatical\nsentences, and incomplete sentences, hence suffering from poor readability. To\nimprove readability, we propose a Contextualized Spoken-to-Written conversion\n(CoS2W) task to address ASR and grammar errors and also transfer the informal\ntext into the formal style with content preserved, utilizing contexts and\nauxiliary information. This task naturally matches the in-context learning\ncapabilities of Large Language Models (LLMs). To facilitate comprehensive\ncomparisons of various LLMs, we construct a document-level Spoken-to-Written\nconversion of ASR Transcripts Benchmark (SWAB) dataset. Using SWAB, we study\nthe impact of different granularity levels on the CoS2W performance, and\npropose methods to exploit contexts and auxiliary information to enhance the\noutputs. Experimental results reveal that LLMs have the potential to excel in\nthe CoS2W task, particularly in grammaticality and formality, our methods\nachieve effective understanding of contexts and auxiliary information by LLMs.\nWe further investigate the effectiveness of using LLMs as evaluators and find\nthat LLM evaluators show strong correlations with human evaluations on rankings\nof faithfulness and formality, which validates the reliability of LLM\nevaluators for the CoS2W task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Speech Recognition (ASR) transcripts exhibit recognition errors and\nvarious spoken language phenomena such as disfluencies, ungrammatical\nsentences, and incomplete sentences, hence suffering from poor readability. To\nimprove readability, we propose a Contextualized Spoken-to-Written conversion\n(CoS2W) task to address ASR and grammar errors and also transfer the informal\ntext into the formal style with content preserved, utilizing contexts and\nauxiliary information. This task naturally matches the in-context learning\ncapabilities of Large Language Models (LLMs). To facilitate comprehensive\ncomparisons of various LLMs, we construct a document-level Spoken-to-Written\nconversion of ASR Transcripts Benchmark (SWAB) dataset. Using SWAB, we study\nthe impact of different granularity levels on the CoS2W performance, and\npropose methods to exploit contexts and auxiliary information to enhance the\noutputs. Experimental results reveal that LLMs have the potential to excel in\nthe CoS2W task, particularly in grammaticality and formality, our methods\nachieve effective understanding of contexts and auxiliary information by LLMs.\nWe further investigate the effectiveness of using LLMs as evaluators and find\nthat LLM evaluators show strong correlations with human evaluations on rankings\nof faithfulness and formality, which validates the reliability of LLM\nevaluators for the CoS2W task."
                },
                "authors": [
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Chong Deng"
                    },
                    {
                        "name": "Qinglin Zhang"
                    },
                    {
                        "name": "Shilin Zhou"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Hai Yu"
                    },
                    {
                        "name": "Wen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Wang"
                },
                "author": "Wen Wang",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09807v1",
                "updated": "2024-12-13T02:48:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    48,
                    36,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T02:48:36Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    48,
                    36,
                    4,
                    348,
                    0
                ],
                "title": "LLM Distillation for Efficient Few-Shot Multiple Choice Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Distillation for Efficient Few-Shot Multiple Choice Question\n  Answering"
                },
                "summary": "Multiple Choice Question Answering (MCQA) is an important problem with\nnumerous real-world applications, such as medicine, law, and education. The\nhigh cost of building MCQA datasets makes few-shot learning pivotal in this\ndomain. While Large Language Models (LLMs) can enable few-shot learning, their\ndirect application in real-world scenarios is often hindered by their high\ncomputational cost. To address this challenge, we propose a simple yet\neffective approach that uses LLMs for data generation and scoring. Our approach\nutilizes LLMs to create MCQA data which contains questions and choices, and to\nassign probability scores to the generated choices. We then use the generated\ndata and LLM-assigned scores to finetune a smaller and more efficient\nencoder-only model, DeBERTa-v3-base by leveraging distillation loss. Extensive\nexperiments on the Massive Multitask Language Understanding (MMLU) benchmark\ndemonstrate that our method improves accuracy from 28.9% to 39.3%, representing\na gain of over 10% compared to a baseline finetuned directly on 5-shot\nexamples. This shows the effectiveness of LLM-driven data generation and\nknowledge distillation for few-shot MCQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple Choice Question Answering (MCQA) is an important problem with\nnumerous real-world applications, such as medicine, law, and education. The\nhigh cost of building MCQA datasets makes few-shot learning pivotal in this\ndomain. While Large Language Models (LLMs) can enable few-shot learning, their\ndirect application in real-world scenarios is often hindered by their high\ncomputational cost. To address this challenge, we propose a simple yet\neffective approach that uses LLMs for data generation and scoring. Our approach\nutilizes LLMs to create MCQA data which contains questions and choices, and to\nassign probability scores to the generated choices. We then use the generated\ndata and LLM-assigned scores to finetune a smaller and more efficient\nencoder-only model, DeBERTa-v3-base by leveraging distillation loss. Extensive\nexperiments on the Massive Multitask Language Understanding (MMLU) benchmark\ndemonstrate that our method improves accuracy from 28.9% to 39.3%, representing\na gain of over 10% compared to a baseline finetuned directly on 5-shot\nexamples. This shows the effectiveness of LLM-driven data generation and\nknowledge distillation for few-shot MCQA."
                },
                "authors": [
                    {
                        "name": "Patrick Sutanto"
                    },
                    {
                        "name": "Joan Santoso"
                    }
                ],
                "author_detail": {
                    "name": "Joan Santoso"
                },
                "author": "Joan Santoso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03514v2",
                "updated": "2024-12-13T02:45:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    45,
                    14,
                    4,
                    348,
                    0
                ],
                "published": "2024-04-04T15:21:22Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    15,
                    21,
                    22,
                    3,
                    95,
                    0
                ],
                "title": "Embedding-Informed Adaptive Retrieval-Augmented Generation of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding-Informed Adaptive Retrieval-Augmented Generation of Large\n  Language Models"
                },
                "summary": "Retrieval-augmented large language models (LLMs) have been remarkably\ncompetent in various NLP tasks. However, it was observed by previous works that\nretrieval is not always helpful, especially when the LLM is already\nknowledgeable on the query to answer. Motivated by this, Adaptive\nRetrieval-Augmented Generation (ARAG) studies retrieving only when the\nknowledge asked by the query is absent in the LLM. Previous works of ARAG\neither require accessing the pre-training corpus or prompting with additional\nmodel inferences. Aiming to avoid such drawbacks, we propose to determine\nwhether the model is knowledgeable on a query via inspecting the\n(contextualized) pre-trained token embeddings of LLMs. We hypothesize that such\nembeddings capture rich information on the model's intrinsic knowledge base,\nwhich enables an efficient way of judging the necessity to retrieve from an\nexternal corpus. Extensive experiments demonstrate our ARAG approach's superior\nperformance across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented large language models (LLMs) have been remarkably\ncompetent in various NLP tasks. However, it was observed by previous works that\nretrieval is not always helpful, especially when the LLM is already\nknowledgeable on the query to answer. Motivated by this, Adaptive\nRetrieval-Augmented Generation (ARAG) studies retrieving only when the\nknowledge asked by the query is absent in the LLM. Previous works of ARAG\neither require accessing the pre-training corpus or prompting with additional\nmodel inferences. Aiming to avoid such drawbacks, we propose to determine\nwhether the model is knowledgeable on a query via inspecting the\n(contextualized) pre-trained token embeddings of LLMs. We hypothesize that such\nembeddings capture rich information on the model's intrinsic knowledge base,\nwhich enables an efficient way of judging the necessity to retrieve from an\nexternal corpus. Extensive experiments demonstrate our ARAG approach's superior\nperformance across various benchmarks."
                },
                "authors": [
                    {
                        "name": "Chengkai Huang"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Kaige Xie"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Julian McAuley"
                    },
                    {
                        "name": "Lina Yao"
                    }
                ],
                "author_detail": {
                    "name": "Lina Yao"
                },
                "author": "Lina Yao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.09025v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.09025v6",
                "updated": "2024-12-13T02:44:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    44,
                    34,
                    4,
                    348,
                    0
                ],
                "published": "2024-02-14T09:01:13Z",
                "published_parsed": [
                    2024,
                    2,
                    14,
                    9,
                    1,
                    13,
                    2,
                    45,
                    0
                ],
                "title": "SLEB: Streamlining LLMs through Redundancy Verification and Elimination\n  of Transformer Blocks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLEB: Streamlining LLMs through Redundancy Verification and Elimination\n  of Transformer Blocks"
                },
                "summary": "Large language models (LLMs) have proven to be highly effective across\nvarious natural language processing tasks. However, their large number of\nparameters poses significant challenges for practical deployment. Pruning, a\ntechnique aimed at reducing the size and complexity of LLMs, offers a potential\nsolution by removing redundant components from the network. Despite the promise\nof pruning, existing methods often struggle to achieve substantial end-to-end\nLLM inference speedup. In this paper, we introduce SLEB, a novel approach\ndesigned to streamline LLMs by eliminating redundant transformer blocks. We\nchoose the transformer block as the fundamental unit for pruning, because LLMs\nexhibit block-level redundancy with high similarity between the outputs of\nneighboring blocks. This choice allows us to effectively enhance the processing\nspeed of LLMs. Our experimental results demonstrate that SLEB outperforms\nprevious LLM pruning methods in accelerating LLM inference while also\nmaintaining superior perplexity and accuracy, making SLEB as a promising\ntechnique for enhancing the efficiency of LLMs. The code is available at:\nhttps://github.com/jiwonsong-dev/SLEB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have proven to be highly effective across\nvarious natural language processing tasks. However, their large number of\nparameters poses significant challenges for practical deployment. Pruning, a\ntechnique aimed at reducing the size and complexity of LLMs, offers a potential\nsolution by removing redundant components from the network. Despite the promise\nof pruning, existing methods often struggle to achieve substantial end-to-end\nLLM inference speedup. In this paper, we introduce SLEB, a novel approach\ndesigned to streamline LLMs by eliminating redundant transformer blocks. We\nchoose the transformer block as the fundamental unit for pruning, because LLMs\nexhibit block-level redundancy with high similarity between the outputs of\nneighboring blocks. This choice allows us to effectively enhance the processing\nspeed of LLMs. Our experimental results demonstrate that SLEB outperforms\nprevious LLM pruning methods in accelerating LLM inference while also\nmaintaining superior perplexity and accuracy, making SLEB as a promising\ntechnique for enhancing the efficiency of LLMs. The code is available at:\nhttps://github.com/jiwonsong-dev/SLEB."
                },
                "authors": [
                    {
                        "name": "Jiwon Song"
                    },
                    {
                        "name": "Kyungseok Oh"
                    },
                    {
                        "name": "Taesu Kim"
                    },
                    {
                        "name": "Hyungjun Kim"
                    },
                    {
                        "name": "Yulhwa Kim"
                    },
                    {
                        "name": "Jae-Joon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jae-Joon Kim"
                },
                "author": "Jae-Joon Kim",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.09025v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.09025v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09796v1",
                "updated": "2024-12-13T02:27:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    27,
                    34,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T02:27:34Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    27,
                    34,
                    4,
                    348,
                    0
                ],
                "title": "AutoPatent: A Multi-Agent Framework for Automatic Patent Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPatent: A Multi-Agent Framework for Automatic Patent Generation"
                },
                "summary": "As the capabilities of Large Language Models (LLMs) continue to advance, the\nfield of patent processing has garnered increased attention within the natural\nlanguage processing community. However, the majority of research has been\nconcentrated on classification tasks, such as patent categorization and\nexamination, or on short text generation tasks like patent summarization and\npatent quizzes. In this paper, we introduce a novel and practical task known as\nDraft2Patent, along with its corresponding D2P benchmark, which challenges LLMs\nto generate full-length patents averaging 17K tokens based on initial drafts.\nPatents present a significant challenge to LLMs due to their specialized\nnature, standardized terminology, and extensive length. We propose a\nmulti-agent framework called AutoPatent which leverages the LLM-based planner\nagent, writer agents, and examiner agent with PGTree and RRAG to generate\nlengthy, intricate, and high-quality complete patent documents. The\nexperimental results demonstrate that our AutoPatent framework significantly\nenhances the ability to generate comprehensive patents across various LLMs.\nFurthermore, we have discovered that patents generated solely with the\nAutoPatent framework based on the Qwen2.5-7B model outperform those produced by\nlarger and more powerful LLMs, such as GPT-4o, Qwen2.5-72B, and LLAMA3.1-70B,\nin both objective metrics and human evaluations. We will make the data and code\navailable upon acceptance at \\url{https://github.com/QiYao-Wang/AutoPatent}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the capabilities of Large Language Models (LLMs) continue to advance, the\nfield of patent processing has garnered increased attention within the natural\nlanguage processing community. However, the majority of research has been\nconcentrated on classification tasks, such as patent categorization and\nexamination, or on short text generation tasks like patent summarization and\npatent quizzes. In this paper, we introduce a novel and practical task known as\nDraft2Patent, along with its corresponding D2P benchmark, which challenges LLMs\nto generate full-length patents averaging 17K tokens based on initial drafts.\nPatents present a significant challenge to LLMs due to their specialized\nnature, standardized terminology, and extensive length. We propose a\nmulti-agent framework called AutoPatent which leverages the LLM-based planner\nagent, writer agents, and examiner agent with PGTree and RRAG to generate\nlengthy, intricate, and high-quality complete patent documents. The\nexperimental results demonstrate that our AutoPatent framework significantly\nenhances the ability to generate comprehensive patents across various LLMs.\nFurthermore, we have discovered that patents generated solely with the\nAutoPatent framework based on the Qwen2.5-7B model outperform those produced by\nlarger and more powerful LLMs, such as GPT-4o, Qwen2.5-72B, and LLAMA3.1-70B,\nin both objective metrics and human evaluations. We will make the data and code\navailable upon acceptance at \\url{https://github.com/QiYao-Wang/AutoPatent}."
                },
                "authors": [
                    {
                        "name": "Qiyao Wang"
                    },
                    {
                        "name": "Shiwen Ni"
                    },
                    {
                        "name": "Huaren Liu"
                    },
                    {
                        "name": "Shule Lu"
                    },
                    {
                        "name": "Guhong Chen"
                    },
                    {
                        "name": "Xi Feng"
                    },
                    {
                        "name": "Chi Wei"
                    },
                    {
                        "name": "Qiang Qu"
                    },
                    {
                        "name": "Hamid Alinejad-Rokny"
                    },
                    {
                        "name": "Yuan Lin"
                    },
                    {
                        "name": "Min Yang"
                    }
                ],
                "author_detail": {
                    "name": "Min Yang"
                },
                "author": "Min Yang",
                "arxiv_comment": "19 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09795v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09795v1",
                "updated": "2024-12-13T02:26:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    58,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T02:26:58Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    58,
                    4,
                    348,
                    0
                ],
                "title": "Is it the model or the metric -- On robustness measures of deeplearning\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Is it the model or the metric -- On robustness measures of deeplearning\n  models"
                },
                "summary": "Determining the robustness of deep learning models is an established and\nongoing challenge within automated decision-making systems. With the advent and\nsuccess of techniques that enable advanced deep learning (DL), these models are\nbeing used in widespread applications, including high-stake ones like\nhealthcare, education, border-control. Therefore, it is critical to understand\nthe limitations of these models and predict their regions of failures, in order\nto create the necessary guardrails for their successful and safe deployment. In\nthis work, we revisit robustness, specifically investigating the sufficiency of\nrobust accuracy (RA), within the context of deepfake detection. We present\nrobust ratio (RR) as a complementary metric, that can quantify the changes to\nthe normalized or probability outcomes under input perturbation. We present a\ncomparison of RA and RR and demonstrate that despite similar RA between models,\nthe models show varying RR under different tolerance (perturbation) levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining the robustness of deep learning models is an established and\nongoing challenge within automated decision-making systems. With the advent and\nsuccess of techniques that enable advanced deep learning (DL), these models are\nbeing used in widespread applications, including high-stake ones like\nhealthcare, education, border-control. Therefore, it is critical to understand\nthe limitations of these models and predict their regions of failures, in order\nto create the necessary guardrails for their successful and safe deployment. In\nthis work, we revisit robustness, specifically investigating the sufficiency of\nrobust accuracy (RA), within the context of deepfake detection. We present\nrobust ratio (RR) as a complementary metric, that can quantify the changes to\nthe normalized or probability outcomes under input perturbation. We present a\ncomparison of RA and RR and demonstrate that despite similar RA between models,\nthe models show varying RR under different tolerance (perturbation) levels."
                },
                "authors": [
                    {
                        "name": "Zhijin Lyu"
                    },
                    {
                        "name": "Yutong Jin"
                    },
                    {
                        "name": "Sneha Das"
                    }
                ],
                "author_detail": {
                    "name": "Sneha Das"
                },
                "author": "Sneha Das",
                "arxiv_comment": "Extended abstract at Northern Lights Deep Learning (NLDL) Conference\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09795v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09795v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11793v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11793v2",
                "updated": "2024-12-13T01:11:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    1,
                    11,
                    25,
                    4,
                    348,
                    0
                ],
                "published": "2024-08-21T17:25:45Z",
                "published_parsed": [
                    2024,
                    8,
                    21,
                    17,
                    25,
                    45,
                    2,
                    234,
                    0
                ],
                "title": "Leveraging Chemistry Foundation Models to Facilitate Structure Focused\n  Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and\n  Materials Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Chemistry Foundation Models to Facilitate Structure Focused\n  Retrieval Augmented Generation in Multi-Agent Workflows for Catalyst and\n  Materials Design"
                },
                "summary": "Molecular property prediction and generative design via deep learning models\nhas been the subject of intense research given its potential to accelerate\ndevelopment of new, high-performance materials. More recently, these workflows\nhave been significantly augmented with the advent of large language models\n(LLMs) and systems of autonomous agents capable of utilizing pre-trained models\nto make predictions in the context of more complex research tasks. While\neffective, there is still room for substantial improvement within agentic\nsystems on the retrieval of salient information for material design tasks.\nWithin this context, alternative uses of predictive deep learning models, such\nas leveraging their latent representations to facilitate cross-modal retrieval\naugmented generation within agentic systems for task-specific materials design,\nhas remained unexplored. Herein, we demonstrate that large, pre-trained\nchemistry foundation models can serve as a basis for enabling\nstructure-focused, semantic chemistry information retrieval for both\nsmall-molecules, complex polymeric materials, and reactions. Additionally, we\nshow the use of chemistry foundation models in conjunction with multi-modal\nmodels such as OpenCLIP facilitate unprecedented queries and information\nretrieval across multiple characterization data domains. Finally, we\ndemonstrate the integration of these models within multi-agent systems to\nfacilitate structure and topological-based natural language queries and\ninformation retrieval for different research tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular property prediction and generative design via deep learning models\nhas been the subject of intense research given its potential to accelerate\ndevelopment of new, high-performance materials. More recently, these workflows\nhave been significantly augmented with the advent of large language models\n(LLMs) and systems of autonomous agents capable of utilizing pre-trained models\nto make predictions in the context of more complex research tasks. While\neffective, there is still room for substantial improvement within agentic\nsystems on the retrieval of salient information for material design tasks.\nWithin this context, alternative uses of predictive deep learning models, such\nas leveraging their latent representations to facilitate cross-modal retrieval\naugmented generation within agentic systems for task-specific materials design,\nhas remained unexplored. Herein, we demonstrate that large, pre-trained\nchemistry foundation models can serve as a basis for enabling\nstructure-focused, semantic chemistry information retrieval for both\nsmall-molecules, complex polymeric materials, and reactions. Additionally, we\nshow the use of chemistry foundation models in conjunction with multi-modal\nmodels such as OpenCLIP facilitate unprecedented queries and information\nretrieval across multiple characterization data domains. Finally, we\ndemonstrate the integration of these models within multi-agent systems to\nfacilitate structure and topological-based natural language queries and\ninformation retrieval for different research tasks."
                },
                "authors": [
                    {
                        "name": "Nathaniel H. Park"
                    },
                    {
                        "name": "Tiffany J. Callahan"
                    },
                    {
                        "name": "James L. Hedrick"
                    },
                    {
                        "name": "Tim Erdmann"
                    },
                    {
                        "name": "Sara Capponi"
                    }
                ],
                "author_detail": {
                    "name": "Sara Capponi"
                },
                "author": "Sara Capponi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11793v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11793v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2206.02508v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2206.02508v4",
                "updated": "2024-12-13T00:34:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    0,
                    34,
                    36,
                    4,
                    348,
                    0
                ],
                "published": "2022-06-06T11:41:02Z",
                "published_parsed": [
                    2022,
                    6,
                    6,
                    11,
                    41,
                    2,
                    0,
                    157,
                    0
                ],
                "title": "Tucker tensor factor models: matricization and mode-wise PCA estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tucker tensor factor models: matricization and mode-wise PCA estimation"
                },
                "summary": "High-dimensional, higher-order tensor data are gaining prominence in a\nvariety of fields, including but not limited to computer vision and network\nanalysis. Tensor factor models, induced from noisy versions of tensor\ndecompositions or factorizations, are natural potent instruments to study a\ncollection of tensor-variate objects that may be dependent or independent.\nHowever, it is still in the early stage of developing statistical inferential\ntheories for the estimation of various low-rank structures, which are customary\nto play the role of signals of tensor factor models. In this paper, we attempt\nto ``decode\" the estimation of a higher-order tensor factor model by leveraging\ntensor matricization. Specifically, we recast it into mode-wise traditional\nhigh-dimensional vector/fiber factor models, enabling the deployment of\nconventional principal components analysis (PCA) for estimation. Demonstrated\nby the Tucker tensor factor model (TuTFaM), which is induced from the noisy\nversion of the widely-used Tucker decomposition, we summarize that estimations\non signal components are essentially mode-wise PCA techniques, and the\ninvolvement of projection and iteration will enhance the signal-to-noise ratio\nto various extent. We establish the inferential theory of the proposed\nestimators, conduct rich simulation experiments, and illustrate how the\nproposed estimations can work in tensor reconstruction, and clustering for\nindependent video and dependent economic datasets, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-dimensional, higher-order tensor data are gaining prominence in a\nvariety of fields, including but not limited to computer vision and network\nanalysis. Tensor factor models, induced from noisy versions of tensor\ndecompositions or factorizations, are natural potent instruments to study a\ncollection of tensor-variate objects that may be dependent or independent.\nHowever, it is still in the early stage of developing statistical inferential\ntheories for the estimation of various low-rank structures, which are customary\nto play the role of signals of tensor factor models. In this paper, we attempt\nto ``decode\" the estimation of a higher-order tensor factor model by leveraging\ntensor matricization. Specifically, we recast it into mode-wise traditional\nhigh-dimensional vector/fiber factor models, enabling the deployment of\nconventional principal components analysis (PCA) for estimation. Demonstrated\nby the Tucker tensor factor model (TuTFaM), which is induced from the noisy\nversion of the widely-used Tucker decomposition, we summarize that estimations\non signal components are essentially mode-wise PCA techniques, and the\ninvolvement of projection and iteration will enhance the signal-to-noise ratio\nto various extent. We establish the inferential theory of the proposed\nestimators, conduct rich simulation experiments, and illustrate how the\nproposed estimations can work in tensor reconstruction, and clustering for\nindependent video and dependent economic datasets, respectively."
                },
                "authors": [
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Guodong Li"
                    },
                    {
                        "name": "Catherine C. Liu"
                    },
                    {
                        "name": "Jianhua Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Guo"
                },
                "author": "Jianhua Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2206.02508v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2206.02508v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04065v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04065v3",
                "updated": "2024-12-12T23:59:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    23,
                    59,
                    23,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-04T17:12:00Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    17,
                    12,
                    0,
                    3,
                    186,
                    0
                ],
                "title": "On the Workflows and Smells of Leaderboard Operations (LBOps): An\n  Exploratory Study of Foundation Model Leaderboards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Workflows and Smells of Leaderboard Operations (LBOps): An\n  Exploratory Study of Foundation Model Leaderboards"
                },
                "summary": "Foundation models (FM), such as large language models (LLMs), which are\nlarge-scale machine learning (ML) models, have demonstrated remarkable\nadaptability in various downstream software engineering (SE) tasks, such as\ncode completion, code understanding, and software development. As a result, FM\nleaderboards have become essential tools for SE teams to compare and select the\nbest third-party FMs for their specific products and purposes. However, the\nlack of standardized guidelines for FM evaluation and comparison threatens the\ntransparency of FM leaderboards and limits stakeholders' ability to perform\neffective FM selection. As a first step towards addressing this challenge, our\nresearch focuses on understanding how these FM leaderboards operate in\nreal-world scenarios (\"leaderboard operations\") and identifying potential\npitfalls and areas for improvement (\"leaderboard smells\"). In this regard, we\ncollect up to 1,045 FM leaderboards from five different sources: GitHub,\nHugging Face Spaces, Papers With Code, spreadsheet and independent platform, to\nexamine their documentation and engage in direct communication with leaderboard\noperators to understand their workflows. Through card sorting and negotiated\nagreement, we identify five distinct workflow patterns and develop a domain\nmodel that captures the key components and their interactions within these\nworkflows. We then identify eight unique types of leaderboard smells in LBOps.\nBy mitigating these smells, SE teams can improve transparency, accountability,\nand collaboration in current LBOps practices, fostering a more robust and\nresponsible ecosystem for FM comparison and selection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models (FM), such as large language models (LLMs), which are\nlarge-scale machine learning (ML) models, have demonstrated remarkable\nadaptability in various downstream software engineering (SE) tasks, such as\ncode completion, code understanding, and software development. As a result, FM\nleaderboards have become essential tools for SE teams to compare and select the\nbest third-party FMs for their specific products and purposes. However, the\nlack of standardized guidelines for FM evaluation and comparison threatens the\ntransparency of FM leaderboards and limits stakeholders' ability to perform\neffective FM selection. As a first step towards addressing this challenge, our\nresearch focuses on understanding how these FM leaderboards operate in\nreal-world scenarios (\"leaderboard operations\") and identifying potential\npitfalls and areas for improvement (\"leaderboard smells\"). In this regard, we\ncollect up to 1,045 FM leaderboards from five different sources: GitHub,\nHugging Face Spaces, Papers With Code, spreadsheet and independent platform, to\nexamine their documentation and engage in direct communication with leaderboard\noperators to understand their workflows. Through card sorting and negotiated\nagreement, we identify five distinct workflow patterns and develop a domain\nmodel that captures the key components and their interactions within these\nworkflows. We then identify eight unique types of leaderboard smells in LBOps.\nBy mitigating these smells, SE teams can improve transparency, accountability,\nand collaboration in current LBOps practices, fostering a more robust and\nresponsible ecosystem for FM comparison and selection."
                },
                "authors": [
                    {
                        "name": "Zhimin Zhao"
                    },
                    {
                        "name": "Abdul Ali Bangash"
                    },
                    {
                        "name": "Filipe Roseiro Côgo"
                    },
                    {
                        "name": "Bram Adams"
                    },
                    {
                        "name": "Ahmed E. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed E. Hassan"
                },
                "author": "Ahmed E. Hassan",
                "arxiv_comment": "awesome foundation model leaderboard list:\n  https://github.com/SAILResearch/awesome-foundation-model-leaderboards",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04065v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04065v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10945v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10945v2",
                "updated": "2024-12-12T23:51:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    23,
                    51,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-08-20T15:34:27Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    15,
                    34,
                    27,
                    1,
                    233,
                    0
                ],
                "title": "HiRED: Attention-Guided Token Dropping for Efficient Inference of\n  High-Resolution Vision-Language Models in Resource-Constrained Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiRED: Attention-Guided Token Dropping for Efficient Inference of\n  High-Resolution Vision-Language Models in Resource-Constrained Environments"
                },
                "summary": "High-resolution Vision-Language Models (VLMs) have been widely used in\nmultimodal tasks to enhance accuracy by preserving detailed image information.\nHowever, these models often generate excessive visual tokens due to encoding\nmultiple partitions of the input image. Processing these excessive visual\ntokens is computationally challenging, especially in resource-constrained\nenvironments with commodity GPUs. To support high-resolution images while\nmeeting resource constraints, we propose High-Resolution Early Dropping\n(HiRED), a token-dropping scheme that operates within a fixed token budget\nbefore the Large Language Model (LLM) stage. HiRED can be integrated with\nexisting high-resolution VLMs in a plug-and-play manner, as it requires no\nadditional training while still maintaining superior accuracy. We strategically\nuse the vision encoder's attention in the initial layers to assess the visual\ncontent of each image partition and allocate the token budget accordingly.\nThen, using the attention in the final layer, we select the most important\nvisual tokens from each partition within the allocated budget, dropping the\nrest. Empirically, when applied to LLaVA-Next-7B on NVIDIA TESLA P40 GPU, HiRED\nwith a 20% token budget increases token generation throughput by 4.7, reduces\nfirst-token generation latency by 15 seconds, and saves 2.3 GB of GPU memory\nfor a single inference. The code is available at\nhttps://github.com/hasanar1f/HiRED.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-resolution Vision-Language Models (VLMs) have been widely used in\nmultimodal tasks to enhance accuracy by preserving detailed image information.\nHowever, these models often generate excessive visual tokens due to encoding\nmultiple partitions of the input image. Processing these excessive visual\ntokens is computationally challenging, especially in resource-constrained\nenvironments with commodity GPUs. To support high-resolution images while\nmeeting resource constraints, we propose High-Resolution Early Dropping\n(HiRED), a token-dropping scheme that operates within a fixed token budget\nbefore the Large Language Model (LLM) stage. HiRED can be integrated with\nexisting high-resolution VLMs in a plug-and-play manner, as it requires no\nadditional training while still maintaining superior accuracy. We strategically\nuse the vision encoder's attention in the initial layers to assess the visual\ncontent of each image partition and allocate the token budget accordingly.\nThen, using the attention in the final layer, we select the most important\nvisual tokens from each partition within the allocated budget, dropping the\nrest. Empirically, when applied to LLaVA-Next-7B on NVIDIA TESLA P40 GPU, HiRED\nwith a 20% token budget increases token generation throughput by 4.7, reduces\nfirst-token generation latency by 15 seconds, and saves 2.3 GB of GPU memory\nfor a single inference. The code is available at\nhttps://github.com/hasanar1f/HiRED."
                },
                "authors": [
                    {
                        "name": "Kazi Hasan Ibn Arif"
                    },
                    {
                        "name": "JinYi Yoon"
                    },
                    {
                        "name": "Dimitrios S. Nikolopoulos"
                    },
                    {
                        "name": "Hans Vandierendonck"
                    },
                    {
                        "name": "Deepu John"
                    },
                    {
                        "name": "Bo Ji"
                    }
                ],
                "author_detail": {
                    "name": "Bo Ji"
                },
                "author": "Bo Ji",
                "arxiv_comment": "Accepted in AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10945v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10945v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.04264v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.04264v5",
                "updated": "2024-12-12T23:17:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    23,
                    17,
                    1,
                    3,
                    347,
                    0
                ],
                "published": "2024-03-17T17:01:45Z",
                "published_parsed": [
                    2024,
                    3,
                    17,
                    17,
                    1,
                    45,
                    6,
                    77,
                    0
                ],
                "title": "Logic Query of Thoughts: Guiding Large Language Models to Answer Complex\n  Logic Queries with Knowledge Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic Query of Thoughts: Guiding Large Language Models to Answer Complex\n  Logic Queries with Knowledge Graphs"
                },
                "summary": "Despite the superb performance in many tasks, large language models (LLMs)\nbear the risk of generating hallucination or even wrong answers when confronted\nwith tasks that demand the accuracy of knowledge. The issue becomes even more\nnoticeable when addressing logic queries that require multiple logic reasoning\nsteps. On the other hand, knowledge graph (KG) based question answering methods\nare capable of accurately identifying the correct answers with the help of\nknowledge graph, yet its accuracy could quickly deteriorate when the knowledge\ngraph itself is sparse and incomplete. It remains a critical challenge on how\nto integrate knowledge graph reasoning with LLMs in a mutually beneficial way\nso as to mitigate both the hallucination problem of LLMs as well as the\nincompleteness issue of knowledge graphs. In this paper, we propose\n'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs\nwith knowledge graph based logic query reasoning. LGOT seamlessly combines\nknowledge graph reasoning and LLMs, effectively breaking down complex logic\nqueries into easy to answer subquestions. Through the utilization of both\nknowledge graph reasoning and LLMs, it successfully derives answers for each\nsubquestion. By aggregating these results and selecting the highest quality\ncandidate answers for each step, LGOT achieves accurate results to complex\nquestions. Our experimental findings demonstrate substantial performance\nenhancements, with up to 20% improvement over ChatGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the superb performance in many tasks, large language models (LLMs)\nbear the risk of generating hallucination or even wrong answers when confronted\nwith tasks that demand the accuracy of knowledge. The issue becomes even more\nnoticeable when addressing logic queries that require multiple logic reasoning\nsteps. On the other hand, knowledge graph (KG) based question answering methods\nare capable of accurately identifying the correct answers with the help of\nknowledge graph, yet its accuracy could quickly deteriorate when the knowledge\ngraph itself is sparse and incomplete. It remains a critical challenge on how\nto integrate knowledge graph reasoning with LLMs in a mutually beneficial way\nso as to mitigate both the hallucination problem of LLMs as well as the\nincompleteness issue of knowledge graphs. In this paper, we propose\n'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs\nwith knowledge graph based logic query reasoning. LGOT seamlessly combines\nknowledge graph reasoning and LLMs, effectively breaking down complex logic\nqueries into easy to answer subquestions. Through the utilization of both\nknowledge graph reasoning and LLMs, it successfully derives answers for each\nsubquestion. By aggregating these results and selecting the highest quality\ncandidate answers for each step, LGOT achieves accurate results to complex\nquestions. Our experimental findings demonstrate substantial performance\nenhancements, with up to 20% improvement over ChatGPT."
                },
                "authors": [
                    {
                        "name": "Lihui Liu"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ruizhong Qiu"
                    },
                    {
                        "name": "Yikun Ban"
                    },
                    {
                        "name": "Eunice Chan"
                    },
                    {
                        "name": "Yangqiu Song"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Hanghang Tong"
                    }
                ],
                "author_detail": {
                    "name": "Hanghang Tong"
                },
                "author": "Hanghang Tong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.04264v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.04264v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03626v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03626v3",
                "updated": "2024-12-12T23:03:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    23,
                    3,
                    54,
                    3,
                    347,
                    0
                ],
                "published": "2024-04-04T17:48:28Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    17,
                    48,
                    28,
                    3,
                    95,
                    0
                ],
                "title": "Training LLMs over Neurally Compressed Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLMs over Neurally Compressed Text"
                },
                "summary": "In this paper, we explore the idea of training large language models (LLMs)\nover highly compressed text. While standard subword tokenizers compress text by\na small factor, neural text compressors can achieve much higher rates of\ncompression. If it were possible to train LLMs directly over neurally\ncompressed text, this would confer advantages in training and serving\nefficiency, as well as easier handling of long text spans. The main obstacle to\nthis goal is that strong compression tends to produce opaque outputs that are\nnot well-suited for learning. In particular, we find that text na\\\"ively\ncompressed via Arithmetic Coding is not readily learnable by LLMs. To overcome\nthis, we propose Equal-Info Windows, a novel compression technique whereby text\nis segmented into blocks that each compress to the same bit length. Using this\nmethod, we demonstrate effective learning over neurally compressed text that\nimproves with scale, and outperforms byte-level baselines by a wide margin on\nperplexity and inference speed benchmarks. While our method delivers worse\nperplexity than subword tokenizers for models trained with the same parameter\ncount, it has the benefit of shorter sequence lengths. Shorter sequence lengths\nrequire fewer autoregressive generation steps, and reduce latency. Finally, we\nprovide extensive analysis of the properties that contribute to learnability,\nand offer concrete suggestions for how to further improve the performance of\nhigh-compression tokenizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore the idea of training large language models (LLMs)\nover highly compressed text. While standard subword tokenizers compress text by\na small factor, neural text compressors can achieve much higher rates of\ncompression. If it were possible to train LLMs directly over neurally\ncompressed text, this would confer advantages in training and serving\nefficiency, as well as easier handling of long text spans. The main obstacle to\nthis goal is that strong compression tends to produce opaque outputs that are\nnot well-suited for learning. In particular, we find that text na\\\"ively\ncompressed via Arithmetic Coding is not readily learnable by LLMs. To overcome\nthis, we propose Equal-Info Windows, a novel compression technique whereby text\nis segmented into blocks that each compress to the same bit length. Using this\nmethod, we demonstrate effective learning over neurally compressed text that\nimproves with scale, and outperforms byte-level baselines by a wide margin on\nperplexity and inference speed benchmarks. While our method delivers worse\nperplexity than subword tokenizers for models trained with the same parameter\ncount, it has the benefit of shorter sequence lengths. Shorter sequence lengths\nrequire fewer autoregressive generation steps, and reduce latency. Finally, we\nprovide extensive analysis of the properties that contribute to learnability,\nand offer concrete suggestions for how to further improve the performance of\nhigh-compression tokenizers."
                },
                "authors": [
                    {
                        "name": "Brian Lester"
                    },
                    {
                        "name": "Jaehoon Lee"
                    },
                    {
                        "name": "Alex Alemi"
                    },
                    {
                        "name": "Jeffrey Pennington"
                    },
                    {
                        "name": "Adam Roberts"
                    },
                    {
                        "name": "Jascha Sohl-Dickstein"
                    },
                    {
                        "name": "Noah Constant"
                    }
                ],
                "author_detail": {
                    "name": "Noah Constant"
                },
                "author": "Noah Constant",
                "arxiv_comment": "Accepted in TMLR https://openreview.net/forum?id=pRvhMSV48t",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03626v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03626v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09745v1",
                "updated": "2024-12-12T22:28:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    22,
                    28,
                    3,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T22:28:03Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    22,
                    28,
                    3,
                    3,
                    347,
                    0
                ],
                "title": "AiEDA: Agentic AI Design Framework for Digital ASIC System Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AiEDA: Agentic AI Design Framework for Digital ASIC System Design"
                },
                "summary": "The paper addresses advancements in Generative Artificial Intelligence\n(GenAI) and digital chip design, highlighting the integration of Large Language\nModels (LLMs) in automating hardware description and design. LLMs, known for\ngenerating human-like content, are now being explored for creating hardware\ndescription languages (HDLs) like Verilog from natural language inputs. This\napproach aims to enhance productivity and reduce costs in VLSI system design.\nThe study introduces \"AiEDA\", a proposed agentic design flow framework for\ndigital ASIC systems, leveraging autonomous AI agents to manage complex design\ntasks. AiEDA is designed to streamline the transition from conceptual design to\nGDSII layout using an open-source toolchain. The framework is demonstrated\nthrough the design of an ultra-low-power digital ASIC for KeyWord Spotting\n(KWS). The use of agentic AI workflows promises to improve design efficiency by\nautomating the integration of multiple design tools, thereby accelerating the\ndevelopment process and addressing the complexities of hardware design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper addresses advancements in Generative Artificial Intelligence\n(GenAI) and digital chip design, highlighting the integration of Large Language\nModels (LLMs) in automating hardware description and design. LLMs, known for\ngenerating human-like content, are now being explored for creating hardware\ndescription languages (HDLs) like Verilog from natural language inputs. This\napproach aims to enhance productivity and reduce costs in VLSI system design.\nThe study introduces \"AiEDA\", a proposed agentic design flow framework for\ndigital ASIC systems, leveraging autonomous AI agents to manage complex design\ntasks. AiEDA is designed to streamline the transition from conceptual design to\nGDSII layout using an open-source toolchain. The framework is demonstrated\nthrough the design of an ultra-low-power digital ASIC for KeyWord Spotting\n(KWS). The use of agentic AI workflows promises to improve design efficiency by\nautomating the integration of multiple design tools, thereby accelerating the\ndevelopment process and addressing the complexities of hardware design."
                },
                "authors": [
                    {
                        "name": "Aditya Patra"
                    },
                    {
                        "name": "Saroj Rout"
                    },
                    {
                        "name": "Arun Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Arun Ravindran"
                },
                "author": "Arun Ravindran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]