[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2408.05996v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05996v1",
                "updated": "2024-08-12T08:46:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:46:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    46,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-based Proactive Caching for Sensing Data in Internet of Vehicles"
                },
                "summary": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensing data (SD) plays an important role in safe-related applications for\nInternet of Vehicles. Proactively caching required sensing data (SD) is a\npivotal strategy for alleviating network congestion and improving data\naccessibility. Despite merits, existing studies predominantly address SD\ncaching within a single time slot, which may not be scalable to scenarios\ninvolving multi-slots. Furthermore, the oversight of service capacity at\ncaching nodes could lead to significant queuing delays in SD reception. To\ntackle these limitations, we jointly consider the problem of anchoring caching\nplacement and requests allocation for SD. A value model incorporating both\ntemporal and spacial characteristics is first proposed to estimate the\nsignificance of different caching decisions. Subsequently, a stochastic integer\nnonlinear programming model is provided to optimize the long-term system\nperformance, which is converted into a series of online optimization problem by\nleveraging the Lyapunov method and linearized via introducing auxiliary\nvariables. To expedite the solution, we provide a binary quantum particle swarm\noptimization based algorithm with quadratic time complexity. Numerical\ninvestigations demonstrate the superiority of proposed algorithms compared with\nother schemes in terms of energy consumption, response latency, and cache-hit\nratio."
                },
                "authors": [
                    {
                        "name": "Yantong Wang"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Hui Ji"
                    },
                    {
                        "name": "Jiande Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jiande Sun"
                },
                "author": "Jiande Sun",
                "arxiv_comment": "14 pages,10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05996v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05996v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19895v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19895v2",
                "updated": "2024-08-12T07:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-29T11:17:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    11,
                    17,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Culsans: An Efficient Snoop-based Coherency Unit for the CVA6 Open\n  Source RISC-V application processor"
                },
                "summary": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Multi-Processing (SMP) based on cache coherency is crucial for\nhigh-end embedded systems like automotive applications. RISC-V is gaining\ntraction, and open-source hardware (OSH) platforms offer solutions to issues\nsuch as IP costs and vendor dependency. Existing multi-core cache-coherent\nRISC-V platforms are complex and not efficient for small embedded core\nclusters. We propose an open-source SystemVerilog implementation of a\nlightweight snoop-based cache-coherent cluster of Linux-capable CVA6 cores. Our\ndesign uses the MOESI protocol via the Arm's AMBA ACE protocol. Evaluated with\nSplash-3 benchmarks, our solution shows up to 32.87% faster performance in a\ndual-core setup and an average improvement of 15.8% over OpenPiton. Synthesized\nusing GF 22nm FDSOI technology, the Cache Coherency Unit occupies only 1.6% of\nthe system area."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Luca Valente"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Enrico Zelioli"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Massimiliano Giacometti"
                    },
                    {
                        "name": "Abdul Basit Sajjad"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "4 pages, 4 figures, DSD2024 and SEAA2024 Works in Progress Session\n  AUG 2024; Updated the acknowledgments",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19895v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19895v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05912v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05912v1",
                "updated": "2024-08-12T03:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Correct Wrong Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correct Wrong Path"
                },
                "summary": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern OOO CPUs have very deep pipelines with large branch misprediction\nrecovery penalties. Speculatively executed instructions on the wrong path can\nsignificantly change cache state, depending on speculation levels. Architects\noften employ trace-driven simulation models in the design exploration stage,\nwhich sacrifice precision for speed. Trace-driven simulators are orders of\nmagnitude faster than execution-driven models, reducing the often hundreds of\nthousands of simulation hours needed to explore new micro-architectural ideas.\nDespite this strong benefit of trace-driven simulation, these often fail to\nadequately model the consequences of wrong path because obtaining them is\nnontrivial. Prior works consider either a positive or negative impact of wrong\npath but not both. Here, we examine wrong path execution in simulation results\nand design a set of infrastructure for enabling wrong-path execution in a trace\ndriven simulator. Our analysis shows the wrong path affects structures on both\nthe instruction and data sides extensively, resulting in performance variations\nranging from $-3.05$\\% to $20.9$\\% when ignoring wrong path. To benefit the\nresearch community and enhance the accuracy of simulators, we opened our traces\nand tracing utility in the hopes that industry can provide wrong-path traces\ngenerated by their internal simulators, enabling academic simulation without\nexposing industry IP."
                },
                "authors": [
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Sankara Prasad Ramesh"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Svilen Kanev"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "Daniel A. Jim√©nez"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "arxiv_comment": "5 pages, 7 Figures, Submited to Computer Architecture Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05912v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05912v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04043v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04043v3",
                "updated": "2024-08-13T13:31:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    31,
                    34,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-07T18:51:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    18,
                    51,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "Ownership in low-level intermediate representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ownership in low-level intermediate representation"
                },
                "summary": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of ownership in high level languages can aid both the programmer\nand the compiler to reason about the validity of memory operations. Previously,\nownership semantics has been used successfully in high level automatic program\nverification to model a reference to data by a first order logic (FOL)\nrepresentation of data instead of maintaining an address map. However,\nownership semantics is not used in low level program verification. We have\nidentified two challenges. First, ownership information is lost when a program\nis compiled to a low level intermediate representation (e.g., in LLVM IR).\nSecond, pointers in low level programs point to bytes using an address map\n(e.g., in unsafe Rust) and thus the verification condition (VC) cannot always\nreplace a pointer by its FOL abstraction. To remedy the situation, we develop\nownership semantics for an LLVM like low level intermediate representation.\nUsing these semantics, the VC can opportunistically model some memory accesses\nby a direct access of a pointer cache that stores byte representation of data.\nThis scheme reduces instances where an address map must be maintained,\nespecially for mostly safe programs that follow ownership semantics. For unsafe\nfunctionality, memory accesses are modelled by operations on an address map and\nwe provide mechanisms to keep the address map and pointer cache in sync. We\nimplement these semantics in SEABMC, a bit precise bounded model checker for\nLLVM. For evaluation, the source programs are assumed to be written in C. Since\nC does not have ownership built in, suitable macros are added that introduce\nand preserve ownership during translation to LLVM like IR for verification.\nThis approach is evaluated on mature open source C code. For both handcrafted\nbenchmarks and practical programs, we observe a speedup of $1.3x-5x$ during SMT\nsolving."
                },
                "authors": [
                    {
                        "name": "Siddharth Priya"
                    },
                    {
                        "name": "Arie Gurfinkel"
                    }
                ],
                "author_detail": {
                    "name": "Arie Gurfinkel"
                },
                "author": "Arie Gurfinkel",
                "arxiv_comment": "FMCAD 2024 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04043v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04043v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v2",
                "updated": "2024-08-11T16:35:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    35,
                    10,
                    6,
                    224,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "Added Section IV - (performance analysis of proposed HPDA\n  construction). The term 'coding delay' is formally defined (page no. 5). 14\n  pages, 10 figures and 9 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.19410v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.19410v2",
                "updated": "2024-08-11T08:07:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    8,
                    7,
                    28,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-29T18:07:58Z",
                "published_parsed": [
                    2024,
                    2,
                    29,
                    18,
                    7,
                    58,
                    3,
                    60,
                    0
                ],
                "title": "Genie: Smart ROS-based Caching for Connected Autonomous Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genie: Smart ROS-based Caching for Connected Autonomous Robots"
                },
                "summary": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the promising future of autonomous robots, several key issues\ncurrently remain that can lead to compromised performance and safety. One such\nissue is latency, where we find that even the latest embedded platforms from\nNVIDIA fail to execute intelligence tasks (e.g., object detection) of\nautonomous vehicles in a real-time fashion. One remedy to this problem is the\npromising paradigm of edge computing. Through collaboration with our industry\npartner, we identify key prohibitive limitations of the current edge mindset:\n(1) servers are not distributed enough and thus, are not close enough to\nvehicles, (2) current proposed edge solutions do not provide substantially\nbetter performance and extra information specific to autonomous vehicles to\nwarrant their cost to the user, and (3) the state-of-the-art solutions are not\ncompatible with popular frameworks used in autonomous systems, particularly the\nRobot Operating System (ROS).\n  To remedy these issues, we provide Genie, an encapsulation technique that can\nenable transparent caching in ROS in a non-intrusive way (i.e., without\nmodifying the source code), can build the cache in a distributed manner (in\ncontrast to traditional central caching methods), and can construct a\ncollective three-dimensional object map to provide substantially better latency\n(even on low-power edge servers) and higher quality data to all vehicles in a\ncertain locality. We fully implement our design on state-of-the-art\nindustry-adopted embedded and edge platforms, using the prominent autonomous\ndriving software Autoware, and find that Genie can enhance the latency of\nAutoware Vision Detector by 82% on average, enable object reusability 31% of\nthe time on average and as much as 67% for the incoming requests, and boost the\nconfidence in its object map considerably over time."
                },
                "authors": [
                    {
                        "name": "Zexin Li"
                    },
                    {
                        "name": "Soroush Bateni"
                    },
                    {
                        "name": "Cong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cong Liu"
                },
                "author": "Cong Liu",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.19410v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.19410v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05646v1",
                "updated": "2024-08-10T22:47:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T22:47:12Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    22,
                    47,
                    12,
                    5,
                    223,
                    0
                ],
                "title": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen Attention: Attention in Low-Rank Space for KV Cache Compression"
                },
                "summary": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Gobinda Saha"
                    },
                    {
                        "name": "Sakshi Choudhary"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "12 page, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05614v1",
                "updated": "2024-08-10T19:17:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "published": "2024-08-10T19:17:46Z",
                "published_parsed": [
                    2024,
                    8,
                    10,
                    19,
                    17,
                    46,
                    5,
                    223,
                    0
                ],
                "title": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICGMM: CXL-enabled Memory Expansion with Intelligent Caching Using\n  Gaussian Mixture Model"
                },
                "summary": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Express Link (CXL) emerges as a solution for wide gap between\ncomputational speed and data communication rates among host and multiple\ndevices. It fosters a unified and coherent memory space between host and CXL\nstorage devices such as such as Solid-state drive (SSD) for memory expansion,\nwith a corresponding DRAM implemented as the device cache. However, this\nintroduces challenges such as substantial cache miss penalties, sub-optimal\ncaching due to data access granularity mismatch between the DRAM \"cache\" and\nSSD \"memory\", and inefficient hardware cache management. To address these\nissues, we propose a novel solution, named ICGMM, which optimizes caching and\neviction directly on hardware, employing a Gaussian Mixture Model (GMM)-based\napproach. We prototype our solution on an FPGA board, which demonstrates a\nnoteworthy improvement compared to the classic Least Recently Used (LRU) cache\nstrategy. We observe a decrease in the cache miss rate ranging from 0.32% to\n6.14%, leading to a substantial 16.23% to 39.14% reduction in the average SSD\naccess latency. Furthermore, when compared to the state-of-the-art Long\nShort-Term Memory (LSTM)-based cache policies, our GMM algorithm on FPGA\nshowcases an impressive latency reduction of over 10,000 times. Remarkably,\nthis is achieved while demanding much fewer hardware resources."
                },
                "authors": [
                    {
                        "name": "Hanqiu Chen"
                    },
                    {
                        "name": "Yitu Wang"
                    },
                    {
                        "name": "Luis Vitorio Cargnini"
                    },
                    {
                        "name": "Mohammadreza Soltaniyeh"
                    },
                    {
                        "name": "Dongyang Li"
                    },
                    {
                        "name": "Gongjin Sun"
                    },
                    {
                        "name": "Pradeep Subedi"
                    },
                    {
                        "name": "Andrew Chang"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Cong Hao"
                    }
                ],
                "author_detail": {
                    "name": "Cong Hao"
                },
                "author": "Cong Hao",
                "arxiv_comment": "This paper is accepted by DAC2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05171v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05171v1",
                "updated": "2024-08-09T16:48:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T16:48:01Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    16,
                    48,
                    1,
                    4,
                    222,
                    0
                ],
                "title": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-resolved measurement of neutron energy isotropy in a\n  sheared-flow-stabilized Z pinch"
                },
                "summary": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Previous measurements of neutron energy using fast plastic scintillators\nwhile operating the Fusion Z Pinch Experiment (FuZE) constrained the energy of\nany yield-producing deuteron beams to less than $4.65 keV$. FuZE has since been\noperated at increasingly higher input power, resulting in increased plasma\ncurrent and larger fusion neutron yields. A detailed experimental study of the\nneutron energy isotropy in these regimes applies more stringent limits to\npossible contributions from beam-target fusion. The FuZE device operated at\n$-25~kV$ charge voltage has resulted in average plasma currents of $370~kA$ and\nD-D fusion neutron yields of $4\\times10^7$ neutrons per discharge. Measurements\nof the neutron energy isotropy under these operating conditions demonstrates\nthe energy of deuteron beams is less than $7.4 \\pm 5.6^\\mathrm{(stat)} \\pm\n3.7^\\mathrm{(syst)}~keV$. Characterization of the detector response has reduced\nthe number of free parameters in the fit of the neutron energy distribution,\nimproving the confidence in the forward-fit method. Gamma backgrounds have been\nmeasured and the impact of these contributions on the isotropy results have\nbeen studied. Additionally, a time dependent measurement of the isotropy has\nbeen resolved for the first time, indicating increases to possible deuteron\nbeam energies at late times. This suggests the possible growth of $m$=0\ninstabilities at the end of the main radiation event but confirms that the\nmajority of the neutron production exhibits isotropy consistent with\nthermonuclear origin."
                },
                "authors": [
                    {
                        "name": "R. A. Ryan"
                    },
                    {
                        "name": "P. E. Tsai"
                    },
                    {
                        "name": "A. R. Johansen"
                    },
                    {
                        "name": "A. Youmans"
                    },
                    {
                        "name": "D. P. Higginson"
                    },
                    {
                        "name": "J. M. Mitrani"
                    },
                    {
                        "name": "C. S. Adams"
                    },
                    {
                        "name": "D. A. Sutherland"
                    },
                    {
                        "name": "B. Levitt"
                    },
                    {
                        "name": "U. Shumlak"
                    }
                ],
                "author_detail": {
                    "name": "U. Shumlak"
                },
                "author": "U. Shumlak",
                "arxiv_comment": "16 pages, 11 figures, submitted to Journal of Nuclear Fusion",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05171v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05171v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04870v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04870v1",
                "updated": "2024-08-09T05:20:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "published": "2024-08-09T05:20:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    5,
                    20,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "ConfusedPilot: Compromising Enterprise Information Integrity and\n  Confidentiality with Copilot for Microsoft 365",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConfusedPilot: Compromising Enterprise Information Integrity and\n  Confidentiality with Copilot for Microsoft 365"
                },
                "summary": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval augmented generation (RAG) is a process where a large language\nmodel (LLM) retrieves useful information from a database and then generates the\nresponses. It is becoming popular in enterprise settings for daily business\noperations. For example, Copilot for Microsoft 365 has accumulated millions of\nbusinesses. However, the security implications of adopting such RAG-based\nsystems are unclear.\n  In this paper, we introduce ConfusedPilot, a class of security\nvulnerabilities of RAG systems that confuse Copilot and cause integrity and\nconfidentiality violations in its responses. First, we investigate a\nvulnerability that embeds malicious text in the modified prompt in RAG,\ncorrupting the responses generated by the LLM. Second, we demonstrate a\nvulnerability that leaks secret data, which leverages the caching mechanism\nduring retrieval. Third, we investigate how both vulnerabilities can be\nexploited to propagate misinformation within the enterprise and ultimately\nimpact its operations, such as sales and manufacturing. We also discuss the\nroot cause of these attacks by investigating the architecture of a RAG-based\nsystem. This study highlights the security vulnerabilities in today's RAG-based\nsystems and proposes design guidelines to secure future RAG-based systems."
                },
                "authors": [
                    {
                        "name": "Ayush RoyChowdhury"
                    },
                    {
                        "name": "Mulong Luo"
                    },
                    {
                        "name": "Prateek Sahu"
                    },
                    {
                        "name": "Sarbartha Banerjee"
                    },
                    {
                        "name": "Mohit Tiwari"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Tiwari"
                },
                "author": "Mohit Tiwari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04870v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04870v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03675v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03675v2",
                "updated": "2024-08-08T01:20:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    8,
                    1,
                    20,
                    13,
                    3,
                    221,
                    0
                ],
                "published": "2024-08-07T10:31:07Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    10,
                    31,
                    7,
                    2,
                    220,
                    0
                ],
                "title": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time"
                },
                "summary": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Guoxia Wang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Shiyao Cui"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Hua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wu"
                },
                "author": "Hua Wu",
                "arxiv_comment": "Accepted by ACL 2024 (main conference, long paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03675v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03675v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.10978v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.10978v2",
                "updated": "2024-08-07T23:48:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    23,
                    48,
                    59,
                    2,
                    220,
                    0
                ],
                "published": "2022-10-20T02:58:36Z",
                "published_parsed": [
                    2022,
                    10,
                    20,
                    2,
                    58,
                    36,
                    3,
                    293,
                    0
                ],
                "title": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey on Edge Data Integrity Verification: Fundamentals\n  and Future Trends"
                },
                "summary": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in edge computing~(EC) have pushed cloud-based data caching\nservices to edge, however, such emerging edge storage comes with numerous\nchallenging and unique security issues. One of them is the problem of edge data\nintegrity verification (EDIV) which coordinates multiple participants (e.g.,\ndata owners and edge nodes) to inspect whether data cached on edge is\nauthentic. To date, various solutions have been proposed to address the EDIV\nproblem, while there is no systematic review. Thus, we offer a comprehensive\nsurvey for the first time, aiming to show current research status, open\nproblems, and potentially promising insights for readers to further investigate\nthis under-explored field. Specifically, we begin by stating the significance\nof the EDIV problem, the integrity verification difference between data cached\non cloud and edge, and three typical system models with corresponding\ninspection processes. To thoroughly assess prior research efforts, we\nsynthesize a universal criteria framework that an effective verification\napproach should satisfy. On top of it, a schematic development timeline is\ndeveloped to reveal the research advance on EDIV in a sequential manner,\nfollowed by a detailed review of the existing EDIV solutions. Finally, we\nhighlight intriguing research challenges and possible directions for future\nwork, along with a discussion on how forthcoming technology, e.g., machine\nlearning and context-aware security, can augment security in EC. Given our\nfindings, some major observations are: there is a noticeable trend to equip\nEDIV solutions with various functions and diversify study scenarios; completing\nEDIV within two types of participants (i.e., data owner and edge nodes) is\ngarnering escalating interest among researchers; although the majority of\nexisting methods rely on cryptography, emerging technology is being explored to\nhandle the EDIV problem."
                },
                "authors": [
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Youyang Qu"
                    },
                    {
                        "name": "Yong Xiang"
                    },
                    {
                        "name": "Md Palash Uddin"
                    },
                    {
                        "name": "Dezhong Peng"
                    },
                    {
                        "name": "Longxiang Gao"
                    }
                ],
                "author_detail": {
                    "name": "Longxiang Gao"
                },
                "author": "Longxiang Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.10978v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.10978v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04107v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04107v1",
                "updated": "2024-08-07T22:10:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T22:10:26Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    22,
                    10,
                    26,
                    2,
                    220,
                    0
                ],
                "title": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference"
                },
                "summary": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code."
                },
                "authors": [
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Haiying Shen"
                    }
                ],
                "author_detail": {
                    "name": "Haiying Shen"
                },
                "author": "Haiying Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04107v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04107v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19547v2",
                "updated": "2024-08-07T20:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    20,
                    43,
                    10,
                    2,
                    220,
                    0
                ],
                "published": "2024-07-28T17:46:15Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    17,
                    46,
                    15,
                    6,
                    210,
                    0
                ],
                "title": "Temporal Feature Matters: A Framework for Diffusion Model Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal Feature Matters: A Framework for Diffusion Model Quantization"
                },
                "summary": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration.."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Yuhang Li"
                    },
                    {
                        "name": "Jiwen Lu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2311.16503",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03652v1",
                "updated": "2024-08-07T09:34:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "published": "2024-08-07T09:34:55Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    9,
                    34,
                    55,
                    2,
                    220,
                    0
                ],
                "title": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search"
                },
                "summary": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task."
                },
                "authors": [
                    {
                        "name": "Ahmed Abdou"
                    },
                    {
                        "name": "Tasneem Mohsen"
                    }
                ],
                "author_detail": {
                    "name": "Tasneem Mohsen"
                },
                "author": "Tasneem Mohsen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03308v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03308v1",
                "updated": "2024-08-06T17:16:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T17:16:19Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    17,
                    16,
                    19,
                    1,
                    219,
                    0
                ],
                "title": "Potential and Limitation of High-Frequency Cores and Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential and Limitation of High-Frequency Cores and Caches"
                },
                "summary": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the potential of cryogenic computing and superconducting\nelectronics as promising alternatives to traditional semiconductor devices. As\nsemiconductor devices face challenges such as increased leakage currents and\nreduced performance at higher temperatures, these novel technologies offer high\nperformance and low power computation. Cryogenic computing operates at\nultra-low temperatures near 77 K, leading to lower leakage currents and\nimproved electron mobility. On the other hand, superconducting electronics,\noperating near 0 K, allow electrons to flow without resistance, offering the\npotential for ultra-low-power, high-speed computation. This study presents a\ncomprehensive performance modeling and analysis of these technologies and\nprovides insights into their potential benefits and limitations. We implement\nmodels of in-order and out-of-order cores operating at high clock frequencies\nassociated with superconducting electronics and cryogenic computing in gem5. We\nevaluate the performance of these components using workloads representative of\nreal-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the\npotential speedups achievable by these components and the limitations posed by\ncache bandwidth. This work provides valuable insights into the performance\nimplications and design trade-offs associated with cryogenic and\nsuperconducting technologies, laying the foundation for future research in this\nfield using gem5."
                },
                "authors": [
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Anusheel Nand"
                    },
                    {
                        "name": "Jason Lowe-Power"
                    }
                ],
                "author_detail": {
                    "name": "Jason Lowe-Power"
                },
                "author": "Jason Lowe-Power",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03308v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03308v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02999v1",
                "updated": "2024-08-06T07:12:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T07:12:09Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    7,
                    12,
                    9,
                    1,
                    219,
                    0
                ],
                "title": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Probabilistic Minimally Adequate Teachers for DFA Learning"
                },
                "summary": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of intelligence in large language models (LLMs) has inspired\ninvestigations into their integration into automata learning. This paper\nintroduces the probabilistic Minimally Adequate Teacher (pMAT) formulation,\nwhich leverages a probabilistic oracle that could give persistent errors\nrandomly during answering the membership queries for deterministic finite\nautomata (DFA) learning. Given the tendency of LLMs to produce hallucinatory\ncontent, we have developed techniques to improve answer accuracy and ensure the\ncorrectness of the learned automata. We propose the $\\mathtt{Discrimination}$\nprompt as well as the $\\mathtt{Verification}$ prompt and explore their\nadvantages over common prompts. Additionally, we compare DFA learning\nperformance between the TTT algorithm and common active learning algorithms. To\naddress the exponential number of persistent errors, we implement a dynamic\nquery cache refinement algorithm that identifies and corrects conflicting\nqueries by combining the active and passive learning algorithms. The empirical\nresults demonstrate the robustness and efficiency of our approach, providing a\ntheoretical foundation for automata learning with LLMs in the loop."
                },
                "authors": [
                    {
                        "name": "Lekai Chen"
                    },
                    {
                        "name": "Ashutosh Trivedi"
                    },
                    {
                        "name": "Alvaro Velasquez"
                    }
                ],
                "author_detail": {
                    "name": "Alvaro Velasquez"
                },
                "author": "Alvaro Velasquez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.FL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02911v1",
                "updated": "2024-08-06T02:51:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "published": "2024-08-06T02:51:22Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    2,
                    51,
                    22,
                    1,
                    219,
                    0
                ],
                "title": "NVPC: A Transparent NVM Page Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NVPC: A Transparent NVM Page Cache"
                },
                "summary": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a compatible utilization of NVM, NVM-specialized kernel file systems\nand NVM-based disk file system accelerators have been proposed. However, these\nstudies only focus on one or several characteristics of NVM, while failing to\nexploit its best practice by putting NVM in the proper position of the whole\nstorage stack. In this paper, we present NVPC, a transparent acceleration to\nexisting kernel file systems with an NVM-enhanced page cache. The acceleration\nlies in two aspects, respectively matching the desperate needs of existing disk\nfile systems: sync writes and cache-missed operations. Besides, the fast DRAM\npage cache is preserved for cache-hit operations. For sync writes, a\nhigh-performance log-based sync absorbing area is provided to redirect data\ndestination from the slow disk to the fast NVM. Meanwhile, the byte-addressable\nfeature of NVM is used to prevent write amplification. For cache-missed\noperations, NVPC makes use of the idle space on NVM to extend the DRAM page\ncache, so that more and larger workloads can fit into the cache. NVPC is\nentirely implemented as a page cache, thus can provide efficient speed-up to\ndisk file systems with full transparency to users and full compatibility to\nlower file systems.\n  In Filebench macro-benchmarks, NVPC achieves at most 3.55x, 2.84x, and 2.64x\nfaster than NOVA, Ext-4, and SPFS. In RocksDB workloads with working set larger\nthan DRAM, NVPC achieves 1.12x, 2.59x, and 2.11x faster than NOVA, Ext-4, and\nSPFS. Meanwhile, NVPC gains positive revenue from NOVA, Ext-4, and SPFS in\n62.5% of the tested cases in our read/write/sync mixed evaluation,\ndemonstrating that NVPC is more balanced and adaptive to complex real-world\nworkloads. Experimental results also show that NVPC is the only method that\naccelerates Ext-4 in particular cases for up to 15.19x, with no slow-down to\nany other use cases."
                },
                "authors": [
                    {
                        "name": "Guoyu Wang"
                    },
                    {
                        "name": "Xilong Che"
                    },
                    {
                        "name": "Haoyang Wei"
                    },
                    {
                        "name": "Shuo Chen"
                    },
                    {
                        "name": "Puyi He"
                    },
                    {
                        "name": "Juncheng Hu"
                    }
                ],
                "author_detail": {
                    "name": "Juncheng Hu"
                },
                "author": "Juncheng Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02409v1",
                "updated": "2024-08-05T12:09:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T12:09:50Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    12,
                    9,
                    50,
                    0,
                    218,
                    0
                ],
                "title": "Electron-beam-induced modification of gold microparticles in an SEM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced modification of gold microparticles in an SEM"
                },
                "summary": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electron-beam-induced conversion of materials in a transmission electron\nmicroscope uses the high power density of a localized electron beam of\nacceleration voltages above 100 kV as an energy source to transform matter at\nthe sub-micron scale. Here, the e-beam-induced transformation of precursor\nmicroparticles employing a low-energy e-beam with an acceleration voltage of 30\nkV in a scanning electron microscope is developed to increase the versatility\nand efficiency of the technique. Under these conditions, the technique can be\nclassified between e-beam lithography, where the e-beam is used to mill holes\nin or grow some different material onto a substrate, and e-beam welding, where\nmatter can be welded together when overcoming the melting phase. Modifying gold\nmicroparticles on an amorphous SiOx substrate reveals the dominant role of\ninelastic electron-matter interaction and subsequent localized heating for the\nobserved melting and vaporization of the precursor microparticles under the\nelectron beam. Monte-Carlo scattering simulations and thermodynamic modeling\nfurther support the findings."
                },
                "authors": [
                    {
                        "name": "Kristina Weinel"
                    },
                    {
                        "name": "Marc Benjamin Hahn"
                    },
                    {
                        "name": "Axel Lubk"
                    },
                    {
                        "name": "Wen Feng"
                    },
                    {
                        "name": "Ignacio Gonzalez Martinez"
                    },
                    {
                        "name": "Bernd B√ºchner"
                    },
                    {
                        "name": "Leonardo Agudo J√°come"
                    }
                ],
                "author_detail": {
                    "name": "Leonardo Agudo J√°come"
                },
                "author": "Leonardo Agudo J√°come",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05235v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05235v1",
                "updated": "2024-08-05T09:07:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "published": "2024-08-05T09:07:06Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    9,
                    7,
                    6,
                    0,
                    218,
                    0
                ],
                "title": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference\n  Serving"
                },
                "summary": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) gain traction, their reliance on power-hungry\nGPUs places ever-increasing energy demands, raising environmental and monetary\nconcerns. Inference dominates LLM workloads, presenting a critical challenge\nfor providers: minimizing energy costs under Service-Level Objectives (SLOs)\nthat ensure optimal user experience. In this paper, we present\n\\textit{throttLL'eM}, a framework that reduces energy consumption while meeting\nSLOs through the use of instance and GPU frequency scaling.\n\\textit{throttLL'eM} features mechanisms that project future KV cache usage and\nbatch size. Leveraging a Machine-Learning (ML) model that receives these\nprojections as inputs, \\textit{throttLL'eM} manages performance at the\niteration level to satisfy SLOs with reduced frequencies and instance sizes. We\nshow that the proposed ML model achieves $R^2$ scores greater than 0.97 and\nmiss-predicts performance by less than 1 iteration per second on average.\nExperimental results on LLM inference traces show that \\textit{throttLL'eM}\nachieves up to 43.8\\% lower energy consumption and an energy efficiency\nimprovement of at least $1.71\\times$ under SLOs, when compared to NVIDIA's\nTriton server."
                },
                "authors": [
                    {
                        "name": "Andreas Kosmas Kakolyris"
                    },
                    {
                        "name": "Dimosthenis Masouros"
                    },
                    {
                        "name": "Petros Vavaroutsos"
                    },
                    {
                        "name": "Sotirios Xydis"
                    },
                    {
                        "name": "Dimitrios Soudris"
                    }
                ],
                "author_detail": {
                    "name": "Dimitrios Soudris"
                },
                "author": "Dimitrios Soudris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05235v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05235v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11912v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11912v3",
                "updated": "2024-08-04T00:58:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    58,
                    4,
                    6,
                    217,
                    0
                ],
                "published": "2024-04-18T05:25:54Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    5,
                    25,
                    54,
                    3,
                    109,
                    0
                ],
                "title": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding"
                },
                "summary": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Xinyu Yang"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "arxiv_comment": "COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11912v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11912v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01890v1",
                "updated": "2024-08-04T00:38:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "published": "2024-08-04T00:38:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    0,
                    38,
                    34,
                    6,
                    217,
                    0
                ],
                "title": "Cross-layer Attention Sharing for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-layer Attention Sharing for Large Language Models"
                },
                "summary": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B."
                },
                "authors": [
                    {
                        "name": "Yongyu Mu"
                    },
                    {
                        "name": "Yuzhang Wu"
                    },
                    {
                        "name": "Yuchun Fan"
                    },
                    {
                        "name": "Chenglong Wang"
                    },
                    {
                        "name": "Hengyu Li"
                    },
                    {
                        "name": "Qiaozhi He"
                    },
                    {
                        "name": "Murun Yang"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "Working in process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01519v1",
                "updated": "2024-08-02T18:25:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-02T18:25:57Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    18,
                    25,
                    57,
                    4,
                    215,
                    0
                ],
                "title": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Material Decomposition Using Spectral Diffusion Posterior Sampling"
                },
                "summary": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many spectral CT applications require accurate material decomposition.\nExisting material decomposition algorithms are often susceptible to significant\nnoise magnification or, in the case of one-step model-based approaches,\nhampered by slow convergence rates and large computational requirements. In\nthis work, we proposed a novel framework - spectral diffusion posterior\nsampling (spectral DPS) - for one-step reconstruction and multi-material\ndecomposition, which combines sophisticated prior information captured by\none-time unsupervised learning and an arbitrary analytic physical system model.\nSpectral DPS is built upon a general DPS framework for nonlinear inverse\nproblems. Several strategies developed in previous work, including jumpstart\nsampling, Jacobian approximation, and multi-step likelihood updates are applied\nfacilitate stable and accurate decompositions. The effectiveness of spectral\nDPS was evaluated on a simulated dual-layer and a kV-switching spectral system\nas well as on a physical cone-beam CT (CBCT) test bench. In simulation studies,\nspectral DPS improved PSNR by 27.49% to 71.93% over baseline DPS and by 26.53%\nto 57.30% over MBMD, depending on the the region of interest. In physical\nphantom study, spectral DPS achieved a <1% error in estimating the mean density\nin a homogeneous region. Compared with baseline DPS, spectral DPS effectively\navoided generating false structures in the homogeneous phantom and reduced the\nvariability around edges. Both simulation and physical phantom studies\ndemonstrated the superior performance of spectral DPS for stable and accurate\nmaterial decomposition."
                },
                "authors": [
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Grace J. Gang"
                    },
                    {
                        "name": "J. Webster Stayman"
                    }
                ],
                "author_detail": {
                    "name": "J. Webster Stayman"
                },
                "author": "J. Webster Stayman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00327v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00327v2",
                "updated": "2024-08-02T07:37:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    2,
                    7,
                    37,
                    51,
                    4,
                    215,
                    0
                ],
                "published": "2024-08-01T07:00:18Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    0,
                    18,
                    3,
                    214,
                    0
                ],
                "title": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-in-Memory (SiM): Reliable, Versatile, and Efficient Data Matching\n  in SSD's NAND Flash Memory Chip for Data Indexing Acceleration"
                },
                "summary": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To index the increasing volume of data, modern data indexes are typically\nstored on SSDs and cached in DRAM. However, searching such an index has\nresulted in significant I/O traffic due to limited access locality and\ninefficient cache utilization. At the heart of index searching is the operation\nof filtering through vast data spans to isolate a small, relevant subset, which\ninvolves basic equality tests rather than the complex arithmetic provided by\nmodern CPUs. This paper introduces the Search-in-Memory (SiM) chip, which\ndemonstrates the feasibility of performing data filtering directly within a\nNAND flash memory chip, transmitting only relevant search results rather than\ncomplete pages. Instead of adding complex circuits, we propose repurposing\nexisting circuitry for efficient and accurate bitwise parallel matching. We\ndemonstrate how different data structures can use our flexible SIMD command\ninterface to offload index searches. This strategy not only frees up the CPU\nfor more computationally demanding tasks, but it also optimizes DRAM usage for\nwrite buffering, significantly lowering energy consumption associated with I/O\ntransmission between the CPU and DRAM. Extensive testing across a wide range of\nworkloads reveals up to a 9X speedup in write-heavy workloads and up to 45%\nenergy savings due to reduced read and write I/O. Furthermore, we achieve\nsignificant reductions in median and tail read latencies of up to 89% and 85%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Yun-Chih Chen"
                    },
                    {
                        "name": "Yuan-Hao Chang"
                    },
                    {
                        "name": "Tei-Wei Kuo"
                    }
                ],
                "author_detail": {
                    "name": "Tei-Wei Kuo"
                },
                "author": "Tei-Wei Kuo",
                "arxiv_comment": "This paper has been accepted for presentation at the The\n  International Conference on Hardware/Software Codesign and System Synthesis\n  (CODES+ISSS) in September, 2024. An extended abstract of this paper was\n  presented in Design, Automation & Test in Europe Conference & Exhibition\n  (DATE), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00327v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00327v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00957v1",
                "updated": "2024-08-01T23:52:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T23:52:43Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    23,
                    52,
                    43,
                    3,
                    214,
                    0
                ],
                "title": "Caching Aided Multi-Tenant Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Aided Multi-Tenant Serverless Computing"
                },
                "summary": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One key to enabling high-performance serverless computing is to mitigate\ncold-starts. Current solutions utilize a warm pool to keep function alive: a\nwarm-start can be analogous to a CPU cache-hit. However, modern cache has\nmultiple hierarchies and the last-level cache is shared among cores, whereas\nthe warm pool is limited to a single tenant for security concerns. Also, the\nwarm pool keep-alive policy can be further optimized using cache replacement\nalgorithms. In this paper, we borrow practical optimizations from caching, and\ndesign FaasCamp, a caching-aided multi-tenant serverless computing framework.\nFaasCamp extends the single-tier warm pool into multi-tiers, with a reclaim\npool introduced enabling secure function instance sharing among tenants. Also,\nFaasCamp leverages machine learning to approximate the optimal cache\nreplacement policy to improve the warm rate. We have implemented a prototype\nand conducted extensive experiments under multiple scenarios. The results show\nthat FaasCamp can outperform existing platforms with minimal overhead."
                },
                "authors": [
                    {
                        "name": "Chu Qiao"
                    },
                    {
                        "name": "Cong Wang"
                    },
                    {
                        "name": "Zhenkai Zhang"
                    },
                    {
                        "name": "Yuede Ji"
                    },
                    {
                        "name": "Xing Gao"
                    }
                ],
                "author_detail": {
                    "name": "Xing Gao"
                },
                "author": "Xing Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.00859v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.00859v2",
                "updated": "2024-08-01T21:21:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    21,
                    21,
                    28,
                    3,
                    214,
                    0
                ],
                "published": "2024-04-01T02:01:28Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    2,
                    1,
                    28,
                    0,
                    92,
                    0
                ],
                "title": "Do language models plan ahead for future tokens?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do language models plan ahead for future tokens?"
                },
                "summary": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale."
                },
                "authors": [
                    {
                        "name": "Wilson Wu"
                    },
                    {
                        "name": "John X. Morris"
                    },
                    {
                        "name": "Lionel Levine"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Levine"
                },
                "author": "Lionel Levine",
                "arxiv_comment": "24 pages, 11 figures. Camera-ready for COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.00859v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.00859v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00539v1",
                "updated": "2024-08-01T13:22:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T13:22:01Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    22,
                    1,
                    3,
                    214,
                    0
                ],
                "title": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intermittent Semi-working Mask: A New Masking Paradigm for LLMs"
                },
                "summary": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance."
                },
                "authors": [
                    {
                        "name": "Mingcong Lu"
                    },
                    {
                        "name": "Jiangcai Zhu"
                    },
                    {
                        "name": "Wang Hao"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Shusheng Zhang"
                    },
                    {
                        "name": "Kailai Shao"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Nan Li"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Xin Lu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lu"
                },
                "author": "Xin Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14361v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14361v2",
                "updated": "2024-08-01T13:21:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    13,
                    21,
                    24,
                    3,
                    214,
                    0
                ],
                "published": "2024-01-25T18:07:50Z",
                "published_parsed": [
                    2024,
                    1,
                    25,
                    18,
                    7,
                    50,
                    3,
                    25,
                    0
                ],
                "title": "MoE-Infinity: Offloading-Efficient MoE Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Infinity: Offloading-Efficient MoE Model Serving"
                },
                "summary": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity"
                },
                "authors": [
                    {
                        "name": "Leyang Xue"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Zhan Lu"
                    },
                    {
                        "name": "Luo Mai"
                    },
                    {
                        "name": "Mahesh Marina"
                    }
                ],
                "author_detail": {
                    "name": "Mahesh Marina"
                },
                "author": "Mahesh Marina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14361v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14361v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.15220v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.15220v4",
                "updated": "2024-08-01T07:51:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    7,
                    51,
                    25,
                    3,
                    214,
                    0
                ],
                "published": "2024-02-23T09:29:19Z",
                "published_parsed": [
                    2024,
                    2,
                    23,
                    9,
                    29,
                    19,
                    4,
                    54,
                    0
                ],
                "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition"
                },
                "summary": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096."
                },
                "authors": [
                    {
                        "name": "Lu Ye"
                    },
                    {
                        "name": "Ze Tao"
                    },
                    {
                        "name": "Yong Huang"
                    },
                    {
                        "name": "Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yang Li"
                },
                "author": "Yang Li",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.15220v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.15220v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00232v1",
                "updated": "2024-08-01T01:57:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "published": "2024-08-01T01:57:09Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    1,
                    57,
                    9,
                    3,
                    214,
                    0
                ],
                "title": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction"
                },
                "summary": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks."
                },
                "authors": [
                    {
                        "name": "Shuai Zhang"
                    },
                    {
                        "name": "Zite Jiang"
                    },
                    {
                        "name": "Haihang You"
                    }
                ],
                "author_detail": {
                    "name": "Haihang You"
                },
                "author": "Haihang You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v2",
                "updated": "2024-08-01T00:41:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    1,
                    0,
                    41,
                    52,
                    3,
                    214,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Towards Variable-Length In-Network Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Variable-Length In-Network Caching"
                },
                "summary": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present StarCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, StarCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement a StarCache prototype on an Intel Tofino\nswitch. Our experimental results show that StarCache can balance highly skewed\nworkloads with various key and value sizes."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00167v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00167v2",
                "updated": "2024-08-13T09:08:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    8,
                    55,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-31T21:33:56Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    21,
                    33,
                    56,
                    2,
                    213,
                    0
                ],
                "title": "Finch: Prompt-guided Key-Value Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finch: Prompt-guided Key-Value Cache Compression"
                },
                "summary": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language model applications, such as Retrieval-Augmented\nGeneration and chatbots, have led to an increased need to process longer input\ncontexts. However, this requirement is hampered by inherent limitations.\nArchitecturally, models are constrained by a context window defined during\ntraining. Additionally, processing extensive texts requires substantial GPU\nmemory. We propose a novel approach, Finch, to compress the input context by\nleveraging the pre-trained model weights of the self-attention. Given a prompt\nand a long text, Finch iteratively identifies the most relevant Key (K) and\nValue (V) pairs over chunks of the text conditioned on the prompt. Only such\npairs are stored in the KV cache, which, within the space constrained by the\ncontext window, ultimately contains a compressed version of the long text. Our\nproposal enables models to consume large inputs even with high compression (up\nto 93x) while preserving semantic integrity without the need for fine-tuning."
                },
                "authors": [
                    {
                        "name": "Giulio Corallo"
                    },
                    {
                        "name": "Paolo Papotti"
                    }
                ],
                "author_detail": {
                    "name": "Paolo Papotti"
                },
                "author": "Paolo Papotti",
                "arxiv_comment": "Accepted for publication at TACL - pre-MIT Press publication version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.00167v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00167v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20485v2",
                "updated": "2024-07-31T02:02:40Z",
                "updated_parsed": [
                    2024,
                    7,
                    31,
                    2,
                    2,
                    40,
                    2,
                    213,
                    0
                ],
                "published": "2024-07-30T01:13:42Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    1,
                    13,
                    42,
                    1,
                    212,
                    0
                ],
                "title": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A2SF: Accumulative Attention Scoring with Forgetting Factor for Token\n  Pruning in Transformer Decoder"
                },
                "summary": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLM) based on transformers are facing memory\nbottleneck issues due to KV cache, especially in long sequence handling.\nPrevious researches proposed KV cache compression techniques that identify\ninsignificant tokens based on Accumulative Attention Scores and removes their\nitems from KV cache, noting that only few tokens play an important role in\nattention operations. However, we have observed that the existing Accumulative\nAttention Score is not suitable for the transformer decoder structure. In the\ndecoder model, the number of times the Attention Score accumulates varies\ndepending on the order of token appearance due to the effect of masking,\ncausing an uneven comparison between tokens. To solve this, we propose\nAccumulative Attention Score with Forgetting Factor (A2SF) technique, which\nintroduces a Forgetting Factor in the Attention Score accumulation process.\nA2SF applies a penalty to the past Attention Score generated from old tokens by\nrepeatedly multiplying the Forgetting Factor to the Attention Score over time.\nTherefore, older tokens receive a larger penalty, providing fairness among\ndifferent ages of tokens. Through the fair comparison among tokens, we can more\neffectively select important tokens. We have verified the accuracy improvement\nthrough A2SF in the OPT and LLaMA models and A2SF improves the accuracy of\nLLaMA 2 by up to 7.8% and 5.1% on 1-shot and 0-shot."
                },
                "authors": [
                    {
                        "name": "Hyun-rae Jo"
                    },
                    {
                        "name": "Dongkun Shin"
                    }
                ],
                "author_detail": {
                    "name": "Dongkun Shin"
                },
                "author": "Dongkun Shin",
                "arxiv_comment": "11 pages(9 pages + reference 2 pages), 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21201v1",
                "updated": "2024-07-30T21:27:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T21:27:00Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    21,
                    27,
                    0,
                    1,
                    212,
                    0
                ],
                "title": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electric field control of magnetocaloric effect in cylindrical MnAs/PZT\n  magnetoelectric composite"
                },
                "summary": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of electric field control of magnetocaloric effect through\nquasi-isostatic compression as a result of the converse piezoelectric effect\nwas demonstrated on cylindrical type magnetoelectric composite MnAs/PZT. It was\nshown that an electric voltage of 100 V corresponding to an electric field of E\n~0.3 kV/mm applied to the walls of the piezoelectric component PZT of the\nMnAs/PZT composite contributes to an increase in the maximum adiabatic\ntemperature change by 0.2 K in the temperature range of the magnetostructural\nphase transition of MnAs ~317 K at magnetic field change of 1.8 T. Calculations\nusing the finite element method have shown that an electric field voltage of\n100 V is capable of creating a quasi-isostatic mechanical stress in the region\ninside a cylindrical PZT tube of ~3 MPa. Moreover, in the region of weak\npressures up to 10 MPa, the contribution to the MCE from piezo compression\nlinearly depends on the electrical voltage that can be used for control the MCE"
                },
                "authors": [
                    {
                        "name": "Abdulkarim A. Amirov"
                    },
                    {
                        "name": "Maksim A. Koliushenkov"
                    },
                    {
                        "name": "Abdula A. Mukhuchev"
                    },
                    {
                        "name": "Dibir M. Yusupov"
                    },
                    {
                        "name": "Valeriya V. Govorina"
                    },
                    {
                        "name": "Dmitriy S. Neznakhin"
                    },
                    {
                        "name": "Gennady A. Govor"
                    },
                    {
                        "name": "Akhmed M. Aliev"
                    }
                ],
                "author_detail": {
                    "name": "Akhmed M. Aliev"
                },
                "author": "Akhmed M. Aliev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21118v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21118v1",
                "updated": "2024-07-30T18:19:38Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T18:19:38Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    18,
                    19,
                    38,
                    1,
                    212,
                    0
                ],
                "title": "Palu: Compressing KV-Cache with Low-Rank Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palu: Compressing KV-Cache with Low-Rank Projection"
                },
                "summary": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Cache compression methods generally sample a KV-Cache of effectual tokens\nor quantize it into lower bits. However, these methods cannot exploit the\nredundancy of the hidden dimension of KV tensors. This paper investigates a\nunique hidden dimension approach called Palu, a novel KV-Cache compression\nframework that utilizes low-rank projection. Palu decomposes the linear layers\ninto low-rank matrices, caches the smaller intermediate states, and\nreconstructs the full keys and values on the fly. To improve accuracy,\ncompression rate, and efficiency, Palu further encompasses (1) a medium-grained\nlow-rank decomposition scheme, (2) an efficient rank search algorithm, (3) a\nlow-rank-aware quantization algorithm, and (4) matrix fusion with optimized GPU\nkernels. Our extensive experiments with popular LLMs show that Palu can\ncompress KV-Cache by more than 91.25% while maintaining a significantly better\naccuracy (up to 1.19 lower perplexity) than state-of-the-art KV-Cache\nquantization methods at a similar or even higher memory usage. When compressing\nKV-Cache for 50%, Palu delivers up to 1.61x end-to-end speedup for the\nattention module. Our code is publicly available at\nhttps://github.com/shadowpa0327/Palu."
                },
                "authors": [
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Wei-Cheng Lin"
                    },
                    {
                        "name": "Chien-Yu Lin"
                    },
                    {
                        "name": "Chong-Yan Chen"
                    },
                    {
                        "name": "Yu-Fang Hu"
                    },
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Luis Ceze"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21118v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21118v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21018v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21018v1",
                "updated": "2024-07-30T17:59:08Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T17:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    59,
                    8,
                    1,
                    212,
                    0
                ],
                "title": "ThinK: Thinner Key Cache by Query-Driven Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThinK: Thinner Key Cache by Query-Driven Pruning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized the field of natural\nlanguage processing, achieving unprecedented performance across a variety of\napplications by leveraging increased model sizes and sequence lengths. However,\nthe associated rise in computational and memory costs poses significant\nchallenges, particularly in managing long sequences due to the quadratic\ncomplexity of the transformer attention mechanism. This paper focuses on the\nlong-context scenario, addressing the inefficiencies in KV cache memory\nconsumption during inference. Unlike existing approaches that optimize the\nmemory based on the sequence lengths, we uncover that the channel dimension of\nthe KV cache exhibits significant redundancy, characterized by unbalanced\nmagnitude distribution and low-rank structure in attention weights. Based on\nthese observations, we propose ThinK, a novel query-dependent KV cache pruning\nmethod designed to minimize attention weight loss while selectively pruning the\nleast significant channels. Our approach not only maintains or enhances model\naccuracy but also achieves a reduction in memory costs by over 20% compared\nwith vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and\nMistral models across various long-sequence datasets confirm the efficacy of\nThinK, setting a new precedent for efficient LLM deployment without\ncompromising performance. We also outline the potential of extending our method\nto value cache pruning, demonstrating ThinK's versatility and broad\napplicability in reducing both memory and computational overheads."
                },
                "authors": [
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Zhanming Jie"
                    },
                    {
                        "name": "Hanze Dong"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Xudong Lu"
                    },
                    {
                        "name": "Aojun Zhou"
                    },
                    {
                        "name": "Amrita Saha"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Doyen Sahoo"
                    }
                ],
                "author_detail": {
                    "name": "Doyen Sahoo"
                },
                "author": "Doyen Sahoo",
                "arxiv_comment": "20 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21018v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21018v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.06944v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.06944v2",
                "updated": "2024-07-30T13:06:36Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    13,
                    6,
                    36,
                    1,
                    212,
                    0
                ],
                "published": "2023-04-14T06:21:57Z",
                "published_parsed": [
                    2023,
                    4,
                    14,
                    6,
                    21,
                    57,
                    4,
                    104,
                    0
                ],
                "title": "SpChar: Characterizing the Sparse Puzzle via Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpChar: Characterizing the Sparse Puzzle via Decision Trees"
                },
                "summary": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse matrix computation is crucial in various modern applications,\nincluding large-scale graph analytics, deep learning, and recommender systems.\nThe performance of sparse kernels varies greatly depending on the structure of\nthe input matrix, making it difficult to gain a comprehensive understanding of\nsparse computation and its relationship to inputs, algorithms, and target\nmachine architecture. Despite extensive research on certain sparse kernels,\nsuch as Sparse Matrix-Vector Multiplication (SpMV), the overall family of\nsparse algorithms has yet to be investigated as a whole. This paper introduces\nSpChar, a workload characterization methodology for general sparse computation.\nSpChar employs tree-based models to identify the most relevant hardware and\ninput characteristics, starting from hardware and input-related metrics\ngathered from Performance Monitoring Counters (PMCs) and matrices. Our analysis\nenables the creation of a characterization loop that facilitates the\noptimization of sparse computation by mapping the impact of architectural\nfeatures to inputs and algorithmic choices. We apply SpChar to more than 600\nmatrices from the SuiteSparse Matrix collection and three state-of-the-art Arm\nCPUs to determine the critical hardware and software characteristics that\naffect sparse computation. In our analysis, we determine that the biggest\nlimiting factors for high-performance sparse computation are (1) the latency of\nthe memory system, (2) the pipeline flush overhead resulting from branch\nmisprediction, and (3) the poor reuse of cached elements. Additionally, we\npropose software and hardware optimizations that designers can implement to\ncreate a platform suitable for sparse computation. We then investigate these\noptimizations using the gem5 simulator to achieve a significant speedup of up\nto 2.63x compared to a CPU where the optimizations are not applied."
                },
                "authors": [
                    {
                        "name": "Francesco Sgherzi"
                    },
                    {
                        "name": "Marco Siracusa"
                    },
                    {
                        "name": "Ivan Fernandez"
                    },
                    {
                        "name": "Adri√† Armejach"
                    },
                    {
                        "name": "Miquel Moret√≥"
                    }
                ],
                "author_detail": {
                    "name": "Miquel Moret√≥"
                },
                "author": "Miquel Moret√≥",
                "arxiv_comment": "27 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.06944v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.06944v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20773v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20773v1",
                "updated": "2024-07-30T12:16:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "published": "2024-07-30T12:16:39Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    12,
                    16,
                    39,
                    1,
                    212,
                    0
                ],
                "title": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UpDown: Programmable fine-grained Events for Scalable Performance on\n  Irregular Applications"
                },
                "summary": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications with irregular data structures, data-dependent control flows and\nfine-grained data transfers (e.g., real-world graph computations) perform\npoorly on cache-based systems. We propose the UpDown accelerator that supports\nfine-grained execution with novel architecture mechanisms - lightweight\nthreading, event-driven scheduling, efficient ultra-short threads, and\nsplit-transaction DRAM access with software-controlled synchronization. These\nhardware primitives support software programmable events, enabling high\nperformance on diverse data structures and algorithms. UpDown also supports\nscalable performance; hardware replication enables programs to scale up\nperformance. Evaluation results show UpDown's flexibility and scalability\nenable it to outperform CPUs on graph mining and analytics computations by up\nto 116-195x geomean speedup and more than 4x speedup over prior accelerators.\nWe show that UpDown generates high memory parallelism (~4.6x over CPU) required\nfor memory intensive graph computations. We present measurements that attribute\nthe performance of UpDown (23x architectural advantage) to its individual\narchitectural mechanisms. Finally, we also analyze the area and power cost of\nUpDown's mechanisms for software programmability."
                },
                "authors": [
                    {
                        "name": "Andronicus Rajasukumar"
                    },
                    {
                        "name": "Jiya Su"
                    },
                    {
                        "name": "Yuqing"
                    },
                    {
                        "name": "Wang"
                    },
                    {
                        "name": "Tianshuo Su"
                    },
                    {
                        "name": "Marziyeh Nourian"
                    },
                    {
                        "name": "Jose M Monsalve Diaz"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Jianru Ding"
                    },
                    {
                        "name": "Wenyi Wang"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Moubarak Jeje"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Yanjing Li"
                    },
                    {
                        "name": "Andrew A. Chien"
                    }
                ],
                "author_detail": {
                    "name": "Andrew A. Chien"
                },
                "arxiv_affiliation": "Ivy",
                "author": "Andrew A. Chien",
                "arxiv_comment": "14 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20773v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20773v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.14928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.14928v3",
                "updated": "2024-07-30T08:39:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    39,
                    52,
                    1,
                    212,
                    0
                ],
                "published": "2023-09-26T13:35:31Z",
                "published_parsed": [
                    2023,
                    9,
                    26,
                    13,
                    35,
                    31,
                    1,
                    269,
                    0
                ],
                "title": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise-Tolerant Few-Shot Unsupervised Adapter for Vision-Language Models"
                },
                "summary": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large-scale vision-language models have achieved\nimpressive performance in various zero-shot image classification tasks. While\nprior studies have demonstrated significant improvements by introducing\nfew-shot labelled target samples, they still require labelling of target\nsamples, which greatly degrades their scalability and generalizability while\nhandling various visual recognition tasks. We design NtUA, a Noise-tolerant\nUnsupervised Adapter that allows the learning of effective target models with\nfew unlabelled target samples. NtUA works as a key-value cache that formulates\nvisual features and predicted pseudo-labels of the few unlabelled target\nsamples as key-value pairs. It consists of two complementary designs. The first\nis adaptive cache formation that combats pseudo-label noises by weighting the\nkey-value pairs according to their prediction confidence. The second is\nknowledge-guided cache refinement, which refines pair values (i.e.,\npseudo-labels) and cache weights by leveraging knowledge distillation from\nlarge-scale vision language models. Extensive experiments show that NtUA\nachieves superior performance consistently across multiple widely adopted\nbenchmarks."
                },
                "authors": [
                    {
                        "name": "Eman Ali"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Haris Khan"
                },
                "author": "Muhammad Haris Khan",
                "arxiv_comment": "Accepted at BMVC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.14928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.14928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03088v2",
                "updated": "2024-07-30T08:19:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    8,
                    19,
                    53,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-03T22:03:28Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    22,
                    3,
                    28,
                    2,
                    94,
                    0
                ],
                "title": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Federated Learning for Wireless Networks: A Demonstration with\n  Channel Estimation"
                },
                "summary": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) offers a privacy-preserving collaborative approach\nfor training models in wireless networks, with channel estimation emerging as a\npromising application. Despite extensive studies on FL-empowered channel\nestimation, the security concerns associated with FL require meticulous\nattention. In a scenario where small base stations (SBSs) serve as local models\ntrained on cached data, and a macro base station (MBS) functions as the global\nmodel setting, an attacker can exploit the vulnerability of FL, launching\nattacks with various adversarial attacks or deployment tactics. In this paper,\nwe analyze such vulnerabilities, corresponding solutions were brought forth,\nand validated through simulation."
                },
                "authors": [
                    {
                        "name": "Zexin Fang"
                    },
                    {
                        "name": "Bin Han"
                    },
                    {
                        "name": "Hans D. Schotten"
                    }
                ],
                "author_detail": {
                    "name": "Hans D. Schotten"
                },
                "author": "Hans D. Schotten",
                "arxiv_comment": "Submitted to IEEE GLOBECOM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.16219v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.16219v3",
                "updated": "2024-07-30T04:01:25Z",
                "updated_parsed": [
                    2024,
                    7,
                    30,
                    4,
                    1,
                    25,
                    1,
                    212,
                    0
                ],
                "published": "2024-04-24T21:35:12Z",
                "published_parsed": [
                    2024,
                    4,
                    24,
                    21,
                    35,
                    12,
                    2,
                    115,
                    0
                ],
                "title": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Increasing the Hit Ratio Hurt Cache Throughput? (Long Version)"
                },
                "summary": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software caches are an intrinsic component of almost every computer system.\nConsequently, caching algorithms, particularly eviction policies, are the topic\nof many papers. Almost all these prior papers evaluate the caching algorithm\nbased on its hit ratio, namely the fraction of requests that are found in the\ncache, as opposed to disk. The hit ratio is viewed as a proxy for traditional\nperformance metrics like system throughput or response time. Intuitively it\nmakes sense that higher hit ratio should lead to higher throughput (and lower\nresponse time), since more requests are found in the cache (low access time) as\nopposed to the disk (high access time).\n  This paper challenges this intuition. We show that increasing the hit ratio\ncan actually hurt the throughput (and response time) for many caching\nalgorithms. Our investigation follows a three-pronged approach involving (i)\nqueueing modeling and analysis, (ii) implementation and measurement, and (iii)\nsimulation to validate the accuracy of the queueing model. We also show that\nthe phenomenon of throughput decreasing at higher hit ratios is likely to be\nmore pronounced in future systems, where the trend is towards faster disks and\nhigher numbers of cores per CPU."
                },
                "authors": [
                    {
                        "name": "Ziyue Qiu"
                    },
                    {
                        "name": "Juncheng Yang"
                    },
                    {
                        "name": "Mor Harchol-Balter"
                    }
                ],
                "author_detail": {
                    "name": "Mor Harchol-Balter"
                },
                "author": "Mor Harchol-Balter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.16219v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.16219v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19637v1",
                "updated": "2024-07-29T01:43:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:43:26Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    43,
                    26,
                    0,
                    211,
                    0
                ],
                "title": "STT-RAM-based Hierarchical In-Memory Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STT-RAM-based Hierarchical In-Memory Computing"
                },
                "summary": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-memory computing promises to overcome the von Neumann bottleneck in\ncomputer systems by performing computations directly within the memory.\nPrevious research has suggested using Spin-Transfer Torque RAM (STT-RAM) for\nin-memory computing due to its non-volatility, low leakage power, high density,\nendurance, and commercial viability. This paper explores hierarchical in-memory\ncomputing, where different levels of the memory hierarchy are augmented with\nprocessing elements to optimize workload execution. The paper investigates\nprocessing in memory (PiM) using non-volatile STT-RAM and processing in cache\n(PiC) using volatile STT-RAM with relaxed retention, which helps mitigate\nSTT-RAM's write latency and energy overheads. We analyze tradeoffs and\noverheads associated with data movement for PiC versus write overheads for PiM\nusing STT-RAMs for various workloads. We examine workload characteristics, such\nas computational intensity and CPU-dependent workloads with limited\ninstruction-level parallelism, and their impact on PiC/PiM tradeoffs. Using\nthese workloads, we evaluate computing in STT-RAM versus SRAM at different\ncache hierarchy levels and explore the potential of heterogeneous STT-RAM cache\narchitectures with various retention times for PiC and CPU-based computing. Our\nexperiments reveal significant advantages of STT-RAM-based PiC over PiM for\nspecific workloads. Finally, we describe open research problems in hierarchical\nin-memory computing architectures to further enhance this paradigm."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kevin Antony Gomez"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/TPDS.2024.3430853",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPDS.2024.3430853",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: IEEE Transactions on Parallel and Distributed Systems (\n  Volume: 35, Issue: 9, September 2024)",
                "arxiv_journal_ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 35,\n  no. 9, pp. 1615-1629, Sept. 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19627v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19627v1",
                "updated": "2024-07-29T01:17:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "published": "2024-07-29T01:17:54Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    1,
                    17,
                    54,
                    0,
                    211,
                    0
                ],
                "title": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIME: Energy-Efficient STT-RAM-based Concurrent Hierarchical In-Memory\n  Processing"
                },
                "summary": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing-in-cache (PiC) and Processing-in-memory (PiM) architectures,\nespecially those utilizing bit-line computing, offer promising solutions to\nmitigate data movement bottlenecks within the memory hierarchy. While previous\nstudies have explored the integration of compute units within individual memory\nlevels, the complexity and potential overheads associated with these designs\nhave often limited their capabilities. This paper introduces a novel PiC/PiM\narchitecture, Concurrent Hierarchical In-Memory Processing (CHIME), which\nstrategically incorporates heterogeneous compute units across multiple levels\nof the memory hierarchy. This design targets the efficient execution of\ndiverse, domain-specific workloads by placing computations closest to the data\nwhere it optimizes performance, energy consumption, data movement costs, and\narea. CHIME employs STT-RAM due to its various advantages in PiC/PiM computing,\nsuch as high density, low leakage, and better resiliency to data corruption\nfrom activating multiple word lines. We demonstrate that CHIME enhances\nconcurrency and improves compute unit utilization at each level of the memory\nhierarchy. We present strategies for exploring the design space, grouping, and\nplacing the compute units across the memory hierarchy. Experiments reveal that,\ncompared to the state-of-the-art bit-line computing approaches, CHIME achieves\nsignificant speedup and energy savings of 57.95% and 78.23% for various\ndomain-specific workloads, while reducing the overheads associated with\nsingle-level compute designs."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    },
                    {
                        "name": "Kevin Gomez"
                    }
                ],
                "author_detail": {
                    "name": "Kevin Gomez"
                },
                "author": "Kevin Gomez",
                "arxiv_comment": "Accepted in 35th IEEE International Conference on\n  Application-specific Systems, Architectures and Processors (ASAP 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19627v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19627v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19612v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19612v1",
                "updated": "2024-07-28T23:43:59Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T23:43:59Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    23,
                    43,
                    59,
                    6,
                    210,
                    0
                ],
                "title": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARC: DVFS-Aware Asymmetric-Retention STT-RAM Caches for Energy-Efficient\n  Multicore Processors"
                },
                "summary": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Relaxed retention (or volatile) spin-transfer torque RAM (STT-RAM) has been\nwidely studied as a way to reduce STT-RAM's write energy and latency overheads.\nGiven a relaxed retention time STT-RAM level one (L1) cache, we analyze the\nimpacts of dynamic voltage and frequency scaling (DVFS) -- a common\noptimization in modern processors -- on STT-RAM L1 cache design. Our analysis\nreveals that, apart from the fact that different applications may require\ndifferent retention times, the clock frequency, which is typically ignored in\nmost STT-RAM studies, may also significantly impact applications' retention\ntime needs. Based on our findings, we propose an asymmetric-retention core\n(ARC) design for multicore architectures. ARC features retention time\nheterogeneity to specialize STT-RAM retention times to applications' needs. We\nalso propose a runtime prediction model to determine the best core on which to\nrun an application, based on the applications' characteristics, their retention\ntime requirements, and available DVFS settings. Results reveal that the\nproposed approach can reduce the average cache energy by 20.19% and overall\nprocessor energy by 7.66%, compared to a homogeneous STT-RAM cache design."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1145/3357526.3357553",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3357526.3357553",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19612v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the international symposium on memory systems, pp.\n  439-450. 2019",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19604v1",
                "updated": "2024-07-28T22:34:20Z",
                "updated_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "published": "2024-07-28T22:34:20Z",
                "published_parsed": [
                    2024,
                    7,
                    28,
                    22,
                    34,
                    20,
                    6,
                    210,
                    0
                ],
                "title": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCART: Predicting STT-RAM Cache Retention Times Using Machine Learning"
                },
                "summary": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prior studies have shown that the retention time of the non-volatile\nspin-transfer torque RAM (STT-RAM) can be relaxed in order to reduce STT-RAM's\nwrite energy and latency. However, since different applications may require\ndifferent retention times, STT-RAM retention times must be critically explored\nto satisfy various applications' needs. This process can be challenging due to\nexploration overhead, and exacerbated by the fact that STT-RAM caches are\nemerging and are not readily available for design time exploration. This paper\nexplores using known and easily obtainable statistics (e.g., SRAM statistics)\nto predict the appropriate STT-RAM retention times, in order to minimize\nexploration overhead. We propose an STT-RAM Cache Retention Time (SCART) model,\nwhich utilizes machine learning to enable design time or runtime prediction of\nright-provisioned STT-RAM retention times for latency or energy optimization.\nExperimental results show that, on average, SCART can reduce the latency and\nenergy by 20.34% and 29.12%, respectively, compared to a homogeneous retention\ntime while reducing the exploration overheads by 52.58% compared to prior work."
                },
                "authors": [
                    {
                        "name": "Dhruv Gajaria"
                    },
                    {
                        "name": "Kyle Kuan"
                    },
                    {
                        "name": "Tosiron Adegbija"
                    }
                ],
                "author_detail": {
                    "name": "Tosiron Adegbija"
                },
                "author": "Tosiron Adegbija",
                "arxiv_doi": "10.1109/IGSC48788.2019.8957182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IGSC48788.2019.8957182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.19604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in: 2019 Tenth International Green and Sustainable\n  Computing Conference (IGSC)",
                "arxiv_journal_ref": "2019 Tenth International Green and Sustainable Computing\n  Conference (IGSC), Alexandria, VA, USA, 2019, pp. 1-7,",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18003v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18003v3",
                "updated": "2024-08-13T09:55:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    9,
                    55,
                    43,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-25T12:56:22Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    12,
                    56,
                    22,
                    3,
                    207,
                    0
                ],
                "title": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache\n  Consumption"
                },
                "summary": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), epitomized by ChatGPT' s release in late 2022,\nhave revolutionized various industries with their advanced language\ncomprehension. However, their efficiency is challenged by the Transformer\narchitecture' s struggle with handling long texts. KV-Cache has emerged as a\npivotal solution to this issue, converting the time complexity of token\ngeneration from quadratic to linear, albeit with increased GPU memory overhead\nproportional to conversation length. With the development of the LLM community\nand academia, various KV-Cache compression methods have been proposed. In this\nreview, we dissect the various properties of KV-Cache and elaborate on various\nmethods currently used to optimize the KV-Cache space usage of LLMs. These\nmethods span the pre-training phase, deployment phase, and inference phase, and\nwe summarize the commonalities and differences among these methods.\nAdditionally, we list some metrics for evaluating the long-text capabilities of\nlarge language models, from both efficiency and capability perspectives. Our\nreview thus sheds light on the evolving landscape of LLM optimization, offering\ninsights into future advancements in this dynamic field."
                },
                "authors": [
                    {
                        "name": "Luohe Shi"
                    },
                    {
                        "name": "Hongyi Zhang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "to be published in CoLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18003v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18003v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19318v1",
                "updated": "2024-07-27T18:26:32Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T18:26:32Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    18,
                    26,
                    32,
                    5,
                    209,
                    0
                ],
                "title": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application State Management (ASM) in the Modern Web and Mobile\n  Applications: A Comprehensive Review"
                },
                "summary": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid evolution of web and mobile applications has necessitated robust\nmechanisms for managing application state to ensure consistency, performance,\nand user-friendliness. This comprehensive review examines the most effective\nApplication State Management (ASM) techniques, categorized into Local State\nManagement, State Management Libraries, and Server-Side State Management. By\nanalyzing popular front end frameworks the study delves into local state\nmanagement mechanisms. It also evaluates the state of front end management\nlibraries, highlighting their implementations, benefits, and limitations.\nServer-side state management techniques, particularly caching, are discussed\nfor their roles in enhancing data retrieval efficiency. This paper offers\nactionable insights for developers to build scalable, responsive applications,\naiming to bridge the gap between theoretical knowledge and practical\napplication. This study's critical analysis and recommendations aim to guide\nfuture research and development in ASM, contributing to the advancement of\nmodern application architecture."
                },
                "authors": [
                    {
                        "name": "Anujkumarsinh Donvir"
                    },
                    {
                        "name": "Apeksha Jain"
                    },
                    {
                        "name": "Pradeep Kumar Saraswathi"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Kumar Saraswathi"
                },
                "author": "Pradeep Kumar Saraswathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19291v1",
                "updated": "2024-07-27T16:20:21Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T16:20:21Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    16,
                    20,
                    21,
                    5,
                    209,
                    0
                ],
                "title": "Symmetric Locality: Definition and Initial Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetric Locality: Definition and Initial Results"
                },
                "summary": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this short paper, we characterize symmetric locality. In designing\nalgorithms, compilers, and systems, data movement is a common bottleneck in\nhigh-performance computation, in which we improve cache and memory performance.\nWe study a special type of data reuse in the form of repeated traversals, or\nre-traversals, which are based on the symmetric group. The cyclic and sawtooth\ntraces are previously known results in symmetric locality, and in this work, we\nwould like to generalize this result for any re-traversal. Then, we also\nprovide an abstract framework for applications in compiler design and machine\nlearning models to improve the memory performance of certain programs."
                },
                "authors": [
                    {
                        "name": "Giordan Escalona"
                    },
                    {
                        "name": "Dylan McKellips"
                    },
                    {
                        "name": "Chen Ding"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ding"
                },
                "author": "Chen Ding",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13996v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13996v2",
                "updated": "2024-07-27T08:52:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    52,
                    39,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-19T03:01:32Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    3,
                    1,
                    32,
                    4,
                    201,
                    0
                ],
                "title": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missile: Fine-Grained, Hardware-Level GPU Resource Isolation for\n  Multi-Tenant DNN Inference"
                },
                "summary": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Colocating high-priority, latency-sensitive (LS) and low-priority,\nbest-effort (BE) DNN inference services reduces the total cost of ownership\n(TCO) of GPU clusters. Limited by bottlenecks such as VRAM channel conflicts\nand PCIe bus contentions, existing GPU sharing solutions are unable to avoid\nresource conflicts among concurrently executing tasks, failing to achieve both\nlow latency for LS tasks and high throughput for BE tasks. To bridge this gap,\nthis paper presents Missile, a general GPU sharing solution for multi-tenant\nDNN inference on NVIDIA GPUs. Missile approximates fine-grained GPU hardware\nresource isolation between multiple LS and BE DNN tasks at software level.\nThrough comprehensive reverse engineering, Missile first reveals a general VRAM\nchannel hash mapping architecture of NVIDIA GPUs and eliminates VRAM channel\nconflicts using software-level cache coloring. It also isolates the PCIe bus\nand fairly allocates PCIe bandwidth using completely fair scheduler. We\nevaluate 12 mainstream DNNs with synthetic and real-world workloads on four\nGPUs. The results show that compared to the state-of-the-art GPU sharing\nsolutions, Missile reduces tail latency for LS services by up to ~50%, achieves\nup to 6.1x BE job throughput, and allocates PCIe bus bandwidth to tenants\non-demand for optimal performance."
                },
                "authors": [
                    {
                        "name": "Yongkang Zhang"
                    },
                    {
                        "name": "Haoxuan Yu"
                    },
                    {
                        "name": "Chenxia Han"
                    },
                    {
                        "name": "Cheng Wang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Huaicheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Huaicheng Li"
                },
                "author": "Huaicheng Li",
                "arxiv_comment": "18 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13996v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13996v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.9; I.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19205v1",
                "updated": "2024-07-27T08:21:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "published": "2024-07-27T08:21:14Z",
                "published_parsed": [
                    2024,
                    7,
                    27,
                    8,
                    21,
                    14,
                    5,
                    209,
                    0
                ],
                "title": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Image2Video Generation: A Closer Look at CLIP Image Embedding's\n  Impact on Spatio-Temporal Cross-Attentions"
                },
                "summary": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the role of CLIP image embeddings within the Stable\nVideo Diffusion (SVD) framework, focusing on their impact on video generation\nquality and computational efficiency. Our findings indicate that CLIP\nembeddings, while crucial for aesthetic quality, do not significantly\ncontribute towards the subject and background consistency of video outputs.\nMoreover, the computationally expensive cross-attention mechanism can be\neffectively replaced by a simpler linear layer. This layer is computed only\nonce at the first diffusion inference step, and its output is then cached and\nreused throughout the inference process, thereby enhancing efficiency while\nmaintaining high-quality outputs. Building on these insights, we introduce the\nVCUT, a training-free approach optimized for efficiency within the SVD\narchitecture. VCUT eliminates temporal cross-attention and replaces spatial\ncross-attention with a one-time computed linear layer, significantly reducing\ncomputational load. The implementation of VCUT leads to a reduction of up to\n322T Multiple-Accumulate Operations (MACs) per video and a decrease in model\nparameters by up to 50M, achieving a 20% reduction in latency compared to the\nbaseline. Our approach demonstrates that conditioning during the Semantic\nBinding stage is sufficient, eliminating the need for continuous computation\nacross all inference steps and setting a new standard for efficient video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Ashkan Taghipour"
                    },
                    {
                        "name": "Morteza Ghahremani"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Aref Miri Rekavandi"
                    },
                    {
                        "name": "Zinuo Li"
                    },
                    {
                        "name": "Hamid Laga"
                    },
                    {
                        "name": "Farid Boussaid"
                    }
                ],
                "author_detail": {
                    "name": "Farid Boussaid"
                },
                "author": "Farid Boussaid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19090v1",
                "updated": "2024-07-26T21:11:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "published": "2024-07-26T21:11:58Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    21,
                    11,
                    58,
                    4,
                    208,
                    0
                ],
                "title": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MetaHive: A Cache-Optimized Metadata Management for Heterogeneous\n  Key-Value Stores"
                },
                "summary": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud key-value (KV) stores provide businesses with a cost-effective and\nadaptive alternative to traditional on-premise data management solutions. KV\nstores frequently consist of heterogeneous clusters, characterized by varying\nhardware specifications of the deployment nodes, with each node potentially\nrunning a distinct version of the KV store software. This heterogeneity is\naccompanied by the diverse metadata that they need to manage. In this study, we\nintroduce MetaHive, a cache-optimized approach to managing metadata in\nheterogeneous KV store clusters. MetaHive disaggregates the original data from\nits associated metadata to promote independence between them, while maintaining\ntheir interconnection during usage. This makes the metadata opaque from the\ndownstream processes and the other KV stores in the cluster. MetaHive also\nensures that the KV and metadata entries are stored in the vicinity of each\nother in memory and storage. This allows MetaHive to optimally utilize the\ncaching mechanism without extra storage read overhead for metadata retrieval.\nWe deploy MetaHive to ensure data integrity in RocksDB and demonstrate its\nrapid data validation with minimal effect on performance."
                },
                "authors": [
                    {
                        "name": "Alireza Heidari"
                    },
                    {
                        "name": "Amirhossein Ahmadi"
                    },
                    {
                        "name": "Zefeng Zhi"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Cloud Databases",
                "arxiv_journal_ref": "VLDB 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18121v1",
                "updated": "2024-07-25T15:29:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T15:29:05Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    15,
                    29,
                    5,
                    3,
                    207,
                    0
                ],
                "title": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Inference of Vision Instruction-Following Models with Elastic\n  Cache"
                },
                "summary": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of instruction-following large vision-language models (LVLMs),\nthe efficient deployment of these models faces challenges, notably due to the\nhigh memory demands of their key-value (KV) caches. Conventional cache\nmanagement strategies for LLMs focus on cache eviction, which often fails to\naddress the specific needs of multimodal instruction-following models.\nRecognizing this gap, in this paper, we introduce Elastic Cache, a novel\napproach that benefits from applying distinct acceleration methods for\ninstruction encoding and output generation stages. We investigate the metrics\nof importance in different stages and propose an importance-driven cache\nmerging strategy to prune redundancy caches. Instead of discarding less\nimportant caches, our strategy identifies important key/value vectors as anchor\npoints. Surrounding less important caches are then merged with these anchors,\nenhancing the preservation of contextual information in the KV caches while\nyielding an arbitrary acceleration ratio. For instruction encoding, we utilize\nthe frequency to evaluate the importance of caches. Regarding output\ngeneration, we prioritize tokens based on their distance with an offset, by\nwhich both the initial and most recent tokens are retained. Results on a range\nof LVLMs demonstrate that Elastic Cache not only boosts efficiency but also\nnotably outperforms existing pruning methods in language generation across\nvarious tasks. Code is available at https://github.com/liuzuyan/ElasticCache"
                },
                "authors": [
                    {
                        "name": "Zuyan Liu"
                    },
                    {
                        "name": "Benlin Liu"
                    },
                    {
                        "name": "Jiahui Wang"
                    },
                    {
                        "name": "Yuhao Dong"
                    },
                    {
                        "name": "Guangyi Chen"
                    },
                    {
                        "name": "Yongming Rao"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Accepted to ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.02750v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.02750v2",
                "updated": "2024-07-25T09:16:05Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    16,
                    5,
                    3,
                    207,
                    0
                ],
                "published": "2024-02-05T06:06:47Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    6,
                    6,
                    47,
                    0,
                    36,
                    0
                ],
                "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache"
                },
                "summary": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI."
                },
                "authors": [
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Jiayi Yuan"
                    },
                    {
                        "name": "Hongye Jin"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Zhaozhuo Xu"
                    },
                    {
                        "name": "Vladimir Braverman"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Xia Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xia Hu"
                },
                "author": "Xia Hu",
                "arxiv_doi": "10.13140/RG.2.2.28167.37282",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.13140/RG.2.2.28167.37282",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2402.02750v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.02750v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ICML2024",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20272v1",
                "updated": "2024-07-25T07:50:17Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T07:50:17Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    7,
                    50,
                    17,
                    3,
                    207,
                    0
                ],
                "title": "An Efficient Inference Framework for Early-exit Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Inference Framework for Early-exit Large Language Models"
                },
                "summary": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building efficient inference framework has gained increasing interests for\nresearch community. Early-exit models, a variant of LLMs, improves the\ninference efficiency of LLMs by skipping rest layers and directly generate\noutput tokens when they are confident enough. However, there is no work of LLM\ninference framework that takes early-exit models into consideration. This is\nnon-trivial as prior art on LLM inference cannot be directly applied to\nearly-exit models. In this work, we solves two key challenges in building\nefficient inference framework for early-exit models: (1) batch inference at\niteration-level granularity; and (2) KV cache management. For the former, we\npropose to process the batch until all sequences surpass the early-exit\nconfidence threshold. For the latter, we propose to fill the KV cache of rest\nlayers before the iteration terminates. Our evaluation shows that, compared\nwith the original vLLM operating at full layers, our solution achieves up to\n1.25x speed up."
                },
                "authors": [
                    {
                        "name": "Ruijie Miao"
                    },
                    {
                        "name": "Yihan Yan"
                    },
                    {
                        "name": "Xinshuo Yao"
                    },
                    {
                        "name": "Tong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Yang"
                },
                "author": "Tong Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17678v1",
                "updated": "2024-07-25T00:27:07Z",
                "updated_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "published": "2024-07-25T00:27:07Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    0,
                    27,
                    7,
                    3,
                    207,
                    0
                ],
                "title": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Training and Serving with Heterogeneous Context Sharding\n  among Attention Heads"
                },
                "summary": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing LLM training and inference frameworks struggle in boosting\nefficiency with sparsity while maintaining the integrity of context and model\narchitecture. Inspired by the sharding concept in database and the fact that\nattention parallelizes over heads on accelerators, we propose Sparsely-Sharded\n(S2) Attention, an attention algorithm that allocates heterogeneous context\npartitions for different attention heads to divide and conquer. S2-Attention\nenforces each attention head to only attend to a partition of contexts\nfollowing a strided sparsity pattern, while the full context is preserved as\nthe union of all the shards. As attention heads are processed in separate\nthread blocks, the context reduction for each head can thus produce end-to-end\nspeed-up and memory reduction. At inference, LLMs trained with S2-Attention can\nthen take the KV cache reduction as free meals with guaranteed model quality\npreserve. In experiments, we show S2-Attentioncan provide as much as (1) 25.3X\nwall-clock attention speed-up over FlashAttention-2, resulting in 6X reduction\nin end-to-end training time and 10X inference latency, (2) on-par model\ntraining quality compared to default attention, (3)perfect needle retrieval\naccuracy over 32K context window. On top of the algorithm, we build DKernel, an\nLLM training and inference kernel library that allows users to customize\nsparsity patterns for their own models. We open-sourced DKerneland make it\ncompatible with Megatron, Pytorch, and vLLM."
                },
                "authors": [
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Barun Patra"
                    },
                    {
                        "name": "Vishrav Chaudhary"
                    },
                    {
                        "name": "Xia Song"
                    }
                ],
                "author_detail": {
                    "name": "Xia Song"
                },
                "author": "Xia Song",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.08711v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.08711v3",
                "updated": "2024-07-24T13:36:03Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    13,
                    36,
                    3,
                    2,
                    206,
                    0
                ],
                "published": "2023-01-20T18:13:38Z",
                "published_parsed": [
                    2023,
                    1,
                    20,
                    18,
                    13,
                    38,
                    4,
                    20,
                    0
                ],
                "title": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust, Secure and Private Cache-aided Scalar Linear Function Retrieval\n  from Distributed System with Blind and Adversarial Servers"
                },
                "summary": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, a distributed server system composed of multiple servers that\nholds some coded files and multiple users that are interested in retrieving the\nlinear functions of the files is investigated, where the servers are robust,\nblind and adversarial in the sense that any $J$ servers can together recover\nall files, while any $I$ colluding servers cannot obtain any information about\nthe files, and at most $A$ servers maliciously provides erroneous information.\nIn addition, the file library must be secure from a wiretapper who obtains all\nthe signals, and the demands of any subset of users must kept private from the\nother users and servers, even if they collude. A coding scheme is proposed by\nincorporating the ideas of Shamir's secret sharing and key superposition into\nthe framework of Placement Delivery Array (PDA), originally proposed to\ncharacterize the single-server coded caching system without any security or\nprivacy constraints. It is shown that PDAs associated to Maddah-Ali and\nNiesen's coded caching scheme results in an achievable\nmemory-storage-communication region, such that the storage size and\ncommunication load were optimal to within a multiplicative gap, except for the\nsmall memory regime when the number of files was smaller than the number of\nusers."
                },
                "authors": [
                    {
                        "name": "Qifa Yan"
                    },
                    {
                        "name": "Xiaohu Tang"
                    },
                    {
                        "name": "Zhengchun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhengchun Zhou"
                },
                "author": "Zhengchun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.08711v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.08711v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15771v2",
                "updated": "2024-07-24T12:56:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    12,
                    56,
                    41,
                    2,
                    206,
                    0
                ],
                "published": "2024-03-13T17:47:39Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    17,
                    47,
                    39,
                    2,
                    73,
                    0
                ],
                "title": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Splitting of Reusable Temporal Monitors for Rare Traffic\n  Violations"
                },
                "summary": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous Vehicles (AVs) are often tested in simulation to estimate the\nprobability they will violate safety specifications. Two common issues arise\nwhen using existing techniques to produce this estimation: If violations occur\nrarely, simple Monte-Carlo sampling techniques can fail to produce efficient\nestimates; if simulation horizons are too long, importance sampling techniques\n(which learn proposal distributions from past simulations) can fail to\nconverge. This paper addresses both issues by interleaving rare-event sampling\ntechniques with online specification monitoring algorithms. We use adaptive\nmulti-level splitting to decompose simulations into partial trajectories, then\ncalculate the distance of those partial trajectories to failure by leveraging\nrobustness metrics from Signal Temporal Logic (STL). By caching those partial\nrobustness metric values, we can efficiently re-use computations across\nmultiple sampling stages. Our experiments on an interstate lane-change scenario\nshow our method is viable for testing simulated AV-pipelines, efficiently\nestimating failure probabilities for STL specifications based on real traffic\nrules. We produce better estimates than Monte-Carlo and importance sampling in\nfewer simulations."
                },
                "authors": [
                    {
                        "name": "Craig Innes"
                    },
                    {
                        "name": "Subramanian Ramamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Subramanian Ramamoorthy"
                },
                "author": "Subramanian Ramamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.15569v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.15569v2",
                "updated": "2024-07-24T08:56:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    24,
                    8,
                    56,
                    11,
                    2,
                    206,
                    0
                ],
                "published": "2024-01-28T05:12:09Z",
                "published_parsed": [
                    2024,
                    1,
                    28,
                    5,
                    12,
                    9,
                    6,
                    28,
                    0
                ],
                "title": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Tuning and Inference for Large Language Models on Textual\n  Graphs"
                },
                "summary": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rich textual and topological information of textual graphs need to be modeled\nin real-world applications such as webpages, e-commerce, and academic articles.\nPractitioners have been long following the path of adopting a shallow text\nencoder and a subsequent graph neural network (GNN) to solve this problem. In\nlight of recent advancements in large language models (LLMs), it is apparent\nthat integrating LLMs for enhanced textual encoding can substantially improve\nthe performance of textual graphs. Nevertheless, the efficiency of these\nmethods poses a significant challenge. In this paper, we propose ENGINE, a\nparameter- and memory-efficient fine-tuning method for textual graphs with an\nLLM encoder. The key insight is to combine the LLMs and GNNs through a tunable\nside structure, which significantly reduces the training complexity without\nimpairing the joint model's capacity. Extensive experiments on textual graphs\ndemonstrate our method's effectiveness by achieving the best model performance,\nmeanwhile having the lowest training cost compared to previous methods.\nMoreover, we introduce two variants with caching and dynamic early exit to\nfurther enhance training and inference speed. Specifically, caching accelerates\nENGINE's training by 12x, and dynamic early exit achieves up to 5x faster\ninference with a negligible performance drop (at maximum 1.17% relevant drop\nacross 7 datasets). Our codes are available at:\nhttps://github.com/ZhuYun97/ENGINE"
                },
                "authors": [
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Yaoke Wang"
                    },
                    {
                        "name": "Haizhou Shi"
                    },
                    {
                        "name": "Siliang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Siliang Tang"
                },
                "author": "Siliang Tang",
                "arxiv_comment": "Accepted by IJCAI2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.15569v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.15569v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.09636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.09636v2",
                "updated": "2024-07-23T17:55:30Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    55,
                    30,
                    1,
                    205,
                    0
                ],
                "published": "2024-03-14T17:59:26Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    17,
                    59,
                    26,
                    3,
                    74,
                    0
                ],
                "title": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference"
                },
                "summary": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget."
                },
                "authors": [
                    {
                        "name": "Piotr Nawrot"
                    },
                    {
                        "name": "Adrian ≈Åa≈Ñcucki"
                    },
                    {
                        "name": "Marcin Chochowski"
                    },
                    {
                        "name": "David Tarjan"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Edoardo M. Ponti"
                },
                "author": "Edoardo M. Ponti",
                "arxiv_journal_ref": "Proceedings of the 41st International Conference on Machine\n  Learning (2024) 37396-37412",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.09636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.09636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16672v1",
                "updated": "2024-07-23T17:42:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T17:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    17,
                    42,
                    57,
                    1,
                    205,
                    0
                ],
                "title": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G at $\\frac{1}{6}g$: The Future of Cislunar Communications"
                },
                "summary": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What will the future of cislunar communications be? The ever-expanding\nhorizons of the space exploration missions, and the need for establishing\nsustainable space communication and navigation infrastructure necessitate to\nthink this question thoroughly. In this article, we examine how some of the\nconcepts of 6G technologies developed for terrestrial networks can be relevant\nin the context of cislunar networks. We discuss how 6G concepts, such as\nreconfigurable intelligent surfaces, quantum-resistant physical layer security,\nprivate information read/write/cache networks, semantic and goal-oriented\ncommunications, information freshness based quality of communication metrics,\nmulti-relay and cooperative networks, hold the potential to shape the future of\ncislunar communications."
                },
                "authors": [
                    {
                        "name": "Sahan Liyanaarachchi"
                    },
                    {
                        "name": "Stavros Mitrolaris"
                    },
                    {
                        "name": "Purbesh Mitra"
                    },
                    {
                        "name": "Sennur Ulukus"
                    }
                ],
                "author_detail": {
                    "name": "Sennur Ulukus"
                },
                "author": "Sennur Ulukus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16303v1",
                "updated": "2024-07-23T08:58:06Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:58:06Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    58,
                    6,
                    1,
                    205,
                    0
                ],
                "title": "Hidden Web Caches Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hidden Web Caches Discovery"
                },
                "summary": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web caches play a crucial role in web performance and scalability. However,\ndetecting cached responses is challenging when web servers do not reliably\ncommunicate the cache status through standardized headers. This paper presents\na novel methodology for cache detection using timing analysis. Our approach\neliminates the dependency on cache status headers, making it applicable to any\nweb server. The methodology relies on sending paired requests using HTTP\nmultiplexing functionality and makes heavy use of cache-busting to control the\norigin of the responses. By measuring the time it takes to receive responses\nfrom paired requests, we can determine if a response is cached or not. In each\npair, one request is cache-busted to force retrieval from the origin server,\nwhile the other request is not and might be served from the cache, if present.\nA faster response time for the non-cache-busted request compared to the\ncache-busted one suggests the first one is coming from the cache. We\nimplemented this approach in a tool and achieved an estimated accuracy of 89.6%\ncompared to state-of-the-art methods based on cache status headers. Leveraging\nour cache detection approach, we conducted a large-scale experiment on the\nTranco Top 50k websites. We identified a significant presence of hidden caches\n(5.8%) that do not advertise themselves through headers. Additionally, we\nemployed our methodology to detect Web Cache Deception (WCD) vulnerabilities in\nthese hidden caches. We discovered that 1.020 of them are susceptible to WCD\nvulnerabilities, potentially leaking sensitive data. Our findings demonstrate\nthe effectiveness of our timing analysis methodology for cache discovery and\nhighlight the importance of a tool that does not rely on cache-communicated\ncache status headers."
                },
                "authors": [
                    {
                        "name": "Matteo Golinelli"
                    },
                    {
                        "name": "Bruno Crispo"
                    }
                ],
                "author_detail": {
                    "name": "Bruno Crispo"
                },
                "author": "Bruno Crispo",
                "arxiv_doi": "10.1145/3678890.3678931",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678890.3678931",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.16303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The definitive Version of Record was published in The 27th\n  International Symposium on Research in Attacks, Intrusions and Defenses (RAID\n  2024), September 30-October 02, 2024, Padua, Italy,\n  https://doi.org/10.1145/3678890.3678931",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v1",
                "updated": "2024-07-23T08:55:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores in a cacheline granularity. Alongside with unleashing unique\nopportunities for a wide range of applications, CXL introduces new challenges\nof data management and crash consistency. Alas, CXL lacks an adequate\nprogramming model, which makes reasoning about the correctness and expected\nbehaviors of algorithms and systems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. Using these transformations, every\nlinearizable algorithm can be easily transformed into its provably correct\nversion in the face of a full-system or sub-system crash. We believe that this\nwork will serve as the stepping stone for systems design and modelling on top\nof CXL, and support the development of future models as software and hardware\nevolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16286v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16286v1",
                "updated": "2024-07-23T08:40:27Z",
                "updated_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "published": "2024-07-23T08:40:27Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    40,
                    27,
                    1,
                    205,
                    0
                ],
                "title": "A deeper look at depth pruning of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A deeper look at depth pruning of LLMs"
                },
                "summary": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are not only resource-intensive to train but\neven more costly to deploy in production. Therefore, recent work has attempted\nto prune blocks of LLMs based on cheap proxies for estimating block importance,\neffectively removing 10% of blocks in well-trained LLaMa-2 and Mistral 7b\nmodels without any significant degradation of downstream metrics. In this\npaper, we explore different block importance metrics by considering adaptive\nmetrics such as Shapley value in addition to static ones explored in prior\nwork. We show that adaptive metrics exhibit a trade-off in performance between\ntasks i.e., improvement on one task may degrade performance on the other due to\ndifferences in the computed block influences. Furthermore, we extend this\nanalysis from a complete block to individual self-attention and feed-forward\nlayers, highlighting the propensity of the self-attention layers to be more\namendable to pruning, even allowing removal of upto 33% of the self-attention\nlayers without incurring any performance degradation on MMLU for Mistral 7b\n(significant reduction in costly maintenance of KV-cache). Finally, we look at\nsimple performance recovery techniques to emulate the pruned layers by training\nlightweight additive bias or low-rank linear adapters. Performance recovery\nusing emulated updates avoids performance degradation for the initial blocks\n(up to 5% absolute improvement on MMLU), which is either competitive or\nsuperior to the learning-based technique."
                },
                "authors": [
                    {
                        "name": "Shoaib Ahmed Siddiqui"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Greg Heinrich"
                    },
                    {
                        "name": "Thomas Breuel"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "David Krueger"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16286v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16286v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15743v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15743v2",
                "updated": "2024-08-13T13:56:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    13,
                    56,
                    14,
                    1,
                    226,
                    0
                ],
                "published": "2024-07-22T15:42:59Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    15,
                    42,
                    59,
                    0,
                    204,
                    0
                ],
                "title": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Aided MIMO Communications: DoF Analysis and Transmitter\n  Optimization"
                },
                "summary": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-aided MIMO communications aims to jointly exploit both coded\ncaching~(CC) and spatial multiplexing gains to enhance communication\nefficiency. In this paper, we first analyze the achievable degrees of\nfreedom~(DoF) in a MIMO-CC system with CC gain \\(t\\), where a server with \\(L\\)\ntransmit antennas communicates with \\(K\\) users, each equipped with \\(G\\)\nreceive antennas. We demonstrate that the enhanced achievable DoF is\n\\(\\max_{\\beta, \\Omega} \\Omega \\beta\\), where the number of users \\(\\Omega\\)\nserved in each transmission is fine-tuned to maximize DoF, and \\(\\beta \\le\n\\min\\big(G, \\nicefrac{L \\binom{\\Omega-1}{t}}{1 + (\\Omega - t -\n1)\\binom{\\Omega-1}{t}}\\big)\\) represents the number of parallel streams decoded\nby each user. Second, we introduce an effective transmit covariance matrix\ndesign aimed at maximizing the symmetric rate, solved iteratively via\nsuccessive convex approximation. Third, we propose a new class of MIMO-CC\nschemes using a novel scheduling mechanism leveraging maximal multicasting\nopportunities to maximize delivery rates at given SNR levels while adhering to\nlinear processing constraints. Lastly, we devise linear multicast beamforming\nstrategies tailored for the flexible scheduling schemes in MIMO-CC systems and\npresent an iterative solution for the efficient design of beamformers.\nExtensive numerical simulations are used to verify the results of the paper."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti T√∂lli"
                    }
                ],
                "author_detail": {
                    "name": "Antti T√∂lli"
                },
                "author": "Antti T√∂lli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15743v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15743v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15309v1",
                "updated": "2024-07-22T14:37:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T14:37:58Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    14,
                    37,
                    58,
                    0,
                    204,
                    0
                ],
                "title": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving"
                },
                "summary": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are widely used across various domains,\nprocessing millions of daily requests. This surge in demand poses significant\nchallenges in optimizing throughput and latency while keeping costs manageable.\nThe Key-Value (KV) cache, a standard method for retaining previous\ncomputations, makes LLM inference highly bounded by memory. While batching\nstrategies can enhance performance, they frequently lead to significant memory\nfragmentation. Even though cutting-edge systems like vLLM mitigate KV cache\nfragmentation using paged Attention mechanisms, they still suffer from\ninefficient memory and computational operations due to the tightly coupled page\nmanagement and computation kernels.\n  This study introduces the vTensor, an innovative tensor structure for LLM\ninference based on GPU virtual memory management (VMM). vTensor addresses\nexisting limitations by decoupling computation from memory defragmentation and\noffering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous\napproach, ensuring efficient, fragmentation-free memory management while\naccommodating various computation kernels across different LLM architectures.\nExperimental results indicate that vTensor achieves an average speedup of 1.86x\nacross different models, with up to 2.42x in multi-turn chat scenarios.\nAdditionally, vTensor provides average speedups of 2.12x and 3.15x in kernel\nevaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton\nprefix-prefilling kernels and vLLM paged Attention kernel, respectively.\nFurthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100\nGPU compared to vLLM, enabling more memory-intensive workloads."
                },
                "authors": [
                    {
                        "name": "Jiale Xu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Weiming Hu"
                    },
                    {
                        "name": "Zihan Liu"
                    },
                    {
                        "name": "Feiyang Wu"
                    },
                    {
                        "name": "Yu Feng"
                    },
                    {
                        "name": "Shixuan Sun"
                    },
                    {
                        "name": "Changxu Shao"
                    },
                    {
                        "name": "Yuhong Guo"
                    },
                    {
                        "name": "Junping Zhao"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jingwen Leng"
                    }
                ],
                "author_detail": {
                    "name": "Jingwen Leng"
                },
                "author": "Jingwen Leng",
                "arxiv_comment": "16 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15581v1",
                "updated": "2024-07-22T12:17:01Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T12:17:01Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    12,
                    17,
                    1,
                    0,
                    204,
                    0
                ],
                "title": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vLSM: Low tail latency and I/O amplification in LSM-based KV stores"
                },
                "summary": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-based key-value (KV) stores are an important component in modern data\ninfrastructures. However, they suffer from high tail latency, in the order of\nseveral seconds, making them less attractive for user-facing applications. In\nthis paper, we introduce the notion of compaction chains and we analyse how\nthey affect tail latency. Then, we show that modern designs reduce tail\nlatency, by trading I/O amplification or require large amounts of memory. Based\non our analysis, we present vLSM, a new KV store design that improves tail\nlatency significantly without compromising on memory or I/O amplification. vLSM\nreduces (a) compaction chain width by using small SSTs and eliminating the\ntiering compaction required in L0 by modern systems and (b) compaction chain\nlength by using a larger than typical growth factor between L1 and L2 and\nintroducing overlap-aware SSTs in L1. We implement vLSM in RocksDB and evaluate\nit using db_bench and YCSB. Our evaluation highlights the underlying trade-off\namong memory requirements, I/O amplification, and tail latency, as well as the\nadvantage of vLSM over current approaches. vLSM improves P99 tail latency by up\nto 4.8x for writes and by up to 12.5x for reads, reduces cumulative write\nstalls by up to 60% while also slightly improves I/O amplification at the same\nmemory budget."
                },
                "authors": [
                    {
                        "name": "Giorgos Xanthakis"
                    },
                    {
                        "name": "Antonios Katsarakis"
                    },
                    {
                        "name": "Giorgos Saloustros"
                    },
                    {
                        "name": "Angelos Bilas"
                    }
                ],
                "author_detail": {
                    "name": "Angelos Bilas"
                },
                "author": "Angelos Bilas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.11055v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.11055v5",
                "updated": "2024-07-22T10:02:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    10,
                    2,
                    57,
                    0,
                    204,
                    0
                ],
                "published": "2022-12-21T14:59:23Z",
                "published_parsed": [
                    2022,
                    12,
                    21,
                    14,
                    59,
                    23,
                    2,
                    355,
                    0
                ],
                "title": "Coalgebraic Satisfiability Checking for Arithmetic $Œº$-Calculi",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coalgebraic Satisfiability Checking for Arithmetic $Œº$-Calculi"
                },
                "summary": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coalgebraic $\\mu$-calculus provides a generic semantic framework for\nfixpoint logics over systems whose branching type goes beyond the standard\nrelational setup, e.g. probabilistic, weighted, or game-based. Previous work on\nthe coalgebraic $\\mu$-calculus includes an exponential-time upper bound on\nsatisfiability checking, which however relies on the availability of tableau\nrules for the next-step modalities that are sufficiently well-behaved in a\nformally defined sense; in particular, rule matches need to be representable by\npolynomial-sized codes, and the sequent duals of the rules need to absorb cut.\nWhile such rule sets have been identified for some important cases, they are\nnot known to exist in all cases of interest, in particular ones involving\neither integer weights as in the graded $\\mu$-calculus, or real-valued weights\nin combination with non-linear arithmetic. In the present work, we prove the\nsame upper complexity bound under more general assumptions, specifically\nregarding the complexity of the (much simpler) satisfiability problem for the\nunderlying one-step logic, roughly described as the nesting-free next-step\nfragment of the logic. The bound is realized by a generic global caching\nalgorithm that supports on-the-fly satisfiability checking. Notably, our\napproach directly accommodates unguarded formulae, and thus avoids use of the\nguardedness transformation. Example applications include new exponential-time\nupper bounds for satisfiability checking in an extension of the graded\n$\\mu$-calculus with polynomial inequalities (including positive Presburger\narithmetic), as well as an extension of the (two-valued) probabilistic\n$\\mu$-calculus with polynomial inequalities."
                },
                "authors": [
                    {
                        "name": "Daniel Hausmann"
                    },
                    {
                        "name": "Lutz Schr√∂der"
                    }
                ],
                "author_detail": {
                    "name": "Lutz Schr√∂der"
                },
                "author": "Lutz Schr√∂der",
                "arxiv_doi": "10.46298/lmcs-20(3:9)2024",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.46298/lmcs-20(3:9)2024",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2212.11055v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.11055v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Logical Methods in Computer Science, Volume 20, Issue 3 (July 23,\n  2024) lmcs:10532",
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "03B70, 03B44",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "F.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15440v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15440v1",
                "updated": "2024-07-22T07:42:57Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T07:42:57Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    7,
                    42,
                    57,
                    0,
                    204,
                    0
                ],
                "title": "The Bicameral Cache: a split cache for vector architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache: a split cache for vector architectures"
                },
                "summary": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bicameral Cache is a cache organization proposal for a vector\narchitecture that segregates data according to their access type,\ndistinguishing scalar from vector references. Its aim is to avoid both types of\nreferences from interfering in each other's data locality, with a special focus\non prioritizing the performance on vector references. The proposed system\nincorporates an additional, non-polluting prefetching mechanism to help\npopulate the long vector cache lines in advance to increase the hit rate by\nfurther exploiting the spatial locality on vector data. Its evaluation was\nconducted on the Cavatools simulator, comparing the performance to a standard\nconventional cache, over different typical vector benchmarks for several vector\nlengths. The results proved the proposed cache speeds up performance on\nstride-1 vector benchmarks, while hardly impacting non-stride-1's. In addition,\nthe prefetching feature consistently provided an additional value."
                },
                "authors": [
                    {
                        "name": "Susana Rebolledo Ruiz"
                    },
                    {
                        "name": "Borja Perez"
                    },
                    {
                        "name": "Jose Luis Bosque"
                    },
                    {
                        "name": "Peter Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Peter Hsu"
                },
                "author": "Peter Hsu",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15440v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15440v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15360v1",
                "updated": "2024-07-22T04:07:26Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T04:07:26Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    4,
                    7,
                    26,
                    0,
                    204,
                    0
                ],
                "title": "Dissecting Multiplication in Transformers: Insights into LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting Multiplication in Transformers: Insights into LLMs"
                },
                "summary": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models have achieved remarkable performance\nacross various natural language processing tasks. However, they often struggle\nwith seemingly easy tasks like arithmetic despite their vast capabilities. This\nstark disparity raise human's concerns about their safe and ethical use, hinder\ntheir widespread adoption.In this paper, we focus on a typical arithmetic task,\ninteger multiplication, to explore and explain the imperfection of transformers\nin this domain. We provide comprehensive analysis of a vanilla transformer\ntrained to perform n-digit integer multiplication. Our observations indicate\nthat the model decomposes multiplication task into multiple parallel subtasks,\nsequentially optimizing each subtask for each digit to complete the final\nmultiplication. Based on observation and analysis, we infer the reasons of\ntransformers deficiencies in multiplication tasks lies in their difficulty in\ncalculating successive carryovers and caching intermediate results, and\nconfirmed this inference through experiments. Guided by these findings, we\npropose improvements to enhance transformers performance on multiplication\ntasks. These enhancements are validated through rigorous testing and\nmathematical modeling, not only enhance transformer's interpretability, but\nalso improve its performance, e.g., we achieve over 99.9% accuracy on 5-digit\ninteger multiplication with a tiny transformer, outperform LLMs GPT-4. Our\nmethod contributes to the broader fields of model understanding and\ninterpretability, paving the way for analyzing more complex tasks and\nTransformer models. This work underscores the importance of explainable AI,\nhelping to build trust in large language models and promoting their adoption in\ncritical applications."
                },
                "authors": [
                    {
                        "name": "Luyu Qiu"
                    },
                    {
                        "name": "Jianing Li"
                    },
                    {
                        "name": "Chi Su"
                    },
                    {
                        "name": "Chen Jason Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15891v1",
                "updated": "2024-07-22T01:12:23Z",
                "updated_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "published": "2024-07-22T01:12:23Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    12,
                    23,
                    0,
                    204,
                    0
                ],
                "title": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads"
                },
                "summary": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The memory and computational demands of Key-Value (KV) cache present\nsignificant challenges for deploying long-context language models. Previous\napproaches attempt to mitigate this issue by selectively dropping tokens, which\nirreversibly erases critical information that might be needed for future\nqueries. In this paper, we propose a novel compression technique for KV cache\nthat preserves all token information. Our investigation reveals that: i) Most\nattention heads primarily focus on the local context; ii) Only a few heads,\ndenoted as retrieval heads, can essentially pay attention to all input tokens.\nThese key observations motivate us to use separate caching strategy for\nattention heads. Therefore, we propose RazorAttention, a training-free KV cache\ncompression algorithm, which maintains a full cache for these crucial retrieval\nheads and discards the remote tokens in non-retrieval heads. Furthermore, we\nintroduce a novel mechanism involving a \"compensation token\" to further recover\nthe information in the dropped tokens. Extensive evaluations across a diverse\nset of large language models (LLMs) demonstrate that RazorAttention achieves a\nreduction in KV cache size by over 70% without noticeable impacts on\nperformance. Additionally, RazorAttention is compatible with FlashAttention,\nrendering it an efficient and plug-and-play solution that enhances LLM\ninference efficiency without overhead or retraining of the original model."
                },
                "authors": [
                    {
                        "name": "Hanlin Tang"
                    },
                    {
                        "name": "Yang Lin"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Qingsen Han"
                    },
                    {
                        "name": "Shikuan Hong"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Gongyi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gongyi Wang"
                },
                "author": "Gongyi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15264v1",
                "updated": "2024-07-21T20:41:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T20:41:39Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    20,
                    41,
                    39,
                    6,
                    203,
                    0
                ],
                "title": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSM-GNN: Large-scale Storage-based Multi-GPU GNN Training by Optimizing\n  Data Transfer Scheme"
                },
                "summary": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) are widely used today in recommendation systems,\nfraud detection, and node/link classification tasks. Real world GNNs continue\nto scale in size and require a large memory footprint for storing graphs and\nembeddings that often exceed the memory capacities of the target GPUs used for\ntraining. To address limited memory capacities, traditional GNN training\napproaches use graph partitioning and sharding techniques to scale up across\nmultiple GPUs within a node and/or scale out across multiple nodes. However,\nthis approach suffers from the high computational costs of graph partitioning\nalgorithms and inefficient communication across GPUs.\n  To address these overheads, we propose Large-scale Storage-based Multi-GPU\nGNN framework (LSM-GNN), a storagebased approach to train GNN models that\nutilizes a novel communication layer enabling GPU software caches to function\nas a system-wide shared cache with low overheads.LSM-GNN incorporates a hybrid\neviction policy that intelligently manages cache space by using both static and\ndynamic node information to significantly enhance cache performance.\nFurthermore, we introduce the Preemptive Victim-buffer Prefetcher (PVP), a\nmechanism for prefetching node feature data from a Victim Buffer located in CPU\npinned-memory to further reduce the pressure on the storage devices.\nExperimental results show that despite the lower compute capabilities and\nmemory capacities, LSM-GNN in a single node with two GPUs offers superior\nperformance over two-node-four-GPU Dist-DGL baseline and provides up to 3.75x\nspeed up on end-to-end epoch time while running large-scale GNN training"
                },
                "authors": [
                    {
                        "name": "Jeongmin Brian Park"
                    },
                    {
                        "name": "Kun Wu"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Zaid Quresh"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Wen-mei Hwu"
                    }
                ],
                "author_detail": {
                    "name": "Wen-mei Hwu"
                },
                "author": "Wen-mei Hwu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15176v1",
                "updated": "2024-07-21T14:23:37Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-21T14:23:37Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    23,
                    37,
                    6,
                    203,
                    0
                ],
                "title": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Farewell to Length Extrapolation, a Training-Free Infinite Context with\n  Finite Attention Scope"
                },
                "summary": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The maximum supported context length is a critical bottleneck limiting the\npractical application of the Large Language Model (LLM). Although existing\nlength extrapolation methods can extend the context of LLMs to millions of\ntokens, these methods all have an explicit upper bound. In this work, we\npropose LongCache, a training-free approach that enables LLM to support an\ninfinite context with finite context scope, through full-context cache\nselection and training-free integration. This effectively frees LLMs from the\nlength extrapolation issue. We validate LongCache on the LongBench and L-Eval\nand demonstrate its performance is on par with traditional full-attention\nmechanisms. Furthermore, we have applied LongCache on mainstream LLMs,\nincluding LLaMA3 and Mistral-v0.3, enabling them to support context lengths of\nat least 400K in Needle-In-A-Haystack tests. We will improve the efficiency of\nLongCache by GPU-aware optimization soon."
                },
                "authors": [
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Kai Lv"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Linlin Li"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v2",
                "updated": "2024-07-21T14:08:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    8,
                    42,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various fields but encounter\nefficiency limitations due to the substantial Key-Value (KV) cache required for\nlong-sequence inference. Recent efforts try to evict non-critical cache\nelements during runtime, thereby reducing cache size within given memory\nbudgets while preserving generation quality. Our reexamination of foundational\nprinciples reveals that prevailing methods aim to minimize an upper bound of\neviction loss, quantified as the L1 distance between the pre- and post-eviction\noutputs of multi-head self-attention mechanisms. Moreover, our analysis\nindicates that the common practices of uniformly assigning budgets across\ndifferent attention heads during cache eviction hinder their budget\nutilization, negatively impacting generation quality. In light of these\nfindings, we propose a simple yet effective adaptive budget allocation\nalgorithm. This algorithm not only optimizes the loss upper bound in theory but\nalso reduces the eviction loss in practice by aligning with the intrinsic\npatterns of self-attention mechanisms. Integrating this algorithm into two\nadvanced methods, we develop Ada-SnapKV and Ada-Pyramid. Extensive evaluations\non 16 datasets and the Needle-in-a-Haystack test confirm that they both\nsignificantly boost performance across various tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various fields but encounter\nefficiency limitations due to the substantial Key-Value (KV) cache required for\nlong-sequence inference. Recent efforts try to evict non-critical cache\nelements during runtime, thereby reducing cache size within given memory\nbudgets while preserving generation quality. Our reexamination of foundational\nprinciples reveals that prevailing methods aim to minimize an upper bound of\neviction loss, quantified as the L1 distance between the pre- and post-eviction\noutputs of multi-head self-attention mechanisms. Moreover, our analysis\nindicates that the common practices of uniformly assigning budgets across\ndifferent attention heads during cache eviction hinder their budget\nutilization, negatively impacting generation quality. In light of these\nfindings, we propose a simple yet effective adaptive budget allocation\nalgorithm. This algorithm not only optimizes the loss upper bound in theory but\nalso reduces the eviction loss in practice by aligning with the intrinsic\npatterns of self-attention mechanisms. Integrating this algorithm into two\nadvanced methods, we develop Ada-SnapKV and Ada-Pyramid. Extensive evaluations\non 16 datasets and the Needle-in-a-Haystack test confirm that they both\nsignificantly boost performance across various tasks."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.00250v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.00250v3",
                "updated": "2024-07-21T11:47:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    11,
                    47,
                    4,
                    6,
                    203,
                    0
                ],
                "published": "2022-12-01T03:35:14Z",
                "published_parsed": [
                    2022,
                    12,
                    1,
                    3,
                    35,
                    14,
                    3,
                    335,
                    0
                ],
                "title": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split Learning without Local Weight Sharing to Enhance Client-side Data\n  Privacy"
                },
                "summary": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Split learning (SL) aims to protect user data privacy by distributing deep\nmodels between client-server and keeping private data locally. In SL training\nwith multiple clients, the local model weights are shared among the clients for\nlocal model update. This paper first reveals data privacy leakage exacerbated\nfrom local weight sharing among the clients in SL through model inversion\nattacks. Then, to reduce the data privacy leakage issue, we propose and analyze\nprivacy-enhanced SL (P-SL) (or SL without local weight sharing). We further\npropose parallelized P-SL to expedite the training process by duplicating\nmultiple server-side model instances without compromising accuracy. Finally, we\nexplore P-SL with late participating clients and devise a server-side\ncache-based training method to address the forgetting phenomenon in SL when\nlate clients join. Experimental results demonstrate that P-SL helps reduce up\nto 50% of client-side data leakage, which essentially achieves a better\nprivacy-accuracy trade-off than the current trend by using differential privacy\nmechanisms. Moreover, P-SL and its cache-based version achieve comparable\naccuracy to baseline SL under various data distributions, while cost less\ncomputation and communication. Additionally, caching-based training in P-SL\nmitigates the negative effect of forgetting, stabilizes the learning, and\nenables practical and low-complexity training in a dynamic environment with\nlate-arriving clients."
                },
                "authors": [
                    {
                        "name": "Ngoc Duy Pham"
                    },
                    {
                        "name": "Tran Khoa Phan"
                    },
                    {
                        "name": "Alsharif Abuadbba"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Doan Nguyen"
                    },
                    {
                        "name": "Naveen Chilamkurti"
                    }
                ],
                "author_detail": {
                    "name": "Naveen Chilamkurti"
                },
                "author": "Naveen Chilamkurti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.00250v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.00250v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08454v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08454v2",
                "updated": "2024-07-21T02:37:11Z",
                "updated_parsed": [
                    2024,
                    7,
                    21,
                    2,
                    37,
                    11,
                    6,
                    203,
                    0
                ],
                "published": "2024-07-11T12:50:42Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    12,
                    50,
                    42,
                    3,
                    193,
                    0
                ],
                "title": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on\n  Long-Context Tasks"
                },
                "summary": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve Large Language Models (LLMs) has become a pressing\nissue because of their huge computational cost in their autoregressive\ngeneration process. To mitigate computational costs, LLMs often employ the KV\nCache technique to improve the generation speed. While improving the\ncomputational efficiency, the storage requirements of the KV cache are\nsubstantial, particularly in long-context scenarios, leading to significant\nmemory consumption. Existing KV cache eviction methods often degrade the\nperformance of LLMs in long-context scenarios due to the information loss\nintroduced by eviction. In this paper, we propose a novel KV cache merging\napproach, called KVMerger, to achieve adaptive KV cache compression for\nlong-context tasks without significant performance degradation under\nconstrained memory budgets. Our approach is inspired by the intriguing\nobservation that key states exhibit high similarity at the token level within a\nsingle sequence. To facilitate merging, we develop an effective yet\nstraightforward merging set identification algorithm to identify suitable KV\nstates for merging. Our merging set identification algorithm stimulates the\nsecond observation that KV cache sparsity, from similarity perspective, is\nindependent of the dataset and remains persistent at the model level.\nSubsequently, we propose a Gaussian kernel weighted merging algorithm to\nselectively merge all states within each merging set. We conduct extensive\nexperiments to demonstrate the effectiveness of KVMerger for long-context tasks\nunder constrained memory budgets, applying it to models including\nLlama2-7B-chat and Llama2-13B-chat. Using the LongBench and ZeroScroll\nbenchmarks, we compare our method with other KV cache compression techniques,\nincluding H2O and CaM, showing that our method achieves superior performance\nacross tasks with both 50% and 35% KV cache budgets."
                },
                "authors": [
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Boxiao Jin"
                    },
                    {
                        "name": "Zhongzhi Yu"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08454v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08454v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.10516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.10516v2",
                "updated": "2024-07-20T22:14:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    22,
                    14,
                    42,
                    5,
                    202,
                    0
                ],
                "published": "2023-03-28T03:55:47Z",
                "published_parsed": [
                    2023,
                    3,
                    28,
                    3,
                    55,
                    47,
                    1,
                    87,
                    0
                ],
                "title": "Distributed Neural Representation for Reactive in situ Visualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Neural Representation for Reactive in situ Visualization"
                },
                "summary": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit neural representations (INRs) have emerged as a powerful tool for\ncompressing large-scale volume data. This opens up new possibilities for in\nsitu visualization. However, the efficient application of INRs to distributed\ndata remains an underexplored area. In this work, we develop a distributed\nvolumetric neural representation and optimize it for in situ visualization. Our\ntechnique eliminates data exchanges between processes, achieving\nstate-of-the-art compression speed, quality and ratios. Our technique also\nenables the implementation of an efficient strategy for caching large-scale\nsimulation data in high temporal frequencies, further facilitating the use of\nreactive in situ visualization in a wider range of scientific problems. We\nintegrate this system with the Ascent infrastructure and evaluate its\nperformance and usability using real-world simulations."
                },
                "authors": [
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Joseph A. Insley"
                    },
                    {
                        "name": "Victor A. Mateevitsi"
                    },
                    {
                        "name": "Silvio Rizzi"
                    },
                    {
                        "name": "Michael E. Papka"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.10516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.10516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14801v1",
                "updated": "2024-07-20T08:21:46Z",
                "updated_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "published": "2024-07-20T08:21:46Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    8,
                    21,
                    46,
                    5,
                    202,
                    0
                ],
                "title": "SquareSort: a cache-oblivious sorting algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SquareSort: a cache-oblivious sorting algorithm"
                },
                "summary": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we consider sorting in the cache-oblivious model of Frigo,\nLeiserson, Prokop, and Ramachandran (1999). We introduce a new simple sorting\nalgorithm in that model which has asymptotically optimal IO complexity\n$O(\\frac{n}{B} \\log_{M/B} n)$, where $n$ is the instance size, $M$ size of the\ncache and $B$ size of a memory block. This is the same as the complexity of the\nbest known cache-oblivious sorting algorithm FunnelSort."
                },
                "authors": [
                    {
                        "name": "Michal Kouck√Ω"
                    },
                    {
                        "name": "Josef Matƒõjka"
                    }
                ],
                "author_detail": {
                    "name": "Josef Matƒõjka"
                },
                "author": "Josef Matƒõjka",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07240v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07240v6",
                "updated": "2024-07-19T21:04:14Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    21,
                    4,
                    14,
                    4,
                    201,
                    0
                ],
                "published": "2023-10-11T07:08:20Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    7,
                    8,
                    20,
                    2,
                    284,
                    0
                ],
                "title": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheGen: KV Cache Compression and Streaming for Fast Large Language\n  Model Serving"
                },
                "summary": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) take on complex tasks, their inputs are\nsupplemented with longer contexts that incorporate domain knowledge. Yet using\nlong contexts is challenging, as nothing can be generated until the whole\ncontext is processed by the LLM. While the context-processing delay can be\nreduced by reusing the KV cache of a context across different inputs, fetching\nthe KV cache, which contains large tensors, over the network can cause high\nextra network delays.\n  CacheGen is a fast context-loading module for LLM systems. First, CacheGen\nuses a custom tensor encoder, leveraging KV cache's distributional properties\nto encode a KV cache into more compact bitstream representations with\nnegligible decoding overhead, to save bandwidth usage. Second, CacheGen adapts\nthe compression level of different parts of a KV cache to cope with changes in\navailable bandwidth, in order to maintain low context-loading delay and high\ngeneration quality. % When available bandwidth drops, CacheGen may raise the\ncompression level for a part of the context or recompute its KV cache on the\nfly. We test CacheGen on popular LLMs and datasets. Compared to the recent\nsystems that reuse the KV cache, CacheGen reduces the KV cache size by 3.5-4.3x\nand the total delay in fetching and processing contexts by 3.2-3.7x with\nnegligible impact on the LLM response quality. Our code is at:\nhttps://github.com/UChi-JCL/CacheGen."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Michael Maire"
                    },
                    {
                        "name": "Henry Hoffmann"
                    },
                    {
                        "name": "Ari Holtzman"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "arxiv_comment": "SIGCOMM'24",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07240v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07240v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14346v1",
                "updated": "2024-07-19T14:28:53Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T14:28:53Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    14,
                    28,
                    53,
                    4,
                    201,
                    0
                ],
                "title": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Retrieval in Sponsored Search by Leveraging Query Context\n  Signals"
                },
                "summary": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately retrieving relevant bid keywords for user queries is critical in\nSponsored Search but remains challenging, particularly for short, ambiguous\nqueries. Existing dense and generative retrieval models often fail to capture\nnuanced user intent in these cases. To address this, we propose an approach to\nenhance query understanding by augmenting queries with rich contextual signals\nderived from web search results and large language models, stored in an online\ncache. Specifically, we use web search titles and snippets to ground queries in\nreal-world information and utilize GPT-4 to generate query rewrites and\nexplanations that clarify user intent. These signals are efficiently integrated\nthrough a Fusion-in-Decoder based Unity architecture, enabling both dense and\ngenerative retrieval with serving costs on par with traditional context-free\nmodels. To address scenarios where context is unavailable in the cache, we\nintroduce context glancing, a curriculum learning strategy that improves model\nrobustness and performance even without contextual signals during inference.\nExtensive offline experiments demonstrate that our context-aware approach\nsubstantially outperforms context-free models. Furthermore, online A/B testing\non a prominent search engine across 160+ countries shows significant\nimprovements in user engagement and revenue."
                },
                "authors": [
                    {
                        "name": "Akash Kumar Mohankumar"
                    },
                    {
                        "name": "Gururaj K"
                    },
                    {
                        "name": "Gagan Madan"
                    },
                    {
                        "name": "Amit Singh"
                    }
                ],
                "author_detail": {
                    "name": "Amit Singh"
                },
                "author": "Amit Singh",
                "arxiv_comment": "8 pages, 8 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.04985v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.04985v5",
                "updated": "2024-07-19T09:37:19Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    9,
                    37,
                    19,
                    4,
                    201,
                    0
                ],
                "published": "2023-12-08T11:47:35Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    11,
                    47,
                    35,
                    4,
                    342,
                    0
                ],
                "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparQ Attention: Bandwidth-Efficient LLM Inference"
                },
                "summary": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computational difficulties of large language model (LLM) inference remain\na significant obstacle to their widespread deployment. The need for many\napplications to support long input sequences and process them in large batches\ntypically causes token-generation to be bottlenecked by data transfer. For this\nreason, we introduce SparQ Attention, a technique for increasing the inference\nthroughput of LLMs by utilising memory bandwidth more efficiently within the\nattention layers, through selective fetching of the cached history. Our\nproposed technique can be applied directly to off-the-shelf LLMs during\ninference, without requiring any modification to the pre-training setup or\nadditional fine-tuning. We show that SparQ Attention brings up to 8x savings in\nattention data transfers without substantial drops in accuracy, by evaluating\nLlama 2 and 3, Mistral, Gemma and Pythia models on a wide range of downstream\ntasks."
                },
                "authors": [
                    {
                        "name": "Luka Ribar"
                    },
                    {
                        "name": "Ivan Chelombiev"
                    },
                    {
                        "name": "Luke Hudlass-Galley"
                    },
                    {
                        "name": "Charlie Blake"
                    },
                    {
                        "name": "Carlo Luschi"
                    },
                    {
                        "name": "Douglas Orr"
                    }
                ],
                "author_detail": {
                    "name": "Douglas Orr"
                },
                "author": "Douglas Orr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.04985v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.04985v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14057v1",
                "updated": "2024-07-19T06:34:45Z",
                "updated_parsed": [
                    2024,
                    7,
                    19,
                    6,
                    34,
                    45,
                    4,
                    201,
                    0
                ],
                "published": "2024-07-19T06:34:45Z",
                "published_parsed": [
                    2024,
                    7,
                    19,
                    6,
                    34,
                    45,
                    4,
                    201,
                    0
                ],
                "title": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference"
                },
                "summary": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The inference of transformer-based large language models consists of two\nsequential stages: 1) a prefilling stage to compute the KV cache of prompts and\ngenerate the first token, and 2) a decoding stage to generate subsequent\ntokens. For long prompts, the KV cache must be computed for all tokens during\nthe prefilling stage, which can significantly increase the time needed to\ngenerate the first token. Consequently, the prefilling stage may become a\nbottleneck in the generation process. An open question remains whether all\nprompt tokens are essential for generating the first token. To answer this, we\nintroduce a novel method, LazyLLM, that selectively computes the KV for tokens\nimportant for the next token prediction in both the prefilling and decoding\nstages. Contrary to static pruning approaches that prune the prompt at once,\nLazyLLM allows language models to dynamically select different subsets of\ntokens from the context in different generation steps, even though they might\nbe pruned in previous steps. Extensive experiments on standard datasets across\nvarious tasks demonstrate that LazyLLM is a generic method that can be\nseamlessly integrated with existing language models to significantly accelerate\nthe generation without fine-tuning. For instance, in the multi-document\nquestion-answering task, LazyLLM accelerates the prefilling stage of the LLama\n2 7B model by 2.34x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Qichen Fu"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "Thomas Merth"
                    },
                    {
                        "name": "Sachin Mehta"
                    },
                    {
                        "name": "Mohammad Rastegari"
                    },
                    {
                        "name": "Mahyar Najibi"
                    }
                ],
                "author_detail": {
                    "name": "Mahyar Najibi"
                },
                "author": "Mahyar Najibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v1",
                "updated": "2024-07-18T18:47:52Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Data-driven Forecasting of Deep Learning Performance on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven Forecasting of Deep Learning Performance on GPUs"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 198% and 19.7% to 3.8% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior works, where both GPT3 and H100 were not\nused to train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13853v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03482v2",
                "updated": "2024-07-18T16:31:29Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    16,
                    31,
                    29,
                    3,
                    200,
                    0
                ],
                "published": "2024-06-05T17:42:05Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    17,
                    42,
                    5,
                    2,
                    157,
                    0
                ],
                "title": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero\n  Overhead"
                },
                "summary": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving LLMs requires substantial memory due to the storage requirements of\nKey-Value (KV) embeddings in the KV cache, which grows with sequence length. An\neffective approach to compress KV cache is quantization. However, traditional\nquantization methods face significant memory overhead due to the need to store\nquantization constants (at least a zero point and a scale) in full precision\nper data block. Depending on the block size, this overhead can add 1 or 2 bits\nper quantized number. We introduce QJL, a new quantization approach that\nconsists of a Johnson-Lindenstrauss (JL) transform followed by sign-bit\nquantization. In contrast to existing methods, QJL eliminates memory overheads\nby removing the need for storing quantization constants. We propose an\nasymmetric estimator for the inner product of two vectors and demonstrate that\napplying QJL to one vector and a standard JL transform without quantization to\nthe other provides an unbiased estimator with minimal distortion. We have\ndeveloped an efficient implementation of the QJL sketch and its corresponding\ninner product estimator, incorporating a lightweight CUDA kernel for optimized\ncomputation. When applied across various LLMs and NLP tasks to quantize the KV\ncache to only 3 bits, QJL demonstrates a more than fivefold reduction in KV\ncache memory usage without compromising accuracy, all while achieving faster\nruntime. Codes are available at \\url{https://github.com/amirzandieh/QJL}."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Insu Han"
                    }
                ],
                "author_detail": {
                    "name": "Insu Han"
                },
                "author": "Insu Han",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.12925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.12925v2",
                "updated": "2024-07-18T09:06:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    9,
                    6,
                    0,
                    3,
                    200,
                    0
                ],
                "published": "2023-09-22T15:23:57Z",
                "published_parsed": [
                    2023,
                    9,
                    22,
                    15,
                    23,
                    57,
                    4,
                    265,
                    0
                ],
                "title": "MCU-Wide Timing Side Channels and Their Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCU-Wide Timing Side Channels and Their Detection"
                },
                "summary": "Microarchitectural timing side channels have been thoroughly investigated as\na security threat in hardware designs featuring shared buffers (e.g., caches)\nor parallelism between attacker and victim task execution. However,\ncontradicting common intuitions, recent activities demonstrate that this threat\nis real even in microcontroller SoCs without such features. In this paper, we\ndescribe SoC-wide timing side channels previously neglected by security\nanalysis and present a new formal method to close this gap. In a case study on\nthe RISC-V Pulpissimo SoC, our method detected a vulnerability to a previously\nunknown attack variant that allows an attacker to obtain information about a\nvictim's memory access behavior. After implementing a conservative fix, we were\nable to verify that the SoC is now secure w.r.t. the considered class of timing\nside channels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Microarchitectural timing side channels have been thoroughly investigated as\na security threat in hardware designs featuring shared buffers (e.g., caches)\nor parallelism between attacker and victim task execution. However,\ncontradicting common intuitions, recent activities demonstrate that this threat\nis real even in microcontroller SoCs without such features. In this paper, we\ndescribe SoC-wide timing side channels previously neglected by security\nanalysis and present a new formal method to close this gap. In a case study on\nthe RISC-V Pulpissimo SoC, our method detected a vulnerability to a previously\nunknown attack variant that allows an attacker to obtain information about a\nvictim's memory access behavior. After implementing a conservative fix, we were\nable to verify that the SoC is now secure w.r.t. the considered class of timing\nside channels."
                },
                "authors": [
                    {
                        "name": "Johannes M√ºller"
                    },
                    {
                        "name": "Anna Lena Duque Ant√≥n"
                    },
                    {
                        "name": "Lucas Deutschmann"
                    },
                    {
                        "name": "Dino Mehmedagiƒá"
                    },
                    {
                        "name": "Cristiano Rodrigues"
                    },
                    {
                        "name": "Daniel Oliveira"
                    },
                    {
                        "name": "Keerthikumara Devarajegowda"
                    },
                    {
                        "name": "Mohammad Rahmani Fadiheh"
                    },
                    {
                        "name": "Sandro Pinto"
                    },
                    {
                        "name": "Dominik Stoffel"
                    },
                    {
                        "name": "Wolfgang Kunz"
                    }
                ],
                "author_detail": {
                    "name": "Wolfgang Kunz"
                },
                "author": "Wolfgang Kunz",
                "arxiv_doi": "10.1145/3649329.3656541",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3649329.3656541",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2309.12925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.12925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This version extends the work of the previous version and was\n  accepted and presented at DAC'24",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v3",
                "updated": "2024-07-18T06:18:04Z",
                "updated_parsed": [
                    2024,
                    7,
                    18,
                    6,
                    18,
                    4,
                    3,
                    200,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongfu Li"
                },
                "author": "Hongfu Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02747v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02747v2",
                "updated": "2024-07-17T23:09:10Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    23,
                    9,
                    10,
                    2,
                    199,
                    0
                ],
                "published": "2024-04-03T13:44:41Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    13,
                    44,
                    41,
                    2,
                    94,
                    0
                ],
                "title": "Faster Diffusion via Temporal Attention Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Diffusion via Temporal Attention Decomposition"
                },
                "summary": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the role of attention mechanism during inference in\ntext-conditional diffusion models. Empirical observations suggest that\ncross-attention outputs converge to a fixed point after several inference\nsteps. The convergence time naturally divides the entire inference process into\ntwo phases: an initial phase for planning text-oriented visual semantics, which\nare then translated into images in a subsequent fidelity-improving phase.\nCross-attention is essential in the initial phase but almost irrelevant\nthereafter. However, self-attention initially plays a minor role but becomes\ncrucial in the second phase. These findings yield a simple and training-free\nmethod known as temporally gating the attention (TGATE), which efficiently\ngenerates images by caching and reusing attention outputs at scheduled time\nsteps. Experimental results show when widely applied to various existing\ntext-conditional diffusion models, TGATE accelerates these models by 10%-50%.\nThe code of TGATE is available at https://github.com/HaozheLiu-ST/T-GATE."
                },
                "authors": [
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Wentian Zhang"
                    },
                    {
                        "name": "Jinheng Xie"
                    },
                    {
                        "name": "Francesco Faccio"
                    },
                    {
                        "name": "Mengmeng Xu"
                    },
                    {
                        "name": "Tao Xiang"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Juan-Manuel Perez-Rua"
                    },
                    {
                        "name": "J√ºrgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "J√ºrgen Schmidhuber"
                },
                "author": "J√ºrgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02747v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02747v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12850v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12850v2",
                "updated": "2024-07-17T16:56:18Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    16,
                    56,
                    18,
                    2,
                    199,
                    0
                ],
                "published": "2024-04-19T12:39:11Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    12,
                    39,
                    11,
                    4,
                    110,
                    0
                ],
                "title": "CaBaFL: Asynchronous Federated Learning via Hierarchical Cache and\n  Feature Balance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaBaFL: Asynchronous Federated Learning via Hierarchical Cache and\n  Feature Balance"
                },
                "summary": "Federated Learning (FL) as a promising distributed machine learning paradigm\nhas been widely adopted in Artificial Intelligence of Things (AIoT)\napplications. However, the efficiency and inference capability of FL is\nseriously limited due to the presence of stragglers and data imbalance across\nmassive AIoT devices, respectively. To address the above challenges, we present\na novel asynchronous FL approach named CaBaFL, which includes a hierarchical\nCache-based aggregation mechanism and a feature Balance-guided device selection\nstrategy. CaBaFL maintains multiple intermediate models simultaneously for\nlocal training. The hierarchical cache-based aggregation mechanism enables each\nintermediate model to be trained on multiple devices to align the training time\nand mitigate the straggler issue. In specific, each intermediate model is\nstored in a low-level cache for local training and when it is trained by\nsufficient local devices, it will be stored in a high-level cache for\naggregation. To address the problem of imbalanced data, the feature\nbalance-guided device selection strategy in CaBaFL adopts the activation\ndistribution as a metric, which enables each intermediate model to be trained\nacross devices with totally balanced data distributions before aggregation.\nExperimental results show that compared with the state-of-the-art FL methods,\nCaBaFL achieves up to 9.26X training acceleration and 19.71\\% accuracy\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) as a promising distributed machine learning paradigm\nhas been widely adopted in Artificial Intelligence of Things (AIoT)\napplications. However, the efficiency and inference capability of FL is\nseriously limited due to the presence of stragglers and data imbalance across\nmassive AIoT devices, respectively. To address the above challenges, we present\na novel asynchronous FL approach named CaBaFL, which includes a hierarchical\nCache-based aggregation mechanism and a feature Balance-guided device selection\nstrategy. CaBaFL maintains multiple intermediate models simultaneously for\nlocal training. The hierarchical cache-based aggregation mechanism enables each\nintermediate model to be trained on multiple devices to align the training time\nand mitigate the straggler issue. In specific, each intermediate model is\nstored in a low-level cache for local training and when it is trained by\nsufficient local devices, it will be stored in a high-level cache for\naggregation. To address the problem of imbalanced data, the feature\nbalance-guided device selection strategy in CaBaFL adopts the activation\ndistribution as a metric, which enables each intermediate model to be trained\nacross devices with totally balanced data distributions before aggregation.\nExperimental results show that compared with the state-of-the-art FL methods,\nCaBaFL achieves up to 9.26X training acceleration and 19.71\\% accuracy\nimprovements."
                },
                "authors": [
                    {
                        "name": "Zeke Xia"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Dengke Yan"
                    },
                    {
                        "name": "Xiaofei Xie"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Anran Li"
                    },
                    {
                        "name": "Junlong Zhou"
                    },
                    {
                        "name": "Mingsong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Mingsong Chen"
                },
                "author": "Mingsong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12850v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12850v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19626v2",
                "updated": "2024-07-17T03:02:49Z",
                "updated_parsed": [
                    2024,
                    7,
                    17,
                    3,
                    2,
                    49,
                    2,
                    199,
                    0
                ],
                "published": "2024-05-30T02:23:50Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    2,
                    23,
                    50,
                    3,
                    151,
                    0
                ],
                "title": "CXL Shared Memory Programming: Barely Distributed and Almost Persistent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL Shared Memory Programming: Barely Distributed and Almost Persistent"
                },
                "summary": "While Compute Express Link (CXL) enables support for cache-coherent shared\nmemory among multiple nodes, it also introduces new types of\nfailures--processes can fail before data does, or data might fail before a\nprocess does. The lack of a failure model for CXL-based shared memory makes it\nchallenging to understand and mitigate these failures.\n  To solve these challenges, in this paper, we describe a model categorizing\nand handling the CXL-based shared memory's failures: data and process failures.\nData failures in CXL-based shared memory render data inaccessible or\ninconsistent for a currently running application. We argue that such failures\nare unlike data failures in distributed storage systems and require\nCXL-specific handling. To address this, we look into traditional data failure\nmitigation techniques like erasure coding and replication and propose new\nsolutions to better handle data failures in CXL-based shared memory systems.\nNext, we look into process failures and compare the failures and potential\nsolutions with PMEM's failure model and programming solutions. We argue that\nalthough PMEM shares some of CXL's characteristics, it does not fully address\nCXL's volatile nature and low access latencies. Finally, taking inspiration\nfrom PMEM programming solutions, we propose techniques to handle these new\nfailures.\n  Thus, this paper is the first work to define the CXL-based shared memory\nfailure model and propose tailored solutions that address challenges specific\nto CXL-based systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Compute Express Link (CXL) enables support for cache-coherent shared\nmemory among multiple nodes, it also introduces new types of\nfailures--processes can fail before data does, or data might fail before a\nprocess does. The lack of a failure model for CXL-based shared memory makes it\nchallenging to understand and mitigate these failures.\n  To solve these challenges, in this paper, we describe a model categorizing\nand handling the CXL-based shared memory's failures: data and process failures.\nData failures in CXL-based shared memory render data inaccessible or\ninconsistent for a currently running application. We argue that such failures\nare unlike data failures in distributed storage systems and require\nCXL-specific handling. To address this, we look into traditional data failure\nmitigation techniques like erasure coding and replication and propose new\nsolutions to better handle data failures in CXL-based shared memory systems.\nNext, we look into process failures and compare the failures and potential\nsolutions with PMEM's failure model and programming solutions. We argue that\nalthough PMEM shares some of CXL's characteristics, it does not fully address\nCXL's volatile nature and low access latencies. Finally, taking inspiration\nfrom PMEM programming solutions, we propose techniques to handle these new\nfailures.\n  Thus, this paper is the first work to define the CXL-based shared memory\nfailure model and propose tailored solutions that address challenges specific\nto CXL-based systems."
                },
                "authors": [
                    {
                        "name": "Yi Xu"
                    },
                    {
                        "name": "Suyash Mahar"
                    },
                    {
                        "name": "Ziheng Liu"
                    },
                    {
                        "name": "Mingyao Shen"
                    },
                    {
                        "name": "Steven Swanson"
                    }
                ],
                "author_detail": {
                    "name": "Steven Swanson"
                },
                "author": "Steven Swanson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12077v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12077v1",
                "updated": "2024-07-16T18:00:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    18,
                    0,
                    0,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T18:00:00Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    18,
                    0,
                    0,
                    1,
                    198,
                    0
                ],
                "title": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill\n  and Extreme KV-Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill\n  and Extreme KV-Cache Compression"
                },
                "summary": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce GoldFinch, a hybrid Linear Attention/Transformer sequence model\nthat uses a new technique to efficiently generate a highly compressed and\nreusable KV-Cache in linear time and space with respect to sequence length.\nGoldFinch stacks our new GOLD transformer on top of an enhanced version of the\nFinch (RWKV-6) architecture. We train up to 1.5B parameter class models of the\nFinch, Llama, and GoldFinch architectures, and find dramatically improved\nmodeling performance relative to both Finch and Llama. Our cache size savings\nincrease linearly with model layer count, ranging from 756-2550 times smaller\nthan the traditional transformer cache for common sizes, enabling inference of\nextremely large context lengths even on limited hardware. Although\nautoregressive generation has O(n) time complexity per token because of\nattention, pre-fill computation of the entire initial cache state for a\nsubmitted context costs only O(1) time per token due to the use of a recurrent\nneural network (RNN) to generate this cache. We release our trained weights and\ntraining code under the Apache 2.0 license for community use."
                },
                "authors": [
                    {
                        "name": "Daniel Goldstein"
                    },
                    {
                        "name": "Fares Obeid"
                    },
                    {
                        "name": "Eric Alcaide"
                    },
                    {
                        "name": "Guangyu Song"
                    },
                    {
                        "name": "Eugene Cheah"
                    }
                ],
                "author_detail": {
                    "name": "Eugene Cheah"
                },
                "author": "Eugene Cheah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12077v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.04877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.04877v2",
                "updated": "2024-07-16T09:05:39Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    5,
                    39,
                    1,
                    198,
                    0
                ],
                "published": "2023-05-08T17:20:30Z",
                "published_parsed": [
                    2023,
                    5,
                    8,
                    17,
                    20,
                    30,
                    0,
                    128,
                    0
                ],
                "title": "Coherently amplified ultrafast imaging using a free-electron\n  interferometer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherently amplified ultrafast imaging using a free-electron\n  interferometer"
                },
                "summary": "Accessing the low-energy non-equilibrium dynamics of materials and their\npolaritons with simultaneous high spatial and temporal resolution has been a\nbold frontier of electron microscopy in recent years. One of the main\nchallenges lies in the ability to retrieve extremely weak signals while\nsimultaneously disentangling amplitude and phase information. Here, we present\nFree-Electron Ramsey Imaging (FERI), a microscopy approach based on\nlight-induced electron modulation that enables coherent amplification of\noptical near-fields in electron imaging. We provide simultaneous time-, space-,\nand phase-resolved measurements of a micro-drum made from a hexagonal boron\nnitride membrane visualizing the sub-cycle dynamics of 2D polariton wavepackets\ntherein. The phase-resolved measurements reveals vortex-anti-vortex\nsingularities on the polariton wavefronts, together with an intriguing\nphenomenon of a traveling wave mimicking the amplitude profile of a standing\nwave. Our experiments show a 20-fold coherent amplification of the near-field\nsignal compared to conventional electron near-field imaging, resolving peak\nfield intensities in the order of ~W/cm2, corresponding to field amplitudes of\na few kV/m. As a result, our work paves the way for spatio-temporal electron\nmicroscopy of biological specimens and quantum materials, exciting yet delicate\nsamples that are currently difficult to investigate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accessing the low-energy non-equilibrium dynamics of materials and their\npolaritons with simultaneous high spatial and temporal resolution has been a\nbold frontier of electron microscopy in recent years. One of the main\nchallenges lies in the ability to retrieve extremely weak signals while\nsimultaneously disentangling amplitude and phase information. Here, we present\nFree-Electron Ramsey Imaging (FERI), a microscopy approach based on\nlight-induced electron modulation that enables coherent amplification of\noptical near-fields in electron imaging. We provide simultaneous time-, space-,\nand phase-resolved measurements of a micro-drum made from a hexagonal boron\nnitride membrane visualizing the sub-cycle dynamics of 2D polariton wavepackets\ntherein. The phase-resolved measurements reveals vortex-anti-vortex\nsingularities on the polariton wavefronts, together with an intriguing\nphenomenon of a traveling wave mimicking the amplitude profile of a standing\nwave. Our experiments show a 20-fold coherent amplification of the near-field\nsignal compared to conventional electron near-field imaging, resolving peak\nfield intensities in the order of ~W/cm2, corresponding to field amplitudes of\na few kV/m. As a result, our work paves the way for spatio-temporal electron\nmicroscopy of biological specimens and quantum materials, exciting yet delicate\nsamples that are currently difficult to investigate."
                },
                "authors": [
                    {
                        "name": "Tomer Bucher"
                    },
                    {
                        "name": "Harel Nahari"
                    },
                    {
                        "name": "Hanan Herzig Sheinfux"
                    },
                    {
                        "name": "Ron Ruimy"
                    },
                    {
                        "name": "Arthur Niedermayr"
                    },
                    {
                        "name": "Raphael Dahan"
                    },
                    {
                        "name": "Qinghui Yan"
                    },
                    {
                        "name": "Yuval Adiv"
                    },
                    {
                        "name": "Michael Yannai"
                    },
                    {
                        "name": "Jialin Chen"
                    },
                    {
                        "name": "Yaniv Kurman"
                    },
                    {
                        "name": "Sang Tae Park"
                    },
                    {
                        "name": "Daniel J. Masiel"
                    },
                    {
                        "name": "Eli Janzen"
                    },
                    {
                        "name": "James H. Edgar"
                    },
                    {
                        "name": "Fabrizio Carbone"
                    },
                    {
                        "name": "Guy Bartal"
                    },
                    {
                        "name": "Shai Tsesses"
                    },
                    {
                        "name": "Frank H. L. Koppens"
                    },
                    {
                        "name": "Giovanni Maria Vanacore"
                    },
                    {
                        "name": "Ido Kaminer"
                    }
                ],
                "author_detail": {
                    "name": "Ido Kaminer"
                },
                "author": "Ido Kaminer",
                "arxiv_doi": "10.1038/s41566-024-01451-w",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41566-024-01451-w",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.04877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.04877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11483v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11483v1",
                "updated": "2024-07-16T08:18:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    18,
                    41,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T08:18:41Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    8,
                    18,
                    41,
                    1,
                    198,
                    0
                ],
                "title": "Performance Analysis of Internet of Vehicles Mesh Networks Based on\n  Actual Switch Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of Internet of Vehicles Mesh Networks Based on\n  Actual Switch Models"
                },
                "summary": "The rapid growth of the automotive industry has exacerbated the conflict\nbetween the complex traffic environment, increasing communication demands, and\nlimited resources. Given the imperative to mitigate traffic and network\ncongestion, analyzing the performance of Internet of Vehicles (IoV) mesh\nnetworks is of great practical significance. Most studies focus solely on\nindividual performance metrics and influencing factors, and the adopted\nsimulation tools, such as OPNET, cannot achieve the dynamic link generation of\nIoV mesh networks. To address these problems, a network performance analysis\nmodel based on actual switches is proposed. First, a typical IoV mesh network\narchitecture is constructed and abstracted into a mathematical model that\ndescribes how the link and topology changes over time. Then, the task\ngeneration model and the task forwarding model based on actual switches are\nproposed to obtain the real traffic distribution of the network. Finally, a\nscientific network performance indicator system is constructed. Simulation\nresults demonstrate that, with rising task traffic and decreasing node caching\ncapacity, the packet loss rate increases, and the task arrival rate decreases\nin the network. The proposed model can effectively evaluate the network\nperformance across various traffic states and provide valuable insights for\nnetwork construction and enhancement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of the automotive industry has exacerbated the conflict\nbetween the complex traffic environment, increasing communication demands, and\nlimited resources. Given the imperative to mitigate traffic and network\ncongestion, analyzing the performance of Internet of Vehicles (IoV) mesh\nnetworks is of great practical significance. Most studies focus solely on\nindividual performance metrics and influencing factors, and the adopted\nsimulation tools, such as OPNET, cannot achieve the dynamic link generation of\nIoV mesh networks. To address these problems, a network performance analysis\nmodel based on actual switches is proposed. First, a typical IoV mesh network\narchitecture is constructed and abstracted into a mathematical model that\ndescribes how the link and topology changes over time. Then, the task\ngeneration model and the task forwarding model based on actual switches are\nproposed to obtain the real traffic distribution of the network. Finally, a\nscientific network performance indicator system is constructed. Simulation\nresults demonstrate that, with rising task traffic and decreasing node caching\ncapacity, the packet loss rate increases, and the task arrival rate decreases\nin the network. The proposed model can effectively evaluate the network\nperformance across various traffic states and provide valuable insights for\nnetwork construction and enhancement."
                },
                "authors": [
                    {
                        "name": "Jialin Hu"
                    },
                    {
                        "name": "Zhiyuan Ren"
                    },
                    {
                        "name": "Wenchi Cheng"
                    },
                    {
                        "name": "Zhiliang Shuai"
                    },
                    {
                        "name": "Zhao Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Li"
                },
                "author": "Zhao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11483v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11483v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11338v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11338v1",
                "updated": "2024-07-16T03:08:41Z",
                "updated_parsed": [
                    2024,
                    7,
                    16,
                    3,
                    8,
                    41,
                    1,
                    198,
                    0
                ],
                "published": "2024-07-16T03:08:41Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    3,
                    8,
                    41,
                    1,
                    198,
                    0
                ],
                "title": "Heterogeneous integration of high endurance ferroelectric and\n  piezoelectric epitaxial BaTiO$_3$ devices on Si",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous integration of high endurance ferroelectric and\n  piezoelectric epitaxial BaTiO$_3$ devices on Si"
                },
                "summary": "Integrating epitaxial BaTiO$_3$ (BTO) with Si is essential for leveraging its\nferroelectric, piezoelectric, and nonlinear optical properties in\nmicroelectronics. Recently, heterogeneous integration approaches that involve\ngrowth of BTO on ideal substrates followed by transfer to a desired substrate\nshow promise of achieving excellent device-quality films. However, beyond\nsimple demonstrations of the existence of ferroelectricity, robust devices with\nhigh endurance were not yet demonstrated on Si using the latter approach. Here,\nusing a novel two-step approach to synthesize epitaxial BTO using pulsed laser\ndeposition (PLD) on water soluble Sr3Al2O7 (SAO) (on SrTiO$_3$ (STO)\nsubstrates), we demonstrate successful integration of high-quality BTO\ncapacitors on Si, with Pr of 7 uC/cm2, Ec 150 kV/cm, ferroelectric and\nelectromechanical endurance of greater than $10^6$ cycles. We further address\nthe challenge of cracking and disintegration of thicker films by first\ntransferring a large area (5 mm x 5 mm) of the templated layer of BTO (~30 nm\nthick) on the desired substrate, followed by the growth of high-quality BTO on\nthis substrate, as revealed by HRXRD and HRSTEM measurements. These templated\nSi substrates offer a versatile platform for integrating any epitaxial complex\noxides with diverse functionalities onto any inorganic substrate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating epitaxial BaTiO$_3$ (BTO) with Si is essential for leveraging its\nferroelectric, piezoelectric, and nonlinear optical properties in\nmicroelectronics. Recently, heterogeneous integration approaches that involve\ngrowth of BTO on ideal substrates followed by transfer to a desired substrate\nshow promise of achieving excellent device-quality films. However, beyond\nsimple demonstrations of the existence of ferroelectricity, robust devices with\nhigh endurance were not yet demonstrated on Si using the latter approach. Here,\nusing a novel two-step approach to synthesize epitaxial BTO using pulsed laser\ndeposition (PLD) on water soluble Sr3Al2O7 (SAO) (on SrTiO$_3$ (STO)\nsubstrates), we demonstrate successful integration of high-quality BTO\ncapacitors on Si, with Pr of 7 uC/cm2, Ec 150 kV/cm, ferroelectric and\nelectromechanical endurance of greater than $10^6$ cycles. We further address\nthe challenge of cracking and disintegration of thicker films by first\ntransferring a large area (5 mm x 5 mm) of the templated layer of BTO (~30 nm\nthick) on the desired substrate, followed by the growth of high-quality BTO on\nthis substrate, as revealed by HRXRD and HRSTEM measurements. These templated\nSi substrates offer a versatile platform for integrating any epitaxial complex\noxides with diverse functionalities onto any inorganic substrate."
                },
                "authors": [
                    {
                        "name": "Asraful Haque"
                    },
                    {
                        "name": "Harshal Jason D'Souza"
                    },
                    {
                        "name": "Shubham Kumar Parate"
                    },
                    {
                        "name": "Rama Satya Sandilya"
                    },
                    {
                        "name": "Srinivasan Raghavan"
                    },
                    {
                        "name": "Pavan Nukala"
                    }
                ],
                "author_detail": {
                    "name": "Pavan Nukala"
                },
                "author": "Pavan Nukala",
                "arxiv_comment": "29 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11338v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11338v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02694v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02694v3",
                "updated": "2024-07-15T22:33:58Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    22,
                    33,
                    58,
                    0,
                    197,
                    0
                ],
                "published": "2024-03-05T06:23:50Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    6,
                    23,
                    50,
                    1,
                    65,
                    0
                ],
                "title": "MeanCache: User-Centric Semantic Cache for Large Language Model Based\n  Web Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MeanCache: User-Centric Semantic Cache for Large Language Model Based\n  Web Services"
                },
                "summary": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT and Llama have revolutionized\nnatural language processing and search engine dynamics. However, these models\nincur exceptionally high computational costs. For instance, GPT-3 consists of\n175 billion parameters, where inference demands billions of floating-point\noperations. Caching is a natural solution to reduce LLM inference costs on\nrepeated queries, which constitute about 31% of the total queries. However,\nexisting caching methods are incapable of finding semantic similarities among\nLLM queries nor do they operate on contextual queries, leading to unacceptable\nfalse hit-and-miss rates. This paper introduces MeanCache, a user-centric\nsemantic cache for LLM-based services that identifies semantically similar\nqueries to determine cache hit or miss. Using MeanCache, the response to a\nuser's semantically similar query can be retrieved from a local cache rather\nthan re-querying the LLM, thus reducing costs, service provider load, and\nenvironmental impact. MeanCache leverages Federated Learning (FL) to\ncollaboratively train a query similarity model without violating user privacy.\nBy placing a local cache in each user's device and using FL, MeanCache reduces\nthe latency and costs and enhances model performance, resulting in lower false\nhit rates. MeanCache also encodes context chains for every cached query,\noffering a simple yet highly effective mechanism to discern contextual query\nresponses from standalone. Our experiments benchmarked against the\nstate-of-the-art caching method, reveal that MeanCache attains an approximately\n17% higher F-score and a 20% increase in precision during semantic cache\nhit-and-miss decisions while performing even better on contextual queries. It\nalso reduces the storage requirement by 83% and accelerates semantic cache\nhit-and-miss decisions by 11%."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Mohamed Elidrisi"
                    },
                    {
                        "name": "Pallavi Kalapatapu"
                    },
                    {
                        "name": "Ammar Ahmed"
                    },
                    {
                        "name": "Ali Anwar"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Ali Gulzar"
                },
                "arxiv_affiliation": "Virginia Tech, USA",
                "author": "Muhammad Ali Gulzar",
                "arxiv_comment": "This study presents the first privacy aware semantic cache for LLMs\n  based on Federated Learning. MeanCache is the first cache that can handle\n  contextual queries efficiently. Total pages 14",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.02694v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02694v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.05740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.05740v2",
                "updated": "2024-07-15T18:38:54Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    18,
                    38,
                    54,
                    0,
                    197,
                    0
                ],
                "published": "2023-07-11T19:08:06Z",
                "published_parsed": [
                    2023,
                    7,
                    11,
                    19,
                    8,
                    6,
                    1,
                    192,
                    0
                ],
                "title": "Minimum Cost Loop Nests for Contraction of a Sparse Tensor with a Tensor\n  Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimum Cost Loop Nests for Contraction of a Sparse Tensor with a Tensor\n  Network"
                },
                "summary": "Sparse tensor decomposition and completion are common in numerous\napplications, ranging from machine learning to computational quantum chemistry.\nTypically, the main bottleneck in optimization of these models are contractions\nof a single large sparse tensor with a network of several dense matrices or\ntensors (SpTTN). Prior works on high-performance tensor decomposition and\ncompletion have focused on performance and scalability optimizations for\nspecific SpTTN kernels. We present algorithms and a runtime system for\nidentifying and executing the most efficient loop nest for any SpTTN kernel. We\nconsider both enumeration of such loop nests for autotuning and efficient\nalgorithms for finding the lowest cost loop-nest for simpler metrics, such as\nbuffer size or cache miss models. Our runtime system identifies the best choice\nof loop nest without user guidance, and also provides a distributed-memory\nparallelization of SpTTN kernels. We evaluate our framework using both\nreal-world and synthetic tensors. Our results demonstrate that our approach\noutperforms available generalized state-of-the-art libraries and matches the\nperformance of specialized codes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse tensor decomposition and completion are common in numerous\napplications, ranging from machine learning to computational quantum chemistry.\nTypically, the main bottleneck in optimization of these models are contractions\nof a single large sparse tensor with a network of several dense matrices or\ntensors (SpTTN). Prior works on high-performance tensor decomposition and\ncompletion have focused on performance and scalability optimizations for\nspecific SpTTN kernels. We present algorithms and a runtime system for\nidentifying and executing the most efficient loop nest for any SpTTN kernel. We\nconsider both enumeration of such loop nests for autotuning and efficient\nalgorithms for finding the lowest cost loop-nest for simpler metrics, such as\nbuffer size or cache miss models. Our runtime system identifies the best choice\nof loop nest without user guidance, and also provides a distributed-memory\nparallelization of SpTTN kernels. We evaluate our framework using both\nreal-world and synthetic tensors. Our results demonstrate that our approach\noutperforms available generalized state-of-the-art libraries and matches the\nperformance of specialized codes."
                },
                "authors": [
                    {
                        "name": "Raghavendra Kanakagiri"
                    },
                    {
                        "name": "Edgar Solomonik"
                    }
                ],
                "author_detail": {
                    "name": "Edgar Solomonik"
                },
                "author": "Edgar Solomonik",
                "arxiv_doi": "10.1145/3626183.3659985",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3626183.3659985",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.05740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.05740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.3; D.1.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10926v1",
                "updated": "2024-07-15T17:25:42Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "published": "2024-07-15T17:25:42Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    17,
                    25,
                    42,
                    0,
                    197,
                    0
                ],
                "title": "In-Loop Filtering via Trained Look-Up Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering via Trained Look-Up Tables"
                },
                "summary": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology for removing the artifacts in\nimage/video coding standards. Recently, neural network-based in-loop filtering\nmethods achieve remarkable coding gains beyond the capability of advanced video\ncoding standards, which becomes a powerful coding tool candidate for future\nvideo coding standards. However, the utilization of deep neural networks brings\nheavy time and computational complexity, and high demands of high-performance\nhardware, which is challenging to apply to the general uses of coding scene. To\naddress this limitation, inspired by explorations in image restoration, we\npropose an efficient and practical in-loop filtering scheme by adopting the\nLook-up Table (LUT). We train the DNN of in-loop filtering within a fixed\nfiltering reference range, and cache the output values of the DNN into a LUT\nvia traversing all possible inputs. At testing time in the coding process, the\nfiltered pixel is generated by locating input pixels (to-be-filtered pixel with\nreference pixels) and interpolating cached filtered pixel values. To further\nenable the large filtering reference range with the limited storage cost of\nLUT, we introduce the enhanced indexing mechanism in the filtering process, and\nclipping/finetuning mechanism in the training. The proposed method is\nimplemented into the Versatile Video Coding (VVC) reference software, VTM-11.0.\nExperimental results show that the ultrafast, very fast, and fast mode of the\nproposed method achieves on average 0.13%/0.34%/0.51%, and 0.10%/0.27%/0.39%\nBD-rate reduction, under the all intra (AI) and random access (RA)\nconfigurations. Especially, our method has friendly time and computational\ncomplexity, only 101%/102%-104%/108% time increase with 0.13-0.93 kMACs/pixel,\nand only 164-1148 KB storage cost for a single model. Our solution may shed\nlight on the journey of practical neural network-based coding tool evolution."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10740v1",
                "updated": "2024-07-15T14:09:00Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "published": "2024-07-15T14:09:00Z",
                "published_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    9,
                    0,
                    0,
                    197,
                    0
                ],
                "title": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TME-Box: Scalable In-Process Isolation through Intel TME-MK Memory\n  Encryption"
                },
                "summary": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient cloud computing relies on in-process isolation to optimize\nperformance by running workloads within a single process. Without heavy-weight\nprocess isolation, memory safety errors pose a significant security threat by\nallowing an adversary to extract or corrupt the private data of other\nco-located tenants. Existing in-process isolation mechanisms are not suitable\nfor modern cloud requirements, e.g., MPK's 16 protection domains are\ninsufficient to isolate thousands of cloud workers per process. Consequently,\ncloud service providers have a strong need for lightweight in-process isolation\non commodity x86 machines.\n  This paper presents TME-Box, a novel isolation technique that enables\nfine-grained and scalable sandboxing on commodity x86 CPUs. By repurposing\nIntel TME-MK, which is intended for the encryption of virtual machines, TME-Box\noffers lightweight and efficient in-process isolation. TME-Box enforces that\nsandboxes use their designated encryption keys for memory interactions through\ncompiler instrumentation. This cryptographic isolation enables fine-grained\naccess control, from single cache lines to full pages, and supports flexible\ndata relocation. In addition, the design of TME-Box allows the efficient\nisolation of up to 32K concurrent sandboxes. We present a performance-optimized\nTME-Box prototype, utilizing x86 segment-based addressing, that showcases\ngeomean performance overheads of 5.2 % for data isolation and 9.7 % for code\nand data isolation, evaluated with the SPEC CPU2017 benchmark suite."
                },
                "authors": [
                    {
                        "name": "Martin Unterguggenberger"
                    },
                    {
                        "name": "Lukas Lamster"
                    },
                    {
                        "name": "David Schrammel"
                    },
                    {
                        "name": "Martin Schwarzl"
                    },
                    {
                        "name": "Stefan Mangard"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Mangard"
                },
                "author": "Stefan Mangard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02350v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02350v3",
                "updated": "2024-07-15T14:00:24Z",
                "updated_parsed": [
                    2024,
                    7,
                    15,
                    14,
                    0,
                    24,
                    0,
                    197,
                    0
                ],
                "published": "2024-07-02T15:16:06Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    15,
                    16,
                    6,
                    1,
                    184,
                    0
                ],
                "title": "Conceptual Codebook Learning for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conceptual Codebook Learning for Vision-Language Models"
                },
                "summary": "In this paper, we propose Conceptual Codebook Learning (CoCoLe), a novel\nfine-tuning method for vision-language models (VLMs) to address the challenge\nof improving the generalization capability of VLMs while fine-tuning them on\ndownstream tasks in a few-shot setting. We recognize that visual concepts, such\nas textures, shapes, and colors are naturally transferable across domains and\nplay a crucial role in generalization tasks. Motivated by this interesting\nfinding, we learn a conceptual codebook consisting of visual concepts as keys\nand conceptual prompts as values, which serves as a link between the image\nencoder's outputs and the text encoder's inputs. Specifically, for a given\nimage, we leverage the codebook to identify the most relevant conceptual\nprompts associated with the class embeddings to perform the classification.\nAdditionally, we incorporate a handcrafted concept cache as a regularization to\nalleviate the overfitting issues in low-shot scenarios. We observe that this\nconceptual codebook learning method is able to achieve enhanced alignment\nbetween visual and linguistic modalities. Extensive experimental results\ndemonstrate that our CoCoLe method remarkably outperforms the existing\nstate-of-the-art methods across various evaluation settings, including\nbase-to-new generalization, cross-dataset evaluation, and domain generalization\ntasks. Detailed ablation studies further confirm the efficacy of each component\nin CoCoLe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we propose Conceptual Codebook Learning (CoCoLe), a novel\nfine-tuning method for vision-language models (VLMs) to address the challenge\nof improving the generalization capability of VLMs while fine-tuning them on\ndownstream tasks in a few-shot setting. We recognize that visual concepts, such\nas textures, shapes, and colors are naturally transferable across domains and\nplay a crucial role in generalization tasks. Motivated by this interesting\nfinding, we learn a conceptual codebook consisting of visual concepts as keys\nand conceptual prompts as values, which serves as a link between the image\nencoder's outputs and the text encoder's inputs. Specifically, for a given\nimage, we leverage the codebook to identify the most relevant conceptual\nprompts associated with the class embeddings to perform the classification.\nAdditionally, we incorporate a handcrafted concept cache as a regularization to\nalleviate the overfitting issues in low-shot scenarios. We observe that this\nconceptual codebook learning method is able to achieve enhanced alignment\nbetween visual and linguistic modalities. Extensive experimental results\ndemonstrate that our CoCoLe method remarkably outperforms the existing\nstate-of-the-art methods across various evaluation settings, including\nbase-to-new generalization, cross-dataset evaluation, and domain generalization\ntasks. Detailed ablation studies further confirm the efficacy of each component\nin CoCoLe."
                },
                "authors": [
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Ke Yu"
                    },
                    {
                        "name": "Siqi Wu"
                    },
                    {
                        "name": "Zhihai He"
                    }
                ],
                "author_detail": {
                    "name": "Zhihai He"
                },
                "author": "Zhihai He",
                "arxiv_comment": "Accepted by ECCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02350v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02350v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.10354v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.10354v1",
                "updated": "2024-07-14T23:15:01Z",
                "updated_parsed": [
                    2024,
                    7,
                    14,
                    23,
                    15,
                    1,
                    6,
                    196,
                    0
                ],
                "published": "2024-07-14T23:15:01Z",
                "published_parsed": [
                    2024,
                    7,
                    14,
                    23,
                    15,
                    1,
                    6,
                    196,
                    0
                ],
                "title": "High Voltage (~2 kV) field-plated Al0.64Ga0.36N-channel HEMTs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage (~2 kV) field-plated Al0.64Ga0.36N-channel HEMTs"
                },
                "summary": "High voltage (~2 kV) AlGaN-channel HEMTs were fabricated with 64% Aluminum\ncomposition in the channel. The average on-resistance was ~75 ohm. mm (~21\nmiliohm. cm^2) for LGD = 20 microns. Breakdown voltage reached >3 kV (tool\nlimit) before passivation however it reduced to ~2 kV after SiN surface\npassivation and field plates. The apparent high breakdown voltage prior to\npassivation can possibly be attributed to the field plate effect of the charged\ntrap states of the surface. The breakdown voltage and RON demonstrated a strong\nlinear correlation in a scattered plot with ~50 measured transistors. In pulsed\nIV measurements with 100 microsecond pulse width and 40 V of off-state bias\n(tool limit), the dynamic RON increased by ~5% compared to DC RON and current\ncollapse was <10%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High voltage (~2 kV) AlGaN-channel HEMTs were fabricated with 64% Aluminum\ncomposition in the channel. The average on-resistance was ~75 ohm. mm (~21\nmiliohm. cm^2) for LGD = 20 microns. Breakdown voltage reached >3 kV (tool\nlimit) before passivation however it reduced to ~2 kV after SiN surface\npassivation and field plates. The apparent high breakdown voltage prior to\npassivation can possibly be attributed to the field plate effect of the charged\ntrap states of the surface. The breakdown voltage and RON demonstrated a strong\nlinear correlation in a scattered plot with ~50 measured transistors. In pulsed\nIV measurements with 100 microsecond pulse width and 40 V of off-state bias\n(tool limit), the dynamic RON increased by ~5% compared to DC RON and current\ncollapse was <10%."
                },
                "authors": [
                    {
                        "name": "Md Tahmidul Alam"
                    },
                    {
                        "name": "Jiahao Chen"
                    },
                    {
                        "name": "Kenneth Stephenson"
                    },
                    {
                        "name": "Md Abdullah-Al Mamun"
                    },
                    {
                        "name": "Abdullah Al Mamun Mazumder"
                    },
                    {
                        "name": "Shubhra S. Pasayat"
                    },
                    {
                        "name": "Asif Khan"
                    },
                    {
                        "name": "Chirag Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Gupta"
                },
                "author": "Chirag Gupta",
                "arxiv_comment": "4 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.10354v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.10354v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04876v2",
                "updated": "2024-07-14T16:12:48Z",
                "updated_parsed": [
                    2024,
                    7,
                    14,
                    16,
                    12,
                    48,
                    6,
                    196,
                    0
                ],
                "published": "2024-07-05T22:07:36Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    22,
                    7,
                    36,
                    4,
                    187,
                    0
                ],
                "title": "Swimming Cylinder Wake Control with Plasma Actuator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Swimming Cylinder Wake Control with Plasma Actuator"
                },
                "summary": "In this research, the effect of utilizing a micro plasma actuator on\ncontrolling the flow through a two-dimensional cylinder is investigated using\nan advanced electrostatic model. Within this model, by solving two elliptic\nequations, the potential distribution and plasma distribution in the solution\ndomain are determined, leading to the generation of a volumetric force. This\nforce is added to the momentum equations as a source term. The Reynolds number\nof the flow is set at 20,000, and the plasma actuator operates in a steady\nmanner. Due to the high Reynolds number, the flow is turbulent, and its\ntime-dependent nature is modeled as unsteady. Plasma actuators are\nsymmetrically mounted at 45 and 90 degrees with respect to the free stream,\nboth above and below the cylinder surfaces. The influence of the actuator\nplacement angle on the flow quality downstream of the cylinder is examined. The\nresults indicate an enhancement of flow quality by 8% to 15% downstream of the\ncylinder. Moreover, improvements of 40% to 55% in the variation of the lift\ncoefficient and 75% to 90% in the drag coefficient are reported. The findings\nreveal superior performance of the actuator positioned at 90 degrees compared\nto the 45-degree orientation. This can be attributed to the proximity of the\n90-degree actuator position to the point of boundary layer separation\ninitiation. Furthermore, using the actuator positioned at 90 degrees and\napplying three different voltages of 10, 13, and 16 kV, the impact of flow\ncontrol on the first cylinder in a tandem arrangement on the downstream flow of\nthe second cylinder is examined. The results demonstrate an enhancement in\ndownstream flow quality of 20% to 26%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this research, the effect of utilizing a micro plasma actuator on\ncontrolling the flow through a two-dimensional cylinder is investigated using\nan advanced electrostatic model. Within this model, by solving two elliptic\nequations, the potential distribution and plasma distribution in the solution\ndomain are determined, leading to the generation of a volumetric force. This\nforce is added to the momentum equations as a source term. The Reynolds number\nof the flow is set at 20,000, and the plasma actuator operates in a steady\nmanner. Due to the high Reynolds number, the flow is turbulent, and its\ntime-dependent nature is modeled as unsteady. Plasma actuators are\nsymmetrically mounted at 45 and 90 degrees with respect to the free stream,\nboth above and below the cylinder surfaces. The influence of the actuator\nplacement angle on the flow quality downstream of the cylinder is examined. The\nresults indicate an enhancement of flow quality by 8% to 15% downstream of the\ncylinder. Moreover, improvements of 40% to 55% in the variation of the lift\ncoefficient and 75% to 90% in the drag coefficient are reported. The findings\nreveal superior performance of the actuator positioned at 90 degrees compared\nto the 45-degree orientation. This can be attributed to the proximity of the\n90-degree actuator position to the point of boundary layer separation\ninitiation. Furthermore, using the actuator positioned at 90 degrees and\napplying three different voltages of 10, 13, and 16 kV, the impact of flow\ncontrol on the first cylinder in a tandem arrangement on the downstream flow of\nthe second cylinder is examined. The results demonstrate an enhancement in\ndownstream flow quality of 20% to 26%."
                },
                "authors": [
                    {
                        "name": "Javad Omidi"
                    }
                ],
                "author_detail": {
                    "name": "Javad Omidi"
                },
                "author": "Javad Omidi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12866v1",
                "updated": "2024-07-13T07:23:07Z",
                "updated_parsed": [
                    2024,
                    7,
                    13,
                    7,
                    23,
                    7,
                    5,
                    195,
                    0
                ],
                "published": "2024-07-13T07:23:07Z",
                "published_parsed": [
                    2024,
                    7,
                    13,
                    7,
                    23,
                    7,
                    5,
                    195,
                    0
                ],
                "title": "Beyond KV Caching: Shared Attention for Efficient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond KV Caching: Shared Attention for Efficient LLMs"
                },
                "summary": "The efficiency of large language models (LLMs) remains a critical challenge,\nparticularly in contexts where computational resources are limited. Traditional\nattention mechanisms in these models, while powerful, require significant\ncomputational and memory resources due to the necessity of recalculating and\nstoring attention weights across different layers. This paper introduces a\nnovel Shared Attention (SA) mechanism, designed to enhance the efficiency of\nLLMs by directly sharing computed attention weights across multiple layers.\nUnlike previous methods that focus on sharing intermediate Key-Value (KV)\ncaches, our approach utilizes the isotropic tendencies of attention\ndistributions observed in advanced LLMs post-pretraining to reduce both the\ncomputational flops and the size of the KV cache required during inference. We\nempirically demonstrate that implementing SA across various LLMs results in\nminimal accuracy loss on standard benchmarks. Our findings suggest that SA not\nonly conserves computational resources but also maintains robust model\nperformance, thereby facilitating the deployment of more efficient LLMs in\nresource-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large language models (LLMs) remains a critical challenge,\nparticularly in contexts where computational resources are limited. Traditional\nattention mechanisms in these models, while powerful, require significant\ncomputational and memory resources due to the necessity of recalculating and\nstoring attention weights across different layers. This paper introduces a\nnovel Shared Attention (SA) mechanism, designed to enhance the efficiency of\nLLMs by directly sharing computed attention weights across multiple layers.\nUnlike previous methods that focus on sharing intermediate Key-Value (KV)\ncaches, our approach utilizes the isotropic tendencies of attention\ndistributions observed in advanced LLMs post-pretraining to reduce both the\ncomputational flops and the size of the KV cache required during inference. We\nempirically demonstrate that implementing SA across various LLMs results in\nminimal accuracy loss on standard benchmarks. Our findings suggest that SA not\nonly conserves computational resources but also maintains robust model\nperformance, thereby facilitating the deployment of more efficient LLMs in\nresource-constrained environments."
                },
                "authors": [
                    {
                        "name": "Bingli Liao"
                    },
                    {
                        "name": "Danilo Vasconcellos Vargas"
                    }
                ],
                "author_detail": {
                    "name": "Danilo Vasconcellos Vargas"
                },
                "author": "Danilo Vasconcellos Vargas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2311.09755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09755v2",
                "updated": "2024-08-12T17:57:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    57,
                    0,
                    0,
                    225,
                    0
                ],
                "published": "2023-11-16T10:30:00Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    10,
                    30,
                    0,
                    3,
                    320,
                    0
                ],
                "title": "On the Impact of Calibration Data in Post-training Quantization and\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of Calibration Data in Post-training Quantization and\n  Pruning"
                },
                "summary": "Quantization and pruning form the foundation of compression for neural\nnetworks, enabling efficient inference for large language models (LLMs).\nRecently, various quantization and pruning techniques have demonstrated\nremarkable performance in a post-training setting. They rely upon calibration\ndata, a small set of unlabeled examples that are used to generate layer\nactivations. However, no prior work has systematically investigated how the\ncalibration data impacts the effectiveness of model compression methods. In\nthis paper, we present the first extensive empirical study on the effect of\ncalibration data upon LLM performance. We trial a variety of quantization and\npruning methods, datasets, tasks, and models. Surprisingly, we find substantial\nvariations in downstream task performance, contrasting existing work that\nsuggests a greater level of robustness to the calibration data. Finally, we\nmake a series of recommendations for the effective use of calibration data in\nLLM quantization and pruning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization and pruning form the foundation of compression for neural\nnetworks, enabling efficient inference for large language models (LLMs).\nRecently, various quantization and pruning techniques have demonstrated\nremarkable performance in a post-training setting. They rely upon calibration\ndata, a small set of unlabeled examples that are used to generate layer\nactivations. However, no prior work has systematically investigated how the\ncalibration data impacts the effectiveness of model compression methods. In\nthis paper, we present the first extensive empirical study on the effect of\ncalibration data upon LLM performance. We trial a variety of quantization and\npruning methods, datasets, tasks, and models. Surprisingly, we find substantial\nvariations in downstream task performance, contrasting existing work that\nsuggests a greater level of robustness to the calibration data. Finally, we\nmake a series of recommendations for the effective use of calibration data in\nLLM quantization and pruning."
                },
                "authors": [
                    {
                        "name": "Miles Williams"
                    },
                    {
                        "name": "Nikolaos Aletras"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Aletras"
                },
                "author": "Nikolaos Aletras",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00798v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00798v4",
                "updated": "2024-08-12T17:54:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    54,
                    32,
                    0,
                    225,
                    0
                ],
                "published": "2024-02-01T17:30:50Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    17,
                    30,
                    50,
                    3,
                    32,
                    0
                ],
                "title": "Formal-LLM: Integrating Formal Language and Natural Language for\n  Controllable LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal-LLM: Integrating Formal Language and Natural Language for\n  Controllable LLM-based Agents"
                },
                "summary": "Recent advancements on Large Language Models (LLMs) enable AI Agents to\nautomatically generate and execute multi-step plans to solve complex tasks.\nHowever, since LLM's content generation process is hardly controllable, current\nLLM-based agents frequently generate invalid or non-executable plans, which\njeopardizes the performance of the generated plans and corrupts users' trust in\nLLM-based agents. In response, this paper proposes a novel \"Formal-LLM\"\nframework for LLM-based agents by integrating the expressiveness of natural\nlanguage and the precision of formal language. Specifically, the framework\nallows agent developers to express their requirements or constraints for the\nplanning process as an automaton. A stack-based LLM plan generation process is\nthen conducted under the supervision of the automaton to ensure that the\ngenerated plan satisfies the constraints, making the planning process\ncontrollable. We conduct experiments on both benchmark tasks and practical\nreal-life tasks, and our framework achieves over 50% overall performance\nincrease, which validates the feasibility and effectiveness of employing\nFormal-LLM to guide the plan generation of agents, preventing the agents from\ngenerating invalid and unsuccessful plans. Further, more controllable LLM-based\nagents can facilitate the broader utilization of LLM in application scenarios\nwhere high validity of planning is essential. The source code of this work is\navailable at https://github.com/agiresearch/Formal-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements on Large Language Models (LLMs) enable AI Agents to\nautomatically generate and execute multi-step plans to solve complex tasks.\nHowever, since LLM's content generation process is hardly controllable, current\nLLM-based agents frequently generate invalid or non-executable plans, which\njeopardizes the performance of the generated plans and corrupts users' trust in\nLLM-based agents. In response, this paper proposes a novel \"Formal-LLM\"\nframework for LLM-based agents by integrating the expressiveness of natural\nlanguage and the precision of formal language. Specifically, the framework\nallows agent developers to express their requirements or constraints for the\nplanning process as an automaton. A stack-based LLM plan generation process is\nthen conducted under the supervision of the automaton to ensure that the\ngenerated plan satisfies the constraints, making the planning process\ncontrollable. We conduct experiments on both benchmark tasks and practical\nreal-life tasks, and our framework achieves over 50% overall performance\nincrease, which validates the feasibility and effectiveness of employing\nFormal-LLM to guide the plan generation of agents, preventing the agents from\ngenerating invalid and unsuccessful plans. Further, more controllable LLM-based\nagents can facilitate the broader utilization of LLM in application scenarios\nwhere high validity of planning is essential. The source code of this work is\navailable at https://github.com/agiresearch/Formal-LLM."
                },
                "authors": [
                    {
                        "name": "Zelong Li"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00798v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00798v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.17012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.17012v2",
                "updated": "2024-08-12T17:53:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    53,
                    13,
                    0,
                    225,
                    0
                ],
                "published": "2023-09-29T06:53:10Z",
                "published_parsed": [
                    2023,
                    9,
                    29,
                    6,
                    53,
                    10,
                    4,
                    272,
                    0
                ],
                "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Cognitive Biases in Large Language Models as Evaluators"
                },
                "summary": "Large Language Models (LLMs) have recently been shown to be effective as\nautomatic evaluators with simple prompting and in-context learning. In this\nwork, we assemble 15 LLMs of four different size ranges and evaluate their\noutput responses by preference ranking from the other LLMs as evaluators, such\nas System Star is better than System Square. We then evaluate the quality of\nranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators\n(CoBBLEr), a benchmark to measure six different cognitive biases in LLM\nevaluation outputs, such as the Egocentric bias where a model prefers to rank\nits own outputs highly in evaluation. We find that LLMs are biased text quality\nevaluators, exhibiting strong indications on our bias benchmark (average of 40%\nof comparisons across all models) within each of their evaluations that\nquestion their robustness as evaluators. Furthermore, we examine the\ncorrelation between human and machine preferences and calculate the average\nRank-Biased Overlap (RBO) score to be 49.6%, indicating that machine\npreferences are misaligned with humans. According to our findings, LLMs may\nstill be unable to be utilized for automatic annotation aligned with human\npreferences. Our project page is at: https://minnesotanlp.github.io/cobbler.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently been shown to be effective as\nautomatic evaluators with simple prompting and in-context learning. In this\nwork, we assemble 15 LLMs of four different size ranges and evaluate their\noutput responses by preference ranking from the other LLMs as evaluators, such\nas System Star is better than System Square. We then evaluate the quality of\nranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators\n(CoBBLEr), a benchmark to measure six different cognitive biases in LLM\nevaluation outputs, such as the Egocentric bias where a model prefers to rank\nits own outputs highly in evaluation. We find that LLMs are biased text quality\nevaluators, exhibiting strong indications on our bias benchmark (average of 40%\nof comparisons across all models) within each of their evaluations that\nquestion their robustness as evaluators. Furthermore, we examine the\ncorrelation between human and machine preferences and calculate the average\nRank-Biased Overlap (RBO) score to be 49.6%, indicating that machine\npreferences are misaligned with humans. According to our findings, LLMs may\nstill be unable to be utilized for automatic annotation aligned with human\npreferences. Our project page is at: https://minnesotanlp.github.io/cobbler."
                },
                "authors": [
                    {
                        "name": "Ryan Koo"
                    },
                    {
                        "name": "Minhwa Lee"
                    },
                    {
                        "name": "Vipul Raheja"
                    },
                    {
                        "name": "Jong Inn Park"
                    },
                    {
                        "name": "Zae Myung Kim"
                    },
                    {
                        "name": "Dongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyeop Kang"
                },
                "author": "Dongyeop Kang",
                "arxiv_comment": "Publishsed at 2024. 29 pages, 9 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.17012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.17012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06337v1",
                "updated": "2024-08-12T17:52:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    52,
                    44,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T17:52:44Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    52,
                    44,
                    0,
                    225,
                    0
                ],
                "title": "Analytical Weak Lensing Shear Inference for Precision Cosmology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analytical Weak Lensing Shear Inference for Precision Cosmology"
                },
                "summary": "Noise bias is a significant source of systematic error in weak gravitational\nlensing measurements that must be corrected to satisfy the stringent standards\nof modern imaging surveys in the era of precision cosmology. This paper reviews\nthe analytical noise bias correction method and provides analytical derivations\ndemonstrating that we can recover shear to its second order using the\n'renoising' noise bias correction approach introduced by Metacalibration. We\nimplement this analytical noise bias correction within the AnaCal shear\nestimation framework and propose several enhancements to the noise bias\ncorrection algorithm. We evaluate the improved AnaCal using simulations\ndesigned to replicate Rubin LSST imaging data. These simulations feature\nsemi-realistic galaxies and stars, complete with representative distributions\nof magnitudes and Galactic spatial density. We conduct tests under various\nobservational challenges, including cosmic rays, defective CCD columns, bright\nstar saturation, bleed trails, and spatially variable point spread functions.\nOur results indicate a multiplicative bias in weak lensing shear recovery of\nless than a few tenths of a percent, meeting LSST DESC requirements without\nrequiring calibration from external image simulations. Additionally, our\nalgorithm achieves rapid processing, handling one galaxy in less than a\nmillisecond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noise bias is a significant source of systematic error in weak gravitational\nlensing measurements that must be corrected to satisfy the stringent standards\nof modern imaging surveys in the era of precision cosmology. This paper reviews\nthe analytical noise bias correction method and provides analytical derivations\ndemonstrating that we can recover shear to its second order using the\n'renoising' noise bias correction approach introduced by Metacalibration. We\nimplement this analytical noise bias correction within the AnaCal shear\nestimation framework and propose several enhancements to the noise bias\ncorrection algorithm. We evaluate the improved AnaCal using simulations\ndesigned to replicate Rubin LSST imaging data. These simulations feature\nsemi-realistic galaxies and stars, complete with representative distributions\nof magnitudes and Galactic spatial density. We conduct tests under various\nobservational challenges, including cosmic rays, defective CCD columns, bright\nstar saturation, bleed trails, and spatially variable point spread functions.\nOur results indicate a multiplicative bias in weak lensing shear recovery of\nless than a few tenths of a percent, meeting LSST DESC requirements without\nrequiring calibration from external image simulations. Additionally, our\nalgorithm achieves rapid processing, handling one galaxy in less than a\nmillisecond."
                },
                "authors": [
                    {
                        "name": "Xiangchong Li"
                    },
                    {
                        "name": "Rachel Mandelbaum"
                    },
                    {
                        "name": "The LSST Dark Energy Science Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "The LSST Dark Energy Science Collaboration"
                },
                "author": "The LSST Dark Energy Science Collaboration",
                "arxiv_comment": "15 pages, 11 figures, submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06333v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06333v1",
                "updated": "2024-08-12T17:50:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    50,
                    2,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T17:50:02Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    50,
                    2,
                    0,
                    225,
                    0
                ],
                "title": "FastFiD: Improve Inference Efficiency of Open Domain Question Answering\n  via Sentence Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastFiD: Improve Inference Efficiency of Open Domain Question Answering\n  via Sentence Selection"
                },
                "summary": "Open Domain Question Answering (ODQA) has been advancing rapidly in recent\ntimes, driven by significant developments in dense passage retrieval and\npretrained language models. Current models typically incorporate the FiD\nframework, which is composed by a neural retriever alongside an encoder-decoder\nneural reader. In the answer generation process, the retriever will retrieve\nnumerous passages (around 100 for instance), each of which is then individually\nencoded by the encoder. Subsequently, the decoder makes predictions based on\nthese encoded passages. Nevertheless, this framework can be relatively\ntime-consuming, particularly due to the extensive length of the gathered\npassages. To address this, we introduce FastFiD in this paper, a novel approach\nthat executes sentence selection on the encoded passages. This aids in\nretaining valuable sentences while reducing the context length required for\ngenerating answers. Experiments on three commonly used datasets (Natural\nQuestions, TriviaQA and ASQA) demonstrate that our method can enhance the\ninference speed by 2.3X-5.7X, while simultaneously maintaining the model's\nperformance. Moreover, an in-depth analysis of the model's attention reveals\nthat the selected sentences indeed hold a substantial contribution towards the\nfinal answer. The codes are publicly available at\nhttps://github.com/thunlp/FastFiD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open Domain Question Answering (ODQA) has been advancing rapidly in recent\ntimes, driven by significant developments in dense passage retrieval and\npretrained language models. Current models typically incorporate the FiD\nframework, which is composed by a neural retriever alongside an encoder-decoder\nneural reader. In the answer generation process, the retriever will retrieve\nnumerous passages (around 100 for instance), each of which is then individually\nencoded by the encoder. Subsequently, the decoder makes predictions based on\nthese encoded passages. Nevertheless, this framework can be relatively\ntime-consuming, particularly due to the extensive length of the gathered\npassages. To address this, we introduce FastFiD in this paper, a novel approach\nthat executes sentence selection on the encoded passages. This aids in\nretaining valuable sentences while reducing the context length required for\ngenerating answers. Experiments on three commonly used datasets (Natural\nQuestions, TriviaQA and ASQA) demonstrate that our method can enhance the\ninference speed by 2.3X-5.7X, while simultaneously maintaining the model's\nperformance. Moreover, an in-depth analysis of the model's attention reveals\nthat the selected sentences indeed hold a substantial contribution towards the\nfinal answer. The codes are publicly available at\nhttps://github.com/thunlp/FastFiD."
                },
                "authors": [
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "ACL 2024 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06333v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06332v1",
                "updated": "2024-08-12T17:48:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    48,
                    55,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T17:48:55Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    48,
                    55,
                    0,
                    225,
                    0
                ],
                "title": "Animate, or Inanimate, That is the Question for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Animate, or Inanimate, That is the Question for Large Language Models"
                },
                "summary": "The cognitive essence of humans is deeply intertwined with the concept of\nanimacy, which plays an essential role in shaping their memory, vision, and\nmulti-layered language understanding. Although animacy appears in language via\nnuanced constraints on verbs and adjectives, it is also learned and refined\nthrough extralinguistic information. Similarly, we assume that the LLMs'\nlimited abilities to understand natural language when processing animacy are\nmotivated by the fact that these models are trained exclusively on text.\n  Hence, the question this paper aims to answer arises: can LLMs, in their\ndigital wisdom, process animacy in a similar way to what humans would do? We\nthen propose a systematic analysis via prompting approaches. In particular, we\nprobe different LLMs by prompting them using animate, inanimate, usual, and\nstranger contexts. Results reveal that, although LLMs have been trained\npredominantly on textual data, they exhibit human-like behavior when faced with\ntypical animate and inanimate entities in alignment with earlier studies.\nHence, LLMs can adapt to understand unconventional situations by recognizing\noddities as animated without needing to interface with unspoken cognitive\ntriggers humans rely on to break down animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cognitive essence of humans is deeply intertwined with the concept of\nanimacy, which plays an essential role in shaping their memory, vision, and\nmulti-layered language understanding. Although animacy appears in language via\nnuanced constraints on verbs and adjectives, it is also learned and refined\nthrough extralinguistic information. Similarly, we assume that the LLMs'\nlimited abilities to understand natural language when processing animacy are\nmotivated by the fact that these models are trained exclusively on text.\n  Hence, the question this paper aims to answer arises: can LLMs, in their\ndigital wisdom, process animacy in a similar way to what humans would do? We\nthen propose a systematic analysis via prompting approaches. In particular, we\nprobe different LLMs by prompting them using animate, inanimate, usual, and\nstranger contexts. Results reveal that, although LLMs have been trained\npredominantly on textual data, they exhibit human-like behavior when faced with\ntypical animate and inanimate entities in alignment with earlier studies.\nHence, LLMs can adapt to understand unconventional situations by recognizing\noddities as animated without needing to interface with unspoken cognitive\ntriggers humans rely on to break down animations."
                },
                "authors": [
                    {
                        "name": "Leonardo Ranaldi"
                    },
                    {
                        "name": "Giulia Pucci"
                    },
                    {
                        "name": "Fabio Massimo Zanzotto"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Massimo Zanzotto"
                },
                "author": "Fabio Massimo Zanzotto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06323v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06323v1",
                "updated": "2024-08-12T17:43:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    43,
                    10,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T17:43:10Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    43,
                    10,
                    0,
                    225,
                    0
                ],
                "title": "Infer-and-widen versus split-and-condition: two tales of selective\n  inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Infer-and-widen versus split-and-condition: two tales of selective\n  inference"
                },
                "summary": "Recent attention has focused on the development of methods for post-selection\ninference. However, the connections between these methods, and the extent to\nwhich one might be preferred to another, remain unclear. In this paper, we\nclassify existing methods for post-selection inference into one of two\nframeworks: infer-and-widen or split-and-condition. The infer-and-widen\nframework produces confidence intervals whose midpoints are biased due to\nselection, and must be wide enough to account for this bias. By contrast,\nsplit-and-condition directly adjusts the intervals' midpoints to account for\nselection. We compare the two frameworks in three vignettes: the winner's\ncurse, maximal contrasts, and inference after the lasso. Our results are\nstriking: in each of these examples, a split-and-condition strategy leads to\nconfidence intervals that are much narrower than the state-of-the-art\ninfer-and-widen proposal, when methods are tuned to yield identical selection\nevents. Furthermore, even an ``oracle\" infer-and-widen confidence interval --\nthe narrowest possible interval that could be theoretically attained via\ninfer-and-widen -- is not necessarily narrower than a feasible\nsplit-and-condition method. Taken together, these results point to\nsplit-and-condition as the most promising framework for post-selection\ninference in real-world settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent attention has focused on the development of methods for post-selection\ninference. However, the connections between these methods, and the extent to\nwhich one might be preferred to another, remain unclear. In this paper, we\nclassify existing methods for post-selection inference into one of two\nframeworks: infer-and-widen or split-and-condition. The infer-and-widen\nframework produces confidence intervals whose midpoints are biased due to\nselection, and must be wide enough to account for this bias. By contrast,\nsplit-and-condition directly adjusts the intervals' midpoints to account for\nselection. We compare the two frameworks in three vignettes: the winner's\ncurse, maximal contrasts, and inference after the lasso. Our results are\nstriking: in each of these examples, a split-and-condition strategy leads to\nconfidence intervals that are much narrower than the state-of-the-art\ninfer-and-widen proposal, when methods are tuned to yield identical selection\nevents. Furthermore, even an ``oracle\" infer-and-widen confidence interval --\nthe narrowest possible interval that could be theoretically attained via\ninfer-and-widen -- is not necessarily narrower than a feasible\nsplit-and-condition method. Taken together, these results point to\nsplit-and-condition as the most promising framework for post-selection\ninference in real-world settings."
                },
                "authors": [
                    {
                        "name": "Ronan Perry"
                    },
                    {
                        "name": "Zichun Xu"
                    },
                    {
                        "name": "Olivia McGough"
                    },
                    {
                        "name": "Daniela Witten"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Witten"
                },
                "author": "Daniela Witten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06323v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06318v1",
                "updated": "2024-08-12T17:39:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    39,
                    1,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T17:39:01Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    39,
                    1,
                    0,
                    225,
                    0
                ],
                "title": "Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take\n  TravelPlanner as an Example",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take\n  TravelPlanner as an Example"
                },
                "summary": "Large language models (LLMs) have brought autonomous agents closer to\nartificial general intelligence (AGI) due to their promising generalization and\nemergent capabilities. There is, however, a lack of studies on how LLM-based\nagents behave, why they could potentially fail, and how to improve them,\nparticularly in demanding real-world planning tasks. In this paper, as an\neffort to fill the gap, we present our study using a realistic benchmark,\nTravelPlanner, where an agent must meet multiple constraints to generate\naccurate plans. We leverage this benchmark to address four key research\nquestions: (1) are LLM agents robust enough to lengthy and noisy contexts when\nit comes to reasoning and planning? (2) can few-shot prompting adversely impact\nthe performance of LLM agents in scenarios with long context? (3) can we rely\non refinement to improve plans, and (4) can fine-tuning LLMs with both positive\nand negative feedback lead to further improvement? Our comprehensive\nexperiments indicate that, firstly, LLMs often fail to attend to crucial parts\nof a long context, despite their ability to handle extensive reference\ninformation and few-shot examples; secondly, they still struggle with analyzing\nthe long plans and cannot provide accurate feedback for refinement; thirdly, we\npropose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive and\nnegative feedback, resulting in substantial gains over Supervised Fine-Tuning\n(SFT). Our findings offer in-depth insights to the community on various aspects\nrelated to real-world planning applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have brought autonomous agents closer to\nartificial general intelligence (AGI) due to their promising generalization and\nemergent capabilities. There is, however, a lack of studies on how LLM-based\nagents behave, why they could potentially fail, and how to improve them,\nparticularly in demanding real-world planning tasks. In this paper, as an\neffort to fill the gap, we present our study using a realistic benchmark,\nTravelPlanner, where an agent must meet multiple constraints to generate\naccurate plans. We leverage this benchmark to address four key research\nquestions: (1) are LLM agents robust enough to lengthy and noisy contexts when\nit comes to reasoning and planning? (2) can few-shot prompting adversely impact\nthe performance of LLM agents in scenarios with long context? (3) can we rely\non refinement to improve plans, and (4) can fine-tuning LLMs with both positive\nand negative feedback lead to further improvement? Our comprehensive\nexperiments indicate that, firstly, LLMs often fail to attend to crucial parts\nof a long context, despite their ability to handle extensive reference\ninformation and few-shot examples; secondly, they still struggle with analyzing\nthe long plans and cannot provide accurate feedback for refinement; thirdly, we\npropose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive and\nnegative feedback, resulting in substantial gains over Supervised Fine-Tuning\n(SFT). Our findings offer in-depth insights to the community on various aspects\nrelated to real-world planning applications."
                },
                "authors": [
                    {
                        "name": "Yanan Chen"
                    },
                    {
                        "name": "Ali Pesaranghader"
                    },
                    {
                        "name": "Tanmana Sadhu"
                    },
                    {
                        "name": "Dong Hoon Yi"
                    }
                ],
                "author_detail": {
                    "name": "Dong Hoon Yi"
                },
                "author": "Dong Hoon Yi",
                "arxiv_comment": "13 pages, 2 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02075v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02075v2",
                "updated": "2024-08-12T17:17:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    17,
                    0,
                    0,
                    225,
                    0
                ],
                "published": "2024-06-04T07:54:31Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    54,
                    31,
                    1,
                    156,
                    0
                ],
                "title": "ReLU-KAN: New Kolmogorov-Arnold Networks that Only Need Matrix Addition,\n  Dot Multiplication, and ReLU",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReLU-KAN: New Kolmogorov-Arnold Networks that Only Need Matrix Addition,\n  Dot Multiplication, and ReLU"
                },
                "summary": "Limited by the complexity of basis function (B-spline) calculations,\nKolmogorov-Arnold Networks (KAN) suffer from restricted parallel computing\ncapability on GPUs. This paper proposes a novel ReLU-KAN implementation that\ninherits the core idea of KAN. By adopting ReLU (Rectified Linear Unit) and\npoint-wise multiplication, we simplify the design of KAN's basis function and\noptimize the computation process for efficient CUDA computing. The proposed\nReLU-KAN architecture can be readily implemented on existing deep learning\nframeworks (e.g., PyTorch) for both inference and training. Experimental\nresults demonstrate that ReLU-KAN achieves a 20x speedup compared to\ntraditional KAN with 4-layer networks. Furthermore, ReLU-KAN exhibits a more\nstable training process with superior fitting ability while preserving the\n\"catastrophic forgetting avoidance\" property of KAN. You can get the code in\nhttps://github.com/quiqi/relu_kan",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limited by the complexity of basis function (B-spline) calculations,\nKolmogorov-Arnold Networks (KAN) suffer from restricted parallel computing\ncapability on GPUs. This paper proposes a novel ReLU-KAN implementation that\ninherits the core idea of KAN. By adopting ReLU (Rectified Linear Unit) and\npoint-wise multiplication, we simplify the design of KAN's basis function and\noptimize the computation process for efficient CUDA computing. The proposed\nReLU-KAN architecture can be readily implemented on existing deep learning\nframeworks (e.g., PyTorch) for both inference and training. Experimental\nresults demonstrate that ReLU-KAN achieves a 20x speedup compared to\ntraditional KAN with 4-layer networks. Furthermore, ReLU-KAN exhibits a more\nstable training process with superior fitting ability while preserving the\n\"catastrophic forgetting avoidance\" property of KAN. You can get the code in\nhttps://github.com/quiqi/relu_kan"
                },
                "authors": [
                    {
                        "name": "Qi Qiu"
                    },
                    {
                        "name": "Tao Zhu"
                    },
                    {
                        "name": "Helin Gong"
                    },
                    {
                        "name": "Liming Chen"
                    },
                    {
                        "name": "Huansheng Ning"
                    }
                ],
                "author_detail": {
                    "name": "Huansheng Ning"
                },
                "author": "Huansheng Ning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02075v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02075v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.00616v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.00616v5",
                "updated": "2024-08-12T16:58:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    58,
                    33,
                    0,
                    225,
                    0
                ],
                "published": "2023-09-01T17:59:56Z",
                "published_parsed": [
                    2023,
                    9,
                    1,
                    17,
                    59,
                    56,
                    4,
                    244,
                    0
                ],
                "title": "OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation"
                },
                "summary": "In this work, we introduce OpenIns3D, a new 3D-input-only framework for 3D\nopen-vocabulary scene understanding. The OpenIns3D framework employs a\n\"Mask-Snap-Lookup\" scheme. The \"Mask\" module learns class-agnostic mask\nproposals in 3D point clouds, the \"Snap\" module generates synthetic scene-level\nimages at multiple scales and leverages 2D vision-language models to extract\ninteresting objects, and the \"Lookup\" module searches through the outcomes of\n\"Snap\" to assign category names to the proposed masks. This approach, yet\nsimple, achieves state-of-the-art performance across a wide range of 3D\nopen-vocabulary tasks, including recognition, object detection, and instance\nsegmentation, on both indoor and outdoor datasets. Moreover, OpenIns3D\nfacilitates effortless switching between different 2D detectors without\nrequiring retraining. When integrated with powerful 2D open-world models, it\nachieves excellent results in scene understanding tasks. Furthermore, when\ncombined with LLM-powered 2D models, OpenIns3D exhibits an impressive\ncapability to comprehend and process highly complex text queries that demand\nintricate reasoning and real-world knowledge. Project page:\nhttps://zheninghuang.github.io/OpenIns3D/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce OpenIns3D, a new 3D-input-only framework for 3D\nopen-vocabulary scene understanding. The OpenIns3D framework employs a\n\"Mask-Snap-Lookup\" scheme. The \"Mask\" module learns class-agnostic mask\nproposals in 3D point clouds, the \"Snap\" module generates synthetic scene-level\nimages at multiple scales and leverages 2D vision-language models to extract\ninteresting objects, and the \"Lookup\" module searches through the outcomes of\n\"Snap\" to assign category names to the proposed masks. This approach, yet\nsimple, achieves state-of-the-art performance across a wide range of 3D\nopen-vocabulary tasks, including recognition, object detection, and instance\nsegmentation, on both indoor and outdoor datasets. Moreover, OpenIns3D\nfacilitates effortless switching between different 2D detectors without\nrequiring retraining. When integrated with powerful 2D open-world models, it\nachieves excellent results in scene understanding tasks. Furthermore, when\ncombined with LLM-powered 2D models, OpenIns3D exhibits an impressive\ncapability to comprehend and process highly complex text queries that demand\nintricate reasoning and real-world knowledge. Project page:\nhttps://zheninghuang.github.io/OpenIns3D/"
                },
                "authors": [
                    {
                        "name": "Zhening Huang"
                    },
                    {
                        "name": "Xiaoyang Wu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Lei Zhu"
                    },
                    {
                        "name": "Joan Lasenby"
                    }
                ],
                "author_detail": {
                    "name": "Joan Lasenby"
                },
                "author": "Joan Lasenby",
                "arxiv_comment": "ECCV 2024. Project page: https://zheninghuang.github.io/OpenIns3D/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.00616v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.00616v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06285v1",
                "updated": "2024-08-12T16:49:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    49,
                    22,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T16:49:22Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    49,
                    22,
                    0,
                    225,
                    0
                ],
                "title": "Synthetic Patient-Physician Dialogue Generation from Clinical Notes\n  Using LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Patient-Physician Dialogue Generation from Clinical Notes\n  Using LLM"
                },
                "summary": "Medical dialogue systems (MDS) enhance patient-physician communication,\nimprove healthcare accessibility, and reduce costs. However, acquiring suitable\ndata to train these systems poses significant challenges. Privacy concerns\nprevent the use of real conversations, necessitating synthetic alternatives.\nSynthetic dialogue generation from publicly available clinical notes offers a\npromising solution to this issue, providing realistic data while safeguarding\nprivacy. Our approach, SynDial, uses a single LLM iteratively with zero-shot\nprompting and a feedback loop to generate and refine high-quality synthetic\ndialogues. The feedback consists of weighted evaluation scores for similarity\nand extractiveness. The iterative process ensures dialogues meet predefined\nthresholds, achieving superior extractiveness as a result of the feedback loop.\nAdditionally, evaluation shows that the generated dialogues excel in factuality\nmetric compared to the baselines and has comparable diversity scores with GPT4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical dialogue systems (MDS) enhance patient-physician communication,\nimprove healthcare accessibility, and reduce costs. However, acquiring suitable\ndata to train these systems poses significant challenges. Privacy concerns\nprevent the use of real conversations, necessitating synthetic alternatives.\nSynthetic dialogue generation from publicly available clinical notes offers a\npromising solution to this issue, providing realistic data while safeguarding\nprivacy. Our approach, SynDial, uses a single LLM iteratively with zero-shot\nprompting and a feedback loop to generate and refine high-quality synthetic\ndialogues. The feedback consists of weighted evaluation scores for similarity\nand extractiveness. The iterative process ensures dialogues meet predefined\nthresholds, achieving superior extractiveness as a result of the feedback loop.\nAdditionally, evaluation shows that the generated dialogues excel in factuality\nmetric compared to the baselines and has comparable diversity scores with GPT4."
                },
                "authors": [
                    {
                        "name": "Trisha Das"
                    },
                    {
                        "name": "Dina Albassam"
                    },
                    {
                        "name": "Jimeng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jimeng Sun"
                },
                "author": "Jimeng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02854v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02854v3",
                "updated": "2024-08-12T16:44:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    44,
                    5,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-05T22:34:28Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    22,
                    34,
                    28,
                    0,
                    218,
                    0
                ],
                "title": "Wiping out the limitations of Large Language Models -- A Taxonomy for\n  Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wiping out the limitations of Large Language Models -- A Taxonomy for\n  Retrieval Augmented Generation"
                },
                "summary": "Current research on RAGs is distributed across various disciplines, and since\nthe technology is evolving very quickly, its unit of analysis is mostly on\ntechnological innovations, rather than applications in business contexts. Thus,\nin this research, we aim to create a taxonomy to conceptualize a comprehensive\noverview of the constituting characteristics that define RAG applications,\nfacilitating the adoption of this technology in the IS community. To the best\nof our knowledge, no RAG application taxonomies have been developed so far. We\ndescribe our methodology for developing the taxonomy, which includes the\ncriteria for selecting papers, an explanation of our rationale for employing a\nLarge Language Model (LLM)-supported approach to extract and identify initial\ncharacteristics, and a concise overview of our systematic process for\nconceptualizing the taxonomy. Our systematic taxonomy development process\nincludes four iterative phases designed to refine and enhance our understanding\nand presentation of RAG's core dimensions. We have developed a total of five\nmeta-dimensions and sixteen dimensions to comprehensively capture the concept\nof Retrieval-Augmented Generation (RAG) applications. When discussing our\nfindings, we also detail the specific research areas and pose key research\nquestions to guide future information system researchers as they explore the\nemerging topics of RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current research on RAGs is distributed across various disciplines, and since\nthe technology is evolving very quickly, its unit of analysis is mostly on\ntechnological innovations, rather than applications in business contexts. Thus,\nin this research, we aim to create a taxonomy to conceptualize a comprehensive\noverview of the constituting characteristics that define RAG applications,\nfacilitating the adoption of this technology in the IS community. To the best\nof our knowledge, no RAG application taxonomies have been developed so far. We\ndescribe our methodology for developing the taxonomy, which includes the\ncriteria for selecting papers, an explanation of our rationale for employing a\nLarge Language Model (LLM)-supported approach to extract and identify initial\ncharacteristics, and a concise overview of our systematic process for\nconceptualizing the taxonomy. Our systematic taxonomy development process\nincludes four iterative phases designed to refine and enhance our understanding\nand presentation of RAG's core dimensions. We have developed a total of five\nmeta-dimensions and sixteen dimensions to comprehensively capture the concept\nof Retrieval-Augmented Generation (RAG) applications. When discussing our\nfindings, we also detail the specific research areas and pose key research\nquestions to guide future information system researchers as they explore the\nemerging topics of RAG systems."
                },
                "authors": [
                    {
                        "name": "Mahei Manhai Li"
                    },
                    {
                        "name": "Irina Nikishina"
                    },
                    {
                        "name": "√ñzge Sevgili"
                    },
                    {
                        "name": "Martin Semmann"
                    }
                ],
                "author_detail": {
                    "name": "Martin Semmann"
                },
                "author": "Martin Semmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02854v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02854v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06279v1",
                "updated": "2024-08-12T16:41:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    41,
                    1,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T16:41:01Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    41,
                    1,
                    0,
                    225,
                    0
                ],
                "title": "PDRs4All. X. ALMA and JWST detection of neutral carbon in the externally\n  irradiated disk d203-506: Undepleted gas-phase carbon",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDRs4All. X. ALMA and JWST detection of neutral carbon in the externally\n  irradiated disk d203-506: Undepleted gas-phase carbon"
                },
                "summary": "The gas-phase abundance of carbon, x_C = C/H, and its depletion factors are\nessential parameters for understanding the gas and solid compositions that are\nultimately incorporated into planets. The majority of protoplanetary disks are\nborn in clusters and, as a result, are exposed to external FUV radiation. These\nFUV photons potentially affect the disk's evolution, chemical composition, and\nline excitation. We present the first detection of the [CI]609um fine-structure\nline of neutral carbon (CI), achieved with ALMA, toward one of these disks,\nd203-506, in the Orion Nebula Cluster. We also report the detection of CI\nforbidden and permitted lines (from electronically excited states up to 10 eV)\nobserved with JWST in the IR. These lines trace the irradiated outer disk and\nphoto-evaporative wind. Contrary to the common belief that these IR lines are\nC+ recombination lines, we find that they are dominated by FUV-pumping of CI\nfollowed by fluorescence cascades. They trace the transition from atomic to\nmolecular gas, and their intensities scale with G0. The lack of outstanding IR\nOI fluorescent emission, however, implies a sharper attenuation of external FUV\nradiation with E > 12 eV (~Lyman-beta). This is related to a lower effective\nFUV dust absorption cross section compared to that of interstellar grains,\nimplying a more prominent role for FUV shielding by the CI photoionization\ncontinuum. The [CI]609um intensity is proportional to N(CI) and can be used to\ninfer x_C. We derive x_C ~ 1.4E-4. This implies that there is no major\ndepletion of volatile carbon compared to x_C measured in the natal cloud,\nhinting at a young disk. We also show that external FUV radiation impacts the\nouter disk and wind by vertically shifting the water freeze-out depth, which\nresults in less efficient grain growth and settling. This shift leads to nearly\nsolar gas-phase C/O abundance ratios in these irradiated layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The gas-phase abundance of carbon, x_C = C/H, and its depletion factors are\nessential parameters for understanding the gas and solid compositions that are\nultimately incorporated into planets. The majority of protoplanetary disks are\nborn in clusters and, as a result, are exposed to external FUV radiation. These\nFUV photons potentially affect the disk's evolution, chemical composition, and\nline excitation. We present the first detection of the [CI]609um fine-structure\nline of neutral carbon (CI), achieved with ALMA, toward one of these disks,\nd203-506, in the Orion Nebula Cluster. We also report the detection of CI\nforbidden and permitted lines (from electronically excited states up to 10 eV)\nobserved with JWST in the IR. These lines trace the irradiated outer disk and\nphoto-evaporative wind. Contrary to the common belief that these IR lines are\nC+ recombination lines, we find that they are dominated by FUV-pumping of CI\nfollowed by fluorescence cascades. They trace the transition from atomic to\nmolecular gas, and their intensities scale with G0. The lack of outstanding IR\nOI fluorescent emission, however, implies a sharper attenuation of external FUV\nradiation with E > 12 eV (~Lyman-beta). This is related to a lower effective\nFUV dust absorption cross section compared to that of interstellar grains,\nimplying a more prominent role for FUV shielding by the CI photoionization\ncontinuum. The [CI]609um intensity is proportional to N(CI) and can be used to\ninfer x_C. We derive x_C ~ 1.4E-4. This implies that there is no major\ndepletion of volatile carbon compared to x_C measured in the natal cloud,\nhinting at a young disk. We also show that external FUV radiation impacts the\nouter disk and wind by vertically shifting the water freeze-out depth, which\nresults in less efficient grain growth and settling. This shift leads to nearly\nsolar gas-phase C/O abundance ratios in these irradiated layers."
                },
                "authors": [
                    {
                        "name": "Javier R. Goicoechea"
                    },
                    {
                        "name": "J. Le Bourlot"
                    },
                    {
                        "name": "J. H. Black"
                    },
                    {
                        "name": "F. Alarc√≥n"
                    },
                    {
                        "name": "E. A. Bergin"
                    },
                    {
                        "name": "O. Bern√©"
                    },
                    {
                        "name": "E. Bron"
                    },
                    {
                        "name": "A. Canin"
                    },
                    {
                        "name": "E. Chapillon"
                    },
                    {
                        "name": "R. Chown"
                    },
                    {
                        "name": "E. Dartois"
                    },
                    {
                        "name": "M. Gerin"
                    },
                    {
                        "name": "E. Habart"
                    },
                    {
                        "name": "T. J. Haworth"
                    },
                    {
                        "name": "C. Joblin"
                    },
                    {
                        "name": "O. Kannavou"
                    },
                    {
                        "name": "F. Le Petit"
                    },
                    {
                        "name": "T. Onaka"
                    },
                    {
                        "name": "E. Peeters"
                    },
                    {
                        "name": "J. Pety"
                    },
                    {
                        "name": "E. Roueff"
                    },
                    {
                        "name": "A. Sidhu"
                    },
                    {
                        "name": "I. Schroetter"
                    },
                    {
                        "name": "B. Tabone"
                    },
                    {
                        "name": "A. G. G. M. Tielens"
                    },
                    {
                        "name": "B. Trahin"
                    },
                    {
                        "name": "D. Van De Putte"
                    },
                    {
                        "name": "S. Vicente"
                    },
                    {
                        "name": "M. Zannese"
                    }
                ],
                "author_detail": {
                    "name": "M. Zannese"
                },
                "author": "M. Zannese",
                "arxiv_comment": "Accepted for publication in A&A Letters. 14 pages including\n  Appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06277v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06277v1",
                "updated": "2024-08-12T16:39:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    39,
                    18,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T16:39:18Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    39,
                    18,
                    0,
                    225,
                    0
                ],
                "title": "Multi-marginal Schr√∂dinger Bridges with Iterative Reference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-marginal Schr√∂dinger Bridges with Iterative Reference"
                },
                "summary": "Practitioners frequently aim to infer an unobserved population trajectory\nusing sample snapshots at multiple time points. For instance, in single-cell\nsequencing, scientists would like to learn how gene expression evolves over\ntime. But sequencing any cell destroys that cell. So we cannot access any\ncell's full trajectory, but we can access snapshot samples from many cells.\nStochastic differential equations are commonly used to analyze systems with\nfull individual-trajectory access; since here we have only sample snapshots,\nthese methods are inapplicable. The deep learning community has recently\nexplored using Schr\\\"odinger bridges (SBs) and their extensions to estimate\nthese dynamics. However, these methods either (1) interpolate between just two\ntime points or (2) require a single fixed reference dynamic within the SB,\nwhich is often just set to be Brownian motion. But learning piecewise from\nadjacent time points can fail to capture long-term dependencies. And\npractitioners are typically able to specify a model class for the reference\ndynamic but not the exact values of the parameters within it. So we propose a\nnew method that (1) learns the unobserved trajectories from sample snapshots\nacross multiple time points and (2) requires specification only of a class of\nreference dynamics, not a single fixed one. In particular, we suggest an\niterative projection method inspired by Schr\\\"odinger bridges; we alternate\nbetween learning a piecewise SB on the unobserved trajectories and using the\nlearned SB to refine our best guess for the dynamics within the reference\nclass. We demonstrate the advantages of our method via a well-known simulated\nparametric model from ecology, simulated and real data from systems biology,\nand real motion-capture data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practitioners frequently aim to infer an unobserved population trajectory\nusing sample snapshots at multiple time points. For instance, in single-cell\nsequencing, scientists would like to learn how gene expression evolves over\ntime. But sequencing any cell destroys that cell. So we cannot access any\ncell's full trajectory, but we can access snapshot samples from many cells.\nStochastic differential equations are commonly used to analyze systems with\nfull individual-trajectory access; since here we have only sample snapshots,\nthese methods are inapplicable. The deep learning community has recently\nexplored using Schr\\\"odinger bridges (SBs) and their extensions to estimate\nthese dynamics. However, these methods either (1) interpolate between just two\ntime points or (2) require a single fixed reference dynamic within the SB,\nwhich is often just set to be Brownian motion. But learning piecewise from\nadjacent time points can fail to capture long-term dependencies. And\npractitioners are typically able to specify a model class for the reference\ndynamic but not the exact values of the parameters within it. So we propose a\nnew method that (1) learns the unobserved trajectories from sample snapshots\nacross multiple time points and (2) requires specification only of a class of\nreference dynamics, not a single fixed one. In particular, we suggest an\niterative projection method inspired by Schr\\\"odinger bridges; we alternate\nbetween learning a piecewise SB on the unobserved trajectories and using the\nlearned SB to refine our best guess for the dynamics within the reference\nclass. We demonstrate the advantages of our method via a well-known simulated\nparametric model from ecology, simulated and real data from systems biology,\nand real motion-capture data."
                },
                "authors": [
                    {
                        "name": "Yunyi Shen"
                    },
                    {
                        "name": "Renato Berlinghieri"
                    },
                    {
                        "name": "Tamara Broderick"
                    }
                ],
                "author_detail": {
                    "name": "Tamara Broderick"
                },
                "author": "Tamara Broderick",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06277v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06277v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06276v2",
                "updated": "2024-08-13T11:05:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    5,
                    10,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-12T16:39:03Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    39,
                    3,
                    0,
                    225,
                    0
                ],
                "title": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Jieyong Kim"
                    },
                    {
                        "name": "Hyunseo Kim"
                    },
                    {
                        "name": "Hyunjin Cho"
                    },
                    {
                        "name": "SeongKu Kang"
                    },
                    {
                        "name": "Buru Chang"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06273v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06273v2",
                "updated": "2024-08-13T14:57:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    57,
                    25,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-12T16:34:56Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    34,
                    56,
                    0,
                    225,
                    0
                ],
                "title": "FuxiTranyu: A Multilingual Large Language Model Trained with Balanced\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FuxiTranyu: A Multilingual Large Language Model Trained with Balanced\n  Data"
                },
                "summary": "Large language models (LLMs) have demonstrated prowess in a wide range of\ntasks. However, many LLMs exhibit significant performance discrepancies between\nhigh- and low-resource languages. To mitigate this challenge, we present\nFuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the\nneed of the research community for balanced and high-performing multilingual\ncapabilities. FuxiTranyu-8B, the base model with 8 billion parameters, is\ntrained from scratch on a meticulously balanced multilingual data repository\nthat contains 600 billion tokens covering 43 natural languages and 16\nprogramming languages. In addition to the base model, we also develop two\ninstruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse\nmultilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined\nwith DPO on a preference dataset for enhanced alignment ability. Extensive\nexperiments on a wide range of multilingual benchmarks demonstrate the\ncompetitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,\nBLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretability\nanalyses at both the neuron and representation level suggest that FuxiTranyu is\nable to learn consistent multilingual representations across different\nlanguages. To promote further research into multilingual LLMs and their working\nmechanisms, we release both the base and instruction-tuned FuxiTranyu models\ntogether with 58 pretraining checkpoints at HuggingFace and Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated prowess in a wide range of\ntasks. However, many LLMs exhibit significant performance discrepancies between\nhigh- and low-resource languages. To mitigate this challenge, we present\nFuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the\nneed of the research community for balanced and high-performing multilingual\ncapabilities. FuxiTranyu-8B, the base model with 8 billion parameters, is\ntrained from scratch on a meticulously balanced multilingual data repository\nthat contains 600 billion tokens covering 43 natural languages and 16\nprogramming languages. In addition to the base model, we also develop two\ninstruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse\nmultilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined\nwith DPO on a preference dataset for enhanced alignment ability. Extensive\nexperiments on a wide range of multilingual benchmarks demonstrate the\ncompetitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,\nBLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretability\nanalyses at both the neuron and representation level suggest that FuxiTranyu is\nable to learn consistent multilingual representations across different\nlanguages. To promote further research into multilingual LLMs and their working\nmechanisms, we release both the base and instruction-tuned FuxiTranyu models\ntogether with 58 pretraining checkpoints at HuggingFace and Github."
                },
                "authors": [
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Shaoyang Xu"
                    },
                    {
                        "name": "Leiyu Pan"
                    },
                    {
                        "name": "Supryadi"
                    },
                    {
                        "name": "Menglong Cui"
                    },
                    {
                        "name": "Jiangcun Du"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Juesi Xiao"
                    },
                    {
                        "name": "Shaolin Zhu"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06273v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06273v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06272v1",
                "updated": "2024-08-12T16:33:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    33,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T16:33:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    33,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "A RAG-Based Question-Answering Solution for Cyber-Attack Investigation\n  and Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A RAG-Based Question-Answering Solution for Cyber-Attack Investigation\n  and Attribution"
                },
                "summary": "In the constantly evolving field of cybersecurity, it is imperative for\nanalysts to stay abreast of the latest attack trends and pertinent information\nthat aids in the investigation and attribution of cyber-attacks. In this work,\nwe introduce the first question-answering (QA) model and its application that\nprovides information to the cybersecurity experts about cyber-attacks\ninvestigations and attribution. Our QA model is based on Retrieval Augmented\nGeneration (RAG) techniques together with a Large Language Model (LLM) and\nprovides answers to the users' queries based on either our knowledge base (KB)\nthat contains curated information about cyber-attacks investigations and\nattribution or on outside resources provided by the users. We have tested and\nevaluated our QA model with various types of questions, including KB-based,\nmetadata-based, specific documents from the KB, and external sources-based\nquestions. We compared the answers for KB-based questions with those from\nOpenAI's GPT-3.5 and the latest GPT-4o LLMs. Our proposed QA model outperforms\nOpenAI's GPT models by providing the source of the answers and overcoming the\nhallucination limitations of the GPT models, which is critical for cyber-attack\ninvestigation and attribution. Additionally, our analysis showed that when the\nRAG QA model is given few-shot examples rather than zero-shot instructions, it\ngenerates better answers compared to cases where no examples are supplied in\naddition to the query.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the constantly evolving field of cybersecurity, it is imperative for\nanalysts to stay abreast of the latest attack trends and pertinent information\nthat aids in the investigation and attribution of cyber-attacks. In this work,\nwe introduce the first question-answering (QA) model and its application that\nprovides information to the cybersecurity experts about cyber-attacks\ninvestigations and attribution. Our QA model is based on Retrieval Augmented\nGeneration (RAG) techniques together with a Large Language Model (LLM) and\nprovides answers to the users' queries based on either our knowledge base (KB)\nthat contains curated information about cyber-attacks investigations and\nattribution or on outside resources provided by the users. We have tested and\nevaluated our QA model with various types of questions, including KB-based,\nmetadata-based, specific documents from the KB, and external sources-based\nquestions. We compared the answers for KB-based questions with those from\nOpenAI's GPT-3.5 and the latest GPT-4o LLMs. Our proposed QA model outperforms\nOpenAI's GPT models by providing the source of the answers and overcoming the\nhallucination limitations of the GPT models, which is critical for cyber-attack\ninvestigation and attribution. Additionally, our analysis showed that when the\nRAG QA model is given few-shot examples rather than zero-shot instructions, it\ngenerates better answers compared to cases where no examples are supplied in\naddition to the query."
                },
                "authors": [
                    {
                        "name": "Sampath Rajapaksha"
                    },
                    {
                        "name": "Ruby Rani"
                    },
                    {
                        "name": "Erisa Karafili"
                    }
                ],
                "author_detail": {
                    "name": "Erisa Karafili"
                },
                "author": "Erisa Karafili",
                "arxiv_comment": "Accepted at SECAI 2024 (ESORICS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06266v1",
                "updated": "2024-08-12T16:24:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    24,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T16:24:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    24,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Anchored Preference Optimization and Contrastive Revisions: Addressing\n  Underspecification in Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchored Preference Optimization and Contrastive Revisions: Addressing\n  Underspecification in Alignment"
                },
                "summary": "Large Language Models (LLMs) are often aligned using contrastive alignment\nobjectives and preference pair datasets. The interaction between model, paired\ndata, and objective makes alignment a complicated procedure, sometimes\nproducing subpar results. We study this and find that (i) preference data gives\na better learning signal when the underlying responses are contrastive, and\n(ii) alignment objectives lead to better performance when they specify more\ncontrol over the model during training. Based on these insights, we introduce\nContrastive Learning from AI Revisions (CLAIR), a data-creation method which\nleads to more contrastive preference pairs, and Anchored Preference\nOptimization (APO), a controllable and more stable alignment objective. We\nalign Llama-3-8B-Instruct using various comparable datasets and alignment\nobjectives and measure MixEval-Hard scores, which correlate highly with human\njudgments. The CLAIR preferences lead to the strongest performance out of all\ndatasets, and APO consistently outperforms less controllable objectives. Our\nbest model, trained on 32K CLAIR preferences with APO, improves\nLlama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code\nis available at https://github.com/ContextualAI/CLAIR_and_APO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are often aligned using contrastive alignment\nobjectives and preference pair datasets. The interaction between model, paired\ndata, and objective makes alignment a complicated procedure, sometimes\nproducing subpar results. We study this and find that (i) preference data gives\na better learning signal when the underlying responses are contrastive, and\n(ii) alignment objectives lead to better performance when they specify more\ncontrol over the model during training. Based on these insights, we introduce\nContrastive Learning from AI Revisions (CLAIR), a data-creation method which\nleads to more contrastive preference pairs, and Anchored Preference\nOptimization (APO), a controllable and more stable alignment objective. We\nalign Llama-3-8B-Instruct using various comparable datasets and alignment\nobjectives and measure MixEval-Hard scores, which correlate highly with human\njudgments. The CLAIR preferences lead to the strongest performance out of all\ndatasets, and APO consistently outperforms less controllable objectives. Our\nbest model, trained on 32K CLAIR preferences with APO, improves\nLlama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code\nis available at https://github.com/ContextualAI/CLAIR_and_APO."
                },
                "authors": [
                    {
                        "name": "Karel D'Oosterlinck"
                    },
                    {
                        "name": "Winnie Xu"
                    },
                    {
                        "name": "Chris Develder"
                    },
                    {
                        "name": "Thomas Demeester"
                    },
                    {
                        "name": "Amanpreet Singh"
                    },
                    {
                        "name": "Christopher Potts"
                    },
                    {
                        "name": "Douwe Kiela"
                    },
                    {
                        "name": "Shikib Mehri"
                    }
                ],
                "author_detail": {
                    "name": "Shikib Mehri"
                },
                "author": "Shikib Mehri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06262v1",
                "updated": "2024-08-12T16:22:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    22,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T16:22:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    22,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "DUNE: A Machine Learning Deep UNet++ based Ensemble Approach to Monthly,\n  Seasonal and Annual Climate Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DUNE: A Machine Learning Deep UNet++ based Ensemble Approach to Monthly,\n  Seasonal and Annual Climate Forecasting"
                },
                "summary": "Capitalizing on the recent availability of ERA5 monthly averaged long-term\ndata records of mean atmospheric and climate fields based on high-resolution\nreanalysis, deep-learning architectures offer an alternative to physics-based\ndaily numerical weather predictions for subseasonal to seasonal (S2S) and\nannual means. A novel Deep UNet++-based Ensemble (DUNE) neural architecture is\nintroduced, employing multi-encoder-decoder structures with residual blocks.\nWhen initialized from a prior month or year, this architecture produced the\nfirst AI-based global monthly, seasonal, or annual mean forecast of 2-meter\ntemperatures (T2m) and sea surface temperatures (SST). ERA5 monthly mean data\nis used as input for T2m over land, SST over oceans, and solar radiation at the\ntop of the atmosphere for each month of 40 years to train the model. Validation\nforecasts are performed for an additional two years, followed by five years of\nforecast evaluations to account for natural annual variability. AI-trained\ninference forecast weights generate forecasts in seconds, enabling ensemble\nseasonal forecasts. Root Mean Squared Error (RMSE), Anomaly Correlation\nCoefficient (ACC), and Heidke Skill Score (HSS) statistics are presented\nglobally and over specific regions. These forecasts outperform persistence,\nclimatology, and multiple linear regression for all domains. DUNE forecasts\ndemonstrate comparable statistical accuracy to NOAA's operational monthly and\nseasonal probabilistic outlook forecasts over the US but at significantly\nhigher resolutions. RMSE and ACC error statistics for other recent AI-based\ndaily forecasts also show superior performance for DUNE-based forecasts. The\nDUNE model's application to an ensemble data assimilation cycle shows\ncomparable forecast accuracy with a single high-resolution model, potentially\neliminating the need for retraining on extrapolated datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Capitalizing on the recent availability of ERA5 monthly averaged long-term\ndata records of mean atmospheric and climate fields based on high-resolution\nreanalysis, deep-learning architectures offer an alternative to physics-based\ndaily numerical weather predictions for subseasonal to seasonal (S2S) and\nannual means. A novel Deep UNet++-based Ensemble (DUNE) neural architecture is\nintroduced, employing multi-encoder-decoder structures with residual blocks.\nWhen initialized from a prior month or year, this architecture produced the\nfirst AI-based global monthly, seasonal, or annual mean forecast of 2-meter\ntemperatures (T2m) and sea surface temperatures (SST). ERA5 monthly mean data\nis used as input for T2m over land, SST over oceans, and solar radiation at the\ntop of the atmosphere for each month of 40 years to train the model. Validation\nforecasts are performed for an additional two years, followed by five years of\nforecast evaluations to account for natural annual variability. AI-trained\ninference forecast weights generate forecasts in seconds, enabling ensemble\nseasonal forecasts. Root Mean Squared Error (RMSE), Anomaly Correlation\nCoefficient (ACC), and Heidke Skill Score (HSS) statistics are presented\nglobally and over specific regions. These forecasts outperform persistence,\nclimatology, and multiple linear regression for all domains. DUNE forecasts\ndemonstrate comparable statistical accuracy to NOAA's operational monthly and\nseasonal probabilistic outlook forecasts over the US but at significantly\nhigher resolutions. RMSE and ACC error statistics for other recent AI-based\ndaily forecasts also show superior performance for DUNE-based forecasts. The\nDUNE model's application to an ensemble data assimilation cycle shows\ncomparable forecast accuracy with a single high-resolution model, potentially\neliminating the need for retraining on extrapolated datasets."
                },
                "authors": [
                    {
                        "name": "Pratik Shukla"
                    },
                    {
                        "name": "Milton Halem"
                    }
                ],
                "author_detail": {
                    "name": "Milton Halem"
                },
                "author": "Milton Halem",
                "arxiv_comment": "Excluding Appendix: 18 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21770v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21770v3",
                "updated": "2024-08-12T16:20:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    20,
                    37,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-31T17:46:51Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    17,
                    46,
                    51,
                    2,
                    213,
                    0
                ],
                "title": "MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware\n  Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware\n  Experts"
                },
                "summary": "We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)\narchitecture designed for pre-training mixed-modal, early-fusion language\nmodels. MoMa processes images and text in arbitrary sequences by dividing\nexpert modules into modality-specific groups. These groups exclusively process\ndesignated tokens while employing learned routing within each group to maintain\nsemantically informed adaptivity. Our empirical results reveal substantial\npre-training efficiency gains through this modality-specific parameter\nallocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,\nfeaturing 4 text experts and 4 image experts, achieves impressive FLOPs\nsavings: 3.7x overall, with 2.6x for text and 5.2x for image processing\ncompared to a compute-equivalent dense baseline, measured by pre-training loss.\nThis outperforms the standard expert-choice MoE with 8 mixed-modal experts,\nwhich achieves 3x overall FLOPs savings (3x for text, 2.8x for image).\nCombining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPs\nsavings to 4.2x overall (text: 3.4x, image: 5.3x), although this combination\nhurts performance in causal inference due to increased sensitivity to router\naccuracy. These results demonstrate MoMa's potential to significantly advance\nthe efficiency of mixed-modal, early-fusion language model pre-training, paving\nthe way for more resource-efficient and capable multimodal AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)\narchitecture designed for pre-training mixed-modal, early-fusion language\nmodels. MoMa processes images and text in arbitrary sequences by dividing\nexpert modules into modality-specific groups. These groups exclusively process\ndesignated tokens while employing learned routing within each group to maintain\nsemantically informed adaptivity. Our empirical results reveal substantial\npre-training efficiency gains through this modality-specific parameter\nallocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,\nfeaturing 4 text experts and 4 image experts, achieves impressive FLOPs\nsavings: 3.7x overall, with 2.6x for text and 5.2x for image processing\ncompared to a compute-equivalent dense baseline, measured by pre-training loss.\nThis outperforms the standard expert-choice MoE with 8 mixed-modal experts,\nwhich achieves 3x overall FLOPs savings (3x for text, 2.8x for image).\nCombining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPs\nsavings to 4.2x overall (text: 3.4x, image: 5.3x), although this combination\nhurts performance in causal inference due to increased sensitivity to router\naccuracy. These results demonstrate MoMa's potential to significantly advance\nthe efficiency of mixed-modal, early-fusion language model pre-training, paving\nthe way for more resource-efficient and capable multimodal AI systems."
                },
                "authors": [
                    {
                        "name": "Xi Victoria Lin"
                    },
                    {
                        "name": "Akshat Shrivastava"
                    },
                    {
                        "name": "Liang Luo"
                    },
                    {
                        "name": "Srinivasan Iyer"
                    },
                    {
                        "name": "Mike Lewis"
                    },
                    {
                        "name": "Gargi Ghosh"
                    },
                    {
                        "name": "Luke Zettlemoyer"
                    },
                    {
                        "name": "Armen Aghajanyan"
                    }
                ],
                "author_detail": {
                    "name": "Armen Aghajanyan"
                },
                "author": "Armen Aghajanyan",
                "arxiv_comment": "v2 -> update related work section v3 -> fix spelling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21770v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21770v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.11643v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.11643v3",
                "updated": "2024-08-12T16:10:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    10,
                    25,
                    0,
                    225,
                    0
                ],
                "published": "2023-02-22T20:39:55Z",
                "published_parsed": [
                    2023,
                    2,
                    22,
                    20,
                    39,
                    55,
                    2,
                    53,
                    0
                ],
                "title": "An Empirical Analysis of Optimal Nonlinear Pricing in\n  Business-to-Business Markets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Analysis of Optimal Nonlinear Pricing in\n  Business-to-Business Markets"
                },
                "summary": "In continuous-choice settings, consumers decide not only on whether to\npurchase a product, but also on how much to purchase. Thus, firms optimize a\nfull price schedule rather than a single price point. This paper provides a\nmethodology to empirically estimate the optimal schedule under\nmulti-dimensional consumer heterogeneity with a focus on B2B applications. We\napply our method to novel data from an educational-services firm that contains\npurchase-size information not only for deals that materialized, but also for\npotential deals that eventually failed. We show that this data, combined with\nidentifying assumptions, helps infer how price sensitivity varies with\n\"customer size\". Using our estimated model, we show that the optimal\nsecond-degree price discrimination (i.e., optimal nonlinear tariff) improves\nthe firm's profit upon linear pricing by at least 8.2%. That said, this\nsecond-degree price discrimination scheme only recovers 7.1% of the gap between\nthe profitability of linear pricing and that of infeasible first degree price\ndiscrimination. We also conduct several further simulation analyses (i)\nempirically quantifying the magnitude by which incentive-compatibility\nconstraints impact the optimal pricing and profits, (ii) comparing the role of\ndemand- v.s. cost-side factors in shaping the optimal price schedule, and (iii)\nstudying the implications of fixed fees for the optimal contract and\nprofitability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In continuous-choice settings, consumers decide not only on whether to\npurchase a product, but also on how much to purchase. Thus, firms optimize a\nfull price schedule rather than a single price point. This paper provides a\nmethodology to empirically estimate the optimal schedule under\nmulti-dimensional consumer heterogeneity with a focus on B2B applications. We\napply our method to novel data from an educational-services firm that contains\npurchase-size information not only for deals that materialized, but also for\npotential deals that eventually failed. We show that this data, combined with\nidentifying assumptions, helps infer how price sensitivity varies with\n\"customer size\". Using our estimated model, we show that the optimal\nsecond-degree price discrimination (i.e., optimal nonlinear tariff) improves\nthe firm's profit upon linear pricing by at least 8.2%. That said, this\nsecond-degree price discrimination scheme only recovers 7.1% of the gap between\nthe profitability of linear pricing and that of infeasible first degree price\ndiscrimination. We also conduct several further simulation analyses (i)\nempirically quantifying the magnitude by which incentive-compatibility\nconstraints impact the optimal pricing and profits, (ii) comparing the role of\ndemand- v.s. cost-side factors in shaping the optimal price schedule, and (iii)\nstudying the implications of fixed fees for the optimal contract and\nprofitability."
                },
                "authors": [
                    {
                        "name": "Soheil Ghili"
                    },
                    {
                        "name": "Russ Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Russ Yoon"
                },
                "author": "Russ Yoon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.11643v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.11643v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06223v1",
                "updated": "2024-08-12T15:24:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    15,
                    24,
                    50,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T15:24:50Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    15,
                    24,
                    50,
                    0,
                    225,
                    0
                ],
                "title": "On Effects of Steering Latent Representation for Large Language Model\n  Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Effects of Steering Latent Representation for Large Language Model\n  Unlearning"
                },
                "summary": "Representation Misdirection for Unlearning (RMU), which steers model\nrepresentation in the intermediate layer to a target random representation, is\nan effective method for large language model (LLM) unlearning. Despite its high\nperformance, the underlying cause and explanation remain underexplored. In this\npaper, we first theoretically demonstrate that steering forget representations\nin the intermediate layer reduces token confidence, causing LLMs to generate\nwrong or nonsense responses. Second, we investigate how the coefficient\ninfluences the alignment of forget-sample representations with the random\ndirection and hint at the optimal coefficient values for effective unlearning\nacross different network layers. Third, we show that RMU unlearned models are\nrobust against adversarial jailbreak attacks. Last, our empirical analysis\nshows that RMU is less effective when applied to the middle and later layers in\nLLMs. To resolve this drawback, we propose Adaptive RMU -- a simple yet\neffective alternative method that makes unlearning effective with most layers.\nExtensive experiments demonstrate that Adaptive RMU significantly improves the\nunlearning performance compared to prior art while incurring no additional\ncomputational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Misdirection for Unlearning (RMU), which steers model\nrepresentation in the intermediate layer to a target random representation, is\nan effective method for large language model (LLM) unlearning. Despite its high\nperformance, the underlying cause and explanation remain underexplored. In this\npaper, we first theoretically demonstrate that steering forget representations\nin the intermediate layer reduces token confidence, causing LLMs to generate\nwrong or nonsense responses. Second, we investigate how the coefficient\ninfluences the alignment of forget-sample representations with the random\ndirection and hint at the optimal coefficient values for effective unlearning\nacross different network layers. Third, we show that RMU unlearned models are\nrobust against adversarial jailbreak attacks. Last, our empirical analysis\nshows that RMU is less effective when applied to the middle and later layers in\nLLMs. To resolve this drawback, we propose Adaptive RMU -- a simple yet\neffective alternative method that makes unlearning effective with most layers.\nExtensive experiments demonstrate that Adaptive RMU significantly improves the\nunlearning performance compared to prior art while incurring no additional\ncomputational cost."
                },
                "authors": [
                    {
                        "name": "Dang Huu-Tien"
                    },
                    {
                        "name": "Trung-Tin Pham"
                    },
                    {
                        "name": "Hoang Thanh-Tung"
                    },
                    {
                        "name": "Naoya Inoue"
                    }
                ],
                "author_detail": {
                    "name": "Naoya Inoue"
                },
                "author": "Naoya Inoue",
                "arxiv_comment": "15 pages, 5 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01310v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01310v3",
                "updated": "2024-08-13T14:00:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    0,
                    25,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-02T15:00:58Z",
                "published_parsed": [
                    2024,
                    8,
                    2,
                    15,
                    0,
                    58,
                    4,
                    215,
                    0
                ],
                "title": "PsybORG+: Modeling and Simulation for Detecting Cognitive Biases in\n  Advanced Persistent Threats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PsybORG+: Modeling and Simulation for Detecting Cognitive Biases in\n  Advanced Persistent Threats"
                },
                "summary": "Advanced Persistent Threats (APTs) bring significant challenges to\ncybersecurity due to their sophisticated and stealthy nature. Traditional\ncybersecurity measures fail to defend against APTs. Cognitive vulnerabilities\ncan significantly influence attackers' decision-making processes, which\npresents an opportunity for defenders to exploit. This work introduces\nPsybORG$^+$, a multi-agent cybersecurity simulation environment designed to\nmodel APT behaviors influenced by cognitive vulnerabilities. A classification\nmodel is built for cognitive vulnerability inference and a simulator is\ndesigned for synthetic data generation. Results show that PsybORG$^+$ can\neffectively model APT attackers with different loss aversion and confirmation\nbias levels. The classification model has at least a 0.83 accuracy rate in\npredicting cognitive vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Persistent Threats (APTs) bring significant challenges to\ncybersecurity due to their sophisticated and stealthy nature. Traditional\ncybersecurity measures fail to defend against APTs. Cognitive vulnerabilities\ncan significantly influence attackers' decision-making processes, which\npresents an opportunity for defenders to exploit. This work introduces\nPsybORG$^+$, a multi-agent cybersecurity simulation environment designed to\nmodel APT behaviors influenced by cognitive vulnerabilities. A classification\nmodel is built for cognitive vulnerability inference and a simulator is\ndesigned for synthetic data generation. Results show that PsybORG$^+$ can\neffectively model APT attackers with different loss aversion and confirmation\nbias levels. The classification model has at least a 0.83 accuracy rate in\npredicting cognitive vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Shuo Huang"
                    },
                    {
                        "name": "Fred Jones"
                    },
                    {
                        "name": "Nikolos Gurney"
                    },
                    {
                        "name": "David Pynadath"
                    },
                    {
                        "name": "Kunal Srivastava"
                    },
                    {
                        "name": "Stoney Trent"
                    },
                    {
                        "name": "Peggy Wu"
                    },
                    {
                        "name": "Quanyan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Quanyan Zhu"
                },
                "author": "Quanyan Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01310v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01310v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06217v1",
                "updated": "2024-08-12T15:16:17Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    15,
                    16,
                    17,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T15:16:17Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    15,
                    16,
                    17,
                    0,
                    225,
                    0
                ],
                "title": "Euclid: The Early Release Observations Lens Search Experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Euclid: The Early Release Observations Lens Search Experiment"
                },
                "summary": "We investigate the ability of the Euclid telescope to detect galaxy-scale\ngravitational lenses. To do so, we perform a systematic visual inspection of\nthe $0.7\\,\\rm{deg}^2$ Euclid ERO data towards the Perseus cluster using both\nthe high-resolution VIS $I_{\\scriptscriptstyle\\rm E}$ band, and the lower\nresolution NISP bands. We inspect every extended source brighter than magnitude\n$23$ in $I_{\\scriptscriptstyle\\rm E}$ with $41$ expert human classifiers. This\namounts to $12\\,086$ stamps of $10^{\\prime\\prime}\\,\\times\\,10^{\\prime\\prime}$.\nWe find $3$ grade A and $13$ grade B candidates. We assess the validity of\nthese $16$ candidates by modelling them and checking that they are consistent\nwith a single source lensed by a plausible mass distribution. Five of the\ncandidates pass this check, five others are rejected by the modelling and six\nare inconclusive. Extrapolating from the five successfully modelled candidates,\nwe infer that the full $14\\,000\\,{\\rm deg}^2$ of the Euclid Wide Survey should\ncontain $100\\,000^{+70\\,000}_{-30\\,000}$ galaxy-galaxy lenses that are both\ndiscoverable through visual inspection and have valid lens models. This is\nconsistent with theoretical forecasts of $170\\,000$ discoverable galaxy-galaxy\nlenses in Euclid. Our five modelled lenses have Einstein radii in the range\n$0.\\!\\!^{\\prime\\prime}68\\,<\\,\\theta_\\mathrm{E}\\,<1.\\!\\!^{\\prime\\prime}24$, but\ntheir Einstein radius distribution is on the higher side when compared to\ntheoretical forecasts. This suggests that our methodology is likely missing\nsmall Einstein radius systems. Whilst it is implausible to visually inspect the\nfull Euclid data set, our results corroborate the promise that Euclid will\nultimately deliver a sample of around $10^5$ galaxy-scale lenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the ability of the Euclid telescope to detect galaxy-scale\ngravitational lenses. To do so, we perform a systematic visual inspection of\nthe $0.7\\,\\rm{deg}^2$ Euclid ERO data towards the Perseus cluster using both\nthe high-resolution VIS $I_{\\scriptscriptstyle\\rm E}$ band, and the lower\nresolution NISP bands. We inspect every extended source brighter than magnitude\n$23$ in $I_{\\scriptscriptstyle\\rm E}$ with $41$ expert human classifiers. This\namounts to $12\\,086$ stamps of $10^{\\prime\\prime}\\,\\times\\,10^{\\prime\\prime}$.\nWe find $3$ grade A and $13$ grade B candidates. We assess the validity of\nthese $16$ candidates by modelling them and checking that they are consistent\nwith a single source lensed by a plausible mass distribution. Five of the\ncandidates pass this check, five others are rejected by the modelling and six\nare inconclusive. Extrapolating from the five successfully modelled candidates,\nwe infer that the full $14\\,000\\,{\\rm deg}^2$ of the Euclid Wide Survey should\ncontain $100\\,000^{+70\\,000}_{-30\\,000}$ galaxy-galaxy lenses that are both\ndiscoverable through visual inspection and have valid lens models. This is\nconsistent with theoretical forecasts of $170\\,000$ discoverable galaxy-galaxy\nlenses in Euclid. Our five modelled lenses have Einstein radii in the range\n$0.\\!\\!^{\\prime\\prime}68\\,<\\,\\theta_\\mathrm{E}\\,<1.\\!\\!^{\\prime\\prime}24$, but\ntheir Einstein radius distribution is on the higher side when compared to\ntheoretical forecasts. This suggests that our methodology is likely missing\nsmall Einstein radius systems. Whilst it is implausible to visually inspect the\nfull Euclid data set, our results corroborate the promise that Euclid will\nultimately deliver a sample of around $10^5$ galaxy-scale lenses."
                },
                "authors": [
                    {
                        "name": "J. A. Acevedo Barroso"
                    },
                    {
                        "name": "C. M. O'Riordan"
                    },
                    {
                        "name": "B. Cl√©ment"
                    },
                    {
                        "name": "C. Tortora"
                    },
                    {
                        "name": "T. E. Collett"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "R. Gavazzi"
                    },
                    {
                        "name": "R. B. Metcalf"
                    },
                    {
                        "name": "V. Busillo"
                    },
                    {
                        "name": "I. T. Andika"
                    },
                    {
                        "name": "R. Cabanac"
                    },
                    {
                        "name": "H. M. Courtois"
                    },
                    {
                        "name": "J. Crook-Mansour"
                    },
                    {
                        "name": "L. Delchambre"
                    },
                    {
                        "name": "G. Despali"
                    },
                    {
                        "name": "L. R. Ecker"
                    },
                    {
                        "name": "A. Franco"
                    },
                    {
                        "name": "P. Holloway"
                    },
                    {
                        "name": "N. Jackson"
                    },
                    {
                        "name": "K. Jahnke"
                    },
                    {
                        "name": "G. Mahler"
                    },
                    {
                        "name": "L. Marchetti"
                    },
                    {
                        "name": "P. Matavulj"
                    },
                    {
                        "name": "A. Melo"
                    },
                    {
                        "name": "M. Meneghetti"
                    },
                    {
                        "name": "L. A. Moustakas"
                    },
                    {
                        "name": "O. M√ºller"
                    },
                    {
                        "name": "A. A. Nucita"
                    },
                    {
                        "name": "A. Paulino-Afonso"
                    },
                    {
                        "name": "J. Pearson"
                    },
                    {
                        "name": "K. Rojas"
                    },
                    {
                        "name": "C. Scarlata"
                    },
                    {
                        "name": "S. Schuldt"
                    },
                    {
                        "name": "S. Serjeant"
                    },
                    {
                        "name": "D. Sluse"
                    },
                    {
                        "name": "S. H. Suyu"
                    },
                    {
                        "name": "M. Vaccari"
                    },
                    {
                        "name": "A. Verma"
                    },
                    {
                        "name": "G. Vernardos"
                    },
                    {
                        "name": "M. Walmsley"
                    },
                    {
                        "name": "H. Bouy"
                    },
                    {
                        "name": "G. L. Walth"
                    },
                    {
                        "name": "D. M. Powell"
                    },
                    {
                        "name": "M. Bolzonella"
                    },
                    {
                        "name": "J. -C. Cuillandre"
                    },
                    {
                        "name": "M. Kluge"
                    },
                    {
                        "name": "T. Saifollahi"
                    },
                    {
                        "name": "M. Schirmer"
                    },
                    {
                        "name": "C. Stone"
                    },
                    {
                        "name": "A. Acebron"
                    },
                    {
                        "name": "L. Bazzanini"
                    },
                    {
                        "name": "A. D√≠az-S√°nchez"
                    },
                    {
                        "name": "N. B. Hogg"
                    },
                    {
                        "name": "L. V. E. Koopmans"
                    },
                    {
                        "name": "S. Kruk"
                    },
                    {
                        "name": "L. Leuzzi"
                    },
                    {
                        "name": "A. Manj√≥n-Garc√≠a"
                    },
                    {
                        "name": "F. Mannucci"
                    },
                    {
                        "name": "B. C. Nagam"
                    },
                    {
                        "name": "R. Pearce-Casey"
                    },
                    {
                        "name": "L. Scharr√©"
                    },
                    {
                        "name": "J. Wilde"
                    },
                    {
                        "name": "B. Altieri"
                    },
                    {
                        "name": "A. Amara"
                    },
                    {
                        "name": "S. Andreon"
                    },
                    {
                        "name": "N. Auricchio"
                    },
                    {
                        "name": "C. Baccigalupi"
                    },
                    {
                        "name": "M. Baldi"
                    },
                    {
                        "name": "A. Balestra"
                    },
                    {
                        "name": "S. Bardelli"
                    },
                    {
                        "name": "A. Basset"
                    },
                    {
                        "name": "P. Battaglia"
                    },
                    {
                        "name": "R. Bender"
                    },
                    {
                        "name": "D. Bonino"
                    },
                    {
                        "name": "E. Branchini"
                    },
                    {
                        "name": "M. Brescia"
                    },
                    {
                        "name": "J. Brinchmann"
                    },
                    {
                        "name": "A. Caillat"
                    },
                    {
                        "name": "S. Camera"
                    },
                    {
                        "name": "G. P. Candini"
                    },
                    {
                        "name": "V. Capobianco"
                    },
                    {
                        "name": "C. Carbone"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "S. Casas"
                    },
                    {
                        "name": "M. Castellano"
                    },
                    {
                        "name": "G. Castignani"
                    },
                    {
                        "name": "S. Cavuoti"
                    },
                    {
                        "name": "A. Cimatti"
                    },
                    {
                        "name": "C. Colodro-Conde"
                    },
                    {
                        "name": "G. Congedo"
                    },
                    {
                        "name": "C. J. Conselice"
                    },
                    {
                        "name": "L. Conversi"
                    },
                    {
                        "name": "Y. Copin"
                    },
                    {
                        "name": "L. Corcione"
                    },
                    {
                        "name": "M. Cropper"
                    },
                    {
                        "name": "A. Da Silva"
                    },
                    {
                        "name": "H. Degaudenzi"
                    },
                    {
                        "name": "G. De Lucia"
                    },
                    {
                        "name": "J. Dinis"
                    },
                    {
                        "name": "F. Dubath"
                    },
                    {
                        "name": "X. Dupac"
                    },
                    {
                        "name": "S. Dusini"
                    },
                    {
                        "name": "M. Farina"
                    },
                    {
                        "name": "S. Farrens"
                    },
                    {
                        "name": "S. Ferriol"
                    },
                    {
                        "name": "M. Frailis"
                    },
                    {
                        "name": "E. Franceschi"
                    },
                    {
                        "name": "S. Galeotta"
                    },
                    {
                        "name": "B. Garilli"
                    },
                    {
                        "name": "K. George"
                    },
                    {
                        "name": "W. Gillard"
                    },
                    {
                        "name": "B. Gillis"
                    },
                    {
                        "name": "C. Giocoli"
                    },
                    {
                        "name": "P. G√≥mez-Alvarez"
                    },
                    {
                        "name": "A. Grazian"
                    },
                    {
                        "name": "F. Grupp"
                    },
                    {
                        "name": "L. Guzzo"
                    },
                    {
                        "name": "S. V. H. Haugan"
                    },
                    {
                        "name": "H. Hoekstra"
                    },
                    {
                        "name": "W. Holmes"
                    },
                    {
                        "name": "I. Hook"
                    },
                    {
                        "name": "F. Hormuth"
                    },
                    {
                        "name": "A. Hornstrup"
                    },
                    {
                        "name": "M. Jhabvala"
                    },
                    {
                        "name": "B. Joachimi"
                    },
                    {
                        "name": "E. Keih√§nen"
                    },
                    {
                        "name": "S. Kermiche"
                    },
                    {
                        "name": "A. Kiessling"
                    },
                    {
                        "name": "B. Kubik"
                    },
                    {
                        "name": "M. Kunz"
                    },
                    {
                        "name": "H. Kurki-Suonio"
                    },
                    {
                        "name": "D. Le Mignant"
                    },
                    {
                        "name": "S. Ligori"
                    },
                    {
                        "name": "P. B. Lilje"
                    },
                    {
                        "name": "V. Lindholm"
                    },
                    {
                        "name": "I. Lloro"
                    },
                    {
                        "name": "G. Mainetti"
                    },
                    {
                        "name": "E. Maiorano"
                    },
                    {
                        "name": "O. Mansutti"
                    },
                    {
                        "name": "S. Marcin"
                    },
                    {
                        "name": "O. Marggraf"
                    },
                    {
                        "name": "M. Martinelli"
                    },
                    {
                        "name": "N. Martinet"
                    },
                    {
                        "name": "F. Marulli"
                    },
                    {
                        "name": "R. Massey"
                    },
                    {
                        "name": "E. Medinaceli"
                    },
                    {
                        "name": "M. Melchior"
                    },
                    {
                        "name": "Y. Mellier"
                    },
                    {
                        "name": "E. Merlin"
                    },
                    {
                        "name": "G. Meylan"
                    },
                    {
                        "name": "M. Moresco"
                    },
                    {
                        "name": "L. Moscardini"
                    },
                    {
                        "name": "E. Munari"
                    },
                    {
                        "name": "R. Nakajima"
                    },
                    {
                        "name": "C. Neissner"
                    },
                    {
                        "name": "R. C. Nichol"
                    },
                    {
                        "name": "S. -M. Niemi"
                    },
                    {
                        "name": "J. W. Nightingale"
                    },
                    {
                        "name": "C. Padilla"
                    },
                    {
                        "name": "S. Paltani"
                    },
                    {
                        "name": "F. Pasian"
                    },
                    {
                        "name": "K. Pedersen"
                    },
                    {
                        "name": "W. J. Percival"
                    },
                    {
                        "name": "V. Pettorino"
                    },
                    {
                        "name": "S. Pires"
                    },
                    {
                        "name": "G. Polenta"
                    },
                    {
                        "name": "M. Poncet"
                    },
                    {
                        "name": "L. A. Popa"
                    },
                    {
                        "name": "L. Pozzetti"
                    },
                    {
                        "name": "F. Raison"
                    },
                    {
                        "name": "R. Rebolo"
                    },
                    {
                        "name": "A. Renzi"
                    },
                    {
                        "name": "J. Rhodes"
                    },
                    {
                        "name": "G. Riccio"
                    },
                    {
                        "name": "E. Romelli"
                    },
                    {
                        "name": "M. Roncarelli"
                    },
                    {
                        "name": "E. Rossetti"
                    },
                    {
                        "name": "R. Saglia"
                    },
                    {
                        "name": "Z. Sakr"
                    },
                    {
                        "name": "A. G. S√°nchez"
                    },
                    {
                        "name": "D. Sapone"
                    },
                    {
                        "name": "P. Schneider"
                    },
                    {
                        "name": "T. Schrabback"
                    },
                    {
                        "name": "A. Secroun"
                    },
                    {
                        "name": "G. Seidel"
                    },
                    {
                        "name": "S. Serrano"
                    },
                    {
                        "name": "C. Sirignano"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "J. Skottfelt"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "J. Steinwagner"
                    },
                    {
                        "name": "P. Tallada-Cresp√≠"
                    },
                    {
                        "name": "D. Tavagnacco"
                    },
                    {
                        "name": "A. N. Taylor"
                    },
                    {
                        "name": "I. Tereno"
                    },
                    {
                        "name": "R. Toledo-Moreo"
                    },
                    {
                        "name": "F. Torradeflot"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "E. A. Valentijn"
                    },
                    {
                        "name": "L. Valenziano"
                    },
                    {
                        "name": "T. Vassallo"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "J. Weller"
                    },
                    {
                        "name": "E. Zucca"
                    },
                    {
                        "name": "C. Burigana"
                    },
                    {
                        "name": "V. Scottez"
                    },
                    {
                        "name": "M. Viel"
                    }
                ],
                "author_detail": {
                    "name": "M. Viel"
                },
                "arxiv_affiliation": "ICSC - Centro Nazionale di Ricerca in High Performance Computing, Big Data e Quantum Computing, Via Magnanelli 2, Bologna, Italy",
                "author": "M. Viel",
                "arxiv_comment": "21 pages, 20 figures, submitted to A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06211v1",
                "updated": "2024-08-12T15:01:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    15,
                    1,
                    33,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T15:01:33Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    15,
                    1,
                    33,
                    0,
                    225,
                    0
                ],
                "title": "Extreme-based causal effect learning with endogenous exposures and a\n  light-tailed error",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extreme-based causal effect learning with endogenous exposures and a\n  light-tailed error"
                },
                "summary": "Endogeneity poses significant challenges in causal inference across various\nresearch domains. This paper proposes a novel approach to identify and estimate\ncausal effects in the presence of endogeneity. We consider a structural\nequation with endogenous exposures and an additive error term. Assuming the\nlight-tailedness of the error term, we show that the causal effect can be\nidentified by contrasting extreme conditional quantiles of the outcome given\nthe exposures. Unlike many existing results, our identification approach does\nnot rely on additional parametric assumptions or auxiliary variables. Building\non the identification result, we develop an EXtreme-based Causal Effect\nLearning (EXCEL) method that estimates the causal effect using extreme quantile\nregression. We establish the consistency of the EXCEL estimator under a general\nadditive structural equation and demonstrate its asymptotic normality in the\nlinear model setting. These results reveal that extreme quantile regression is\ninvulnerable to endogeneity when the error term is light-tailed, which is not\nappreciated in the literature to our knowledge. The EXCEL method is applied to\ncausal inference problems with invalid instruments to construct a valid\nconfidence set for the causal effect. Simulations and data analysis of an\nautomobile sale dataset show the effectiveness of our method in addressing\nendogeneity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Endogeneity poses significant challenges in causal inference across various\nresearch domains. This paper proposes a novel approach to identify and estimate\ncausal effects in the presence of endogeneity. We consider a structural\nequation with endogenous exposures and an additive error term. Assuming the\nlight-tailedness of the error term, we show that the causal effect can be\nidentified by contrasting extreme conditional quantiles of the outcome given\nthe exposures. Unlike many existing results, our identification approach does\nnot rely on additional parametric assumptions or auxiliary variables. Building\non the identification result, we develop an EXtreme-based Causal Effect\nLearning (EXCEL) method that estimates the causal effect using extreme quantile\nregression. We establish the consistency of the EXCEL estimator under a general\nadditive structural equation and demonstrate its asymptotic normality in the\nlinear model setting. These results reveal that extreme quantile regression is\ninvulnerable to endogeneity when the error term is light-tailed, which is not\nappreciated in the literature to our knowledge. The EXCEL method is applied to\ncausal inference problems with invalid instruments to construct a valid\nconfidence set for the causal effect. Simulations and data analysis of an\nautomobile sale dataset show the effectiveness of our method in addressing\nendogeneity."
                },
                "authors": [
                    {
                        "name": "Ruoyu Wang"
                    },
                    {
                        "name": "Wang Miao"
                    }
                ],
                "author_detail": {
                    "name": "Wang Miao"
                },
                "author": "Wang Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06573v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06573v2",
                "updated": "2024-08-12T14:52:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    52,
                    49,
                    0,
                    225,
                    0
                ],
                "published": "2023-12-11T17:58:06Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    17,
                    58,
                    6,
                    0,
                    345,
                    0
                ],
                "title": "ControlNet-XS: Rethinking the Control of Text-to-Image Diffusion Models\n  as Feedback-Control Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ControlNet-XS: Rethinking the Control of Text-to-Image Diffusion Models\n  as Feedback-Control Systems"
                },
                "summary": "The field of image synthesis has made tremendous strides forward in the last\nyears. Besides defining the desired output image with text-prompts, an\nintuitive approach is to additionally use spatial guidance in form of an image,\nsuch as a depth map. In state-of-the-art approaches, this guidance is realized\nby a separate controlling model that controls a pre-trained image generation\nnetwork, such as a latent diffusion model. Understanding this process from a\ncontrol system perspective shows that it forms a feedback-control system, where\nthe control module receives a feedback signal from the generation process and\nsends a corrective signal back. When analysing existing systems, we observe\nthat the feedback signals are timely sparse and have a small number of bits. As\na consequence, there can be long delays between newly generated features and\nthe respective corrective signals for these features. It is known that this\ndelay is the most unwanted aspect of any control system. In this work, we take\nan existing controlling network (ControlNet) and change the communication\nbetween the controlling network and the generation process to be of\nhigh-frequency and with large-bandwidth. By doing so, we are able to\nconsiderably improve the quality of the generated images, as well as the\nfidelity of the control. Also, the controlling network needs noticeably fewer\nparameters and hence is about twice as fast during inference and training time.\nAnother benefit of small-sized models is that they help to democratise our\nfield and are likely easier to understand. We call our proposed network\nControlNet-XS. When comparing with the state-of-the-art approaches, we\noutperform them for pixel-level guidance, such as depth, canny-edges, and\nsemantic segmentation, and are on a par for loose keypoint-guidance of human\nposes. All code and pre-trained models will be made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of image synthesis has made tremendous strides forward in the last\nyears. Besides defining the desired output image with text-prompts, an\nintuitive approach is to additionally use spatial guidance in form of an image,\nsuch as a depth map. In state-of-the-art approaches, this guidance is realized\nby a separate controlling model that controls a pre-trained image generation\nnetwork, such as a latent diffusion model. Understanding this process from a\ncontrol system perspective shows that it forms a feedback-control system, where\nthe control module receives a feedback signal from the generation process and\nsends a corrective signal back. When analysing existing systems, we observe\nthat the feedback signals are timely sparse and have a small number of bits. As\na consequence, there can be long delays between newly generated features and\nthe respective corrective signals for these features. It is known that this\ndelay is the most unwanted aspect of any control system. In this work, we take\nan existing controlling network (ControlNet) and change the communication\nbetween the controlling network and the generation process to be of\nhigh-frequency and with large-bandwidth. By doing so, we are able to\nconsiderably improve the quality of the generated images, as well as the\nfidelity of the control. Also, the controlling network needs noticeably fewer\nparameters and hence is about twice as fast during inference and training time.\nAnother benefit of small-sized models is that they help to democratise our\nfield and are likely easier to understand. We call our proposed network\nControlNet-XS. When comparing with the state-of-the-art approaches, we\noutperform them for pixel-level guidance, such as depth, canny-edges, and\nsemantic segmentation, and are on a par for loose keypoint-guidance of human\nposes. All code and pre-trained models will be made publicly available."
                },
                "authors": [
                    {
                        "name": "Denis Zavadski"
                    },
                    {
                        "name": "Johann-Friedrich Feiden"
                    },
                    {
                        "name": "Carsten Rother"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Rother"
                },
                "author": "Carsten Rother",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06573v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06573v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03459v2",
                "updated": "2024-08-12T14:45:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    45,
                    34,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-06T22:11:00Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    22,
                    11,
                    0,
                    1,
                    219,
                    0
                ],
                "title": "On the Generalization of Preference Learning with DPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Generalization of Preference Learning with DPO"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities but\noften struggle to align with human preferences, leading to harmful or\nundesirable outputs. Preference learning, which trains models to distinguish\nbetween preferred and non-preferred responses based on human feedback, has\nbecome a crucial component for ensuring that LLMs align with human values.\nDespite the widespread adoption in real-world systems, a thorough theoretical\nunderstanding of the generalization guarantees for these models remain lacking.\nThis paper bridges that gap by introducing a new theoretical framework to\nanalyze the generalization guarantees of models trained with direct preference\noptimization (DPO). While existing generalization theory often focuses on\noverparameterized models achieving near-optimal loss or models independent of\nthe training process, our framework rigorously assesses how well models\ngeneralize after a finite number of gradient steps, reflecting real-world LLM\ntraining practices. By analyzing the reward margin associated with each sample\nand its trajectory throughout training, we can effectively bound the\ngeneralization error. We derive learning guarantees showing that, under\nspecific conditions, models trained with DPO can correctly discern preferred\nresponses on unseen data with high probability. These insights are empirically\nvalidated on contemporary LLMs, underscoring the practical relevance of our\ntheoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities but\noften struggle to align with human preferences, leading to harmful or\nundesirable outputs. Preference learning, which trains models to distinguish\nbetween preferred and non-preferred responses based on human feedback, has\nbecome a crucial component for ensuring that LLMs align with human values.\nDespite the widespread adoption in real-world systems, a thorough theoretical\nunderstanding of the generalization guarantees for these models remain lacking.\nThis paper bridges that gap by introducing a new theoretical framework to\nanalyze the generalization guarantees of models trained with direct preference\noptimization (DPO). While existing generalization theory often focuses on\noverparameterized models achieving near-optimal loss or models independent of\nthe training process, our framework rigorously assesses how well models\ngeneralize after a finite number of gradient steps, reflecting real-world LLM\ntraining practices. By analyzing the reward margin associated with each sample\nand its trajectory throughout training, we can effectively bound the\ngeneralization error. We derive learning guarantees showing that, under\nspecific conditions, models trained with DPO can correctly discern preferred\nresponses on unseen data with high probability. These insights are empirically\nvalidated on contemporary LLMs, underscoring the practical relevance of our\ntheoretical findings."
                },
                "authors": [
                    {
                        "name": "Shawn Im"
                    },
                    {
                        "name": "Yixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Li"
                },
                "author": "Yixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06195v1",
                "updated": "2024-08-12T14:42:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    42,
                    13,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T14:42:13Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    42,
                    13,
                    0,
                    225,
                    0
                ],
                "title": "Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers"
                },
                "summary": "This paper introduces rStar, a self-play mutual reasoning approach that\nsignificantly improves reasoning capabilities of small language models (SLMs)\nwithout fine-tuning or superior models. rStar decouples reasoning into a\nself-play mutual generation-discrimination process. First, a target SLM\naugments the Monte Carlo Tree Search (MCTS) with a rich set of human-like\nreasoning actions to construct higher quality reasoning trajectories. Next,\nanother SLM, with capabilities similar to the target SLM, acts as a\ndiscriminator to verify each trajectory generated by the target SLM. The\nmutually agreed reasoning trajectories are considered mutual consistent, thus\nare more likely to be correct. Extensive experiments across five SLMs\ndemonstrate rStar can effectively solve diverse reasoning problems, including\nGSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K\naccuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for\nMistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct. Code will be\navailable at https://github.com/zhentingqi/rStar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces rStar, a self-play mutual reasoning approach that\nsignificantly improves reasoning capabilities of small language models (SLMs)\nwithout fine-tuning or superior models. rStar decouples reasoning into a\nself-play mutual generation-discrimination process. First, a target SLM\naugments the Monte Carlo Tree Search (MCTS) with a rich set of human-like\nreasoning actions to construct higher quality reasoning trajectories. Next,\nanother SLM, with capabilities similar to the target SLM, acts as a\ndiscriminator to verify each trajectory generated by the target SLM. The\nmutually agreed reasoning trajectories are considered mutual consistent, thus\nare more likely to be correct. Extensive experiments across five SLMs\ndemonstrate rStar can effectively solve diverse reasoning problems, including\nGSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K\naccuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for\nMistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct. Code will be\navailable at https://github.com/zhentingqi/rStar."
                },
                "authors": [
                    {
                        "name": "Zhenting Qi"
                    },
                    {
                        "name": "Mingyuan Ma"
                    },
                    {
                        "name": "Jiahang Xu"
                    },
                    {
                        "name": "Li Lyna Zhang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06189v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06189v1",
                "updated": "2024-08-12T14:40:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    40,
                    9,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T14:40:09Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    40,
                    9,
                    0,
                    225,
                    0
                ],
                "title": "The Planck SZiFi catalogues: a new set of Planck catalogues of\n  Sunyaev-Zeldovich-detected galaxy clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Planck SZiFi catalogues: a new set of Planck catalogues of\n  Sunyaev-Zeldovich-detected galaxy clusters"
                },
                "summary": "We introduce the Planck SZiFi catalogues, a new set of 10 catalogues of\ngalaxy clusters detected through their thermal Sunyaev-Zeldovich (tSZ)\nsignature. The catalogues are produced by applying the SZiFi cluster finder to\nthe Planck PR3 temperature data down to a signal-to-noise threshold of 5. They\nspan three frequency channel combinations (100-857 GHz, 100-545 GHz, and\n100-353 GHz) and 7 of them are constructed by spectrally deprojecting the\nCosmic Infrared Background (CIB). This approach allows us, for the first time\nin the context of cluster finding, to carefully assess the impact of the\ncluster-correlated CIB on the recovered cluster tSZ observables, which we find\nto be negligible. In addition, we quantify the impact of the relativistic\ncorrections to the tSZ signal, finding them to be at the 5-10% level for the\ncluster tSZ amplitude but negligible for the signal-to-noise. We compile our\ncatalogues into a single Planck SZiFi master catalogue containing a total of\n1499 detections. We cross-match the master catalogue with several external tSZ\nand X-ray cluster catalogues, setting a lower bound on the purity of our\nbaseline catalogue of 95% and 99% at a minimum signal-to-noise of 5 and 6,\nrespectively. We validate our cluster detection pipeline by applying it to\nsynthetic observations, recovering cluster number counts for which we are able\nto produce a theoretical prediction that accurately describes them. This\nvalidation exercise indicates that our catalogues are well-suited for\ncosmological inference. The Planck SZiFi master catalogue will become publicly\navailable at\ngithub.com/inigozubeldia/szifi/tree/main/planck_szifi_master_catalogue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Planck SZiFi catalogues, a new set of 10 catalogues of\ngalaxy clusters detected through their thermal Sunyaev-Zeldovich (tSZ)\nsignature. The catalogues are produced by applying the SZiFi cluster finder to\nthe Planck PR3 temperature data down to a signal-to-noise threshold of 5. They\nspan three frequency channel combinations (100-857 GHz, 100-545 GHz, and\n100-353 GHz) and 7 of them are constructed by spectrally deprojecting the\nCosmic Infrared Background (CIB). This approach allows us, for the first time\nin the context of cluster finding, to carefully assess the impact of the\ncluster-correlated CIB on the recovered cluster tSZ observables, which we find\nto be negligible. In addition, we quantify the impact of the relativistic\ncorrections to the tSZ signal, finding them to be at the 5-10% level for the\ncluster tSZ amplitude but negligible for the signal-to-noise. We compile our\ncatalogues into a single Planck SZiFi master catalogue containing a total of\n1499 detections. We cross-match the master catalogue with several external tSZ\nand X-ray cluster catalogues, setting a lower bound on the purity of our\nbaseline catalogue of 95% and 99% at a minimum signal-to-noise of 5 and 6,\nrespectively. We validate our cluster detection pipeline by applying it to\nsynthetic observations, recovering cluster number counts for which we are able\nto produce a theoretical prediction that accurately describes them. This\nvalidation exercise indicates that our catalogues are well-suited for\ncosmological inference. The Planck SZiFi master catalogue will become publicly\navailable at\ngithub.com/inigozubeldia/szifi/tree/main/planck_szifi_master_catalogue."
                },
                "authors": [
                    {
                        "name": "√ç√±igo Zubeldia"
                    },
                    {
                        "name": "Jean-Baptiste Melin"
                    },
                    {
                        "name": "Jens Chluba"
                    },
                    {
                        "name": "Richard Battye"
                    }
                ],
                "author_detail": {
                    "name": "Richard Battye"
                },
                "author": "Richard Battye",
                "arxiv_comment": "32 pages, 22 figures, submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06189v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06189v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17442v2",
                "updated": "2024-08-12T14:36:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    36,
                    32,
                    0,
                    225,
                    0
                ],
                "published": "2024-02-27T11:57:28Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    11,
                    57,
                    28,
                    1,
                    58,
                    0
                ],
                "title": "Insights from the Usage of the Ansible Lightspeed Code Completion\n  Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insights from the Usage of the Ansible Lightspeed Code Completion\n  Service"
                },
                "summary": "The availability of Large Language Models (LLMs) which can generate code, has\nmade it possible to create tools that improve developer productivity.\nIntegrated development environments or IDEs which developers use to write\nsoftware are often used as an interface to interact with LLMs. Although many\nsuch tools have been released, almost all of them focus on general-purpose\nprogramming languages. Domain-specific languages, such as those crucial for\nInformation Technology (IT) automation, have not received much attention.\nAnsible is one such YAML-based IT automation-specific language. Ansible\nLightspeed is an LLM-based service designed explicitly to generate Ansible YAML\ngiven natural language prompt.\n  This paper first presents the design and implementation of the Ansible\nLightspeed service. We then evaluate its utility to developers using diverse\nindicators, including extended utilization, analysis of user rejected\nsuggestions, as well as analysis of user sentiments. The analysis is based on\ndata collected for 10,696 real users including 3,910 returning users. The code\nfor Ansible Lightspeed service and the analysis framework is made available for\nothers to use.\n  To our knowledge, our study is the first to involve thousands of users in\nevaluating code assistants for domain-specific languages. We propose an\nimproved version of user acceptance rate and we are the first code completion\ntool to present N-Day user retention figures. With our findings we provide\ninsights into the effectiveness of small, dedicated models in a domain-specific\ncontext. We hope this work serves as a reference for software engineering and\nmachine learning researchers exploring code completion services for\ndomain-specific languages in particular and programming languages in general.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The availability of Large Language Models (LLMs) which can generate code, has\nmade it possible to create tools that improve developer productivity.\nIntegrated development environments or IDEs which developers use to write\nsoftware are often used as an interface to interact with LLMs. Although many\nsuch tools have been released, almost all of them focus on general-purpose\nprogramming languages. Domain-specific languages, such as those crucial for\nInformation Technology (IT) automation, have not received much attention.\nAnsible is one such YAML-based IT automation-specific language. Ansible\nLightspeed is an LLM-based service designed explicitly to generate Ansible YAML\ngiven natural language prompt.\n  This paper first presents the design and implementation of the Ansible\nLightspeed service. We then evaluate its utility to developers using diverse\nindicators, including extended utilization, analysis of user rejected\nsuggestions, as well as analysis of user sentiments. The analysis is based on\ndata collected for 10,696 real users including 3,910 returning users. The code\nfor Ansible Lightspeed service and the analysis framework is made available for\nothers to use.\n  To our knowledge, our study is the first to involve thousands of users in\nevaluating code assistants for domain-specific languages. We propose an\nimproved version of user acceptance rate and we are the first code completion\ntool to present N-Day user retention figures. With our findings we provide\ninsights into the effectiveness of small, dedicated models in a domain-specific\ncontext. We hope this work serves as a reference for software engineering and\nmachine learning researchers exploring code completion services for\ndomain-specific languages in particular and programming languages in general."
                },
                "authors": [
                    {
                        "name": "Priyam Sahoo"
                    },
                    {
                        "name": "Saurabh Pujar"
                    },
                    {
                        "name": "Ganesh Nalawade"
                    },
                    {
                        "name": "Richard Gebhardt"
                    },
                    {
                        "name": "Louis Mandel"
                    },
                    {
                        "name": "Luca Buratti"
                    }
                ],
                "author_detail": {
                    "name": "Luca Buratti"
                },
                "author": "Luca Buratti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06186v1",
                "updated": "2024-08-12T14:34:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    34,
                    6,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T14:34:06Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    34,
                    6,
                    0,
                    225,
                    0
                ],
                "title": "Improving Structural Diversity of Blackbox LLMs via\n  Chain-of-Specification Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Structural Diversity of Blackbox LLMs via\n  Chain-of-Specification Prompting"
                },
                "summary": "The capability to generate diverse text is a key challenge facing large\nlanguage models (LLMs). Thus far, diversity has been studied via metrics such\nas $n$-gram diversity or diversity of BERT embeddings. However, for these kinds\nof diversity, the user has little control over the dimensions along which\ndiversity is considered. For example, in the poetry domain, one might desire\ndiversity in terms of rhyme and meter, whereas in the code domain, one might\ndesire diversity in terms of the kinds of expressions used to solve a problem.\nWe propose a diversity metric called structural diversity, where the user\nprovides a mapping from generated text to features capturing the kinds of\ndiversity that they care about. In addition, we propose a novel strategy called\nchain-of-specification (CoS) prompting for improving diversity by first having\nthe LLM generate a specification encoding one instance of structural features,\nand then prompting the LLM to generate text that satisfies these features;\nnotably, our strategy works with blackbox LLMs. In our experiments, we show\nthat for structural diversity in the poetry and code domains, CoS significantly\nimproves diversity compared to several baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capability to generate diverse text is a key challenge facing large\nlanguage models (LLMs). Thus far, diversity has been studied via metrics such\nas $n$-gram diversity or diversity of BERT embeddings. However, for these kinds\nof diversity, the user has little control over the dimensions along which\ndiversity is considered. For example, in the poetry domain, one might desire\ndiversity in terms of rhyme and meter, whereas in the code domain, one might\ndesire diversity in terms of the kinds of expressions used to solve a problem.\nWe propose a diversity metric called structural diversity, where the user\nprovides a mapping from generated text to features capturing the kinds of\ndiversity that they care about. In addition, we propose a novel strategy called\nchain-of-specification (CoS) prompting for improving diversity by first having\nthe LLM generate a specification encoding one instance of structural features,\nand then prompting the LLM to generate text that satisfies these features;\nnotably, our strategy works with blackbox LLMs. In our experiments, we show\nthat for structural diversity in the poetry and code domains, CoS significantly\nimproves diversity compared to several baselines."
                },
                "authors": [
                    {
                        "name": "Halley Young"
                    },
                    {
                        "name": "Yimeng Zeng"
                    },
                    {
                        "name": "Jacob Gardner"
                    },
                    {
                        "name": "Osbert Bastani"
                    }
                ],
                "author_detail": {
                    "name": "Osbert Bastani"
                },
                "author": "Osbert Bastani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03745v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03745v3",
                "updated": "2024-08-12T14:13:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    13,
                    15,
                    0,
                    225,
                    0
                ],
                "published": "2024-04-04T18:34:32Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    18,
                    34,
                    32,
                    3,
                    95,
                    0
                ],
                "title": "Fakes of Varying Shades: How Warning Affects Human Perception and\n  Engagement Regarding LLM Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fakes of Varying Shades: How Warning Affects Human Perception and\n  Engagement Regarding LLM Hallucinations"
                },
                "summary": "The widespread adoption and transformative effects of large language models\n(LLMs) have sparked concerns regarding their capacity to produce inaccurate and\nfictitious content, referred to as `hallucinations'. Given the potential risks\nassociated with hallucinations, humans should be able to identify them. This\nresearch aims to understand the human perception of LLM hallucinations by\nsystematically varying the degree of hallucination (genuine, minor\nhallucination, major hallucination) and examining its interaction with warning\n(i.e., a warning of potential inaccuracies: absent vs. present). Participants\n(N=419) from Prolific rated the perceived accuracy and engaged with content\n(e.g., like, dislike, share) in a Q/A format. Participants ranked content as\ntruthful in the order of genuine, minor hallucination, and major hallucination,\nand user engagement behaviors mirrored this pattern. More importantly, we\nobserved that warning improved the detection of hallucination without\nsignificantly affecting the perceived truthfulness of genuine content. We\nconclude by offering insights for future tools to aid human detection of\nhallucinations. All survey materials, demographic questions, and post-session\nquestions are available at:\nhttps://github.com/MahjabinNahar/fakes-of-varying-shades-survey-materials",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption and transformative effects of large language models\n(LLMs) have sparked concerns regarding their capacity to produce inaccurate and\nfictitious content, referred to as `hallucinations'. Given the potential risks\nassociated with hallucinations, humans should be able to identify them. This\nresearch aims to understand the human perception of LLM hallucinations by\nsystematically varying the degree of hallucination (genuine, minor\nhallucination, major hallucination) and examining its interaction with warning\n(i.e., a warning of potential inaccuracies: absent vs. present). Participants\n(N=419) from Prolific rated the perceived accuracy and engaged with content\n(e.g., like, dislike, share) in a Q/A format. Participants ranked content as\ntruthful in the order of genuine, minor hallucination, and major hallucination,\nand user engagement behaviors mirrored this pattern. More importantly, we\nobserved that warning improved the detection of hallucination without\nsignificantly affecting the perceived truthfulness of genuine content. We\nconclude by offering insights for future tools to aid human detection of\nhallucinations. All survey materials, demographic questions, and post-session\nquestions are available at:\nhttps://github.com/MahjabinNahar/fakes-of-varying-shades-survey-materials"
                },
                "authors": [
                    {
                        "name": "Mahjabin Nahar"
                    },
                    {
                        "name": "Haeseung Seo"
                    },
                    {
                        "name": "Eun-Ju Lee"
                    },
                    {
                        "name": "Aiping Xiong"
                    },
                    {
                        "name": "Dongwon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongwon Lee"
                },
                "author": "Dongwon Lee",
                "arxiv_comment": "Accepted at COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03745v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03745v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04660v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04660v2",
                "updated": "2024-08-12T14:12:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    12,
                    23,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-05T20:01:10Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    20,
                    1,
                    10,
                    0,
                    218,
                    0
                ],
                "title": "XMainframe: A Large Language Model for Mainframe Modernization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XMainframe: A Large Language Model for Mainframe Modernization"
                },
                "summary": "Mainframe operating systems, despite their inception in the 1940s, continue\nto support critical sectors like finance and government. However, these systems\nare often viewed as outdated, requiring extensive maintenance and\nmodernization. Addressing this challenge necessitates innovative tools that can\nunderstand and interact with legacy codebases. To this end, we introduce\nXMainframe, a state-of-the-art large language model (LLM) specifically designed\nwith knowledge of mainframe legacy systems and COBOL codebases. Our solution\ninvolves the creation of an extensive data collection pipeline to produce\nhigh-quality training datasets, enhancing XMainframe's performance in this\nspecialized domain. Additionally, we present MainframeBench, a comprehensive\nbenchmark for assessing mainframe knowledge, including multiple-choice\nquestions, question answering, and COBOL code summarization. Our empirical\nevaluations demonstrate that XMainframe consistently outperforms existing\nstate-of-the-art LLMs across these tasks. Specifically, XMainframe achieves 30%\nhigher accuracy than DeepSeek-Coder on multiple-choice questions, doubles the\nBLEU score of Mixtral-Instruct 8x7B on question answering, and scores six times\nhigher than GPT-3.5 on COBOL summarization. Our work highlights the potential\nof XMainframe to drive significant advancements in managing and modernizing\nlegacy systems, thereby enhancing productivity and saving time for software\ndevelopers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe operating systems, despite their inception in the 1940s, continue\nto support critical sectors like finance and government. However, these systems\nare often viewed as outdated, requiring extensive maintenance and\nmodernization. Addressing this challenge necessitates innovative tools that can\nunderstand and interact with legacy codebases. To this end, we introduce\nXMainframe, a state-of-the-art large language model (LLM) specifically designed\nwith knowledge of mainframe legacy systems and COBOL codebases. Our solution\ninvolves the creation of an extensive data collection pipeline to produce\nhigh-quality training datasets, enhancing XMainframe's performance in this\nspecialized domain. Additionally, we present MainframeBench, a comprehensive\nbenchmark for assessing mainframe knowledge, including multiple-choice\nquestions, question answering, and COBOL code summarization. Our empirical\nevaluations demonstrate that XMainframe consistently outperforms existing\nstate-of-the-art LLMs across these tasks. Specifically, XMainframe achieves 30%\nhigher accuracy than DeepSeek-Coder on multiple-choice questions, doubles the\nBLEU score of Mixtral-Instruct 8x7B on question answering, and scores six times\nhigher than GPT-3.5 on COBOL summarization. Our work highlights the potential\nof XMainframe to drive significant advancements in managing and modernizing\nlegacy systems, thereby enhancing productivity and saving time for software\ndevelopers."
                },
                "authors": [
                    {
                        "name": "Anh T. V. Dau"
                    },
                    {
                        "name": "Hieu Trung Dao"
                    },
                    {
                        "name": "Anh Tuan Nguyen"
                    },
                    {
                        "name": "Hieu Trung Tran"
                    },
                    {
                        "name": "Phong X. Nguyen"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04660v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04660v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10620v2",
                "updated": "2024-08-12T14:07:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    7,
                    32,
                    0,
                    225,
                    0
                ],
                "published": "2024-05-17T08:33:27Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    8,
                    33,
                    27,
                    4,
                    138,
                    0
                ],
                "title": "MC-GPT: Empowering Vision-and-Language Navigation with Memory Map and\n  Reasoning Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC-GPT: Empowering Vision-and-Language Navigation with Memory Map and\n  Reasoning Chains"
                },
                "summary": "In the Vision-and-Language Navigation (VLN) task, the agent is required to\nnavigate to a destination following a natural language instruction. While\nlearning-based approaches have been a major solution to the task, they suffer\nfrom high training costs and lack of interpretability. Recently, Large Language\nModels (LLMs) have emerged as a promising tool for VLN due to their strong\ngeneralization capabilities. However, existing LLM-based methods face\nlimitations in memory construction and diversity of navigation strategies. To\naddress these challenges, we propose a suite of techniques. Firstly, we\nintroduce a method to maintain a topological map that stores navigation\nhistory, retaining information about viewpoints, objects, and their spatial\nrelationships. This map also serves as a global action space. Additionally, we\npresent a Navigation Chain of Thoughts module, leveraging human navigation\nexamples to enrich navigation strategy diversity. Finally, we establish a\npipeline that integrates navigational memory and strategies with perception and\naction prediction modules. Experimental results on the REVERIE and R2R datasets\nshow that our method effectively enhances the navigation ability of the LLM and\nimproves the interpretability of navigation reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the Vision-and-Language Navigation (VLN) task, the agent is required to\nnavigate to a destination following a natural language instruction. While\nlearning-based approaches have been a major solution to the task, they suffer\nfrom high training costs and lack of interpretability. Recently, Large Language\nModels (LLMs) have emerged as a promising tool for VLN due to their strong\ngeneralization capabilities. However, existing LLM-based methods face\nlimitations in memory construction and diversity of navigation strategies. To\naddress these challenges, we propose a suite of techniques. Firstly, we\nintroduce a method to maintain a topological map that stores navigation\nhistory, retaining information about viewpoints, objects, and their spatial\nrelationships. This map also serves as a global action space. Additionally, we\npresent a Navigation Chain of Thoughts module, leveraging human navigation\nexamples to enrich navigation strategy diversity. Finally, we establish a\npipeline that integrates navigational memory and strategies with perception and\naction prediction modules. Experimental results on the REVERIE and R2R datasets\nshow that our method effectively enhances the navigation ability of the LLM and\nimproves the interpretability of navigation reasoning."
                },
                "authors": [
                    {
                        "name": "Zhaohuan Zhan"
                    },
                    {
                        "name": "Lisha Yu"
                    },
                    {
                        "name": "Sijie Yu"
                    },
                    {
                        "name": "Guang Tan"
                    }
                ],
                "author_detail": {
                    "name": "Guang Tan"
                },
                "author": "Guang Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00487v2",
                "updated": "2024-08-12T14:06:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    6,
                    48,
                    0,
                    225,
                    0
                ],
                "published": "2024-06-29T16:34:23Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    16,
                    34,
                    23,
                    5,
                    181,
                    0
                ],
                "title": "It's Morphing Time: Unleashing the Potential of Multiple LLMs via\n  Multi-objective Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It's Morphing Time: Unleashing the Potential of Multiple LLMs via\n  Multi-objective Optimization"
                },
                "summary": "In this paper, we introduce a novel approach for large language model merging\nvia black-box multi-objective optimization algorithms. The goal of model\nmerging is to combine multiple models, each excelling in different tasks, into\na single model that outperforms any of the individual source models. However,\nmodel merging faces two significant challenges: First, existing methods rely\nheavily on human intuition and customized strategies to tackle multiple tasks.\nSecond, it's difficult to search for the great model merging configuration in\nlimited evaluations. To address these challenges, we propose a multi-objective\noptimization based model merging method named MM-MO. The proposed method can\nautomatically search merging configurations for multiple tasks with\nmulti-objective optimization algorithms. Moreover, to obtain high-quality model\nmerging configurations within a limited number of evaluation iterations, we\nhave made several improvements to multi-objective Bayesian optimization\nspecifically for model merging scenarios. First, we introduced a weak-to-strong\nmethod to improve the acquisition strategy. Second, we employed Fisher\ninformation to select configurations, further increasing the chances of\ndiscovering superior model merging configurations. Third, we designed a\nsparsity metric as an additional optimization objective to enhance the model's\ngeneralization performance across different tasks. We conducted comprehensive\nexperiments with other mainstream model merging methods, demonstrating that our\nmethod consistently outperforms them. Moreover, performance improvements are\nobserved even on the tasks not explicitly targeted as optimization objectives,\nindicating that our method enhances the overall potential of the model. ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a novel approach for large language model merging\nvia black-box multi-objective optimization algorithms. The goal of model\nmerging is to combine multiple models, each excelling in different tasks, into\na single model that outperforms any of the individual source models. However,\nmodel merging faces two significant challenges: First, existing methods rely\nheavily on human intuition and customized strategies to tackle multiple tasks.\nSecond, it's difficult to search for the great model merging configuration in\nlimited evaluations. To address these challenges, we propose a multi-objective\noptimization based model merging method named MM-MO. The proposed method can\nautomatically search merging configurations for multiple tasks with\nmulti-objective optimization algorithms. Moreover, to obtain high-quality model\nmerging configurations within a limited number of evaluation iterations, we\nhave made several improvements to multi-objective Bayesian optimization\nspecifically for model merging scenarios. First, we introduced a weak-to-strong\nmethod to improve the acquisition strategy. Second, we employed Fisher\ninformation to select configurations, further increasing the chances of\ndiscovering superior model merging configurations. Third, we designed a\nsparsity metric as an additional optimization objective to enhance the model's\ngeneralization performance across different tasks. We conducted comprehensive\nexperiments with other mainstream model merging methods, demonstrating that our\nmethod consistently outperforms them. Moreover, performance improvements are\nobserved even on the tasks not explicitly targeted as optimization objectives,\nindicating that our method enhances the overall potential of the model. ..."
                },
                "authors": [
                    {
                        "name": "Bingdong Li"
                    },
                    {
                        "name": "Zixiang Di"
                    },
                    {
                        "name": "Yanting Yang"
                    },
                    {
                        "name": "Hong Qian"
                    },
                    {
                        "name": "Peng Yang"
                    },
                    {
                        "name": "Hao Hao"
                    },
                    {
                        "name": "Ke Tang"
                    },
                    {
                        "name": "Aimin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Aimin Zhou"
                },
                "author": "Aimin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05025v2",
                "updated": "2024-08-12T13:57:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    13,
                    57,
                    42,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-09T12:26:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    12,
                    26,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "Rag and Roll: An End-to-End Evaluation of Indirect Prompt Manipulations\n  in LLM-based Application Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rag and Roll: An End-to-End Evaluation of Indirect Prompt Manipulations\n  in LLM-based Application Frameworks"
                },
                "summary": "Retrieval Augmented Generation (RAG) is a technique commonly used to equip\nmodels with out of distribution knowledge. This process involves collecting,\nindexing, retrieving, and providing information to an LLM for generating\nresponses. Despite its growing popularity due to its flexibility and low cost,\nthe security implications of RAG have not been extensively studied. The data\nfor such systems are often collected from public sources, providing an attacker\na gateway for indirect prompt injections to manipulate the responses of the\nmodel. In this paper, we investigate the security of RAG systems against\nend-to-end indirect prompt manipulations. First, we review existing RAG\nframework pipelines, deriving a prototypical architecture and identifying\ncritical parameters. We then examine prior works searching for techniques that\nattackers can use to perform indirect prompt manipulations. Finally, we\nimplemented Rag 'n Roll, a framework to determine the effectiveness of attacks\nagainst end-to-end RAG applications. Our results show that existing attacks are\nmostly optimized to boost the ranking of malicious documents during the\nretrieval phase. However, a higher rank does not immediately translate into a\nreliable attack. Most attacks, against various configurations, settle around a\n40% success rate, which could rise to 60% when considering ambiguous answers as\nsuccessful attacks (those that include the expected benign one as well).\nAdditionally, when using unoptimized documents, attackers deploying two of them\n(or more) for a target query can achieve similar results as those using\noptimized ones. Finally, exploration of the configuration space of a RAG showed\nlimited impact in thwarting the attacks, where the most successful combination\nseverely undermines functionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) is a technique commonly used to equip\nmodels with out of distribution knowledge. This process involves collecting,\nindexing, retrieving, and providing information to an LLM for generating\nresponses. Despite its growing popularity due to its flexibility and low cost,\nthe security implications of RAG have not been extensively studied. The data\nfor such systems are often collected from public sources, providing an attacker\na gateway for indirect prompt injections to manipulate the responses of the\nmodel. In this paper, we investigate the security of RAG systems against\nend-to-end indirect prompt manipulations. First, we review existing RAG\nframework pipelines, deriving a prototypical architecture and identifying\ncritical parameters. We then examine prior works searching for techniques that\nattackers can use to perform indirect prompt manipulations. Finally, we\nimplemented Rag 'n Roll, a framework to determine the effectiveness of attacks\nagainst end-to-end RAG applications. Our results show that existing attacks are\nmostly optimized to boost the ranking of malicious documents during the\nretrieval phase. However, a higher rank does not immediately translate into a\nreliable attack. Most attacks, against various configurations, settle around a\n40% success rate, which could rise to 60% when considering ambiguous answers as\nsuccessful attacks (those that include the expected benign one as well).\nAdditionally, when using unoptimized documents, attackers deploying two of them\n(or more) for a target query can achieve similar results as those using\noptimized ones. Finally, exploration of the configuration space of a RAG showed\nlimited impact in thwarting the attacks, where the most successful combination\nseverely undermines functionality."
                },
                "authors": [
                    {
                        "name": "Gianluca De Stefano"
                    },
                    {
                        "name": "Lea Sch√∂nherr"
                    },
                    {
                        "name": "Giancarlo Pellegrino"
                    }
                ],
                "author_detail": {
                    "name": "Giancarlo Pellegrino"
                },
                "author": "Giancarlo Pellegrino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06152v1",
                "updated": "2024-08-12T13:48:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    13,
                    48,
                    6,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T13:48:06Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    13,
                    48,
                    6,
                    0,
                    225,
                    0
                ],
                "title": "Palantir: Towards Efficient Super Resolution for Ultra-high-definition\n  Live Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palantir: Towards Efficient Super Resolution for Ultra-high-definition\n  Live Streaming"
                },
                "summary": "Neural enhancement through super-resolution deep neural networks opens up new\npossibilities for ultra-high-definition live streaming over existing encoding\nand networking infrastructure. Yet, the heavy SR DNN inference overhead leads\nto severe deployment challenges. To reduce the overhead, existing systems\npropose to apply DNN-based SR only on selected anchor frames while upscaling\nnon-anchor frames via the lightweight reusing-based SR approach. However,\nframe-level scheduling is coarse-grained and fails to deliver optimal\nefficiency. In this work, we propose Palantir, the first neural-enhanced UHD\nlive streaming system with fine-grained patch-level scheduling. In the\npresented solutions, two novel techniques are incorporated to make good\nscheduling decisions for inference overhead optimization and reduce the\nscheduling latency. Firstly, under the guidance of our pioneering and\ntheoretical analysis, Palantir constructs a directed acyclic graph (DAG) for\nlightweight yet accurate quality estimation under any possible anchor patch\nset. Secondly, to further optimize the scheduling latency, Palantir improves\nparallelizability by refactoring the computation subprocedure of the estimation\nprocess into a sparse matrix-matrix multiplication operation. The evaluation\nresults suggest that Palantir incurs a negligible scheduling latency accounting\nfor less than 5.7% of the end-to-end latency requirement. When compared to the\nstate-of-the-art real-time frame-level scheduling strategy, Palantir reduces\nthe energy overhead of SR-integrated mobile clients by 38.1% at most (and 22.4%\non average) and the monetary costs of cloud-based SR by 80.1% at most (and\n38.4% on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural enhancement through super-resolution deep neural networks opens up new\npossibilities for ultra-high-definition live streaming over existing encoding\nand networking infrastructure. Yet, the heavy SR DNN inference overhead leads\nto severe deployment challenges. To reduce the overhead, existing systems\npropose to apply DNN-based SR only on selected anchor frames while upscaling\nnon-anchor frames via the lightweight reusing-based SR approach. However,\nframe-level scheduling is coarse-grained and fails to deliver optimal\nefficiency. In this work, we propose Palantir, the first neural-enhanced UHD\nlive streaming system with fine-grained patch-level scheduling. In the\npresented solutions, two novel techniques are incorporated to make good\nscheduling decisions for inference overhead optimization and reduce the\nscheduling latency. Firstly, under the guidance of our pioneering and\ntheoretical analysis, Palantir constructs a directed acyclic graph (DAG) for\nlightweight yet accurate quality estimation under any possible anchor patch\nset. Secondly, to further optimize the scheduling latency, Palantir improves\nparallelizability by refactoring the computation subprocedure of the estimation\nprocess into a sparse matrix-matrix multiplication operation. The evaluation\nresults suggest that Palantir incurs a negligible scheduling latency accounting\nfor less than 5.7% of the end-to-end latency requirement. When compared to the\nstate-of-the-art real-time frame-level scheduling strategy, Palantir reduces\nthe energy overhead of SR-integrated mobile clients by 38.1% at most (and 22.4%\non average) and the monetary costs of cloud-based SR by 80.1% at most (and\n38.4% on average)."
                },
                "authors": [
                    {
                        "name": "Xinqi Jin"
                    },
                    {
                        "name": "Zhui Zhu"
                    },
                    {
                        "name": "Xikai Sun"
                    },
                    {
                        "name": "Fan Dang"
                    },
                    {
                        "name": "Jiangchuan Liu"
                    },
                    {
                        "name": "Jingao Xu"
                    },
                    {
                        "name": "Kebin Liu"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Yunhao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunhao Liu"
                },
                "author": "Yunhao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06149v1",
                "updated": "2024-08-12T13:43:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    13,
                    43,
                    41,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T13:43:41Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    13,
                    43,
                    41,
                    0,
                    225,
                    0
                ],
                "title": "On the Peril of Inferring Phytoplankton Properties from Remote-Sensing\n  Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Peril of Inferring Phytoplankton Properties from Remote-Sensing\n  Observations"
                },
                "summary": "Since 1978, sensors on remote-sensing satellites have provided global,\nmulti-band images at optical wavelengths to assess ocean color. In parallel,\nsophisticated radiative transfer models account for attenuation and emission by\nthe Earth's atmosphere and ocean, thereby estimating the water-leaving radiance\nor and remote-sensing reflectance Rrs. From these Rrs measurements, estimates\nof the absorption and scattering by seawater are inferred. We emphasize an\ninherent, physical degeneracy in the radiative transfer equation that relates\nRrs to the absorption and backscattering coefficients a and b_b, aka inherent\noptical properties (IOPs). Because Rrs depends solely on the ratio of b_b to a,\nmeaning one cannot retrieve independent functions for the non-water IOPs, a_nw\nand b_bnw, without a priori knowledge. Moreover, water generally dominates\nscattering at blue wavelengths and absorption at red wavelengths, further\nlimiting retrievals of IOPs in the presence of noise. We demonstrate that all\nprevious and current multi-spectral satellite observations lack the statistical\npower to measure more than 3 parameters total to describe a_nw and b_bnw. Due\nto the ubiquitous exponential-like absorption by color dissolved organic matter\nat short wavelengths (l<500nm), multi-spectral Rrs do not permit the detection\nof phytoplankton absorption a_ph without very strict priors. Furthermore, such\npriors lead to biased and uncertain retrievals of a_ph. Hyperspectral\nobservations may recover a 4th and possibly 5th parameter describing only one\nor two aspects of the complexity of a_ph. These results cast doubt on decades\nof literature on IOP retrievals, including estimates of phytoplankton growth\nand biomass. We further conclude that NASA/PACE will greatly enhance our\nability to measure the phytoplankton biomass of Earth, but challenges remain in\nresolving the IOPs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since 1978, sensors on remote-sensing satellites have provided global,\nmulti-band images at optical wavelengths to assess ocean color. In parallel,\nsophisticated radiative transfer models account for attenuation and emission by\nthe Earth's atmosphere and ocean, thereby estimating the water-leaving radiance\nor and remote-sensing reflectance Rrs. From these Rrs measurements, estimates\nof the absorption and scattering by seawater are inferred. We emphasize an\ninherent, physical degeneracy in the radiative transfer equation that relates\nRrs to the absorption and backscattering coefficients a and b_b, aka inherent\noptical properties (IOPs). Because Rrs depends solely on the ratio of b_b to a,\nmeaning one cannot retrieve independent functions for the non-water IOPs, a_nw\nand b_bnw, without a priori knowledge. Moreover, water generally dominates\nscattering at blue wavelengths and absorption at red wavelengths, further\nlimiting retrievals of IOPs in the presence of noise. We demonstrate that all\nprevious and current multi-spectral satellite observations lack the statistical\npower to measure more than 3 parameters total to describe a_nw and b_bnw. Due\nto the ubiquitous exponential-like absorption by color dissolved organic matter\nat short wavelengths (l<500nm), multi-spectral Rrs do not permit the detection\nof phytoplankton absorption a_ph without very strict priors. Furthermore, such\npriors lead to biased and uncertain retrievals of a_ph. Hyperspectral\nobservations may recover a 4th and possibly 5th parameter describing only one\nor two aspects of the complexity of a_ph. These results cast doubt on decades\nof literature on IOP retrievals, including estimates of phytoplankton growth\nand biomass. We further conclude that NASA/PACE will greatly enhance our\nability to measure the phytoplankton biomass of Earth, but challenges remain in\nresolving the IOPs."
                },
                "authors": [
                    {
                        "name": "J. Xavier Prochaska"
                    },
                    {
                        "name": "Robert J. Frouin"
                    }
                ],
                "author_detail": {
                    "name": "Robert J. Frouin"
                },
                "arxiv_affiliation": "Scripps Institution of Oceanography, University of California, San Diego",
                "author": "Robert J. Frouin",
                "arxiv_comment": "submitted to Nature Communications; 30 pages, 20 figures (3 main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06142v1",
                "updated": "2024-08-12T13:37:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    13,
                    37,
                    31,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T13:37:31Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    13,
                    37,
                    31,
                    0,
                    225,
                    0
                ],
                "title": "Med42-v2: A Suite of Clinical LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med42-v2: A Suite of Clinical LLMs"
                },
                "summary": "Med42-v2 introduces a suite of clinical large language models (LLMs) designed\nto address the limitations of generic models in healthcare settings. These\nmodels are built on Llama3 architecture and fine-tuned using specialized\nclinical data. They underwent multi-stage preference alignment to effectively\nrespond to natural prompts. While generic models are often preference-aligned\nto avoid answering clinical queries as a precaution, Med42-v2 is specifically\ntrained to overcome this limitation, enabling its use in clinical settings.\nMed42-v2 models demonstrate superior performance compared to the original\nLlama3 models in both 8B and 70B parameter configurations and GPT-4 across\nvarious medical benchmarks. These LLMs are developed to understand clinical\nqueries, perform reasoning tasks, and provide valuable assistance in clinical\nenvironments. The models are now publicly available at\n\\href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med42-v2 introduces a suite of clinical large language models (LLMs) designed\nto address the limitations of generic models in healthcare settings. These\nmodels are built on Llama3 architecture and fine-tuned using specialized\nclinical data. They underwent multi-stage preference alignment to effectively\nrespond to natural prompts. While generic models are often preference-aligned\nto avoid answering clinical queries as a precaution, Med42-v2 is specifically\ntrained to overcome this limitation, enabling its use in clinical settings.\nMed42-v2 models demonstrate superior performance compared to the original\nLlama3 models in both 8B and 70B parameter configurations and GPT-4 across\nvarious medical benchmarks. These LLMs are developed to understand clinical\nqueries, perform reasoning tasks, and provide valuable assistance in clinical\nenvironments. The models are now publicly available at\n\\href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}."
                },
                "authors": [
                    {
                        "name": "Cl√©ment Christophe"
                    },
                    {
                        "name": "Praveen K Kanithi"
                    },
                    {
                        "name": "Tathagata Raha"
                    },
                    {
                        "name": "Shadab Khan"
                    },
                    {
                        "name": "Marco AF Pimentel"
                    }
                ],
                "author_detail": {
                    "name": "Marco AF Pimentel"
                },
                "author": "Marco AF Pimentel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04655v2",
                "updated": "2024-08-12T13:20:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    13,
                    20,
                    36,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-05T11:27:51Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    11,
                    27,
                    51,
                    0,
                    218,
                    0
                ],
                "title": "Strong and weak alignment of large language models with human values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strong and weak alignment of large language models with human values"
                },
                "summary": "Minimizing negative impacts of Artificial Intelligent (AI) systems on human\nsocieties without human supervision requires them to be able to align with\nhuman values. However, most current work only addresses this issue from a\ntechnical point of view, e.g., improving current methods relying on\nreinforcement learning from human feedback, neglecting what it means and is\nrequired for alignment to occur. Here, we propose to distinguish strong and\nweak value alignment. Strong alignment requires cognitive abilities (either\nhuman-like or different from humans) such as understanding and reasoning about\nagents' intentions and their ability to causally produce desired effects. We\nargue that this is required for AI systems like large language models (LLMs) to\nbe able to recognize situations presenting a risk that human values may be\nflouted. To illustrate this distinction, we present a series of prompts showing\nChatGPT's, Gemini's and Copilot's failures to recognize some of these\nsituations. We moreover analyze word embeddings to show that the nearest\nneighbors of some human values in LLMs differ from humans' semantic\nrepresentations. We then propose a new thought experiment that we call \"the\nChinese room with a word transition dictionary\", in extension of John Searle's\nfamous proposal. We finally mention current promising research directions\ntowards a weak alignment, which could produce statistically satisfying answers\nin a number of common situations, however so far without ensuring any truth\nvalue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimizing negative impacts of Artificial Intelligent (AI) systems on human\nsocieties without human supervision requires them to be able to align with\nhuman values. However, most current work only addresses this issue from a\ntechnical point of view, e.g., improving current methods relying on\nreinforcement learning from human feedback, neglecting what it means and is\nrequired for alignment to occur. Here, we propose to distinguish strong and\nweak value alignment. Strong alignment requires cognitive abilities (either\nhuman-like or different from humans) such as understanding and reasoning about\nagents' intentions and their ability to causally produce desired effects. We\nargue that this is required for AI systems like large language models (LLMs) to\nbe able to recognize situations presenting a risk that human values may be\nflouted. To illustrate this distinction, we present a series of prompts showing\nChatGPT's, Gemini's and Copilot's failures to recognize some of these\nsituations. We moreover analyze word embeddings to show that the nearest\nneighbors of some human values in LLMs differ from humans' semantic\nrepresentations. We then propose a new thought experiment that we call \"the\nChinese room with a word transition dictionary\", in extension of John Searle's\nfamous proposal. We finally mention current promising research directions\ntowards a weak alignment, which could produce statistically satisfying answers\nin a number of common situations, however so far without ensuring any truth\nvalue."
                },
                "authors": [
                    {
                        "name": "Mehdi Khamassi"
                    },
                    {
                        "name": "Marceau Nahon"
                    },
                    {
                        "name": "Raja Chatila"
                    }
                ],
                "author_detail": {
                    "name": "Raja Chatila"
                },
                "author": "Raja Chatila",
                "arxiv_comment": "Accepted for publication in Scientific Reports, special issue on AI\n  aligment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13659v2",
                "updated": "2024-08-12T12:52:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    52,
                    54,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-18T16:35:48Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    16,
                    35,
                    48,
                    3,
                    200,
                    0
                ],
                "title": "Quantifying uncertainty in area and regression coefficient estimation\n  from remote sensing maps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying uncertainty in area and regression coefficient estimation\n  from remote sensing maps"
                },
                "summary": "Remote sensing map products are used to obtain estimates of environmental\nquantities, such as deforested area or the effect of conservation zones on\ndeforestation. However, the quality of map products varies, and - because maps\nare outputs of complex machine learning algorithms that take in a variety of\nremotely sensed variables as inputs - errors are difficult to characterize.\nWithout capturing the biases that may be present, naive calculations of\npopulation-level estimates from such maps are statistically invalid. In this\npaper, we compare several uncertainty quantification methods - stratification,\nOlofsson area estimation method, and prediction-powered inference - that\ncombine a small amount of randomly sampled ground truth data with large-scale\nremote sensing map products to generate statistically valid estimates. Applying\nthese methods across four remote sensing use cases in area and regression\ncoefficient estimation, we find that they result in estimates that are more\nreliable than naively using the map product as if it were 100% accurate and\nhave lower uncertainty than using only the ground truth and ignoring the map\nproduct. Prediction-powered inference uses ground truth data to correct for\nbias in the map product estimate and (unlike stratification) does not require\nus to choose a map product before sampling. This is the first work to (1) apply\nprediction-powered inference to remote sensing estimation tasks, and (2)\nperform uncertainty quantification on remote sensing regression coefficients\nwithout assumptions on the structure of map product errors. To improve the\nutility of machine learning-generated remote sensing maps for downstream\napplications, we recommend that map producers provide a holdout ground truth\ndataset to be used for calibration in uncertainty quantification alongside\ntheir maps.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote sensing map products are used to obtain estimates of environmental\nquantities, such as deforested area or the effect of conservation zones on\ndeforestation. However, the quality of map products varies, and - because maps\nare outputs of complex machine learning algorithms that take in a variety of\nremotely sensed variables as inputs - errors are difficult to characterize.\nWithout capturing the biases that may be present, naive calculations of\npopulation-level estimates from such maps are statistically invalid. In this\npaper, we compare several uncertainty quantification methods - stratification,\nOlofsson area estimation method, and prediction-powered inference - that\ncombine a small amount of randomly sampled ground truth data with large-scale\nremote sensing map products to generate statistically valid estimates. Applying\nthese methods across four remote sensing use cases in area and regression\ncoefficient estimation, we find that they result in estimates that are more\nreliable than naively using the map product as if it were 100% accurate and\nhave lower uncertainty than using only the ground truth and ignoring the map\nproduct. Prediction-powered inference uses ground truth data to correct for\nbias in the map product estimate and (unlike stratification) does not require\nus to choose a map product before sampling. This is the first work to (1) apply\nprediction-powered inference to remote sensing estimation tasks, and (2)\nperform uncertainty quantification on remote sensing regression coefficients\nwithout assumptions on the structure of map product errors. To improve the\nutility of machine learning-generated remote sensing maps for downstream\napplications, we recommend that map producers provide a holdout ground truth\ndataset to be used for calibration in uncertainty quantification alongside\ntheir maps."
                },
                "authors": [
                    {
                        "name": "Kerri Lu"
                    },
                    {
                        "name": "Stephen Bates"
                    },
                    {
                        "name": "Sherrie Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sherrie Wang"
                },
                "author": "Sherrie Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.13659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06109v1",
                "updated": "2024-08-12T12:47:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T12:47:28Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    47,
                    28,
                    0,
                    225,
                    0
                ],
                "title": "Inferring directed spectral information flow between mixed-frequency\n  time series",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring directed spectral information flow between mixed-frequency\n  time series"
                },
                "summary": "Identifying directed spectral information flow between multivariate time\nseries is important for many applications in finance, climate, geophysics and\nneuroscience. Spectral Granger causality (SGC) is a prediction-based measure\ncharacterizing directed information flow at specific oscillatory frequencies.\nHowever, traditional vector autoregressive (VAR) approaches are insufficient to\nassess SGC when time series have mixed frequencies (MF) or are coupled by\nnonlinearity. Here we propose a time-frequency canonical correlation analysis\napproach (\"MF-TFCCA\") to assess the strength and driving frequency of spectral\ninformation flow. We validate the approach with intensive computer simulations\non MF time series under various interaction conditions and assess statistical\nsignificance of the estimate with surrogate data. We further apply MF-TFCCA to\nreal-life finance, climate and neuroscience data. Our analysis framework\nprovides an exploratory and computationally efficient approach to quantify\ndirected information flow between MF time series in the presence of complex and\nnonlinear interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying directed spectral information flow between multivariate time\nseries is important for many applications in finance, climate, geophysics and\nneuroscience. Spectral Granger causality (SGC) is a prediction-based measure\ncharacterizing directed information flow at specific oscillatory frequencies.\nHowever, traditional vector autoregressive (VAR) approaches are insufficient to\nassess SGC when time series have mixed frequencies (MF) or are coupled by\nnonlinearity. Here we propose a time-frequency canonical correlation analysis\napproach (\"MF-TFCCA\") to assess the strength and driving frequency of spectral\ninformation flow. We validate the approach with intensive computer simulations\non MF time series under various interaction conditions and assess statistical\nsignificance of the estimate with surrogate data. We further apply MF-TFCCA to\nreal-life finance, climate and neuroscience data. Our analysis framework\nprovides an exploratory and computationally efficient approach to quantify\ndirected information flow between MF time series in the presence of complex and\nnonlinear interactions."
                },
                "authors": [
                    {
                        "name": "Qiqi Xian"
                    },
                    {
                        "name": "Zhe Sage Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Sage Chen"
                },
                "author": "Zhe Sage Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06103v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06103v1",
                "updated": "2024-08-12T12:43:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    43,
                    30,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T12:43:30Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    43,
                    30,
                    0,
                    225,
                    0
                ],
                "title": "Method-of-Moments Inference for GLMs and Doubly Robust Functionals under\n  Proportional Asymptotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Method-of-Moments Inference for GLMs and Doubly Robust Functionals under\n  Proportional Asymptotics"
                },
                "summary": "In this paper, we consider the estimation of regression coefficients and\nsignal-to-noise (SNR) ratio in high-dimensional Generalized Linear Models\n(GLMs), and explore their implications in inferring popular estimands such as\naverage treatment effects in high-dimensional observational studies. Under the\n``proportional asymptotic'' regime and Gaussian covariates with known\n(population) covariance $\\Sigma$, we derive Consistent and Asymptotically\nNormal (CAN) estimators of our targets of inference through a Method-of-Moments\ntype of estimators that bypasses estimation of high dimensional nuisance\nfunctions and hyperparameter tuning altogether. Additionally, under\nnon-Gaussian covariates, we demonstrate universality of our results under\ncertain additional assumptions on the regression coefficients and $\\Sigma$. We\nalso demonstrate that knowing $\\Sigma$ is not essential to our proposed\nmethodology when the sample covariance matrix estimator is invertible. Finally,\nwe complement our theoretical results with numerical experiments and\ncomparisons with existing literature.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we consider the estimation of regression coefficients and\nsignal-to-noise (SNR) ratio in high-dimensional Generalized Linear Models\n(GLMs), and explore their implications in inferring popular estimands such as\naverage treatment effects in high-dimensional observational studies. Under the\n``proportional asymptotic'' regime and Gaussian covariates with known\n(population) covariance $\\Sigma$, we derive Consistent and Asymptotically\nNormal (CAN) estimators of our targets of inference through a Method-of-Moments\ntype of estimators that bypasses estimation of high dimensional nuisance\nfunctions and hyperparameter tuning altogether. Additionally, under\nnon-Gaussian covariates, we demonstrate universality of our results under\ncertain additional assumptions on the regression coefficients and $\\Sigma$. We\nalso demonstrate that knowing $\\Sigma$ is not essential to our proposed\nmethodology when the sample covariance matrix estimator is invertible. Finally,\nwe complement our theoretical results with numerical experiments and\ncomparisons with existing literature."
                },
                "authors": [
                    {
                        "name": "Xingyu Chen"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Rajarshi Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Rajarshi Mukherjee"
                },
                "author": "Rajarshi Mukherjee",
                "arxiv_comment": "16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06103v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06103v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11046v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11046v3",
                "updated": "2024-08-12T12:41:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    41,
                    57,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-08T12:32:10Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    12,
                    32,
                    10,
                    0,
                    190,
                    0
                ],
                "title": "A Survey on LoRA of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on LoRA of Large Language Models"
                },
                "summary": "Low-Rank Adaptation~(LoRA), which updates the dense neural network layers\nwith pluggable low-rank matrices, is one of the best performed parameter\nefficient fine-tuning paradigms. Furthermore, it has significant advantages in\ncross-task generalization and privacy-preserving. Hence, LoRA has gained much\nattention recently, and the number of related literature demonstrates\nexponential growth. It is necessary to conduct a comprehensive overview of the\ncurrent progress on LoRA. This survey categorizes and reviews the progress from\nthe perspectives of (1) downstream adaptation improving variants that improve\nLoRA's performance on downstream tasks; (2) cross-task generalization methods\nthat mix multiple LoRA plugins to achieve cross-task generalization; (3)\nefficiency-improving methods that boost the computation-efficiency of LoRA; (4)\ndata privacy-preserving methods that use LoRA in federated learning; (5)\napplication. Besides, this survey also discusses the future directions in this\nfield. At last, we provide a Github\npage~\\footnote{\\href{https://github.com/ZJU-LLMs/Awesome-LoRAs.git}{https://github.com/ZJU-LLMs/Awesome-LoRAs.git}}\nfor readers to check the updates and initiate discussions on this survey paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation~(LoRA), which updates the dense neural network layers\nwith pluggable low-rank matrices, is one of the best performed parameter\nefficient fine-tuning paradigms. Furthermore, it has significant advantages in\ncross-task generalization and privacy-preserving. Hence, LoRA has gained much\nattention recently, and the number of related literature demonstrates\nexponential growth. It is necessary to conduct a comprehensive overview of the\ncurrent progress on LoRA. This survey categorizes and reviews the progress from\nthe perspectives of (1) downstream adaptation improving variants that improve\nLoRA's performance on downstream tasks; (2) cross-task generalization methods\nthat mix multiple LoRA plugins to achieve cross-task generalization; (3)\nefficiency-improving methods that boost the computation-efficiency of LoRA; (4)\ndata privacy-preserving methods that use LoRA in federated learning; (5)\napplication. Besides, this survey also discusses the future directions in this\nfield. At last, we provide a Github\npage~\\footnote{\\href{https://github.com/ZJU-LLMs/Awesome-LoRAs.git}{https://github.com/ZJU-LLMs/Awesome-LoRAs.git}}\nfor readers to check the updates and initiate discussions on this survey paper."
                },
                "authors": [
                    {
                        "name": "Yuren Mao"
                    },
                    {
                        "name": "Yuhang Ge"
                    },
                    {
                        "name": "Yijiang Fan"
                    },
                    {
                        "name": "Wenyi Xu"
                    },
                    {
                        "name": "Yu Mi"
                    },
                    {
                        "name": "Zhonghao Hu"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "author": "Yunjun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11046v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11046v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06094v1",
                "updated": "2024-08-12T12:25:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    25,
                    25,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T12:25:25Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    25,
                    25,
                    0,
                    225,
                    0
                ],
                "title": "Mapping the longitudinal magnetic field in the atmosphere of an active\n  region plage from the inversion of the near-ultraviolet CLASP2.1\n  spectropolarimetric data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping the longitudinal magnetic field in the atmosphere of an active\n  region plage from the inversion of the near-ultraviolet CLASP2.1\n  spectropolarimetric data"
                },
                "summary": "We apply the HanleRT Tenerife Inversion Code to the spectro-polarimetric\nobservations obtained by the Chromospheric LAyer SpectroPolarimeter. This\nsuborbital space experiment measured the variation with wavelength of the four\nStokes parameters in the near-ultraviolet spectral region of the Mg II h & k\nlines over a solar disk area containing part of an active region plage and the\nedge of a sunspot penumbra. We infer the stratification of the temperature, the\nelectron density, the line of-sight velocity, the micro-turbulent velocity, and\nthe longitudinal component of the magnetic field from the observed intensity\nand circular polarization profiles. The inferred model atmosphere shows larger\ntemperature and electron density in the plage and the superpenumbra regions\nthan in the quiet regions. The shape of the plage region in terms of its\nbrightness is similar to the pattern of the inferred longitudinal component of\nthe magnetic field in the chromosphere, as well as to that of the overlying\nmoss observed by AIA in the 171 A band, which suggests a similar magnetic\norigin for the heating in both the plage and the moss region. Moreover, this\nheating is particularly significant in the regions with larger inferred\nmagnetic flux. In contrast, in the superpenumbra, the regions with larger\nelectron density and temperature are usually found in between these regions\nwith larger magnetic flux, suggesting that the details of the heating mechanism\nin the chromosphere of the superpenumbra may be different to those in the\nplage, but with the magnetic field still playing a key role.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We apply the HanleRT Tenerife Inversion Code to the spectro-polarimetric\nobservations obtained by the Chromospheric LAyer SpectroPolarimeter. This\nsuborbital space experiment measured the variation with wavelength of the four\nStokes parameters in the near-ultraviolet spectral region of the Mg II h & k\nlines over a solar disk area containing part of an active region plage and the\nedge of a sunspot penumbra. We infer the stratification of the temperature, the\nelectron density, the line of-sight velocity, the micro-turbulent velocity, and\nthe longitudinal component of the magnetic field from the observed intensity\nand circular polarization profiles. The inferred model atmosphere shows larger\ntemperature and electron density in the plage and the superpenumbra regions\nthan in the quiet regions. The shape of the plage region in terms of its\nbrightness is similar to the pattern of the inferred longitudinal component of\nthe magnetic field in the chromosphere, as well as to that of the overlying\nmoss observed by AIA in the 171 A band, which suggests a similar magnetic\norigin for the heating in both the plage and the moss region. Moreover, this\nheating is particularly significant in the regions with larger inferred\nmagnetic flux. In contrast, in the superpenumbra, the regions with larger\nelectron density and temperature are usually found in between these regions\nwith larger magnetic flux, suggesting that the details of the heating mechanism\nin the chromosphere of the superpenumbra may be different to those in the\nplage, but with the magnetic field still playing a key role."
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Tanaus√∫ del Pino Alem√°n"
                    },
                    {
                        "name": "Javier Trujillo Bueno"
                    },
                    {
                        "name": "Ryohko Ishikawa"
                    },
                    {
                        "name": "Ernest Alsina Ballester"
                    },
                    {
                        "name": "David E. McKenzie"
                    },
                    {
                        "name": "Luca Belluzzi"
                    },
                    {
                        "name": "Donguk Song"
                    },
                    {
                        "name": "Takenori J. Okamoto"
                    },
                    {
                        "name": "Ken Kobayashi"
                    },
                    {
                        "name": "Laurel A. Rachmeler"
                    },
                    {
                        "name": "Christian Bethge"
                    },
                    {
                        "name": "Fr√©d√©ric Auch√®re"
                    }
                ],
                "author_detail": {
                    "name": "Fr√©d√©ric Auch√®re"
                },
                "author": "Fr√©d√©ric Auch√®re",
                "arxiv_comment": "Accepted for publication in the Astrophysical Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.07032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.07032v2",
                "updated": "2024-08-12T12:11:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    11,
                    52,
                    0,
                    225,
                    0
                ],
                "published": "2023-11-13T02:31:16Z",
                "published_parsed": [
                    2023,
                    11,
                    13,
                    2,
                    31,
                    16,
                    0,
                    317,
                    0
                ],
                "title": "ExpNote: Black-box Large Language Models are Better Task Solvers with\n  Experience Notebook",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpNote: Black-box Large Language Models are Better Task Solvers with\n  Experience Notebook"
                },
                "summary": "Black-box Large Language Models (LLMs) have shown great power in solving\nvarious tasks and are considered general problem solvers. However, LLMs still\nfail in many specific tasks although understand the task instruction. In this\npaper, we focus on the problem of boosting the ability of black-box LLMs to\nsolve downstream tasks. We propose ExpNote, an automated framework to help LLMs\nbetter adapt to unfamiliar tasks through reflecting and noting experiences from\ntraining data and retrieving them from external memory during testing. We\nevaluate ExpNote on multiple tasks and the experimental results demonstrate\nthat the proposed method significantly improves the performance of black-box\nLLMs. The data and code are available at\nhttps://github.com/forangel2014/ExpNote",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box Large Language Models (LLMs) have shown great power in solving\nvarious tasks and are considered general problem solvers. However, LLMs still\nfail in many specific tasks although understand the task instruction. In this\npaper, we focus on the problem of boosting the ability of black-box LLMs to\nsolve downstream tasks. We propose ExpNote, an automated framework to help LLMs\nbetter adapt to unfamiliar tasks through reflecting and noting experiences from\ntraining data and retrieving them from external memory during testing. We\nevaluate ExpNote on multiple tasks and the experimental results demonstrate\nthat the proposed method significantly improves the performance of black-box\nLLMs. The data and code are available at\nhttps://github.com/forangel2014/ExpNote"
                },
                "authors": [
                    {
                        "name": "Wangtao Sun"
                    },
                    {
                        "name": "Xuanqing Yu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "arxiv_comment": "EMNLP 2023 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.07032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.07032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06087v1",
                "updated": "2024-08-12T12:04:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    4,
                    14,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T12:04:14Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    4,
                    14,
                    0,
                    225,
                    0
                ],
                "title": "Building Decision Making Models Through Language Model Regime",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Decision Making Models Through Language Model Regime"
                },
                "summary": "We propose a novel approach for decision making problems leveraging the\ngeneralization capabilities of large language models (LLMs). Traditional\nmethods such as expert systems, planning algorithms, and reinforcement learning\noften exhibit limited generalization, typically requiring the training of new\nmodels for each unique task. In contrast, LLMs demonstrate remarkable success\nin generalizing across varied language tasks, inspiring a new strategy for\ntraining decision making models. Our approach, referred to as \"Learning then\nUsing\" (LTU), entails a two-stage process. Initially, the \\textit{learning}\nphase develops a robust foundational decision making model by integrating\ndiverse knowledge from various domains and decision making contexts. The\nsubsequent \\textit{using} phase refines this foundation model for specific\ndecision making scenarios. Distinct from other studies that employ LLMs for\ndecision making through supervised learning, our LTU method embraces a\nversatile training methodology that combines broad pre-training with targeted\nfine-tuning. Experiments in e-commerce domains such as advertising and search\noptimization have shown that LTU approach outperforms traditional supervised\nlearning regimes in decision making capabilities and generalization. The LTU\napproach is the first practical training architecture for both single-step and\nmulti-step decision making tasks combined with LLMs, which can be applied\nbeyond game and robot domains. It provides a robust and adaptable framework for\ndecision making, enhances the effectiveness and flexibility of various systems\nin tackling various challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel approach for decision making problems leveraging the\ngeneralization capabilities of large language models (LLMs). Traditional\nmethods such as expert systems, planning algorithms, and reinforcement learning\noften exhibit limited generalization, typically requiring the training of new\nmodels for each unique task. In contrast, LLMs demonstrate remarkable success\nin generalizing across varied language tasks, inspiring a new strategy for\ntraining decision making models. Our approach, referred to as \"Learning then\nUsing\" (LTU), entails a two-stage process. Initially, the \\textit{learning}\nphase develops a robust foundational decision making model by integrating\ndiverse knowledge from various domains and decision making contexts. The\nsubsequent \\textit{using} phase refines this foundation model for specific\ndecision making scenarios. Distinct from other studies that employ LLMs for\ndecision making through supervised learning, our LTU method embraces a\nversatile training methodology that combines broad pre-training with targeted\nfine-tuning. Experiments in e-commerce domains such as advertising and search\noptimization have shown that LTU approach outperforms traditional supervised\nlearning regimes in decision making capabilities and generalization. The LTU\napproach is the first practical training architecture for both single-step and\nmulti-step decision making tasks combined with LLMs, which can be applied\nbeyond game and robot domains. It provides a robust and adaptable framework for\ndecision making, enhances the effectiveness and flexibility of various systems\nin tackling various challenges."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Haoxiang Liu"
                    },
                    {
                        "name": "Feijun Jiang"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaifu Zhang"
                },
                "author": "Kaifu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.01043v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.01043v4",
                "updated": "2024-08-12T11:53:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    11,
                    53,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2023-11-02T07:23:33Z",
                "published_parsed": [
                    2023,
                    11,
                    2,
                    7,
                    23,
                    33,
                    3,
                    306,
                    0
                ],
                "title": "LLM4Drive: A Survey of Large Language Models for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Drive: A Survey of Large Language Models for Autonomous Driving"
                },
                "summary": "Autonomous driving technology, a catalyst for revolutionizing transportation\nand urban mobility, has the tend to transition from rule-based systems to\ndata-driven strategies. Traditional module-based systems are constrained by\ncumulative errors among cascaded modules and inflexible pre-set rules. In\ncontrast, end-to-end autonomous driving systems have the potential to avoid\nerror accumulation due to their fully data-driven training process, although\nthey often lack transparency due to their \"black box\" nature, complicating the\nvalidation and traceability of decisions. Recently, large language models\n(LLMs) have demonstrated abilities including understanding context, logical\nreasoning, and generating answers. A natural thought is to utilize these\nabilities to empower autonomous driving. By combining LLM with foundation\nvision models, it could open the door to open-world understanding, reasoning,\nand few-shot learning, which current autonomous driving systems are lacking. In\nthis paper, we systematically review a research line about \\textit{Large\nLanguage Models for Autonomous Driving (LLM4AD)}. This study evaluates the\ncurrent state of technological advancements, distinctly outlining the principal\nchallenges and prospective directions for the field. For the convenience of\nresearchers in academia and industry, we provide real-time updates on the\nlatest advances in the field as well as relevant open-source resources via the\ndesignated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving technology, a catalyst for revolutionizing transportation\nand urban mobility, has the tend to transition from rule-based systems to\ndata-driven strategies. Traditional module-based systems are constrained by\ncumulative errors among cascaded modules and inflexible pre-set rules. In\ncontrast, end-to-end autonomous driving systems have the potential to avoid\nerror accumulation due to their fully data-driven training process, although\nthey often lack transparency due to their \"black box\" nature, complicating the\nvalidation and traceability of decisions. Recently, large language models\n(LLMs) have demonstrated abilities including understanding context, logical\nreasoning, and generating answers. A natural thought is to utilize these\nabilities to empower autonomous driving. By combining LLM with foundation\nvision models, it could open the door to open-world understanding, reasoning,\nand few-shot learning, which current autonomous driving systems are lacking. In\nthis paper, we systematically review a research line about \\textit{Large\nLanguage Models for Autonomous Driving (LLM4AD)}. This study evaluates the\ncurrent state of technological advancements, distinctly outlining the principal\nchallenges and prospective directions for the field. For the convenience of\nresearchers in academia and industry, we provide real-time updates on the\nlatest advances in the field as well as relevant open-source resources via the\ndesignated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD."
                },
                "authors": [
                    {
                        "name": "Zhenjie Yang"
                    },
                    {
                        "name": "Xiaosong Jia"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "arxiv_comment": "GitHub Repo: https://github.com/Thinklab-SJTU/Awesome-LLM4AD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.01043v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.01043v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06073v1",
                "updated": "2024-08-12T11:47:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    11,
                    47,
                    52,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T11:47:52Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    11,
                    47,
                    52,
                    0,
                    225,
                    0
                ],
                "title": "Neural Ordinary Differential Equations for Model Order Reduction of\n  Stiff Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Ordinary Differential Equations for Model Order Reduction of\n  Stiff Systems"
                },
                "summary": "Neural Ordinary Differential Equations (ODEs) represent a significant\nadvancement at the intersection of machine learning and dynamical systems,\noffering a continuous-time analog to discrete neural networks. Despite their\npromise, deploying neural ODEs in practical applications often encounters the\nchallenge of stiffness, a condition where rapid variations in some components\nof the solution demand prohibitively small time steps for explicit solvers.\nThis work addresses the stiffness issue when employing neural ODEs for model\norder reduction by introducing a suitable reparametrization in time. The\nconsidered map is data-driven and it is induced by the adaptive time-stepping\nof an implicit solver on a reference solution. We show the map produces a\nnonstiff system that can be cheaply solved with an explicit time integration\nscheme. The original, stiff, time dynamic is recovered by means of a map learnt\nby a neural network that connects the state space to the time\nreparametrization. We validate our method through extensive experiments,\ndemonstrating improvements in efficiency for the neural ODE inference while\nmaintaining robustness and accuracy. The neural network model also showcases\ngood generalization properties for times beyond the training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Ordinary Differential Equations (ODEs) represent a significant\nadvancement at the intersection of machine learning and dynamical systems,\noffering a continuous-time analog to discrete neural networks. Despite their\npromise, deploying neural ODEs in practical applications often encounters the\nchallenge of stiffness, a condition where rapid variations in some components\nof the solution demand prohibitively small time steps for explicit solvers.\nThis work addresses the stiffness issue when employing neural ODEs for model\norder reduction by introducing a suitable reparametrization in time. The\nconsidered map is data-driven and it is induced by the adaptive time-stepping\nof an implicit solver on a reference solution. We show the map produces a\nnonstiff system that can be cheaply solved with an explicit time integration\nscheme. The original, stiff, time dynamic is recovered by means of a map learnt\nby a neural network that connects the state space to the time\nreparametrization. We validate our method through extensive experiments,\ndemonstrating improvements in efficiency for the neural ODE inference while\nmaintaining robustness and accuracy. The neural network model also showcases\ngood generalization properties for times beyond the training data."
                },
                "authors": [
                    {
                        "name": "Matteo Caldana"
                    },
                    {
                        "name": "Jan S. Hesthaven"
                    }
                ],
                "author_detail": {
                    "name": "Jan S. Hesthaven"
                },
                "author": "Jan S. Hesthaven",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65L99, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06069v1",
                "updated": "2024-08-12T11:41:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    11,
                    41,
                    7,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T11:41:07Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    11,
                    41,
                    7,
                    0,
                    225,
                    0
                ],
                "title": "Fully Bayesian Differential Gaussian Processes through Stochastic\n  Differential Equations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully Bayesian Differential Gaussian Processes through Stochastic\n  Differential Equations"
                },
                "summary": "Traditional deep Gaussian processes model the data evolution using a discrete\nhierarchy, whereas differential Gaussian processes (DIFFGPs) represent the\nevolution as an infinitely deep Gaussian process. However, prior DIFFGP methods\noften overlook the uncertainty of kernel hyperparameters and assume them to be\nfixed and time-invariant, failing to leverage the unique synergy between\ncontinuous-time models and approximate inference. In this work, we propose a\nfully Bayesian approach that treats the kernel hyperparameters as random\nvariables and constructs coupled stochastic differential equations (SDEs) to\nlearn their posterior distribution and that of inducing points. By\nincorporating estimation uncertainty on hyperparameters, our method enhances\nthe model's flexibility and adaptability to complex dynamics. Additionally, our\napproach provides a time-varying, comprehensive, and realistic posterior\napproximation through coupling variables using SDE methods. Experimental\nresults demonstrate the advantages of our method over traditional approaches,\nshowcasing its superior performance in terms of flexibility, accuracy, and\nother metrics. Our work opens up exciting research avenues for advancing\nBayesian inference and offers a powerful modeling tool for continuous-time\nGaussian processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional deep Gaussian processes model the data evolution using a discrete\nhierarchy, whereas differential Gaussian processes (DIFFGPs) represent the\nevolution as an infinitely deep Gaussian process. However, prior DIFFGP methods\noften overlook the uncertainty of kernel hyperparameters and assume them to be\nfixed and time-invariant, failing to leverage the unique synergy between\ncontinuous-time models and approximate inference. In this work, we propose a\nfully Bayesian approach that treats the kernel hyperparameters as random\nvariables and constructs coupled stochastic differential equations (SDEs) to\nlearn their posterior distribution and that of inducing points. By\nincorporating estimation uncertainty on hyperparameters, our method enhances\nthe model's flexibility and adaptability to complex dynamics. Additionally, our\napproach provides a time-varying, comprehensive, and realistic posterior\napproximation through coupling variables using SDE methods. Experimental\nresults demonstrate the advantages of our method over traditional approaches,\nshowcasing its superior performance in terms of flexibility, accuracy, and\nother metrics. Our work opens up exciting research avenues for advancing\nBayesian inference and offers a powerful modeling tool for continuous-time\nGaussian processes."
                },
                "authors": [
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Zhiqi Lin"
                    },
                    {
                        "name": "Min Chen"
                    },
                    {
                        "name": "Junmei Yang"
                    },
                    {
                        "name": "Delu Zeng"
                    },
                    {
                        "name": "John Paisley"
                    }
                ],
                "author_detail": {
                    "name": "John Paisley"
                },
                "author": "John Paisley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06063v1",
                "updated": "2024-08-12T11:29:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    11,
                    29,
                    54,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T11:29:54Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    11,
                    29,
                    54,
                    0,
                    225,
                    0
                ],
                "title": "TruVRF: Towards Triple-Granularity Verification on Machine Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TruVRF: Towards Triple-Granularity Verification on Machine Unlearning"
                },
                "summary": "The concept of the right to be forgotten has led to growing interest in\nmachine unlearning, but reliable validation methods are lacking, creating\nopportunities for dishonest model providers to mislead data contributors.\nTraditional invasive methods like backdoor injection are not feasible for\nlegacy data. To address this, we introduce TruVRF, a non-invasive unlearning\nverification framework operating at class-, volume-, and sample-level\ngranularities. TruVRF includes three Unlearning-Metrics designed to detect\ndifferent types of dishonest servers: Neglecting, Lazy, and Deceiving.\nUnlearning-Metric-I checks class alignment, Unlearning-Metric-II verifies\nsample count, and Unlearning-Metric-III confirms specific sample deletion.\nEvaluations on three datasets show TruVRF's robust performance, with over 90%\naccuracy for Metrics I and III, and a 4.8% to 8.2% inference deviation for\nMetric II. TruVRF also demonstrates generalizability and practicality across\nvarious conditions and with state-of-the-art unlearning frameworks like SISA\nand Amnesiac Unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The concept of the right to be forgotten has led to growing interest in\nmachine unlearning, but reliable validation methods are lacking, creating\nopportunities for dishonest model providers to mislead data contributors.\nTraditional invasive methods like backdoor injection are not feasible for\nlegacy data. To address this, we introduce TruVRF, a non-invasive unlearning\nverification framework operating at class-, volume-, and sample-level\ngranularities. TruVRF includes three Unlearning-Metrics designed to detect\ndifferent types of dishonest servers: Neglecting, Lazy, and Deceiving.\nUnlearning-Metric-I checks class alignment, Unlearning-Metric-II verifies\nsample count, and Unlearning-Metric-III confirms specific sample deletion.\nEvaluations on three datasets show TruVRF's robust performance, with over 90%\naccuracy for Metrics I and III, and a 4.8% to 8.2% inference deviation for\nMetric II. TruVRF also demonstrates generalizability and practicality across\nvarious conditions and with state-of-the-art unlearning frameworks like SISA\nand Amnesiac Unlearning."
                },
                "authors": [
                    {
                        "name": "Chunyi Zhou"
                    },
                    {
                        "name": "Anmin Fu"
                    },
                    {
                        "name": "Zhiyang Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyang Dai"
                },
                "author": "Zhiyang Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06765v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06765v3",
                "updated": "2024-08-12T10:55:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    10,
                    55,
                    38,
                    0,
                    225,
                    0
                ],
                "published": "2024-03-11T14:35:45Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    14,
                    35,
                    45,
                    0,
                    71,
                    0
                ],
                "title": "ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large\n  Language Model"
                },
                "summary": "The internet has brought both benefits and harms to society. A prime example\nof the latter is misinformation, including conspiracy theories, which flood the\nweb. Recent advances in natural language processing, particularly the emergence\nof large language models (LLMs), have improved the prospects of accurate\nmisinformation detection. However, most LLM-based approaches to conspiracy\ntheory detection focus only on binary classification and fail to account for\nthe important relationship between misinformation and affective features (i.e.,\nsentiment and emotions). Driven by a comprehensive analysis of conspiracy text\nthat reveals its distinctive affective features, we propose ConspEmoLLM, the\nfirst open-source LLM that integrates affective information and is able to\nperform diverse tasks relating to conspiracy theories. These tasks include not\nonly conspiracy theory detection, but also classification of theory type and\ndetection of related discussion (e.g., opinions towards theories). ConspEmoLLM\nis fine-tuned based on an emotion-oriented LLM using our novel ConDID dataset,\nwhich includes five tasks to support LLM instruction tuning and evaluation. We\ndemonstrate that when applied to these tasks, ConspEmoLLM largely outperforms\nseveral open-source general domain LLMs and ChatGPT, as well as an LLM that has\nbeen fine-tuned using ConDID, but which does not use affective features. This\nproject will be released on https://github.com/lzw108/ConspEmoLLM/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The internet has brought both benefits and harms to society. A prime example\nof the latter is misinformation, including conspiracy theories, which flood the\nweb. Recent advances in natural language processing, particularly the emergence\nof large language models (LLMs), have improved the prospects of accurate\nmisinformation detection. However, most LLM-based approaches to conspiracy\ntheory detection focus only on binary classification and fail to account for\nthe important relationship between misinformation and affective features (i.e.,\nsentiment and emotions). Driven by a comprehensive analysis of conspiracy text\nthat reveals its distinctive affective features, we propose ConspEmoLLM, the\nfirst open-source LLM that integrates affective information and is able to\nperform diverse tasks relating to conspiracy theories. These tasks include not\nonly conspiracy theory detection, but also classification of theory type and\ndetection of related discussion (e.g., opinions towards theories). ConspEmoLLM\nis fine-tuned based on an emotion-oriented LLM using our novel ConDID dataset,\nwhich includes five tasks to support LLM instruction tuning and evaluation. We\ndemonstrate that when applied to these tasks, ConspEmoLLM largely outperforms\nseveral open-source general domain LLMs and ChatGPT, as well as an LLM that has\nbeen fine-tuned using ConDID, but which does not use affective features. This\nproject will be released on https://github.com/lzw108/ConspEmoLLM/."
                },
                "authors": [
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Boyang Liu"
                    },
                    {
                        "name": "Paul Thompson"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Ananiadou"
                },
                "author": "Sophia Ananiadou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06765v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06765v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06050v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06050v1",
                "updated": "2024-08-12T10:55:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    10,
                    55,
                    29,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T10:55:29Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    10,
                    55,
                    29,
                    0,
                    225,
                    0
                ],
                "title": "What Ails Generative Structure-based Drug Design: Too Little or Too Much\n  Expressivity?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Ails Generative Structure-based Drug Design: Too Little or Too Much\n  Expressivity?"
                },
                "summary": "Several generative models with elaborate training and sampling procedures\nhave been proposed recently to accelerate structure-based drug design (SBDD);\nhowever, perplexingly, their empirical performance turns out to be suboptimal.\nWe seek to better understand this phenomenon from both theoretical and\nempirical perspectives. Since most of these models apply graph neural networks\n(GNNs), one may suspect that they inherit the representational limitations of\nGNNs. We analyze this aspect, establishing the first such results for\nprotein-ligand complexes. A plausible counterview may attribute the\nunderperformance of these models to their excessive parameterizations, inducing\nexpressivity at the expense of generalization. We also investigate this\npossibility with a simple metric-aware approach that learns an economical\nsurrogate for affinity to infer an unlabelled molecular graph and optimizes for\nlabels conditioned on this graph and molecular properties. The resulting model\nachieves state-of-the-art results using 100x fewer trainable parameters and\naffords up to 1000x speedup. Collectively, our findings underscore the need to\nreassess and redirect the existing paradigm and efforts for SBDD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several generative models with elaborate training and sampling procedures\nhave been proposed recently to accelerate structure-based drug design (SBDD);\nhowever, perplexingly, their empirical performance turns out to be suboptimal.\nWe seek to better understand this phenomenon from both theoretical and\nempirical perspectives. Since most of these models apply graph neural networks\n(GNNs), one may suspect that they inherit the representational limitations of\nGNNs. We analyze this aspect, establishing the first such results for\nprotein-ligand complexes. A plausible counterview may attribute the\nunderperformance of these models to their excessive parameterizations, inducing\nexpressivity at the expense of generalization. We also investigate this\npossibility with a simple metric-aware approach that learns an economical\nsurrogate for affinity to infer an unlabelled molecular graph and optimizes for\nlabels conditioned on this graph and molecular properties. The resulting model\nachieves state-of-the-art results using 100x fewer trainable parameters and\naffords up to 1000x speedup. Collectively, our findings underscore the need to\nreassess and redirect the existing paradigm and efforts for SBDD."
                },
                "authors": [
                    {
                        "name": "Rafa≈Ç Karczewski"
                    },
                    {
                        "name": "Samuel Kaski"
                    },
                    {
                        "name": "Markus Heinonen"
                    },
                    {
                        "name": "Vikas Garg"
                    }
                ],
                "author_detail": {
                    "name": "Vikas Garg"
                },
                "author": "Vikas Garg",
                "arxiv_comment": "25 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06050v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06050v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06046v1",
                "updated": "2024-08-12T10:35:18Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    10,
                    35,
                    18,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T10:35:18Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    10,
                    35,
                    18,
                    0,
                    225,
                    0
                ],
                "title": "Identifying Total Causal Effects in Linear Models under Partial\n  Homoscedasticity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying Total Causal Effects in Linear Models under Partial\n  Homoscedasticity"
                },
                "summary": "A fundamental challenge of scientific research is inferring causal relations\nbased on observed data. One commonly used approach involves utilizing\nstructural causal models that postulate noisy functional relations among\ninteracting variables. A directed graph naturally represents these models and\nreflects the underlying causal structure. However, classical identifiability\nresults suggest that, without conducting additional experiments, this causal\ngraph can only be identified up to a Markov equivalence class of\nindistinguishable models. Recent research has shown that focusing on linear\nrelations with equal error variances can enable the identification of the\ncausal structure from mere observational data. Nonetheless, practitioners are\noften primarily interested in the effects of specific interventions, rendering\nthe complete identification of the causal structure unnecessary. In this work,\nwe investigate the extent to which less restrictive assumptions of partial\nhomoscedasticity are sufficient for identifying the causal effects of interest.\nFurthermore, we construct mathematically rigorous confidence regions for total\ncausal effects under structure uncertainty and explore the performance gain of\nrelying on stricter error assumptions in a simulation study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A fundamental challenge of scientific research is inferring causal relations\nbased on observed data. One commonly used approach involves utilizing\nstructural causal models that postulate noisy functional relations among\ninteracting variables. A directed graph naturally represents these models and\nreflects the underlying causal structure. However, classical identifiability\nresults suggest that, without conducting additional experiments, this causal\ngraph can only be identified up to a Markov equivalence class of\nindistinguishable models. Recent research has shown that focusing on linear\nrelations with equal error variances can enable the identification of the\ncausal structure from mere observational data. Nonetheless, practitioners are\noften primarily interested in the effects of specific interventions, rendering\nthe complete identification of the causal structure unnecessary. In this work,\nwe investigate the extent to which less restrictive assumptions of partial\nhomoscedasticity are sufficient for identifying the causal effects of interest.\nFurthermore, we construct mathematically rigorous confidence regions for total\ncausal effects under structure uncertainty and explore the performance gain of\nrelying on stricter error assumptions in a simulation study."
                },
                "authors": [
                    {
                        "name": "David Strieder"
                    },
                    {
                        "name": "Mathias Drton"
                    }
                ],
                "author_detail": {
                    "name": "Mathias Drton"
                },
                "author": "Mathias Drton",
                "arxiv_comment": "Accepted for the International Conference on Probabilistic Graphical\n  Models (PGM) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62D20, 62H22",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06037v1",
                "updated": "2024-08-12T09:59:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    9,
                    59,
                    47,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T09:59:47Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    9,
                    59,
                    47,
                    0,
                    225,
                    0
                ],
                "title": "Hyperion: Unveiling DApp Inconsistencies using LLM and Dataflow-Guided\n  Symbolic Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperion: Unveiling DApp Inconsistencies using LLM and Dataflow-Guided\n  Symbolic Execution"
                },
                "summary": "The rapid advancement of blockchain platforms has significantly accelerated\nthe growth of decentralized applications (DApps). Similar to traditional\napplications, DApps integrate front-end descriptions that showcase their\nfeatures to attract users, and back-end smart contracts for executing their\nbusiness logic. However, inconsistencies between the features promoted in\nfront-end descriptions and those actually implemented in the contract can\nconfuse users and undermine DApps's trustworthiness. In this paper, we first\nconducted an empirical study to identify seven types of inconsistencies, each\nexemplified by a real-world DApp. Furthermore, we introduce HYPERION, an\napproach designed to automatically identify inconsistencies between front-end\ndescriptions and back-end code implementation in DApps. This method leverages a\nfine-tuned large language model LLaMA2 to analyze DApp descriptions and employs\ndataflow-guided symbolic execution for contract bytecode analysis. Finally,\nHYPERION reports the inconsistency based on predefined detection patterns. The\nexperiment on our ground truth dataset consisting of 54 DApps shows that\nHYPERION reaches 84.06% overall recall and 92.06% overall precision in\nreporting DApp inconsistencies. We also implement HYPERION to analyze 835\nreal-world DApps. The experimental results show that HYPERION discovers 459\nreal-world DApps containing at least one inconsistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of blockchain platforms has significantly accelerated\nthe growth of decentralized applications (DApps). Similar to traditional\napplications, DApps integrate front-end descriptions that showcase their\nfeatures to attract users, and back-end smart contracts for executing their\nbusiness logic. However, inconsistencies between the features promoted in\nfront-end descriptions and those actually implemented in the contract can\nconfuse users and undermine DApps's trustworthiness. In this paper, we first\nconducted an empirical study to identify seven types of inconsistencies, each\nexemplified by a real-world DApp. Furthermore, we introduce HYPERION, an\napproach designed to automatically identify inconsistencies between front-end\ndescriptions and back-end code implementation in DApps. This method leverages a\nfine-tuned large language model LLaMA2 to analyze DApp descriptions and employs\ndataflow-guided symbolic execution for contract bytecode analysis. Finally,\nHYPERION reports the inconsistency based on predefined detection patterns. The\nexperiment on our ground truth dataset consisting of 54 DApps shows that\nHYPERION reaches 84.06% overall recall and 92.06% overall precision in\nreporting DApp inconsistencies. We also implement HYPERION to analyze 835\nreal-world DApps. The experimental results show that HYPERION discovers 459\nreal-world DApps containing at least one inconsistency."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Xingwei Lin"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Qingyuan Zhong"
                    },
                    {
                        "name": "Lei Xiao"
                    },
                    {
                        "name": "Renke Huang"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "Accepted by ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2205.10034v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2205.10034v3",
                "updated": "2024-08-12T09:23:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    9,
                    23,
                    13,
                    0,
                    225,
                    0
                ],
                "published": "2022-05-20T09:09:27Z",
                "published_parsed": [
                    2022,
                    5,
                    20,
                    9,
                    9,
                    27,
                    4,
                    140,
                    0
                ],
                "title": "MoESys: A Distributed and Efficient Mixture-of-Experts Training and\n  Inference System for Internet Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoESys: A Distributed and Efficient Mixture-of-Experts Training and\n  Inference System for Internet Services"
                },
                "summary": "While modern internet services, such as chatbots, search engines, and online\nadvertising, demand the use of large-scale deep neural networks (DNNs),\ndistributed training and inference over heterogeneous computing systems are\ndesired to facilitate these DNN models. Mixture-of-Experts (MoE) is one the\nmost common strategies to lower the cost of training subject to the overall\nsize of models/data through gating and parallelism in a divide-and-conquer\nfashion. While DeepSpeed has made efforts in carrying out large-scale MoE\ntraining over heterogeneous infrastructures, the efficiency of training and\ninference could be further improved from several system aspects, including load\nbalancing, communication/computation efficiency, and memory footprint limits.\nIn this work, we present a novel MoESys that boosts efficiency in both\nlarge-scale training and inference. Specifically, in the training procedure,\nthe proposed MoESys adopts an Elastic MoE training strategy with 2D prefetch\nand Fusion communication over Hierarchical storage, so as to enjoy efficient\nparallelisms. For scalable inference in a single node, especially when the\nmodel size is larger than GPU memory, MoESys builds the CPU-GPU memory jointly\ninto a ring of sections to load the model, and executes the computation tasks\nacross the memory sections in a round-robin manner for efficient inference. We\ncarried out extensive experiments to evaluate MoESys, where MoESys successfully\ntrains a Unified Feature Optimization (UFO) model with a Sparsely-Gated\nMixture-of-Experts model of 12B parameters in 8 days on 48 A100 GPU cards. The\ncomparison against the state-of-the-art shows that MoESys outperformed\nDeepSpeed with 33% higher throughput (tokens per second) in training and 13%\nhigher throughput in inference in general. Particularly, under unbalanced MoE\nTasks, e.g., UFO, MoESys achieved 64% higher throughput with 18% lower memory\nfootprints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While modern internet services, such as chatbots, search engines, and online\nadvertising, demand the use of large-scale deep neural networks (DNNs),\ndistributed training and inference over heterogeneous computing systems are\ndesired to facilitate these DNN models. Mixture-of-Experts (MoE) is one the\nmost common strategies to lower the cost of training subject to the overall\nsize of models/data through gating and parallelism in a divide-and-conquer\nfashion. While DeepSpeed has made efforts in carrying out large-scale MoE\ntraining over heterogeneous infrastructures, the efficiency of training and\ninference could be further improved from several system aspects, including load\nbalancing, communication/computation efficiency, and memory footprint limits.\nIn this work, we present a novel MoESys that boosts efficiency in both\nlarge-scale training and inference. Specifically, in the training procedure,\nthe proposed MoESys adopts an Elastic MoE training strategy with 2D prefetch\nand Fusion communication over Hierarchical storage, so as to enjoy efficient\nparallelisms. For scalable inference in a single node, especially when the\nmodel size is larger than GPU memory, MoESys builds the CPU-GPU memory jointly\ninto a ring of sections to load the model, and executes the computation tasks\nacross the memory sections in a round-robin manner for efficient inference. We\ncarried out extensive experiments to evaluate MoESys, where MoESys successfully\ntrains a Unified Feature Optimization (UFO) model with a Sparsely-Gated\nMixture-of-Experts model of 12B parameters in 8 days on 48 A100 GPU cards. The\ncomparison against the state-of-the-art shows that MoESys outperformed\nDeepSpeed with 33% higher throughput (tokens per second) in training and 13%\nhigher throughput in inference in general. Particularly, under unbalanced MoE\nTasks, e.g., UFO, MoESys achieved 64% higher throughput with 18% lower memory\nfootprints."
                },
                "authors": [
                    {
                        "name": "Dianhai Yu"
                    },
                    {
                        "name": "Liang Shen"
                    },
                    {
                        "name": "Hongxiang Hao"
                    },
                    {
                        "name": "Weibao Gong"
                    },
                    {
                        "name": "Huachao Wu"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Lirong Dai"
                    },
                    {
                        "name": "Haoyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Haoyi Xiong"
                },
                "author": "Haoyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2205.10034v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2205.10034v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06003v1",
                "updated": "2024-08-12T08:52:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    52,
                    14,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:52:14Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    52,
                    14,
                    0,
                    225,
                    0
                ],
                "title": "LUT Tensor Core: Lookup Table Enables Efficient Low-Bit LLM Inference\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUT Tensor Core: Lookup Table Enables Efficient Low-Bit LLM Inference\n  Acceleration"
                },
                "summary": "As large language model (LLM) inference demands ever-greater resources, there\nis a rapid growing trend of using low-bit weights to shrink memory usage and\nboost inference efficiency. However, these low-bit LLMs introduce the need for\nmixed-precision matrix multiplication (mpGEMM), which is a crucial yet\nunder-explored operation that involves multiplying lower-precision weights with\nhigher-precision activations. Unfortunately, current hardware does not natively\nsupport mpGEMM, resulting in indirect and inefficient dequantization-based\nimplementations.\n  To address the mpGEMM requirements in low-bit LLMs, we explored the lookup\ntable (LUT)-based approach for mpGEMM. However, a conventional LUT\nimplementation falls short of its potential. To fully harness the power of\nLUT-based mpGEMM, we introduce LUT Tensor Core, a software-hardware co-design\noptimized for low-bit LLM inference. Specifically, we introduce software-based\noperator fusion and table symmetrization techniques to optimize table\nprecompute and table storage, respectively. Then, LUT Tensor Core proposes the\nhardware design featuring an elongated tiling shape design to enhance table\nreuse and a bit-serial design to support various precision combinations in\nmpGEMM. Moreover, we design an end-to-end compilation stack with new\ninstructions for LUT-based mpGEMM, enabling efficient LLM compilation and\noptimizations. The evaluation on low-bit LLMs (e.g., BitNet, LLAMA) shows that\nLUT Tensor Core achieves more than a magnitude of improvements on both compute\ndensity and energy efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language model (LLM) inference demands ever-greater resources, there\nis a rapid growing trend of using low-bit weights to shrink memory usage and\nboost inference efficiency. However, these low-bit LLMs introduce the need for\nmixed-precision matrix multiplication (mpGEMM), which is a crucial yet\nunder-explored operation that involves multiplying lower-precision weights with\nhigher-precision activations. Unfortunately, current hardware does not natively\nsupport mpGEMM, resulting in indirect and inefficient dequantization-based\nimplementations.\n  To address the mpGEMM requirements in low-bit LLMs, we explored the lookup\ntable (LUT)-based approach for mpGEMM. However, a conventional LUT\nimplementation falls short of its potential. To fully harness the power of\nLUT-based mpGEMM, we introduce LUT Tensor Core, a software-hardware co-design\noptimized for low-bit LLM inference. Specifically, we introduce software-based\noperator fusion and table symmetrization techniques to optimize table\nprecompute and table storage, respectively. Then, LUT Tensor Core proposes the\nhardware design featuring an elongated tiling shape design to enhance table\nreuse and a bit-serial design to support various precision combinations in\nmpGEMM. Moreover, we design an end-to-end compilation stack with new\ninstructions for LUT-based mpGEMM, enabling efficient LLM compilation and\noptimizations. The evaluation on low-bit LLMs (e.g., BitNet, LLAMA) shows that\nLUT Tensor Core achieves more than a magnitude of improvements on both compute\ndensity and energy efficiency."
                },
                "authors": [
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Jianyu Wei"
                    },
                    {
                        "name": "Zhichen Zeng"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Jilong Xue"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05992v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05992v1",
                "updated": "2024-08-12T08:40:20Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    40,
                    20,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:40:20Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    40,
                    20,
                    0,
                    225,
                    0
                ],
                "title": "Transfer learning of state-based potential games for process\n  optimization in decentralized manufacturing systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer learning of state-based potential games for process\n  optimization in decentralized manufacturing systems"
                },
                "summary": "This paper presents a novel transfer learning approach in state-based\npotential games (TL-SbPGs) for enhancing distributed self-optimization in\nmanufacturing systems. The approach focuses on the practical relevant\nindustrial setting where sharing and transferring gained knowledge among\nsimilar-behaved players improves the self-learning mechanism in large-scale\nsystems. With TL-SbPGs, the gained knowledge can be reused by other players to\noptimize their policies, thereby improving the learning outcomes of the players\nand accelerating the learning process. To accomplish this goal, we develop\ntransfer learning concepts and similarity criteria for players, which offer two\ndistinct settings: (a) predefined similarities between players and (b)\ndynamically inferred similarities between players during training. We formally\nprove the applicability of the SbPG framework in transfer learning.\nAdditionally, we introduce an efficient method to determine the optimal timing\nand weighting of the transfer learning procedure during the training phase.\nThrough experiments on a laboratory-scale testbed, we demonstrate that TL-SbPGs\nsignificantly boost production efficiency while reducing power consumption of\nthe production schedules while also outperforming native SbPGs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel transfer learning approach in state-based\npotential games (TL-SbPGs) for enhancing distributed self-optimization in\nmanufacturing systems. The approach focuses on the practical relevant\nindustrial setting where sharing and transferring gained knowledge among\nsimilar-behaved players improves the self-learning mechanism in large-scale\nsystems. With TL-SbPGs, the gained knowledge can be reused by other players to\noptimize their policies, thereby improving the learning outcomes of the players\nand accelerating the learning process. To accomplish this goal, we develop\ntransfer learning concepts and similarity criteria for players, which offer two\ndistinct settings: (a) predefined similarities between players and (b)\ndynamically inferred similarities between players during training. We formally\nprove the applicability of the SbPG framework in transfer learning.\nAdditionally, we introduce an efficient method to determine the optimal timing\nand weighting of the transfer learning procedure during the training phase.\nThrough experiments on a laboratory-scale testbed, we demonstrate that TL-SbPGs\nsignificantly boost production efficiency while reducing power consumption of\nthe production schedules while also outperforming native SbPGs."
                },
                "authors": [
                    {
                        "name": "Steve Yuwono"
                    },
                    {
                        "name": "Dorothea Schwung"
                    },
                    {
                        "name": "Andreas Schwung"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Schwung"
                },
                "author": "Andreas Schwung",
                "arxiv_comment": "This pre-print was submitted to Computers in Industry on May 02, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05992v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05992v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.04191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.04191v2",
                "updated": "2024-08-12T08:34:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    34,
                    26,
                    0,
                    225,
                    0
                ],
                "published": "2024-01-08T19:06:59Z",
                "published_parsed": [
                    2024,
                    1,
                    8,
                    19,
                    6,
                    59,
                    0,
                    8,
                    0
                ],
                "title": "Dense Hopfield Networks in the Teacher-Student Setting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dense Hopfield Networks in the Teacher-Student Setting"
                },
                "summary": "Dense Hopfield networks are known for their feature to prototype transition\nand adversarial robustness. However, previous theoretical studies have been\nmostly concerned with their storage capacity. We bridge this gap by studying\nthe phase diagram of p-body Hopfield networks in the teacher-student setting of\nan unsupervised learning problem, uncovering ferromagnetic phases reminiscent\nof the prototype and feature learning regimes. On the Nishimori line, we find\nthe critical size of the training set necessary for efficient pattern\nretrieval. Interestingly, we find that that the paramagnetic to ferromagnetic\ntransition of the teacher-student setting coincides with the paramagnetic to\nspin-glass transition of the direct model, i.e. with random patterns. Outside\nof the Nishimori line, we investigate the learning performance in relation to\nthe inference temperature and dataset noise. Moreover, we show that using a\nlarger p for the student than the teacher gives the student an extensive\ntolerance to noise. We then derive a closed-form expression measuring the\nadversarial robustness of such a student at zero temperature, corroborating the\npositive correlation between number of parameters and robustness observed in\nlarge neural networks. We also use our model to clarify why the prototype phase\nof modern Hopfield networks is adversarially robust.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dense Hopfield networks are known for their feature to prototype transition\nand adversarial robustness. However, previous theoretical studies have been\nmostly concerned with their storage capacity. We bridge this gap by studying\nthe phase diagram of p-body Hopfield networks in the teacher-student setting of\nan unsupervised learning problem, uncovering ferromagnetic phases reminiscent\nof the prototype and feature learning regimes. On the Nishimori line, we find\nthe critical size of the training set necessary for efficient pattern\nretrieval. Interestingly, we find that that the paramagnetic to ferromagnetic\ntransition of the teacher-student setting coincides with the paramagnetic to\nspin-glass transition of the direct model, i.e. with random patterns. Outside\nof the Nishimori line, we investigate the learning performance in relation to\nthe inference temperature and dataset noise. Moreover, we show that using a\nlarger p for the student than the teacher gives the student an extensive\ntolerance to noise. We then derive a closed-form expression measuring the\nadversarial robustness of such a student at zero temperature, corroborating the\npositive correlation between number of parameters and robustness observed in\nlarge neural networks. We also use our model to clarify why the prototype phase\nof modern Hopfield networks is adversarially robust."
                },
                "authors": [
                    {
                        "name": "Robin Th√©riault"
                    },
                    {
                        "name": "Daniele Tantari"
                    }
                ],
                "author_detail": {
                    "name": "Daniele Tantari"
                },
                "author": "Daniele Tantari",
                "arxiv_doi": "10.21468/SciPostPhys.17.2.040",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.21468/SciPostPhys.17.2.040",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.04191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.04191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "34 pages, 9 figures, updated to match published version, implemented\n  minor changes proposed in referee reports",
                "arxiv_journal_ref": "SciPost Phys. 17 (2024) 040",
                "arxiv_primary_category": {
                    "term": "cond-mat.dis-nn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.MP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05990v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05990v1",
                "updated": "2024-08-12T08:33:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    33,
                    9,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:33:09Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    33,
                    9,
                    0,
                    225,
                    0
                ],
                "title": "Parameters Inference for Nonlinear Wave Equations with Markovian\n  Switching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameters Inference for Nonlinear Wave Equations with Markovian\n  Switching"
                },
                "summary": "Traditional partial differential equations with constant coefficients often\nstruggle to capture abrupt changes in real-world phenomena, leading to the\ndevelopment of variable coefficient PDEs and Markovian switching models.\nRecently, research has introduced the concept of PDEs with Markov switching\nmodels, established their well-posedness and presented numerical methods.\nHowever, there has been limited discussion on parameter estimation for the jump\ncoefficients in these models. This paper addresses this gap by focusing on\nparameter inference for the wave equation with Markovian switching. We propose\na Bayesian statistical framework using discrete sparse Bayesian learning to\nestablish its convergence and a uniform error bound. Our method requires fewer\nassumptions and enables independent parameter inference for each segment by\nallowing different underlying structures for the parameter estimation problem\nwithin each segmented time interval. The effectiveness of our approach is\ndemonstrated through three numerical cases, which involve noisy spatiotemporal\ndata from different wave equations with Markovian switching. The results show\nstrong performance in parameter estimation for variable coefficient PDEs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional partial differential equations with constant coefficients often\nstruggle to capture abrupt changes in real-world phenomena, leading to the\ndevelopment of variable coefficient PDEs and Markovian switching models.\nRecently, research has introduced the concept of PDEs with Markov switching\nmodels, established their well-posedness and presented numerical methods.\nHowever, there has been limited discussion on parameter estimation for the jump\ncoefficients in these models. This paper addresses this gap by focusing on\nparameter inference for the wave equation with Markovian switching. We propose\na Bayesian statistical framework using discrete sparse Bayesian learning to\nestablish its convergence and a uniform error bound. Our method requires fewer\nassumptions and enables independent parameter inference for each segment by\nallowing different underlying structures for the parameter estimation problem\nwithin each segmented time interval. The effectiveness of our approach is\ndemonstrated through three numerical cases, which involve noisy spatiotemporal\ndata from different wave equations with Markovian switching. The results show\nstrong performance in parameter estimation for variable coefficient PDEs."
                },
                "authors": [
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zhikun Zhang"
                    },
                    {
                        "name": "Xiangjun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangjun Wang"
                },
                "author": "Xiangjun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05990v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03095v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03095v3",
                "updated": "2024-08-12T08:27:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    27,
                    56,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-06T10:52:41Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    10,
                    52,
                    41,
                    1,
                    219,
                    0
                ],
                "title": "TestART: Improving LLM-based Unit Test via Co-evolution of Automated\n  Generation and Repair Iteration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TestART: Improving LLM-based Unit Test via Co-evolution of Automated\n  Generation and Repair Iteration"
                },
                "summary": "Unit test is crucial for detecting bugs in individual program units but\nconsumes time and effort. The existing automated unit test generation methods\nare mainly based on search-based software testing (SBST) and language models to\nliberate developers. Recently, large language models (LLMs) have demonstrated\nremarkable reasoning and generation capabilities. However, several problems\nlimit their ability to generate high-quality test cases: (1) LLMs may generate\ninvalid test cases under insufficient context, resulting in compilation errors;\n(2) Lack of test and coverage feedback information may cause runtime errors and\nlow coverage rates. (3) The repetitive suppression problem causes LLMs to get\nstuck into the repetition loop of self-repair or re-generation attempts. In\nthis paper, we propose TestART, a novel unit test generation method that\nleverages the strengths of LLMs while overcoming the limitations mentioned.\nTestART improves LLM-based unit test via co-evolution of automated generation\nand repair iteration. TestART leverages the template-based repair technique to\nfix bugs in LLM-generated test cases, using prompt injection to guide the\nnext-step automated generation and avoid repetition suppression. Furthermore,\nTestART extracts coverage information from the passed test cases and utilizes\nit as testing feedback to enhance the sufficiency of the final test case. This\nsynergy between generation and repair elevates the quality, effectiveness, and\nreadability of the produced test cases significantly beyond previous methods.\nIn comparative experiments, the pass rate of TestART-generated test cases is\n78.55%, which is approximately 18% higher than both the ChatGPT-4.0 model and\nthe same ChatGPT-3.5-based method ChatUniTest. It also achieves an impressive\nline coverage rate of 90.96% on the focal methods that passed the test,\nexceeding EvoSuite by 3.4%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit test is crucial for detecting bugs in individual program units but\nconsumes time and effort. The existing automated unit test generation methods\nare mainly based on search-based software testing (SBST) and language models to\nliberate developers. Recently, large language models (LLMs) have demonstrated\nremarkable reasoning and generation capabilities. However, several problems\nlimit their ability to generate high-quality test cases: (1) LLMs may generate\ninvalid test cases under insufficient context, resulting in compilation errors;\n(2) Lack of test and coverage feedback information may cause runtime errors and\nlow coverage rates. (3) The repetitive suppression problem causes LLMs to get\nstuck into the repetition loop of self-repair or re-generation attempts. In\nthis paper, we propose TestART, a novel unit test generation method that\nleverages the strengths of LLMs while overcoming the limitations mentioned.\nTestART improves LLM-based unit test via co-evolution of automated generation\nand repair iteration. TestART leverages the template-based repair technique to\nfix bugs in LLM-generated test cases, using prompt injection to guide the\nnext-step automated generation and avoid repetition suppression. Furthermore,\nTestART extracts coverage information from the passed test cases and utilizes\nit as testing feedback to enhance the sufficiency of the final test case. This\nsynergy between generation and repair elevates the quality, effectiveness, and\nreadability of the produced test cases significantly beyond previous methods.\nIn comparative experiments, the pass rate of TestART-generated test cases is\n78.55%, which is approximately 18% higher than both the ChatGPT-4.0 model and\nthe same ChatGPT-3.5-based method ChatUniTest. It also achieves an impressive\nline coverage rate of 90.96% on the focal methods that passed the test,\nexceeding EvoSuite by 3.4%."
                },
                "authors": [
                    {
                        "name": "Siqi Gu"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Quanjun Zhang"
                    },
                    {
                        "name": "Fangyuan Tian"
                    },
                    {
                        "name": "Jianyi Zhou"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03095v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03095v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21335v2",
                "updated": "2024-08-12T08:21:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    21,
                    36,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-31T04:57:06Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    57,
                    6,
                    2,
                    213,
                    0
                ],
                "title": "On-the-fly Point Feature Representation for Point Clouds Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-the-fly Point Feature Representation for Point Clouds Analysis"
                },
                "summary": "Point cloud analysis is challenging due to its unique characteristics of\nunorderness, sparsity and irregularity. Prior works attempt to capture local\nrelationships by convolution operations or attention mechanisms, exploiting\ngeometric information from coordinates implicitly. These methods, however, are\ninsufficient to describe the explicit local geometry, e.g., curvature and\norientation. In this paper, we propose On-the-fly Point Feature Representation\n(OPFR), which captures abundant geometric information explicitly through Curve\nFeature Generator module. This is inspired by Point Feature Histogram (PFH)\nfrom computer vision community. However, the utilization of vanilla PFH\nencounters great difficulties when applied to large datasets and dense point\nclouds, as it demands considerable time for feature generation. In contrast, we\nintroduce the Local Reference Constructor module, which approximates the local\ncoordinate systems based on triangle sets. Owing to this, our OPFR only\nrequires extra 1.56ms for inference (65x faster than vanilla PFH) and 0.012M\nmore parameters, and it can serve as a versatile plug-and-play module for\nvarious backbones, particularly MLP-based and Transformer-based backbones\nexamined in this study. Additionally, we introduce the novel Hierarchical\nSampling module aimed at enhancing the quality of triangle sets, thereby\nensuring robustness of the obtained geometric features. Our proposed method\nimproves overall accuracy (OA) on ModelNet40 from 90.7% to 94.5% (+3.8%) for\nclassification, and OA on S3DIS Area-5 from 86.4% to 90.0% (+3.6%) for semantic\nsegmentation, respectively, building upon PointNet++ backbone. When integrated\nwith Point Transformer backbone, we achieve state-of-the-art results on both\ntasks: 94.8% OA on ModelNet40 and 91.7% OA on S3DIS Area-5.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point cloud analysis is challenging due to its unique characteristics of\nunorderness, sparsity and irregularity. Prior works attempt to capture local\nrelationships by convolution operations or attention mechanisms, exploiting\ngeometric information from coordinates implicitly. These methods, however, are\ninsufficient to describe the explicit local geometry, e.g., curvature and\norientation. In this paper, we propose On-the-fly Point Feature Representation\n(OPFR), which captures abundant geometric information explicitly through Curve\nFeature Generator module. This is inspired by Point Feature Histogram (PFH)\nfrom computer vision community. However, the utilization of vanilla PFH\nencounters great difficulties when applied to large datasets and dense point\nclouds, as it demands considerable time for feature generation. In contrast, we\nintroduce the Local Reference Constructor module, which approximates the local\ncoordinate systems based on triangle sets. Owing to this, our OPFR only\nrequires extra 1.56ms for inference (65x faster than vanilla PFH) and 0.012M\nmore parameters, and it can serve as a versatile plug-and-play module for\nvarious backbones, particularly MLP-based and Transformer-based backbones\nexamined in this study. Additionally, we introduce the novel Hierarchical\nSampling module aimed at enhancing the quality of triangle sets, thereby\nensuring robustness of the obtained geometric features. Our proposed method\nimproves overall accuracy (OA) on ModelNet40 from 90.7% to 94.5% (+3.8%) for\nclassification, and OA on S3DIS Area-5 from 86.4% to 90.0% (+3.6%) for semantic\nsegmentation, respectively, building upon PointNet++ backbone. When integrated\nwith Point Transformer backbone, we achieve state-of-the-art results on both\ntasks: 94.8% OA on ModelNet40 and 91.7% OA on S3DIS Area-5."
                },
                "authors": [
                    {
                        "name": "Jiangyi Wang"
                    },
                    {
                        "name": "Zhongyao Cheng"
                    },
                    {
                        "name": "Na Zhao"
                    },
                    {
                        "name": "Jun Cheng"
                    },
                    {
                        "name": "Xulei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xulei Yang"
                },
                "author": "Xulei Yang",
                "arxiv_doi": "10.1145/3664647.3680700",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3680700",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.21335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM MM 2024",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15740v2",
                "updated": "2024-08-12T08:21:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    21,
                    32,
                    0,
                    225,
                    0
                ],
                "published": "2024-03-23T06:36:32Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    6,
                    36,
                    32,
                    5,
                    83,
                    0
                ],
                "title": "Protecting Copyrighted Material with Unique Identifiers in Large\n  Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting Copyrighted Material with Unique Identifiers in Large\n  Language Model Training"
                },
                "summary": "A major public concern regarding the training of large language models (LLMs)\nis whether they abusing copyrighted online text. Previous membership inference\nmethods may be misled by similar examples in vast amounts of training data.\nAdditionally, these methods are often too complex for general users to\nunderstand and use, making them centralized, lacking transparency, and\ntrustworthiness. To address these issues, we propose an alternative\n\\textit{insert-and-detection} methodology, advocating that web users and\ncontent platforms employ \\textbf{\\textit{unique identifiers}} for reliable and\nindependent membership inference. Users and platforms can create their own\nidentifiers, embed them in copyrighted text, and independently detect them in\nfuture LLMs. As an initial demonstration, we introduce \\textit{ghost\nsentences}, a primitive form of unique identifiers, consisting primarily of\npassphrases made up of random words. By embedding one ghost sentences in a few\ncopyrighted texts, users can detect its membership using a perplexity test and\na \\textit{user-friendly} last-$k$ words test. The perplexity test is based on\nthe fact that LLMs trained on natural language should exhibit high perplexity\nwhen encountering unnatural passphrases. As the repetition increases, users can\nleverage the verbatim memorization ability of LLMs to perform a last-$k$ words\ntest by chatting with LLMs without writing any code. Both tests offer rigorous\nstatistical guarantees for membership inference. For LLaMA-13B, a perplexity\ntest on 30 ghost sentences with an average of 7 repetitions in 148K examples\nyields a 0.891 ROC AUC. For the last-$k$ words test with OpenLLaMA-3B, 11 out\nof 16 users, with an average of 24 examples each, successfully identify their\ndata from 1.8M examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major public concern regarding the training of large language models (LLMs)\nis whether they abusing copyrighted online text. Previous membership inference\nmethods may be misled by similar examples in vast amounts of training data.\nAdditionally, these methods are often too complex for general users to\nunderstand and use, making them centralized, lacking transparency, and\ntrustworthiness. To address these issues, we propose an alternative\n\\textit{insert-and-detection} methodology, advocating that web users and\ncontent platforms employ \\textbf{\\textit{unique identifiers}} for reliable and\nindependent membership inference. Users and platforms can create their own\nidentifiers, embed them in copyrighted text, and independently detect them in\nfuture LLMs. As an initial demonstration, we introduce \\textit{ghost\nsentences}, a primitive form of unique identifiers, consisting primarily of\npassphrases made up of random words. By embedding one ghost sentences in a few\ncopyrighted texts, users can detect its membership using a perplexity test and\na \\textit{user-friendly} last-$k$ words test. The perplexity test is based on\nthe fact that LLMs trained on natural language should exhibit high perplexity\nwhen encountering unnatural passphrases. As the repetition increases, users can\nleverage the verbatim memorization ability of LLMs to perform a last-$k$ words\ntest by chatting with LLMs without writing any code. Both tests offer rigorous\nstatistical guarantees for membership inference. For LLaMA-13B, a perplexity\ntest on 30 ghost sentences with an average of 7 repetitions in 148K examples\nyields a 0.891 ROC AUC. For the last-$k$ words test with OpenLLaMA-3B, 11 out\nof 16 users, with an average of 24 examples each, successfully identify their\ndata from 1.8M examples."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Linchao Zhu"
                    },
                    {
                        "name": "Ruijie Quan"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05982v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05982v1",
                "updated": "2024-08-12T08:17:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    17,
                    14,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:17:14Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    17,
                    14,
                    0,
                    225,
                    0
                ],
                "title": "Exploring and Learning Structure: Active Inference Approach in\n  Navigational Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring and Learning Structure: Active Inference Approach in\n  Navigational Agents"
                },
                "summary": "Drawing inspiration from animal navigation strategies, we introduce a novel\ncomputational model for navigation and mapping, rooted in biologically inspired\nprinciples. Animals exhibit remarkable navigation abilities by efficiently\nusing memory, imagination, and strategic decision-making to navigate complex\nand aliased environments. Building on these insights, we integrate traditional\ncognitive mapping approaches with an Active Inference Framework (AIF) to learn\nan environment structure in a few steps. Through the incorporation of\ntopological mapping for long-term memory and AIF for navigation planning and\nstructure learning, our model can dynamically apprehend environmental\nstructures and expand its internal map with predicted beliefs during\nexploration. Comparative experiments with the Clone-Structured Graph (CSCG)\nmodel highlight our model's ability to rapidly learn environmental structures\nin a single episode, with minimal navigation overlap. this is achieved without\nprior knowledge of the dimensions of the environment or the type of\nobservations, showcasing its robustness and effectiveness in navigating\nambiguous environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drawing inspiration from animal navigation strategies, we introduce a novel\ncomputational model for navigation and mapping, rooted in biologically inspired\nprinciples. Animals exhibit remarkable navigation abilities by efficiently\nusing memory, imagination, and strategic decision-making to navigate complex\nand aliased environments. Building on these insights, we integrate traditional\ncognitive mapping approaches with an Active Inference Framework (AIF) to learn\nan environment structure in a few steps. Through the incorporation of\ntopological mapping for long-term memory and AIF for navigation planning and\nstructure learning, our model can dynamically apprehend environmental\nstructures and expand its internal map with predicted beliefs during\nexploration. Comparative experiments with the Clone-Structured Graph (CSCG)\nmodel highlight our model's ability to rapidly learn environmental structures\nin a single episode, with minimal navigation overlap. this is achieved without\nprior knowledge of the dimensions of the environment or the type of\nobservations, showcasing its robustness and effectiveness in navigating\nambiguous environments."
                },
                "authors": [
                    {
                        "name": "Daria de Tinguy"
                    },
                    {
                        "name": "Tim Verbelen"
                    },
                    {
                        "name": "Bart Dhoedt"
                    }
                ],
                "author_detail": {
                    "name": "Bart Dhoedt"
                },
                "author": "Bart Dhoedt",
                "arxiv_comment": "IWAI workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05982v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05982v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05974v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05974v1",
                "updated": "2024-08-12T08:02:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    2,
                    37,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:02:37Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    2,
                    37,
                    0,
                    225,
                    0
                ],
                "title": "Unseen No More: Unlocking the Potential of CLIP for Generative Zero-shot\n  HOI Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unseen No More: Unlocking the Potential of CLIP for Generative Zero-shot\n  HOI Detection"
                },
                "summary": "Zero-shot human-object interaction (HOI) detector is capable of generalizing\nto HOI categories even not encountered during training. Inspired by the\nimpressive zero-shot capabilities offered by CLIP, latest methods strive to\nleverage CLIP embeddings for improving zero-shot HOI detection. However, these\nembedding-based methods train the classifier on seen classes only, inevitably\nresulting in seen-unseen confusion for the model during inference. Besides, we\nfind that using prompt-tuning and adapters further increases the gap between\nseen and unseen accuracy. To tackle this challenge, we present the first\ngeneration-based model using CLIP for zero-shot HOI detection, coined HOIGen.\nIt allows to unlock the potential of CLIP for feature generation instead of\nfeature extraction only. To achieve it, we develop a CLIP-injected feature\ngenerator in accordance with the generation of human, object and union\nfeatures. Then, we extract realistic features of seen samples and mix them with\nsynthetic features together, allowing the model to train seen and unseen\nclasses jointly. To enrich the HOI scores, we construct a generative prototype\nbank in a pairwise HOI recognition branch, and a multi-knowledge prototype bank\nin an image-wise HOI recognition branch, respectively. Extensive experiments on\nHICO-DET benchmark demonstrate our HOIGen achieves superior performance for\nboth seen and unseen classes under various zero-shot settings, compared with\nother top-performing methods. Code is available at:\nhttps://github.com/soberguo/HOIGen",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-shot human-object interaction (HOI) detector is capable of generalizing\nto HOI categories even not encountered during training. Inspired by the\nimpressive zero-shot capabilities offered by CLIP, latest methods strive to\nleverage CLIP embeddings for improving zero-shot HOI detection. However, these\nembedding-based methods train the classifier on seen classes only, inevitably\nresulting in seen-unseen confusion for the model during inference. Besides, we\nfind that using prompt-tuning and adapters further increases the gap between\nseen and unseen accuracy. To tackle this challenge, we present the first\ngeneration-based model using CLIP for zero-shot HOI detection, coined HOIGen.\nIt allows to unlock the potential of CLIP for feature generation instead of\nfeature extraction only. To achieve it, we develop a CLIP-injected feature\ngenerator in accordance with the generation of human, object and union\nfeatures. Then, we extract realistic features of seen samples and mix them with\nsynthetic features together, allowing the model to train seen and unseen\nclasses jointly. To enrich the HOI scores, we construct a generative prototype\nbank in a pairwise HOI recognition branch, and a multi-knowledge prototype bank\nin an image-wise HOI recognition branch, respectively. Extensive experiments on\nHICO-DET benchmark demonstrate our HOIGen achieves superior performance for\nboth seen and unseen classes under various zero-shot settings, compared with\nother top-performing methods. Code is available at:\nhttps://github.com/soberguo/HOIGen"
                },
                "authors": [
                    {
                        "name": "Yixin Guo"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Jianghao Li"
                    },
                    {
                        "name": "Weimin Wang"
                    },
                    {
                        "name": "Qi Jia"
                    }
                ],
                "author_detail": {
                    "name": "Qi Jia"
                },
                "author": "Qi Jia",
                "arxiv_doi": "10.1145/3664647.3680927",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3664647.3680927",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.05974v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05974v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM MM 2024",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05968v1",
                "updated": "2024-08-12T07:49:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    49,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T07:49:28Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    49,
                    28,
                    0,
                    225,
                    0
                ],
                "title": "Nob-MIAs: Non-biased Membership Inference Attacks Assessment on Large\n  Language Models with Ex-Post Dataset Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nob-MIAs: Non-biased Membership Inference Attacks Assessment on Large\n  Language Models with Ex-Post Dataset Construction"
                },
                "summary": "The rise of Large Language Models (LLMs) has triggered legal and ethical\nconcerns, especially regarding the unauthorized use of copyrighted materials in\ntheir training datasets. This has led to lawsuits against tech companies\naccused of using protected content without permission. Membership Inference\nAttacks (MIAs) aim to detect whether specific documents were used in a given\nLLM pretraining, but their effectiveness is undermined by biases such as\ntime-shifts and n-gram overlaps.\n  This paper addresses the evaluation of MIAs on LLMs with partially inferable\ntraining sets, under the ex-post hypothesis, which acknowledges inherent\ndistributional biases between members and non-members datasets. We propose and\nvalidate algorithms to create ``non-biased'' and ``non-classifiable'' datasets\nfor fairer MIA assessment. Experiments using the Gutenberg dataset on OpenLamma\nand Pythia show that neutralizing known biases alone is insufficient. Our\nmethods produce non-biased ex-post datasets with AUC-ROC scores comparable to\nthose previously obtained on genuinely random datasets, validating our\napproach. Globally, MIAs yield results close to random, with only one being\neffective on both random and our datasets, but its performance decreases when\nbias is removed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) has triggered legal and ethical\nconcerns, especially regarding the unauthorized use of copyrighted materials in\ntheir training datasets. This has led to lawsuits against tech companies\naccused of using protected content without permission. Membership Inference\nAttacks (MIAs) aim to detect whether specific documents were used in a given\nLLM pretraining, but their effectiveness is undermined by biases such as\ntime-shifts and n-gram overlaps.\n  This paper addresses the evaluation of MIAs on LLMs with partially inferable\ntraining sets, under the ex-post hypothesis, which acknowledges inherent\ndistributional biases between members and non-members datasets. We propose and\nvalidate algorithms to create ``non-biased'' and ``non-classifiable'' datasets\nfor fairer MIA assessment. Experiments using the Gutenberg dataset on OpenLamma\nand Pythia show that neutralizing known biases alone is insufficient. Our\nmethods produce non-biased ex-post datasets with AUC-ROC scores comparable to\nthose previously obtained on genuinely random datasets, validating our\napproach. Globally, MIAs yield results close to random, with only one being\neffective on both random and our datasets, but its performance decreases when\nbias is removed."
                },
                "authors": [
                    {
                        "name": "C√©dric Eichler"
                    },
                    {
                        "name": "Nathan Champeil"
                    },
                    {
                        "name": "Nicolas Anciaux"
                    },
                    {
                        "name": "Alexandra Bensamoun"
                    },
                    {
                        "name": "Heber Hwang Arcolezi"
                    },
                    {
                        "name": "Jos√© Maria De Fuentes"
                    }
                ],
                "author_detail": {
                    "name": "Jos√© Maria De Fuentes"
                },
                "author": "Jos√© Maria De Fuentes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.18279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.18279v2",
                "updated": "2024-08-12T07:14:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    14,
                    0,
                    0,
                    225,
                    0
                ],
                "published": "2023-05-29T17:50:33Z",
                "published_parsed": [
                    2023,
                    5,
                    29,
                    17,
                    50,
                    33,
                    0,
                    149,
                    0
                ],
                "title": "Contextual Object Detection with Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Object Detection with Multimodal Large Language Models"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) are remarkable in\nvision-language tasks, such as image captioning and question answering, but\nlack the essential perception ability, i.e., object detection. In this work, we\naddress this limitation by introducing a novel research problem of contextual\nobject detection -- understanding visible objects within different human-AI\ninteractive contexts. Three representative scenarios are investigated,\nincluding the language cloze test, visual captioning, and question answering.\nMoreover, we present ContextDET, a unified multimodal model that is capable of\nend-to-end differentiable modeling of visual-language contexts, so as to\nlocate, identify, and associate visual objects with language inputs for\nhuman-AI interaction. Our ContextDET involves three key submodels: (i) a visual\nencoder for extracting visual representations, (ii) a pre-trained LLM for\nmultimodal context decoding, and (iii) a visual decoder for predicting bounding\nboxes given contextual object words. The new generate-then-detect framework\nenables us to detect object words within human vocabulary. Extensive\nexperiments show the advantages of ContextDET on our proposed CODE benchmark,\nopen-vocabulary detection, and referring image segmentation. Github:\nhttps://github.com/yuhangzang/ContextDET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) are remarkable in\nvision-language tasks, such as image captioning and question answering, but\nlack the essential perception ability, i.e., object detection. In this work, we\naddress this limitation by introducing a novel research problem of contextual\nobject detection -- understanding visible objects within different human-AI\ninteractive contexts. Three representative scenarios are investigated,\nincluding the language cloze test, visual captioning, and question answering.\nMoreover, we present ContextDET, a unified multimodal model that is capable of\nend-to-end differentiable modeling of visual-language contexts, so as to\nlocate, identify, and associate visual objects with language inputs for\nhuman-AI interaction. Our ContextDET involves three key submodels: (i) a visual\nencoder for extracting visual representations, (ii) a pre-trained LLM for\nmultimodal context decoding, and (iii) a visual decoder for predicting bounding\nboxes given contextual object words. The new generate-then-detect framework\nenables us to detect object words within human vocabulary. Extensive\nexperiments show the advantages of ContextDET on our proposed CODE benchmark,\nopen-vocabulary detection, and referring image segmentation. Github:\nhttps://github.com/yuhangzang/ContextDET."
                },
                "authors": [
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Jun Han"
                    },
                    {
                        "name": "Kaiyang Zhou"
                    },
                    {
                        "name": "Chen Change Loy"
                    }
                ],
                "author_detail": {
                    "name": "Chen Change Loy"
                },
                "author": "Chen Change Loy",
                "arxiv_comment": "IJCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.18279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.18279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05948v1",
                "updated": "2024-08-12T06:48:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    48,
                    43,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T06:48:43Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    48,
                    43,
                    0,
                    225,
                    0
                ],
                "title": "ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge\n  Graph QA datasets with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge\n  Graph QA datasets with Large Language Models"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) and conversational\nassistants necessitates dynamic, scalable, and configurable conversational\ndatasets for training and evaluation. These datasets must accommodate diverse\nuser interaction modes, including text and voice, each presenting unique\nmodeling challenges. Knowledge Graphs (KGs), with their structured and evolving\nnature, offer an ideal foundation for current and precise knowledge. Although\nhuman-curated KG-based conversational datasets exist, they struggle to keep\npace with the rapidly changing user information needs. We present ConvKGYarn, a\nscalable method for generating up-to-date and configurable conversational KGQA\ndatasets. Qualitative psychometric analyses confirm our method can generate\nhigh-quality datasets rivaling a popular conversational KGQA dataset while\noffering it at scale and covering a wide range of human-interaction\nconfigurations. We showcase its utility by testing LLMs on diverse\nconversations - exploring model behavior on conversational KGQA sets with\ndifferent configurations grounded in the same KG fact set. Our results\nhighlight the ability of ConvKGYarn to improve KGQA foundations and evaluate\nparametric knowledge of LLMs, thus offering a robust solution to the constantly\nevolving landscape of conversational assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) and conversational\nassistants necessitates dynamic, scalable, and configurable conversational\ndatasets for training and evaluation. These datasets must accommodate diverse\nuser interaction modes, including text and voice, each presenting unique\nmodeling challenges. Knowledge Graphs (KGs), with their structured and evolving\nnature, offer an ideal foundation for current and precise knowledge. Although\nhuman-curated KG-based conversational datasets exist, they struggle to keep\npace with the rapidly changing user information needs. We present ConvKGYarn, a\nscalable method for generating up-to-date and configurable conversational KGQA\ndatasets. Qualitative psychometric analyses confirm our method can generate\nhigh-quality datasets rivaling a popular conversational KGQA dataset while\noffering it at scale and covering a wide range of human-interaction\nconfigurations. We showcase its utility by testing LLMs on diverse\nconversations - exploring model behavior on conversational KGQA sets with\ndifferent configurations grounded in the same KG fact set. Our results\nhighlight the ability of ConvKGYarn to improve KGQA foundations and evaluate\nparametric knowledge of LLMs, thus offering a robust solution to the constantly\nevolving landscape of conversational assistants."
                },
                "authors": [
                    {
                        "name": "Ronak Pradeep"
                    },
                    {
                        "name": "Daniel Lee"
                    },
                    {
                        "name": "Ali Mousavi"
                    },
                    {
                        "name": "Jeff Pound"
                    },
                    {
                        "name": "Yisi Sang"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Ihab Ilyas"
                    },
                    {
                        "name": "Saloni Potdar"
                    },
                    {
                        "name": "Mostafa Arefiyan"
                    },
                    {
                        "name": "Yunyao Li"
                    }
                ],
                "author_detail": {
                    "name": "Yunyao Li"
                },
                "author": "Yunyao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05944v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05944v1",
                "updated": "2024-08-12T06:45:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    45,
                    6,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T06:45:06Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    45,
                    6,
                    0,
                    225,
                    0
                ],
                "title": "Uncertainty Quantification of Spectral Estimator and MLE for Orthogonal\n  Group Synchronization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty Quantification of Spectral Estimator and MLE for Orthogonal\n  Group Synchronization"
                },
                "summary": "Orthogonal group synchronization aims to recover orthogonal group elements\nfrom their noisy pairwise measurements. It has found numerous applications\nincluding computer vision, imaging science, and community detection. Due to the\northogonal constraints, it is often challenging to find the least squares\nestimator in presence of noise. In the recent years, semidefinite relaxation\n(SDR) and spectral methods have proven to be powerful tools in recovering the\ngroup elements. In particular, under additive Gaussian noise, the SDR exactly\nproduces the maximum likelihood estimator (MLE), and both MLE and spectral\nmethods are able to achieve near-optimal statistical error. In this work, we\ntake one step further to quantify the uncertainty of the MLE and spectral\nestimators by considering their distributions. By leveraging the orthogonality\nconstraints in the likelihood function, we obtain a second-order expansion of\nthe MLE and spectral estimator with the leading terms as an anti-symmetric\nGaussian random matrix that is on the tangent space of the orthogonal matrix.\nThis also implies state-of-the-art min-max risk bounds as a by-product. Our\nworks provide a general theoretical framework that is potentially useful to\nfind an approximate distribution of the estimators arising from many\nstatistical inference problems with manifold constraints. The numerical\nexperiments confirm our theoretical contribution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orthogonal group synchronization aims to recover orthogonal group elements\nfrom their noisy pairwise measurements. It has found numerous applications\nincluding computer vision, imaging science, and community detection. Due to the\northogonal constraints, it is often challenging to find the least squares\nestimator in presence of noise. In the recent years, semidefinite relaxation\n(SDR) and spectral methods have proven to be powerful tools in recovering the\ngroup elements. In particular, under additive Gaussian noise, the SDR exactly\nproduces the maximum likelihood estimator (MLE), and both MLE and spectral\nmethods are able to achieve near-optimal statistical error. In this work, we\ntake one step further to quantify the uncertainty of the MLE and spectral\nestimators by considering their distributions. By leveraging the orthogonality\nconstraints in the likelihood function, we obtain a second-order expansion of\nthe MLE and spectral estimator with the leading terms as an anti-symmetric\nGaussian random matrix that is on the tangent space of the orthogonal matrix.\nThis also implies state-of-the-art min-max risk bounds as a by-product. Our\nworks provide a general theoretical framework that is potentially useful to\nfind an approximate distribution of the estimators arising from many\nstatistical inference problems with manifold constraints. The numerical\nexperiments confirm our theoretical contribution."
                },
                "authors": [
                    {
                        "name": "Ziliang Samuel Zhong"
                    },
                    {
                        "name": "Shuyang Ling"
                    }
                ],
                "author_detail": {
                    "name": "Shuyang Ling"
                },
                "author": "Shuyang Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05944v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05944v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20613v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20613v2",
                "updated": "2024-08-12T06:36:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    36,
                    10,
                    0,
                    225,
                    0
                ],
                "published": "2024-05-31T04:05:09Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    4,
                    5,
                    9,
                    4,
                    152,
                    0
                ],
                "title": "FineRadScore: A Radiology Report Line-by-Line Evaluation Technique\n  Generating Corrections with Severity Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineRadScore: A Radiology Report Line-by-Line Evaluation Technique\n  Generating Corrections with Severity Scores"
                },
                "summary": "The current gold standard for evaluating generated chest x-ray (CXR) reports\nis through radiologist annotations. However, this process can be extremely\ntime-consuming and costly, especially when evaluating large numbers of reports.\nIn this work, we present FineRadScore, a Large Language Model (LLM)-based\nautomated evaluation metric for generated CXR reports. Given a candidate report\nand a ground-truth report, FineRadScore gives the minimum number of\nline-by-line corrections required to go from the candidate to the ground-truth\nreport. Additionally, FineRadScore provides an error severity rating with each\ncorrection and generates comments explaining why the correction was needed. We\ndemonstrate that FineRadScore's corrections and error severity scores align\nwith radiologist opinions. We also show that, when used to judge the quality of\nthe report as a whole, FineRadScore aligns with radiologists as well as current\nstate-of-the-art automated CXR evaluation metrics. Finally, we analyze\nFineRadScore's shortcomings to provide suggestions for future improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current gold standard for evaluating generated chest x-ray (CXR) reports\nis through radiologist annotations. However, this process can be extremely\ntime-consuming and costly, especially when evaluating large numbers of reports.\nIn this work, we present FineRadScore, a Large Language Model (LLM)-based\nautomated evaluation metric for generated CXR reports. Given a candidate report\nand a ground-truth report, FineRadScore gives the minimum number of\nline-by-line corrections required to go from the candidate to the ground-truth\nreport. Additionally, FineRadScore provides an error severity rating with each\ncorrection and generates comments explaining why the correction was needed. We\ndemonstrate that FineRadScore's corrections and error severity scores align\nwith radiologist opinions. We also show that, when used to judge the quality of\nthe report as a whole, FineRadScore aligns with radiologists as well as current\nstate-of-the-art automated CXR evaluation metrics. Finally, we analyze\nFineRadScore's shortcomings to provide suggestions for future improvements."
                },
                "authors": [
                    {
                        "name": "Alyssa Huang"
                    },
                    {
                        "name": "Oishi Banerjee"
                    },
                    {
                        "name": "Kay Wu"
                    },
                    {
                        "name": "Eduardo Pontes Reis"
                    },
                    {
                        "name": "Pranav Rajpurkar"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Rajpurkar"
                },
                "author": "Pranav Rajpurkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20613v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20613v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05941v1",
                "updated": "2024-08-12T06:36:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    36,
                    8,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T06:36:08Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    36,
                    8,
                    0,
                    225,
                    0
                ],
                "title": "Multimodal Large Language Models for Phishing Webpage Detection and\n  Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models for Phishing Webpage Detection and\n  Identification"
                },
                "summary": "To address the challenging problem of detecting phishing webpages,\nresearchers have developed numerous solutions, in particular those based on\nmachine learning (ML) algorithms. Among these, brand-based phishing detection\nthat uses models from Computer Vision to detect if a given webpage is imitating\na well-known brand has received widespread attention. However, such models are\ncostly and difficult to maintain, as they need to be retrained with labeled\ndataset that has to be regularly and continuously collected. Besides, they also\nneed to maintain a good reference list of well-known websites and related\nmeta-data for effective performance.\n  In this work, we take steps to study the efficacy of large language models\n(LLMs), in particular the multimodal LLMs, in detecting phishing webpages.\nGiven that the LLMs are pretrained on a large corpus of data, we aim to make\nuse of their understanding of different aspects of a webpage (logo, theme,\nfavicon, etc.) to identify the brand of a given webpage and compare the\nidentified brand with the domain name in the URL to detect a phishing attack.\nWe propose a two-phase system employing LLMs in both phases: the first phase\nfocuses on brand identification, while the second verifies the domain. We carry\nout comprehensive evaluations on a newly collected dataset. Our experiments\nshow that the LLM-based system achieves a high detection rate at high\nprecision; importantly, it also provides interpretable evidence for the\ndecisions. Our system also performs significantly better than a\nstate-of-the-art brand-based phishing detection system while demonstrating\nrobustness against two known adversarial attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address the challenging problem of detecting phishing webpages,\nresearchers have developed numerous solutions, in particular those based on\nmachine learning (ML) algorithms. Among these, brand-based phishing detection\nthat uses models from Computer Vision to detect if a given webpage is imitating\na well-known brand has received widespread attention. However, such models are\ncostly and difficult to maintain, as they need to be retrained with labeled\ndataset that has to be regularly and continuously collected. Besides, they also\nneed to maintain a good reference list of well-known websites and related\nmeta-data for effective performance.\n  In this work, we take steps to study the efficacy of large language models\n(LLMs), in particular the multimodal LLMs, in detecting phishing webpages.\nGiven that the LLMs are pretrained on a large corpus of data, we aim to make\nuse of their understanding of different aspects of a webpage (logo, theme,\nfavicon, etc.) to identify the brand of a given webpage and compare the\nidentified brand with the domain name in the URL to detect a phishing attack.\nWe propose a two-phase system employing LLMs in both phases: the first phase\nfocuses on brand identification, while the second verifies the domain. We carry\nout comprehensive evaluations on a newly collected dataset. Our experiments\nshow that the LLM-based system achieves a high detection rate at high\nprecision; importantly, it also provides interpretable evidence for the\ndecisions. Our system also performs significantly better than a\nstate-of-the-art brand-based phishing detection system while demonstrating\nrobustness against two known adversarial attacks."
                },
                "authors": [
                    {
                        "name": "Jehyun Lee"
                    },
                    {
                        "name": "Peiyuan Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Dinil Mon Divakaran"
                    }
                ],
                "author_detail": {
                    "name": "Dinil Mon Divakaran"
                },
                "author": "Dinil Mon Divakaran",
                "arxiv_comment": "To appear in eCrime 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06363v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06363v3",
                "updated": "2024-08-12T06:17:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    17,
                    21,
                    0,
                    225,
                    0
                ],
                "published": "2023-12-11T13:11:04Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    13,
                    11,
                    4,
                    0,
                    345,
                    0
                ],
                "title": "MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples"
                },
                "summary": "Although In-Context Learning (ICL) brings remarkable performance gains to\nLarge Language Models (LLMs), the improvements remain lower than fine-tuning on\ndownstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT),\na novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning by\nfully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). We\npropose the Multi-Modal Hub (M-Hub), a unified module that captures various\nmulti-modal features according to different inputs and objectives. Based on\nM-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textual\nfeatures and subsequently generate outputs conditioned on the textual-guided\nvisual features. Moreover, leveraging the flexibility of M-Hub, we design a\nvariety of in-context demonstrations. Extensive experiments on a diverse range\nof downstream multi-modal tasks demonstrate that MMICT significantly\noutperforms traditional fine-tuning strategy and the vanilla ICT method that\ndirectly takes the concatenation of all information from different modalities\nas input. Our implementation is available at:\nhttps://github.com/KDEGroup/MMICT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although In-Context Learning (ICL) brings remarkable performance gains to\nLarge Language Models (LLMs), the improvements remain lower than fine-tuning on\ndownstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT),\na novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning by\nfully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). We\npropose the Multi-Modal Hub (M-Hub), a unified module that captures various\nmulti-modal features according to different inputs and objectives. Based on\nM-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textual\nfeatures and subsequently generate outputs conditioned on the textual-guided\nvisual features. Moreover, leveraging the flexibility of M-Hub, we design a\nvariety of in-context demonstrations. Extensive experiments on a diverse range\nof downstream multi-modal tasks demonstrate that MMICT significantly\noutperforms traditional fine-tuning strategy and the vanilla ICT method that\ndirectly takes the concatenation of all information from different modalities\nas input. Our implementation is available at:\nhttps://github.com/KDEGroup/MMICT."
                },
                "authors": [
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Enwei Zhang"
                    },
                    {
                        "name": "Yuting Gao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "arxiv_comment": "TOMM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06363v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06363v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.13762v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.13762v4",
                "updated": "2024-08-12T06:16:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    16,
                    47,
                    0,
                    225,
                    0
                ],
                "published": "2023-08-26T04:54:47Z",
                "published_parsed": [
                    2023,
                    8,
                    26,
                    4,
                    54,
                    47,
                    5,
                    238,
                    0
                ],
                "title": "In situ observation of chemistry in Rydberg molecules within a coherent\n  solvent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In situ observation of chemistry in Rydberg molecules within a coherent\n  solvent"
                },
                "summary": "We often infer the state of systems in nature indirectly, for example, in\nhigh-energy physics by the interaction of particles with an ambient medium. We\nadapt this principle to energies $9$ orders of magnitude smaller, to classify\nthe final state of exotic molecules after internal conversion of their\nelectronic state, through their interaction with an ambient quantum fluid, a\nBose-Einstein condensate (BEC). The BEC is the ground-state of a million\nbosonic atoms near zero temperature, and a single embedded ultra-long range\nRydberg molecule can coherently excite waves in this fluid, which carry\ntelltale signatures of its dynamics. Bond lengths exceeding a micrometer allow\nus to observe the molecular fingerprint on the BEC in-situ, via optical\nmicroscopy. Interpreting images in comparison with simulations strongly\nsuggests that the molecular electronic state rapidly converts from the\ninitially excited S and D orbitals to a much more complex molecular state\n(called \"trilobite''), marked by a maximally localized electron. This internal\nconversion liberates energy, such that one expects final-state particles to\nmove rapidly through the medium, which is however ruled out by comparing\nexperiment and simulations. The molecule thus must strongly decelerate in the\nmedium, for which we propose a plausible mechanism. Our experiment demonstrates\na medium that facilitates and records an electronic state change of embedded\nexotic molecules in ultra-cold chemistry, with sufficient sensitivity to\nconstrain velocities of final-state particles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We often infer the state of systems in nature indirectly, for example, in\nhigh-energy physics by the interaction of particles with an ambient medium. We\nadapt this principle to energies $9$ orders of magnitude smaller, to classify\nthe final state of exotic molecules after internal conversion of their\nelectronic state, through their interaction with an ambient quantum fluid, a\nBose-Einstein condensate (BEC). The BEC is the ground-state of a million\nbosonic atoms near zero temperature, and a single embedded ultra-long range\nRydberg molecule can coherently excite waves in this fluid, which carry\ntelltale signatures of its dynamics. Bond lengths exceeding a micrometer allow\nus to observe the molecular fingerprint on the BEC in-situ, via optical\nmicroscopy. Interpreting images in comparison with simulations strongly\nsuggests that the molecular electronic state rapidly converts from the\ninitially excited S and D orbitals to a much more complex molecular state\n(called \"trilobite''), marked by a maximally localized electron. This internal\nconversion liberates energy, such that one expects final-state particles to\nmove rapidly through the medium, which is however ruled out by comparing\nexperiment and simulations. The molecule thus must strongly decelerate in the\nmedium, for which we propose a plausible mechanism. Our experiment demonstrates\na medium that facilitates and records an electronic state change of embedded\nexotic molecules in ultra-cold chemistry, with sufficient sensitivity to\nconstrain velocities of final-state particles."
                },
                "authors": [
                    {
                        "name": "Felix Engel"
                    },
                    {
                        "name": "Shiva Kant Tiwari"
                    },
                    {
                        "name": "Tilman Pfau"
                    },
                    {
                        "name": "Sebastian W√ºster"
                    },
                    {
                        "name": "Florian Meinert"
                    }
                ],
                "author_detail": {
                    "name": "Florian Meinert"
                },
                "author": "Florian Meinert",
                "arxiv_doi": "10.1103/PhysRevResearch.6.033150",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevResearch.6.033150",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2308.13762v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.13762v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages and 11 figures",
                "arxiv_journal_ref": "Phys. Rev. Research 6, 033150 (2024)",
                "arxiv_primary_category": {
                    "term": "cond-mat.quant-gas",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.quant-gas",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05933v1",
                "updated": "2024-08-12T06:16:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    16,
                    37,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T06:16:37Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    16,
                    37,
                    0,
                    225,
                    0
                ],
                "title": "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case\n  Study with Locally Deployed Ollama Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case\n  Study with Locally Deployed Ollama Models"
                },
                "summary": "With the growing demand for offline PDF chatbots in automotive industrial\nproduction environments, optimizing the deployment of large language models\n(LLMs) in local, low-performance settings has become increasingly important.\nThis study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques\nfor processing complex automotive industry documents using locally deployed\nOllama models. Based on the Langchain framework, we propose a multi-dimensional\noptimization approach for Ollama's local RAG implementation. Our method\naddresses key challenges in automotive document processing, including\nmulti-column layouts and technical specifications. We introduce improvements in\nPDF processing, retrieval mechanisms, and context compression, tailored to the\nunique characteristics of automotive industry documents. Additionally, we\ndesign custom classes supporting embedding pipelines and an agent supporting\nself-RAG based on LangGraph best practices. To evaluate our approach, we\nconstructed a proprietary dataset comprising typical automotive industry\ndocuments, including technical reports and corporate regulations. We compared\nour optimized RAG model and self-RAG agent against a naive RAG baseline across\nthree datasets: our automotive industry dataset, QReCC, and CoQA. Results\ndemonstrate significant improvements in context precision, context recall,\nanswer relevancy, and faithfulness, with particularly notable performance on\nthe automotive industry dataset. Our optimization scheme provides an effective\nsolution for deploying local RAG systems in the automotive sector, addressing\nthe specific needs of PDF chatbots in industrial production environments. This\nresearch has important implications for advancing information processing and\nintelligent production in the automotive industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for offline PDF chatbots in automotive industrial\nproduction environments, optimizing the deployment of large language models\n(LLMs) in local, low-performance settings has become increasingly important.\nThis study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques\nfor processing complex automotive industry documents using locally deployed\nOllama models. Based on the Langchain framework, we propose a multi-dimensional\noptimization approach for Ollama's local RAG implementation. Our method\naddresses key challenges in automotive document processing, including\nmulti-column layouts and technical specifications. We introduce improvements in\nPDF processing, retrieval mechanisms, and context compression, tailored to the\nunique characteristics of automotive industry documents. Additionally, we\ndesign custom classes supporting embedding pipelines and an agent supporting\nself-RAG based on LangGraph best practices. To evaluate our approach, we\nconstructed a proprietary dataset comprising typical automotive industry\ndocuments, including technical reports and corporate regulations. We compared\nour optimized RAG model and self-RAG agent against a naive RAG baseline across\nthree datasets: our automotive industry dataset, QReCC, and CoQA. Results\ndemonstrate significant improvements in context precision, context recall,\nanswer relevancy, and faithfulness, with particularly notable performance on\nthe automotive industry dataset. Our optimization scheme provides an effective\nsolution for deploying local RAG systems in the automotive sector, addressing\nthe specific needs of PDF chatbots in industrial production environments. This\nresearch has important implications for advancing information processing and\nintelligent production in the automotive industry."
                },
                "authors": [
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Zejun Kang"
                    },
                    {
                        "name": "Xing Han"
                    }
                ],
                "author_detail": {
                    "name": "Xing Han"
                },
                "author": "Xing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05931v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05931v1",
                "updated": "2024-08-12T05:58:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    5,
                    58,
                    21,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T05:58:21Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    5,
                    58,
                    21,
                    0,
                    225,
                    0
                ],
                "title": "Study of a red clump giant, KIC~11087027, with high rotation and strong\n  infrared excess -- Evidence of tidal interaction for high lithium abundance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of a red clump giant, KIC~11087027, with high rotation and strong\n  infrared excess -- Evidence of tidal interaction for high lithium abundance"
                },
                "summary": "This paper presents results from Kepler photometric light curves and\nhigh-resolution spectroscopic study of a super Li-rich giant KIC11087027. Using\nthe light curve analysis, we measured the star's rotational period P$_{\\rm\nrot}$=30.4$\\pm$0.1~days, which translates to rotational velocity V$_{\\rm\nrot}$=19.5 $\\pm$ 1.7~km s$^{-1}$. Star's location in the HR-diagram, derived\nvalues of $^{12}C/^{13}C$ = 7$\\pm$1 and $[C/N]=-0.95\\pm 0.2$, and the inferred\nasteroseismic parameters from secondary calibration based on spectra suggest\nstar is a low-mass red clump giant in the He-core burning phase. Using Gaia\ndata, we found evidence of variation in radial velocity and proper motion,\nindicative of presence of an unresolved binary. The large V$_{\\rm rot}$ is\nprobably a result of tidal synchronization combined with the after-effects of\nHe-flash, in which the size of the star is reduced significantly. The\nsimultaneous presence of features like high rotation, very high Li abundance,\nstrong dust shell, and strong flares in a single star is relatively uncommon,\nsuggesting that the star experiencing tidal synchronization has recently\nundergone He-flash. The results pose a question whether the binary interaction,\nhence the high rotation, is a prerequisite for dredging-up of the high amounts\nof Li from the interior to the photosphere during or immediately after the\nHe-flash event.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents results from Kepler photometric light curves and\nhigh-resolution spectroscopic study of a super Li-rich giant KIC11087027. Using\nthe light curve analysis, we measured the star's rotational period P$_{\\rm\nrot}$=30.4$\\pm$0.1~days, which translates to rotational velocity V$_{\\rm\nrot}$=19.5 $\\pm$ 1.7~km s$^{-1}$. Star's location in the HR-diagram, derived\nvalues of $^{12}C/^{13}C$ = 7$\\pm$1 and $[C/N]=-0.95\\pm 0.2$, and the inferred\nasteroseismic parameters from secondary calibration based on spectra suggest\nstar is a low-mass red clump giant in the He-core burning phase. Using Gaia\ndata, we found evidence of variation in radial velocity and proper motion,\nindicative of presence of an unresolved binary. The large V$_{\\rm rot}$ is\nprobably a result of tidal synchronization combined with the after-effects of\nHe-flash, in which the size of the star is reduced significantly. The\nsimultaneous presence of features like high rotation, very high Li abundance,\nstrong dust shell, and strong flares in a single star is relatively uncommon,\nsuggesting that the star experiencing tidal synchronization has recently\nundergone He-flash. The results pose a question whether the binary interaction,\nhence the high rotation, is a prerequisite for dredging-up of the high amounts\nof Li from the interior to the photosphere during or immediately after the\nHe-flash event."
                },
                "authors": [
                    {
                        "name": "Raghubar Singh"
                    },
                    {
                        "name": "Anohita Mallick"
                    },
                    {
                        "name": "Bacham E. Reddy"
                    },
                    {
                        "name": "Jeewan C. Pandey"
                    },
                    {
                        "name": "Gang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Gang Zhao"
                },
                "author": "Gang Zhao",
                "arxiv_comment": "9 pages, 3 figures, 2 tables, accepted",
                "arxiv_journal_ref": "ApJ Letters 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05931v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05931v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05928v1",
                "updated": "2024-08-12T05:40:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    5,
                    40,
                    21,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T05:40:21Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    5,
                    40,
                    21,
                    0,
                    225,
                    0
                ],
                "title": "Adapting General Disentanglement-Based Speaker Anonymization for\n  Enhanced Emotion Preservation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adapting General Disentanglement-Based Speaker Anonymization for\n  Enhanced Emotion Preservation"
                },
                "summary": "A general disentanglement-based speaker anonymization system typically\nseparates speech into content, speaker, and prosody features using individual\nencoders. This paper explores how to adapt such a system when a new speech\nattribute, for example, emotion, needs to be preserved to a greater extent.\nWhile existing systems are good at anonymizing speaker embeddings, they are not\ndesigned to preserve emotion. Two strategies for this are examined. First, we\nshow that integrating emotion embeddings from a pre-trained emotion encoder can\nhelp preserve emotional cues, even though this approach slightly compromises\nprivacy protection. Alternatively, we propose an emotion compensation strategy\nas a post-processing step applied to anonymized speaker embeddings. This\nconceals the original speaker's identity and reintroduces the emotional traits\nlost during speaker embedding anonymization. Specifically, we model the emotion\nattribute using support vector machines to learn separate boundaries for each\nemotion. During inference, the original speaker embedding is processed in two\nways: one, by an emotion indicator to predict emotion and select the\nemotion-matched SVM accurately; and two, by a speaker anonymizer to conceal\nspeaker characteristics. The anonymized speaker embedding is then modified\nalong the corresponding SVM boundary towards an enhanced emotional direction to\nsave the emotional cues. The proposed strategies are also expected to be useful\nfor adapting a general disentanglement-based speaker anonymization system to\npreserve other target paralinguistic attributes, with potential for a range of\ndownstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A general disentanglement-based speaker anonymization system typically\nseparates speech into content, speaker, and prosody features using individual\nencoders. This paper explores how to adapt such a system when a new speech\nattribute, for example, emotion, needs to be preserved to a greater extent.\nWhile existing systems are good at anonymizing speaker embeddings, they are not\ndesigned to preserve emotion. Two strategies for this are examined. First, we\nshow that integrating emotion embeddings from a pre-trained emotion encoder can\nhelp preserve emotional cues, even though this approach slightly compromises\nprivacy protection. Alternatively, we propose an emotion compensation strategy\nas a post-processing step applied to anonymized speaker embeddings. This\nconceals the original speaker's identity and reintroduces the emotional traits\nlost during speaker embedding anonymization. Specifically, we model the emotion\nattribute using support vector machines to learn separate boundaries for each\nemotion. During inference, the original speaker embedding is processed in two\nways: one, by an emotion indicator to predict emotion and select the\nemotion-matched SVM accurately; and two, by a speaker anonymizer to conceal\nspeaker characteristics. The anonymized speaker embedding is then modified\nalong the corresponding SVM boundary towards an enhanced emotional direction to\nsave the emotional cues. The proposed strategies are also expected to be useful\nfor adapting a general disentanglement-based speaker anonymization system to\npreserve other target paralinguistic attributes, with potential for a range of\ndownstream tasks."
                },
                "authors": [
                    {
                        "name": "Xiaoxiao Miao"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Natalia Tomashenko"
                    },
                    {
                        "name": "Donny Cheng Lock Soh"
                    },
                    {
                        "name": "Ian Mcloughlin"
                    }
                ],
                "author_detail": {
                    "name": "Ian Mcloughlin"
                },
                "author": "Ian Mcloughlin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05927v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05927v1",
                "updated": "2024-08-12T05:33:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    5,
                    33,
                    45,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T05:33:45Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    5,
                    33,
                    45,
                    0,
                    225,
                    0
                ],
                "title": "A Simple Early Exiting Framework for Accelerated Sampling in Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple Early Exiting Framework for Accelerated Sampling in Diffusion\n  Models"
                },
                "summary": "Diffusion models have shown remarkable performance in generation problems\nover various domains including images, videos, text, and audio. A practical\nbottleneck of diffusion models is their sampling speed, due to the repeated\nevaluation of score estimation networks during the inference. In this work, we\npropose a novel framework capable of adaptively allocating compute required for\nthe score estimation, thereby reducing the overall sampling time of diffusion\nmodels. We observe that the amount of computation required for the score\nestimation may vary along the time step for which the score is estimated. Based\non this observation, we propose an early-exiting scheme, where we skip the\nsubset of parameters in the score estimation network during the inference,\nbased on a time-dependent exit schedule. Using the diffusion models for image\nsynthesis, we show that our method could significantly improve the sampling\nthroughput of the diffusion models without compromising image quality.\nFurthermore, we also demonstrate that our method seamlessly integrates with\nvarious types of solvers for faster sampling, capitalizing on their\ncompatibility to enhance overall efficiency. The source code and our\nexperiments are available at \\url{https://github.com/taehong-moon/ee-diffusion}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have shown remarkable performance in generation problems\nover various domains including images, videos, text, and audio. A practical\nbottleneck of diffusion models is their sampling speed, due to the repeated\nevaluation of score estimation networks during the inference. In this work, we\npropose a novel framework capable of adaptively allocating compute required for\nthe score estimation, thereby reducing the overall sampling time of diffusion\nmodels. We observe that the amount of computation required for the score\nestimation may vary along the time step for which the score is estimated. Based\non this observation, we propose an early-exiting scheme, where we skip the\nsubset of parameters in the score estimation network during the inference,\nbased on a time-dependent exit schedule. Using the diffusion models for image\nsynthesis, we show that our method could significantly improve the sampling\nthroughput of the diffusion models without compromising image quality.\nFurthermore, we also demonstrate that our method seamlessly integrates with\nvarious types of solvers for faster sampling, capitalizing on their\ncompatibility to enhance overall efficiency. The source code and our\nexperiments are available at \\url{https://github.com/taehong-moon/ee-diffusion}"
                },
                "authors": [
                    {
                        "name": "Taehong Moon"
                    },
                    {
                        "name": "Moonseok Choi"
                    },
                    {
                        "name": "EungGu Yun"
                    },
                    {
                        "name": "Jongmin Yoon"
                    },
                    {
                        "name": "Gayoung Lee"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Juho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Juho Lee"
                },
                "author": "Juho Lee",
                "arxiv_comment": "ICML 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05927v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05927v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12968v2",
                "updated": "2024-08-12T04:48:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    4,
                    48,
                    11,
                    0,
                    225,
                    0
                ],
                "published": "2024-03-19T17:59:56Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    59,
                    56,
                    1,
                    79,
                    0
                ],
                "title": "LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic\n  Prompt Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic\n  Prompt Compression"
                },
                "summary": "This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x. Our code is available at\nhttps://aka.ms/LLMLingua-2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x. Our code is available at\nhttps://aka.ms/LLMLingua-2."
                },
                "authors": [
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Menglin Xia"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Jue Zhang"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Victor R√ºhle"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "H. Vicky Zhao"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "arxiv_comment": "Accepted at Findings of ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05918v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05918v1",
                "updated": "2024-08-12T04:46:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    4,
                    46,
                    55,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T04:46:55Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    4,
                    46,
                    55,
                    0,
                    225,
                    0
                ],
                "title": "PAFormer: Part Aware Transformer for Person Re-identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAFormer: Part Aware Transformer for Person Re-identification"
                },
                "summary": "Within the domain of person re-identification (ReID), partial ReID methods\nare considered mainstream, aiming to measure feature distances through\ncomparisons of body parts between samples. However, in practice, previous\nmethods often lack sufficient awareness of anatomical aspect of body parts,\nresulting in the failure to capture features of the same body parts across\ndifferent samples. To address this issue, we introduce \\textbf{Part Aware\nTransformer (PAFormer)}, a pose estimation based ReID model which can perform\nprecise part-to-part comparison. In order to inject part awareness to pose\ntokens, we introduce learnable parameters called `pose token' which estimate\nthe correlation between each body part and partial regions of the image.\nNotably, at inference phase, PAFormer operates without additional modules\nrelated to body part localization, which is commonly used in previous ReID\nmethodologies leveraging pose estimation models. Additionally, leveraging the\nenhanced awareness of body parts, PAFormer suggests the use of a learning-based\nvisibility predictor to estimate the degree of occlusion for each body part.\nAlso, we introduce a teacher forcing technique using ground truth visibility\nscores which enables PAFormer to be trained only with visible parts. A set of\nextensive experiments show that our method outperforms existing approaches on\nwell-known ReID benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the domain of person re-identification (ReID), partial ReID methods\nare considered mainstream, aiming to measure feature distances through\ncomparisons of body parts between samples. However, in practice, previous\nmethods often lack sufficient awareness of anatomical aspect of body parts,\nresulting in the failure to capture features of the same body parts across\ndifferent samples. To address this issue, we introduce \\textbf{Part Aware\nTransformer (PAFormer)}, a pose estimation based ReID model which can perform\nprecise part-to-part comparison. In order to inject part awareness to pose\ntokens, we introduce learnable parameters called `pose token' which estimate\nthe correlation between each body part and partial regions of the image.\nNotably, at inference phase, PAFormer operates without additional modules\nrelated to body part localization, which is commonly used in previous ReID\nmethodologies leveraging pose estimation models. Additionally, leveraging the\nenhanced awareness of body parts, PAFormer suggests the use of a learning-based\nvisibility predictor to estimate the degree of occlusion for each body part.\nAlso, we introduce a teacher forcing technique using ground truth visibility\nscores which enables PAFormer to be trained only with visible parts. A set of\nextensive experiments show that our method outperforms existing approaches on\nwell-known ReID benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Hyeono Jung"
                    },
                    {
                        "name": "Jangwon Lee"
                    },
                    {
                        "name": "Jiwon Yoo"
                    },
                    {
                        "name": "Dami Ko"
                    },
                    {
                        "name": "Gyeonghwan Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyeonghwan Kim"
                },
                "author": "Gyeonghwan Kim",
                "arxiv_comment": "34 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05918v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05918v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.06839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.06839v2",
                "updated": "2024-08-12T03:53:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    35,
                    0,
                    225,
                    0
                ],
                "published": "2023-10-10T17:59:58Z",
                "published_parsed": [
                    2023,
                    10,
                    10,
                    17,
                    59,
                    58,
                    1,
                    283,
                    0
                ],
                "title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios\n  via Prompt Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios\n  via Prompt Compression"
                },
                "summary": "In long context scenarios, large language models (LLMs) face three main\nchallenges: higher computational cost, performance reduction, and position\nbias. Research indicates that LLM performance hinges on the density and\nposition of key information in the input prompt. Inspired by these findings, we\npropose LongLLMLingua for prompt compression towards improving LLMs' perception\nof the key information to simultaneously address the three challenges. Our\nextensive evaluation across various long context scenarios demonstrates that\nLongLLMLingua not only enhances performance but also significantly reduces\ncosts and latency. For instance, in the NaturalQuestions benchmark,\nLongLLMLingua boosts performance by up to 21.4% with around 4x fewer tokens in\nGPT-3.5-Turbo, leading to substantial cost savings. It achieves a 94.0% cost\nreduction in the LooGLE benchmark. Moreover, when compressing prompts of about\n10k tokens at ratios of 2x-6x, LongLLMLingua can accelerate end-to-end latency\nby 1.4x-2.6x. Our code is available at https://aka.ms/LongLLMLingua.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In long context scenarios, large language models (LLMs) face three main\nchallenges: higher computational cost, performance reduction, and position\nbias. Research indicates that LLM performance hinges on the density and\nposition of key information in the input prompt. Inspired by these findings, we\npropose LongLLMLingua for prompt compression towards improving LLMs' perception\nof the key information to simultaneously address the three challenges. Our\nextensive evaluation across various long context scenarios demonstrates that\nLongLLMLingua not only enhances performance but also significantly reduces\ncosts and latency. For instance, in the NaturalQuestions benchmark,\nLongLLMLingua boosts performance by up to 21.4% with around 4x fewer tokens in\nGPT-3.5-Turbo, leading to substantial cost savings. It achieves a 94.0% cost\nreduction in the LooGLE benchmark. Moreover, when compressing prompts of about\n10k tokens at ratios of 2x-6x, LongLLMLingua can accelerate end-to-end latency\nby 1.4x-2.6x. Our code is available at https://aka.ms/LongLLMLingua."
                },
                "authors": [
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.06839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.06839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05911v1",
                "updated": "2024-08-12T03:52:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    52,
                    11,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:52:11Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    52,
                    11,
                    0,
                    225,
                    0
                ],
                "title": "A New Pipeline For Generating Instruction Dataset via RAG and Self\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Pipeline For Generating Instruction Dataset via RAG and Self\n  Fine-Tuning"
                },
                "summary": "With the rapid development of large language models in recent years, there\nhas been an increasing demand for domain-specific Agents that can cater to the\nunique needs of enterprises and organizations. Unlike general models, which\nstrive for broad coverage, these specialized Agents rely on focused datasets\ntailored to their intended applications. This research proposes a pipeline that\nleverages the power of LLMs and the Retrieval-Augmented Generation related\nframework to construct high-quality instruction datasets for fine-tuning on\nspecific domains using custom document collections. By ingesting\ndomain-specific documents, the pipeline generates relevant and contextually\nappropriate instructions, thus effectively creating a comprehensive dataset for\nfine-tuning LLMs on the target domain. This approach overcomes the limitations\nof traditional dataset creation methods, which often rely on manual curation or\nweb-scraping techniques that may introduce noise and irrelevant data. Notably,\nour pipeline offers a dynamic solution that can quickly adapt to updates or\nmodifications in the domain-specific document collection, eliminating the need\nfor complete retraining. Additionally, it addresses the challenge of data\nscarcity by enabling the generation of instruction datasets from a limited set\nof initial documents, rendering it suitable for unpopular or specialized\ndomains where comprehensive datasets are scarce. As a case study, we apply this\napproach to the domain of psychiatry, a field requiring specialized knowledge\nand sensitive handling of patient information. The resulting fine-tuned LLM\ndemonstrates showcases the viability of the proposed approach and underscores\nits potential for widespread adoption across various industries and domains\nwhere tailored, accurate, and contextually relevant language models are\nindispensable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models in recent years, there\nhas been an increasing demand for domain-specific Agents that can cater to the\nunique needs of enterprises and organizations. Unlike general models, which\nstrive for broad coverage, these specialized Agents rely on focused datasets\ntailored to their intended applications. This research proposes a pipeline that\nleverages the power of LLMs and the Retrieval-Augmented Generation related\nframework to construct high-quality instruction datasets for fine-tuning on\nspecific domains using custom document collections. By ingesting\ndomain-specific documents, the pipeline generates relevant and contextually\nappropriate instructions, thus effectively creating a comprehensive dataset for\nfine-tuning LLMs on the target domain. This approach overcomes the limitations\nof traditional dataset creation methods, which often rely on manual curation or\nweb-scraping techniques that may introduce noise and irrelevant data. Notably,\nour pipeline offers a dynamic solution that can quickly adapt to updates or\nmodifications in the domain-specific document collection, eliminating the need\nfor complete retraining. Additionally, it addresses the challenge of data\nscarcity by enabling the generation of instruction datasets from a limited set\nof initial documents, rendering it suitable for unpopular or specialized\ndomains where comprehensive datasets are scarce. As a case study, we apply this\napproach to the domain of psychiatry, a field requiring specialized knowledge\nand sensitive handling of patient information. The resulting fine-tuned LLM\ndemonstrates showcases the viability of the proposed approach and underscores\nits potential for widespread adoption across various industries and domains\nwhere tailored, accurate, and contextually relevant language models are\nindispensable."
                },
                "authors": [
                    {
                        "name": "Chih-Wei Song"
                    },
                    {
                        "name": "Yu-Kai Lee"
                    },
                    {
                        "name": "Yin-Te Tsai"
                    }
                ],
                "author_detail": {
                    "name": "Yin-Te Tsai"
                },
                "author": "Yin-Te Tsai",
                "arxiv_comment": "5 pages, SCA 2024: The 7th IEEE International Workshop on Smart\n  Computing & Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03179v2",
                "updated": "2024-08-12T03:31:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    31,
                    57,
                    0,
                    225,
                    0
                ],
                "published": "2024-04-04T03:28:57Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    3,
                    28,
                    57,
                    3,
                    95,
                    0
                ],
                "title": "UniAV: Unified Audio-Visual Perception for Multi-Task Video Event\n  Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniAV: Unified Audio-Visual Perception for Multi-Task Video Event\n  Localization"
                },
                "summary": "Video localization tasks aim to temporally locate specific instances in\nvideos, including temporal action localization (TAL), sound event detection\n(SED) and audio-visual event localization (AVEL). Existing methods\nover-specialize on each task, overlooking the fact that these instances often\noccur in the same video to form the complete video content. In this work, we\npresent UniAV, a Unified Audio-Visual perception network, to achieve joint\nlearning of TAL, SED and AVEL tasks for the first time. UniAV can leverage\ndiverse data available in task-specific datasets, allowing the model to learn\nand share mutually beneficial knowledge across tasks and modalities. To tackle\nthe challenges posed by substantial variations in datasets\n(size/domain/duration) and distinct task characteristics, we propose to\nuniformly encode visual and audio modalities of all videos to derive generic\nrepresentations, while also designing task-specific experts to capture unique\nknowledge for each task. Besides, we develop a unified language-aware\nclassifier by utilizing a pre-trained text encoder, enabling the model to\nflexibly detect various types of instances and previously unseen ones by simply\nchanging prompts during inference. UniAV outperforms its single-task\ncounterparts by a large margin with fewer parameters, achieving on-par or\nsuperior performances compared to state-of-the-art task-specific methods across\nActivityNet 1.3, DESED and UnAV-100 benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video localization tasks aim to temporally locate specific instances in\nvideos, including temporal action localization (TAL), sound event detection\n(SED) and audio-visual event localization (AVEL). Existing methods\nover-specialize on each task, overlooking the fact that these instances often\noccur in the same video to form the complete video content. In this work, we\npresent UniAV, a Unified Audio-Visual perception network, to achieve joint\nlearning of TAL, SED and AVEL tasks for the first time. UniAV can leverage\ndiverse data available in task-specific datasets, allowing the model to learn\nand share mutually beneficial knowledge across tasks and modalities. To tackle\nthe challenges posed by substantial variations in datasets\n(size/domain/duration) and distinct task characteristics, we propose to\nuniformly encode visual and audio modalities of all videos to derive generic\nrepresentations, while also designing task-specific experts to capture unique\nknowledge for each task. Besides, we develop a unified language-aware\nclassifier by utilizing a pre-trained text encoder, enabling the model to\nflexibly detect various types of instances and previously unseen ones by simply\nchanging prompts during inference. UniAV outperforms its single-task\ncounterparts by a large margin with fewer parameters, achieving on-par or\nsuperior performances compared to state-of-the-art task-specific methods across\nActivityNet 1.3, DESED and UnAV-100 benchmarks."
                },
                "authors": [
                    {
                        "name": "Tiantian Geng"
                    },
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Yanfu Zhang"
                    },
                    {
                        "name": "Jinming Duan"
                    },
                    {
                        "name": "Weili Guan"
                    },
                    {
                        "name": "Feng Zheng"
                    },
                    {
                        "name": "Ling shao"
                    }
                ],
                "author_detail": {
                    "name": "Ling shao"
                },
                "author": "Ling shao",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05803v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05803v2",
                "updated": "2024-08-12T03:29:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    29,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-05-09T14:38:53Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    14,
                    38,
                    53,
                    3,
                    130,
                    0
                ],
                "title": "Boosting Multimodal Large Language Models with Visual Tokens Withdrawal\n  for Rapid Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Multimodal Large Language Models with Visual Tokens Withdrawal\n  for Rapid Inference"
                },
                "summary": "Multimodal large language models (MLLMs) demand considerable computations for\ninference due to the extensive parameters and the additional input tokens\nneeded for visual information representation. Herein, we introduce Visual\nTokens Withdrawal (VTW), a plug-and-play module to boost MLLMs for rapid\ninference. Our approach is inspired by two intriguing phenomena we have\nobserved: (1) the attention sink phenomenon that is prevalent in LLMs also\npersists in MLLMs, suggesting that initial tokens and nearest tokens receive\nthe majority of attention, while middle vision tokens garner minimal attention\nin deep layers; (2) the presence of information migration, which implies that\nvisual information is transferred to subsequent text tokens within the first\nfew layers of MLLMs. As per our findings, we conclude that vision tokens are\nunnecessary in the deep layers of MLLMs. Thus, we strategically withdraw them\nat a certain layer, enabling only text tokens to engage in subsequent layers.\nTo pinpoint the ideal layer for VTW, we initially analyze a limited set of tiny\ndatasets and choose the first layer that meets the Kullback-Leibler divergence\ncriterion. Our VTW approach can cut computational overhead by over 40\\% across\ndiverse multimodal tasks while maintaining performance. Our code is released at\n\\url{https://github.com/lzhxmu/VTW}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) demand considerable computations for\ninference due to the extensive parameters and the additional input tokens\nneeded for visual information representation. Herein, we introduce Visual\nTokens Withdrawal (VTW), a plug-and-play module to boost MLLMs for rapid\ninference. Our approach is inspired by two intriguing phenomena we have\nobserved: (1) the attention sink phenomenon that is prevalent in LLMs also\npersists in MLLMs, suggesting that initial tokens and nearest tokens receive\nthe majority of attention, while middle vision tokens garner minimal attention\nin deep layers; (2) the presence of information migration, which implies that\nvisual information is transferred to subsequent text tokens within the first\nfew layers of MLLMs. As per our findings, we conclude that vision tokens are\nunnecessary in the deep layers of MLLMs. Thus, we strategically withdraw them\nat a certain layer, enabling only text tokens to engage in subsequent layers.\nTo pinpoint the ideal layer for VTW, we initially analyze a limited set of tiny\ndatasets and choose the first layer that meets the Kullback-Leibler divergence\ncriterion. Our VTW approach can cut computational overhead by over 40\\% across\ndiverse multimodal tasks while maintaining performance. Our code is released at\n\\url{https://github.com/lzhxmu/VTW}."
                },
                "authors": [
                    {
                        "name": "Zhihang Lin"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Luxi Lin"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05803v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05803v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.12258v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.12258v3",
                "updated": "2024-08-12T03:23:09Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    23,
                    9,
                    0,
                    225,
                    0
                ],
                "published": "2023-01-28T17:30:47Z",
                "published_parsed": [
                    2023,
                    1,
                    28,
                    17,
                    30,
                    47,
                    5,
                    28,
                    0
                ],
                "title": "Cross-domain Neural Pitch and Periodicity Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-domain Neural Pitch and Periodicity Estimation"
                },
                "summary": "Pitch is a foundational aspect of our perception of audio signals. Pitch\ncontours are commonly used to analyze speech and music signals and as input\nfeatures for many audio tasks, including music transcription, singing voice\nsynthesis, and prosody editing. In this paper, we describe a set of techniques\nfor improving the accuracy of widely-used neural pitch and periodicity\nestimators to achieve state-of-the-art performance on both speech and music. We\nalso introduce a novel entropy-based method for extracting periodicity and\nper-frame voiced-unvoiced classifications from statistical inference-based\npitch estimators (e.g., neural networks), and show how to train a neural pitch\nestimator to simultaneously handle both speech and music data (i.e.,\ncross-domain estimation) without performance degradation. Our estimator\nimplementations run 11.2x faster than real-time on a Intel i9-9820X 10-core\n3.30 GHz CPU$\\unicode{x2014}$approaching the speed of state-of-the-art\nDSP-based pitch estimators$\\unicode{x2014}$or 408x faster than real-time on a\nNVIDIA GeForce RTX 3090 GPU. We release all of our code and models as\nPitch-Estimating Neural Networks (penn), an open-source, pip-installable Python\nmodule for training, evaluating, and performing inference with pitch- and\nperiodicity-estimating neural networks. The code for penn is available at\nhttps://github.com/interactiveaudiolab/penn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pitch is a foundational aspect of our perception of audio signals. Pitch\ncontours are commonly used to analyze speech and music signals and as input\nfeatures for many audio tasks, including music transcription, singing voice\nsynthesis, and prosody editing. In this paper, we describe a set of techniques\nfor improving the accuracy of widely-used neural pitch and periodicity\nestimators to achieve state-of-the-art performance on both speech and music. We\nalso introduce a novel entropy-based method for extracting periodicity and\nper-frame voiced-unvoiced classifications from statistical inference-based\npitch estimators (e.g., neural networks), and show how to train a neural pitch\nestimator to simultaneously handle both speech and music data (i.e.,\ncross-domain estimation) without performance degradation. Our estimator\nimplementations run 11.2x faster than real-time on a Intel i9-9820X 10-core\n3.30 GHz CPU$\\unicode{x2014}$approaching the speed of state-of-the-art\nDSP-based pitch estimators$\\unicode{x2014}$or 408x faster than real-time on a\nNVIDIA GeForce RTX 3090 GPU. We release all of our code and models as\nPitch-Estimating Neural Networks (penn), an open-source, pip-installable Python\nmodule for training, evaluating, and performing inference with pitch- and\nperiodicity-estimating neural networks. The code for penn is available at\nhttps://github.com/interactiveaudiolab/penn."
                },
                "authors": [
                    {
                        "name": "Max Morrison"
                    },
                    {
                        "name": "Caedon Hsieh"
                    },
                    {
                        "name": "Nathan Pruyne"
                    },
                    {
                        "name": "Bryan Pardo"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Pardo"
                },
                "author": "Bryan Pardo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.12258v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.12258v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07089v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07089v3",
                "updated": "2024-08-12T02:44:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    2,
                    44,
                    33,
                    0,
                    225,
                    0
                ],
                "published": "2024-05-11T20:20:58Z",
                "published_parsed": [
                    2024,
                    5,
                    11,
                    20,
                    20,
                    58,
                    5,
                    132,
                    0
                ],
                "title": "SonifyAR: Context-Aware Sound Generation in Augmented Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SonifyAR: Context-Aware Sound Generation in Augmented Reality"
                },
                "summary": "Sound plays a crucial role in enhancing user experience and immersiveness in\nAugmented Reality (AR). However, current platforms lack support for AR sound\nauthoring due to limited interaction types, challenges in collecting and\nspecifying context information, and difficulty in acquiring matching sound\nassets. We present SonifyAR, an LLM-based AR sound authoring system that\ngenerates context-aware sound effects for AR experiences. SonifyAR expands the\ncurrent design space of AR sound and implements a Programming by Demonstration\n(PbD) pipeline to automatically collect contextual information of AR events,\nincluding virtual content semantics and real world context. This context\ninformation is then processed by a large language model to acquire sound\neffects with Recommendation, Retrieval, Generation, and Transfer methods. To\nevaluate the usability and performance of our system, we conducted a user study\nwith eight participants and created five example applications, including an\nAR-based science experiment, an improving case for AR headset safety, and an\nassisting example for low vision AR users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sound plays a crucial role in enhancing user experience and immersiveness in\nAugmented Reality (AR). However, current platforms lack support for AR sound\nauthoring due to limited interaction types, challenges in collecting and\nspecifying context information, and difficulty in acquiring matching sound\nassets. We present SonifyAR, an LLM-based AR sound authoring system that\ngenerates context-aware sound effects for AR experiences. SonifyAR expands the\ncurrent design space of AR sound and implements a Programming by Demonstration\n(PbD) pipeline to automatically collect contextual information of AR events,\nincluding virtual content semantics and real world context. This context\ninformation is then processed by a large language model to acquire sound\neffects with Recommendation, Retrieval, Generation, and Transfer methods. To\nevaluate the usability and performance of our system, we conducted a user study\nwith eight participants and created five example applications, including an\nAR-based science experiment, an improving case for AR headset safety, and an\nassisting example for low vision AR users."
                },
                "authors": [
                    {
                        "name": "Xia Su"
                    },
                    {
                        "name": "Jon E. Froehlich"
                    },
                    {
                        "name": "Eunyee Koh"
                    },
                    {
                        "name": "Chang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xiao"
                },
                "author": "Chang Xiao",
                "arxiv_comment": "To appear in UIST2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07089v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07089v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18312v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18312v3",
                "updated": "2024-08-12T02:43:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    2,
                    43,
                    32,
                    0,
                    225,
                    0
                ],
                "published": "2024-06-26T12:51:37Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    12,
                    51,
                    37,
                    2,
                    178,
                    0
                ],
                "title": "AI-native Memory: A Pathway from LLMs Towards AGI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-native Memory: A Pathway from LLMs Towards AGI"
                },
                "summary": "Large language models (LLMs) have demonstrated the world with the sparks of\nartificial general intelligence (AGI). One opinion, especially from some\nstartups working on LLMs, argues that an LLM with nearly unlimited context\nlength can realize AGI. However, they might be too optimistic about the\nlong-context capability of (existing) LLMs -- (1) Recent literature has shown\nthat their effective context length is significantly smaller than their claimed\ncontext length; and (2) Our reasoning-in-a-haystack experiments further\ndemonstrate that simultaneously finding the relevant information from a long\ncontext and conducting (simple) reasoning is nearly impossible. In this paper,\nwe envision a pathway from LLMs to AGI through the integration of\n\\emph{memory}. We believe that AGI should be a system where LLMs serve as core\nprocessors. In addition to raw data, the memory in this system would store a\nlarge number of important conclusions derived from reasoning processes.\nCompared with retrieval-augmented generation (RAG) that merely processing raw\ndata, this approach not only connects semantically related information closer,\nbut also simplifies complex inferences at the time of querying. As an\nintermediate stage, the memory will likely be in the form of natural language\ndescriptions, which can be directly consumed by users too. Ultimately, every\nagent/person should have its own large personal model, a deep neural network\nmodel (thus \\emph{AI-native}) that parameterizes and compresses all types of\nmemory, even the ones cannot be described by natural languages. Finally, we\ndiscuss the significant potential of AI-native memory as the transformative\ninfrastructure for (proactive) engagement, personalization, distribution, and\nsocial in the AGI era, as well as the incurred privacy and security challenges\nwith preliminary solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated the world with the sparks of\nartificial general intelligence (AGI). One opinion, especially from some\nstartups working on LLMs, argues that an LLM with nearly unlimited context\nlength can realize AGI. However, they might be too optimistic about the\nlong-context capability of (existing) LLMs -- (1) Recent literature has shown\nthat their effective context length is significantly smaller than their claimed\ncontext length; and (2) Our reasoning-in-a-haystack experiments further\ndemonstrate that simultaneously finding the relevant information from a long\ncontext and conducting (simple) reasoning is nearly impossible. In this paper,\nwe envision a pathway from LLMs to AGI through the integration of\n\\emph{memory}. We believe that AGI should be a system where LLMs serve as core\nprocessors. In addition to raw data, the memory in this system would store a\nlarge number of important conclusions derived from reasoning processes.\nCompared with retrieval-augmented generation (RAG) that merely processing raw\ndata, this approach not only connects semantically related information closer,\nbut also simplifies complex inferences at the time of querying. As an\nintermediate stage, the memory will likely be in the form of natural language\ndescriptions, which can be directly consumed by users too. Ultimately, every\nagent/person should have its own large personal model, a deep neural network\nmodel (thus \\emph{AI-native}) that parameterizes and compresses all types of\nmemory, even the ones cannot be described by natural languages. Finally, we\ndiscuss the significant potential of AI-native memory as the transformative\ninfrastructure for (proactive) engagement, personalization, distribution, and\nsocial in the AGI era, as well as the incurred privacy and security challenges\nwith preliminary solutions."
                },
                "authors": [
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Zai Zheng"
                    },
                    {
                        "name": "Jiale Wei"
                    },
                    {
                        "name": "Xiang Ying"
                    },
                    {
                        "name": "Felix Tao"
                    },
                    {
                        "name": "Mindverse Team"
                    }
                ],
                "author_detail": {
                    "name": "Mindverse Team"
                },
                "author": "Mindverse Team",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18312v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18312v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.04325v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.04325v5",
                "updated": "2024-08-12T02:38:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    2,
                    38,
                    11,
                    0,
                    225,
                    0
                ],
                "published": "2023-06-07T10:45:02Z",
                "published_parsed": [
                    2023,
                    6,
                    7,
                    10,
                    45,
                    2,
                    2,
                    158,
                    0
                ],
                "title": "Last Week with ChatGPT: A Weibo Study on Social Perspective Regarding\n  ChatGPT for Education and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last Week with ChatGPT: A Weibo Study on Social Perspective Regarding\n  ChatGPT for Education and Beyond"
                },
                "summary": "The application of AI-powered tools has piqued the interest of many fields,\nparticularly in the academic community. This study uses ChatGPT, currently the\nmost powerful and popular AI tool, as a representative example to analyze how\nthe Chinese public perceives the potential of large language models (LLMs) for\neducational and general purposes. Although facing accessibility challenges, we\nfound that the number of discussions on ChatGPT per month is 16 times that of\nErnie Bot developed by Baidu, the most popular alternative product to ChatGPT\nin the mainland, making ChatGPT a more suitable subject for our analysis. The\nstudy also serves as the first effort to investigate the changes in public\nopinion as AI technologies become more advanced and intelligent. The analysis\nreveals that, upon first encounters with advanced AI that was not yet highly\ncapable, some social media users believed that AI advancements would benefit\neducation and society, while others feared that advanced AI, like ChatGPT,\nwould make humans feel inferior and lead to problems such as cheating and a\ndecline in moral principles. The majority of users remained neutral.\nInterestingly, with the rapid development and improvement of AI capabilities,\npublic attitudes have tended to shift in a positive direction. We present a\nthorough analysis of the trending shift and a roadmap to ensure the ethical\napplication of ChatGPT-like models in education and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of AI-powered tools has piqued the interest of many fields,\nparticularly in the academic community. This study uses ChatGPT, currently the\nmost powerful and popular AI tool, as a representative example to analyze how\nthe Chinese public perceives the potential of large language models (LLMs) for\neducational and general purposes. Although facing accessibility challenges, we\nfound that the number of discussions on ChatGPT per month is 16 times that of\nErnie Bot developed by Baidu, the most popular alternative product to ChatGPT\nin the mainland, making ChatGPT a more suitable subject for our analysis. The\nstudy also serves as the first effort to investigate the changes in public\nopinion as AI technologies become more advanced and intelligent. The analysis\nreveals that, upon first encounters with advanced AI that was not yet highly\ncapable, some social media users believed that AI advancements would benefit\neducation and society, while others feared that advanced AI, like ChatGPT,\nwould make humans feel inferior and lead to problems such as cheating and a\ndecline in moral principles. The majority of users remained neutral.\nInterestingly, with the rapid development and improvement of AI capabilities,\npublic attitudes have tended to shift in a positive direction. We present a\nthorough analysis of the trending shift and a roadmap to ensure the ethical\napplication of ChatGPT-like models in education and beyond."
                },
                "authors": [
                    {
                        "name": "Yao Tian"
                    },
                    {
                        "name": "Chengwei Tong"
                    },
                    {
                        "name": "Lik-Hang Lee"
                    },
                    {
                        "name": "Reza Hadi Mogavi"
                    },
                    {
                        "name": "Yong Liao"
                    },
                    {
                        "name": "Pengyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Pengyuan Zhou"
                },
                "author": "Pengyuan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.04325v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.04325v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05897v1",
                "updated": "2024-08-12T02:32:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    2,
                    32,
                    45,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T02:32:45Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    2,
                    32,
                    45,
                    0,
                    225,
                    0
                ],
                "title": "TRIZ-GPT: An LLM-augmented method for problem-solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIZ-GPT: An LLM-augmented method for problem-solving"
                },
                "summary": "TRIZ, the Theory of Inventive Problem Solving, is derived from a\ncomprehensive analysis of patents across various domains, offering a framework\nand practical tools for problem-solving. Despite its potential to foster\ninnovative solutions, the complexity and abstractness of TRIZ methodology often\nmake its acquisition and application challenging. This often requires users to\nhave a deep understanding of the theory, as well as substantial practical\nexperience and knowledge across various disciplines. The advent of Large\nLanguage Models (LLMs) presents an opportunity to address these challenges by\nleveraging their extensive knowledge bases and reasoning capabilities for\ninnovative solution generation within TRIZ-based problem-solving process. This\nstudy explores and evaluates the application of LLMs within the TRIZ-based\nproblem-solving process. The construction of TRIZ case collections establishes\na solid empirical foundation for our experiments and offers valuable resources\nto the TRIZ community. A specifically designed workflow, utilizing step-by-step\nreasoning and evaluation-validated prompt strategies, effectively transforms\nconcrete problems into TRIZ problems and finally generates inventive solutions.\nFinally, we present a case study in mechanical engineering field that\nhighlights the practical application of this LLM-augmented method. It showcases\nGPT-4's ability to generate solutions that closely resonate with original\nsolutions and suggests more implementation mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIZ, the Theory of Inventive Problem Solving, is derived from a\ncomprehensive analysis of patents across various domains, offering a framework\nand practical tools for problem-solving. Despite its potential to foster\ninnovative solutions, the complexity and abstractness of TRIZ methodology often\nmake its acquisition and application challenging. This often requires users to\nhave a deep understanding of the theory, as well as substantial practical\nexperience and knowledge across various disciplines. The advent of Large\nLanguage Models (LLMs) presents an opportunity to address these challenges by\nleveraging their extensive knowledge bases and reasoning capabilities for\ninnovative solution generation within TRIZ-based problem-solving process. This\nstudy explores and evaluates the application of LLMs within the TRIZ-based\nproblem-solving process. The construction of TRIZ case collections establishes\na solid empirical foundation for our experiments and offers valuable resources\nto the TRIZ community. A specifically designed workflow, utilizing step-by-step\nreasoning and evaluation-validated prompt strategies, effectively transforms\nconcrete problems into TRIZ problems and finally generates inventive solutions.\nFinally, we present a case study in mechanical engineering field that\nhighlights the practical application of this LLM-augmented method. It showcases\nGPT-4's ability to generate solutions that closely resonate with original\nsolutions and suggests more implementation mechanisms."
                },
                "authors": [
                    {
                        "name": "Liuqing Chen"
                    },
                    {
                        "name": "Yaxuan Song"
                    },
                    {
                        "name": "Shixian Ding"
                    },
                    {
                        "name": "Lingyun Sun"
                    },
                    {
                        "name": "Peter Childs"
                    },
                    {
                        "name": "Haoyu Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Zuo"
                },
                "author": "Haoyu Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05751v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05751v3",
                "updated": "2024-08-12T02:15:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    2,
                    15,
                    55,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-08T09:03:49Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    9,
                    3,
                    49,
                    0,
                    190,
                    0
                ],
                "title": "TransformerPayne: enhancing spectral emulation accuracy and data\n  efficiency by capturing long-range correlations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TransformerPayne: enhancing spectral emulation accuracy and data\n  efficiency by capturing long-range correlations"
                },
                "summary": "Stellar spectra emulators often rely on large grids and tend to reach a\nplateau in emulation accuracy, leading to significant systematic errors when\ninferring stellar properties. Our study explores the use of Transformer models\nto capture long-range information in spectra, comparing their performance to\nThe Payne emulator (a fully connected multilayer perceptron), an expanded\nversion of The Payne, and a convolutional-based emulator. We tested these\nmodels on synthetic spectra grids, evaluating their performance by analyzing\nemulation residuals and assessing the quality of spectral parameter inference.\nThe newly introduced TransformerPayne emulator outperformed all other tested\nmodels, achieving a mean absolute error (MAE) of approximately 0.15% when\ntrained on the full grid. The most significant improvements were observed in\ngrids containing between 1000 and 10,000 spectra, with TransformerPayne showing\n2 to 5 times better performance than the scaled-up version of The Payne.\nAdditionally, TransformerPayne demonstrated superior fine-tuning capabilities,\nallowing for pretraining on one spectral model grid before transferring to\nanother. This fine-tuning approach enabled up to a tenfold reduction in\ntraining grid size compared to models trained from scratch. Analysis of\nTransformerPayne's attention maps revealed that they encode interpretable\nfeatures common across many spectral lines of chosen elements. While scaling up\nThe Payne to a larger network reduced its MAE from 1.2% to 0.3% when trained on\nthe full dataset, TransformerPayne consistently achieved the lowest MAE across\nall tests. The inductive biases of the TransformerPayne emulator enhance\naccuracy, data efficiency, and interpretability for spectral emulation compared\nto existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stellar spectra emulators often rely on large grids and tend to reach a\nplateau in emulation accuracy, leading to significant systematic errors when\ninferring stellar properties. Our study explores the use of Transformer models\nto capture long-range information in spectra, comparing their performance to\nThe Payne emulator (a fully connected multilayer perceptron), an expanded\nversion of The Payne, and a convolutional-based emulator. We tested these\nmodels on synthetic spectra grids, evaluating their performance by analyzing\nemulation residuals and assessing the quality of spectral parameter inference.\nThe newly introduced TransformerPayne emulator outperformed all other tested\nmodels, achieving a mean absolute error (MAE) of approximately 0.15% when\ntrained on the full grid. The most significant improvements were observed in\ngrids containing between 1000 and 10,000 spectra, with TransformerPayne showing\n2 to 5 times better performance than the scaled-up version of The Payne.\nAdditionally, TransformerPayne demonstrated superior fine-tuning capabilities,\nallowing for pretraining on one spectral model grid before transferring to\nanother. This fine-tuning approach enabled up to a tenfold reduction in\ntraining grid size compared to models trained from scratch. Analysis of\nTransformerPayne's attention maps revealed that they encode interpretable\nfeatures common across many spectral lines of chosen elements. While scaling up\nThe Payne to a larger network reduced its MAE from 1.2% to 0.3% when trained on\nthe full dataset, TransformerPayne consistently achieved the lowest MAE across\nall tests. The inductive biases of the TransformerPayne emulator enhance\naccuracy, data efficiency, and interpretability for spectral emulation compared\nto existing methods."
                },
                "authors": [
                    {
                        "name": "Tomasz R√≥≈ºa≈Ñski"
                    },
                    {
                        "name": "Yuan-Sen Ting"
                    },
                    {
                        "name": "Maja Jab≈Ço≈Ñska"
                    }
                ],
                "author_detail": {
                    "name": "Maja Jab≈Ço≈Ñska"
                },
                "author": "Maja Jab≈Ço≈Ñska",
                "arxiv_comment": "25 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05751v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05751v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00958v2",
                "updated": "2024-08-12T02:08:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    2,
                    8,
                    3,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-01T04:29:35Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    4,
                    29,
                    35,
                    0,
                    183,
                    0
                ],
                "title": "Universal Approximation Theory: The basic theory for large language\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Approximation Theory: The basic theory for large language\n  models"
                },
                "summary": "Language models have emerged as a critical area of focus in artificial\nintelligence, particularly with the introduction of groundbreaking innovations\nlike ChatGPT. Large-scale Transformer networks have quickly become the leading\napproach for advancing natural language processing algorithms. Built on the\nTransformer architecture, these models enable interactions that closely mimic\nhuman communication and, equipped with extensive knowledge, can even assist in\nguiding human tasks. Despite their impressive capabilities and growing\ncomplexity, a key question remains-the theoretical foundations of large\nlanguage models (LLMs). What makes Transformer so effective for powering\nintelligent language applications, such as translation and coding? What\nunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme\nenhance the fine-tuning of LLMs? And what supports the practicality of pruning\nLLMs? To address these critical questions and explore the technological\nstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) to\noffer a theoretical backdrop, shedding light on the mechanisms that underpin\nthese advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models have emerged as a critical area of focus in artificial\nintelligence, particularly with the introduction of groundbreaking innovations\nlike ChatGPT. Large-scale Transformer networks have quickly become the leading\napproach for advancing natural language processing algorithms. Built on the\nTransformer architecture, these models enable interactions that closely mimic\nhuman communication and, equipped with extensive knowledge, can even assist in\nguiding human tasks. Despite their impressive capabilities and growing\ncomplexity, a key question remains-the theoretical foundations of large\nlanguage models (LLMs). What makes Transformer so effective for powering\nintelligent language applications, such as translation and coding? What\nunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme\nenhance the fine-tuning of LLMs? And what supports the practicality of pruning\nLLMs? To address these critical questions and explore the technological\nstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) to\noffer a theoretical backdrop, shedding light on the mechanisms that underpin\nthese advancements."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21670v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21670v3",
                "updated": "2024-08-12T01:50:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    1,
                    50,
                    23,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-31T15:13:39Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    15,
                    13,
                    39,
                    2,
                    213,
                    0
                ],
                "title": "Universal Approximation Theory: Foundations for Parallelism in Neural\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Approximation Theory: Foundations for Parallelism in Neural\n  Networks"
                },
                "summary": "Neural networks are increasingly evolving towards training large models with\nbig data, a method that has demonstrated superior performance across many\ntasks. However, this approach introduces an urgent problem: current deep\nlearning models are predominantly serial, meaning that as the number of network\nlayers increases, so do the training and inference times. This is unacceptable\nif deep learning is to continue advancing. Therefore, this paper proposes a\ndeep learning parallelization strategy based on the Universal Approximation\nTheorem (UAT). From this foundation, we designed a parallel network called\nPara-Former to test our theory. Unlike traditional serial models, the inference\ntime of Para-Former does not increase with the number of layers, significantly\naccelerating the inference speed of multi-layer networks. Experimental results\nvalidate the effectiveness of this network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks are increasingly evolving towards training large models with\nbig data, a method that has demonstrated superior performance across many\ntasks. However, this approach introduces an urgent problem: current deep\nlearning models are predominantly serial, meaning that as the number of network\nlayers increases, so do the training and inference times. This is unacceptable\nif deep learning is to continue advancing. Therefore, this paper proposes a\ndeep learning parallelization strategy based on the Universal Approximation\nTheorem (UAT). From this foundation, we designed a parallel network called\nPara-Former to test our theory. Unlike traditional serial models, the inference\ntime of Para-Former does not increase with the number of layers, significantly\naccelerating the inference speed of multi-layer networks. Experimental results\nvalidate the effectiveness of this network."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21670v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21670v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05002v2",
                "updated": "2024-08-12T01:48:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    1,
                    48,
                    42,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-06T05:46:28Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    5,
                    46,
                    28,
                    1,
                    219,
                    0
                ],
                "title": "An Empirical Study on Challenges for LLM Developers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on Challenges for LLM Developers"
                },
                "summary": "In recent years, large language models (LLMs) have seen rapid advancements,\nsignificantly impacting various fields such as natural language processing, and\nsoftware engineering. These LLMs, exemplified by OpenAI's ChatGPT, have\nrevolutionized the way we approach language understanding and generation tasks.\nHowever, in contrast to traditional software development practices, LLM\ndevelopment introduces new challenges for AI developers in design,\nimplementation, and deployment. These challenges span different areas (such as\nprompts, APIs, and plugins), requiring developers to navigate unique\nmethodologies and considerations specific to LLM development.\n  Despite the profound influence of LLMs, to the best of our knowledge, these\nchallenges have not been thoroughly investigated in previous empirical studies.\nTo fill this gap, we present the first comprehensive study on understanding the\nchallenges faced by LLM developers. Specifically, we crawl and analyze 29,057\nrelevant questions from a popular OpenAI developer forum. We first examine\ntheir popularity and difficulty. After manually analyzing 2,364 sampled\nquestions, we construct a taxonomy of challenges faced by LLM developers. Based\non this taxonomy, we summarize a set of findings and actionable implications\nfor LLM-related stakeholders, including developers and providers (especially\nthe OpenAI organization).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have seen rapid advancements,\nsignificantly impacting various fields such as natural language processing, and\nsoftware engineering. These LLMs, exemplified by OpenAI's ChatGPT, have\nrevolutionized the way we approach language understanding and generation tasks.\nHowever, in contrast to traditional software development practices, LLM\ndevelopment introduces new challenges for AI developers in design,\nimplementation, and deployment. These challenges span different areas (such as\nprompts, APIs, and plugins), requiring developers to navigate unique\nmethodologies and considerations specific to LLM development.\n  Despite the profound influence of LLMs, to the best of our knowledge, these\nchallenges have not been thoroughly investigated in previous empirical studies.\nTo fill this gap, we present the first comprehensive study on understanding the\nchallenges faced by LLM developers. Specifically, we crawl and analyze 29,057\nrelevant questions from a popular OpenAI developer forum. We first examine\ntheir popularity and difficulty. After manually analyzing 2,364 sampled\nquestions, we construct a taxonomy of challenges faced by LLM developers. Based\non this taxonomy, we summarize a set of findings and actionable implications\nfor LLM-related stakeholders, including developers and providers (especially\nthe OpenAI organization)."
                },
                "authors": [
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Chaoyang Gao"
                    },
                    {
                        "name": "Chunyang Chen"
                    },
                    {
                        "name": "Guangbei Zhang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "29 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01528v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01528v3",
                "updated": "2024-08-12T01:44:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    1,
                    44,
                    26,
                    0,
                    225,
                    0
                ],
                "published": "2024-02-02T16:15:24Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    16,
                    15,
                    24,
                    4,
                    33,
                    0
                ],
                "title": "Decoding Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Speculative Decoding"
                },
                "summary": "Speculative Decoding is a widely used technique to speed up inference for\nLarge Language Models (LLMs) without sacrificing quality. When performing\ninference, speculative decoding uses a smaller draft model to generate\nspeculative tokens and then uses the target LLM to verify those draft tokens.\nThe speedup provided by speculative decoding heavily depends on the choice of\nthe draft model. In this work, we perform a detailed study comprising over 350\nexperiments with LLaMA-65B and OPT-66B using speculative decoding and delineate\nthe factors that affect the performance gain provided by speculative decoding.\nOur experiments indicate that the performance of speculative decoding depends\nheavily on the latency of the draft model, and the draft model's capability in\nlanguage modeling does not correlate strongly with its performance in\nspeculative decoding. Based on these insights we explore a new design space for\ndraft models and design hardware-efficient draft models for speculative\ndecoding. Our newly designed draft model for LLaMA-65B can provide 111% higher\nthroughput than existing draft models and can generalize further to the LLaMA-2\nmodel family and supervised fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative Decoding is a widely used technique to speed up inference for\nLarge Language Models (LLMs) without sacrificing quality. When performing\ninference, speculative decoding uses a smaller draft model to generate\nspeculative tokens and then uses the target LLM to verify those draft tokens.\nThe speedup provided by speculative decoding heavily depends on the choice of\nthe draft model. In this work, we perform a detailed study comprising over 350\nexperiments with LLaMA-65B and OPT-66B using speculative decoding and delineate\nthe factors that affect the performance gain provided by speculative decoding.\nOur experiments indicate that the performance of speculative decoding depends\nheavily on the latency of the draft model, and the draft model's capability in\nlanguage modeling does not correlate strongly with its performance in\nspeculative decoding. Based on these insights we explore a new design space for\ndraft models and design hardware-efficient draft models for speculative\ndecoding. Our newly designed draft model for LLaMA-65B can provide 111% higher\nthroughput than existing draft models and can generalize further to the LLaMA-2\nmodel family and supervised fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Minghao Yan"
                    },
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01528v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01528v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10885v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10885v3",
                "updated": "2024-08-12T01:24:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    1,
                    24,
                    33,
                    0,
                    225,
                    0
                ],
                "published": "2024-05-17T16:22:52Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    16,
                    22,
                    52,
                    4,
                    138,
                    0
                ],
                "title": "FA-Depth: Toward Fast and Accurate Self-supervised Monocular Depth\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FA-Depth: Toward Fast and Accurate Self-supervised Monocular Depth\n  Estimation"
                },
                "summary": "Most existing methods often rely on complex models to predict scene depth\nwith high accuracy, resulting in slow inference that is not conducive to\ndeployment. To better balance precision and speed, we first designed SmallDepth\nbased on sparsity. Second, to enhance the feature representation ability of\nSmallDepth during training under the condition of equal complexity during\ninference, we propose an equivalent transformation module(ETM). Third, to\nimprove the ability of each layer in the case of a fixed SmallDepth to perceive\ndifferent context information and improve the robustness of SmallDepth to the\nleft-right direction and illumination changes, we propose pyramid loss. Fourth,\nto further improve the accuracy of SmallDepth, we utilized the proposed\nfunction approximation loss (APX) to transfer knowledge in the pretrained\nHQDecv2, obtained by optimizing the previous HQDec to address grid artifacts in\nsome regions, to SmallDepth. Extensive experiments demonstrate that each\nproposed component improves the precision of SmallDepth without changing the\ncomplexity of SmallDepth during inference, and the developed approach achieves\nstate-of-the-art results on KITTI at an inference speed of more than 500 frames\nper second and with approximately 2 M parameters. The code and models will be\npublicly available at https://github.com/fwucas/FA-Depth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most existing methods often rely on complex models to predict scene depth\nwith high accuracy, resulting in slow inference that is not conducive to\ndeployment. To better balance precision and speed, we first designed SmallDepth\nbased on sparsity. Second, to enhance the feature representation ability of\nSmallDepth during training under the condition of equal complexity during\ninference, we propose an equivalent transformation module(ETM). Third, to\nimprove the ability of each layer in the case of a fixed SmallDepth to perceive\ndifferent context information and improve the robustness of SmallDepth to the\nleft-right direction and illumination changes, we propose pyramid loss. Fourth,\nto further improve the accuracy of SmallDepth, we utilized the proposed\nfunction approximation loss (APX) to transfer knowledge in the pretrained\nHQDecv2, obtained by optimizing the previous HQDec to address grid artifacts in\nsome regions, to SmallDepth. Extensive experiments demonstrate that each\nproposed component improves the precision of SmallDepth without changing the\ncomplexity of SmallDepth during inference, and the developed approach achieves\nstate-of-the-art results on KITTI at an inference speed of more than 500 frames\nper second and with approximately 2 M parameters. The code and models will be\npublicly available at https://github.com/fwucas/FA-Depth."
                },
                "authors": [
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Jun Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Jun Cheng"
                },
                "author": "Jun Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10885v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10885v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14755v2",
                "updated": "2024-08-12T01:09:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    1,
                    9,
                    36,
                    0,
                    225,
                    0
                ],
                "published": "2024-05-23T16:21:57Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    16,
                    21,
                    57,
                    3,
                    144,
                    0
                ],
                "title": "Large language models can be zero-shot anomaly detectors for time\n  series?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can be zero-shot anomaly detectors for time\n  series?"
                },
                "summary": "Recent studies have shown the ability of large language models to perform a\nvariety of tasks, including time series forecasting. The flexible nature of\nthese models allows them to be used for many applications. In this paper, we\npresent a novel study of large language models used for the challenging task of\ntime series anomaly detection. This problem entails two aspects novel for LLMs:\nthe need for the model to identify part of the input sequence (or multiple\nparts) as anomalous; and the need for it to work with time series data rather\nthan the traditional text input. We introduce sigllm, a framework for time\nseries anomaly detection using large language models. Our framework includes a\ntime-series-to-text conversion module, as well as end-to-end pipelines that\nprompt language models to perform time series anomaly detection. We investigate\ntwo paradigms for testing the abilities of large language models to perform the\ndetection task. First, we present a prompt-based detection method that directly\nasks a language model to indicate which elements of the input are anomalies.\nSecond, we leverage the forecasting capability of a large language model to\nguide the anomaly detection process. We evaluated our framework on 11 datasets\nspanning various sources and 10 pipelines. We show that the forecasting method\nsignificantly outperformed the prompting method in all 11 datasets with respect\nto the F1 score. Moreover, while large language models are capable of finding\nanomalies, state-of-the-art deep learning models are still superior in\nperformance, achieving results 30% better than large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown the ability of large language models to perform a\nvariety of tasks, including time series forecasting. The flexible nature of\nthese models allows them to be used for many applications. In this paper, we\npresent a novel study of large language models used for the challenging task of\ntime series anomaly detection. This problem entails two aspects novel for LLMs:\nthe need for the model to identify part of the input sequence (or multiple\nparts) as anomalous; and the need for it to work with time series data rather\nthan the traditional text input. We introduce sigllm, a framework for time\nseries anomaly detection using large language models. Our framework includes a\ntime-series-to-text conversion module, as well as end-to-end pipelines that\nprompt language models to perform time series anomaly detection. We investigate\ntwo paradigms for testing the abilities of large language models to perform the\ndetection task. First, we present a prompt-based detection method that directly\nasks a language model to indicate which elements of the input are anomalies.\nSecond, we leverage the forecasting capability of a large language model to\nguide the anomaly detection process. We evaluated our framework on 11 datasets\nspanning various sources and 10 pipelines. We show that the forecasting method\nsignificantly outperformed the prompting method in all 11 datasets with respect\nto the F1 score. Moreover, while large language models are capable of finding\nanomalies, state-of-the-art deep learning models are still superior in\nperformance, achieving results 30% better than large language models."
                },
                "authors": [
                    {
                        "name": "Sarah Alnegheimish"
                    },
                    {
                        "name": "Linh Nguyen"
                    },
                    {
                        "name": "Laure Berti-Equille"
                    },
                    {
                        "name": "Kalyan Veeramachaneni"
                    }
                ],
                "author_detail": {
                    "name": "Kalyan Veeramachaneni"
                },
                "author": "Kalyan Veeramachaneni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17728v2",
                "updated": "2024-08-12T00:54:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    0,
                    54,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-05-28T01:07:06Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    1,
                    7,
                    6,
                    1,
                    149,
                    0
                ],
                "title": "Facilitating Holistic Evaluations with LLMs: Insights from\n  Scenario-Based Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facilitating Holistic Evaluations with LLMs: Insights from\n  Scenario-Based Experiments"
                },
                "summary": "Workshop courses designed to foster creativity are gaining popularity.\nHowever, even experienced faculty teams find it challenging to realize a\nholistic evaluation that accommodates diverse perspectives. Adequate\ndeliberation is essential to integrate varied assessments, but faculty often\nlack the time for such exchanges. Deriving an average score without discussion\nundermines the purpose of a holistic evaluation. Therefore, this paper explores\nthe use of a Large Language Model (LLM) as a facilitator to integrate diverse\nfaculty assessments. Scenario-based experiments were conducted to determine if\nthe LLM could integrate diverse evaluations and explain the underlying\npedagogical theories to faculty. The results were noteworthy, showing that the\nLLM can effectively facilitate faculty discussions. Additionally, the LLM\ndemonstrated the capability to create evaluation criteria by generalizing a\nsingle scenario-based experiment, leveraging its already acquired pedagogical\ndomain knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Workshop courses designed to foster creativity are gaining popularity.\nHowever, even experienced faculty teams find it challenging to realize a\nholistic evaluation that accommodates diverse perspectives. Adequate\ndeliberation is essential to integrate varied assessments, but faculty often\nlack the time for such exchanges. Deriving an average score without discussion\nundermines the purpose of a holistic evaluation. Therefore, this paper explores\nthe use of a Large Language Model (LLM) as a facilitator to integrate diverse\nfaculty assessments. Scenario-based experiments were conducted to determine if\nthe LLM could integrate diverse evaluations and explain the underlying\npedagogical theories to faculty. The results were noteworthy, showing that the\nLLM can effectively facilitate faculty discussions. Additionally, the LLM\ndemonstrated the capability to create evaluation criteria by generalizing a\nsingle scenario-based experiment, leveraging its already acquired pedagogical\ndomain knowledge."
                },
                "authors": [
                    {
                        "name": "Toru Ishida"
                    },
                    {
                        "name": "Tongxi Liu"
                    },
                    {
                        "name": "Hailong Wang"
                    },
                    {
                        "name": "William K. Cheunga"
                    }
                ],
                "author_detail": {
                    "name": "William K. Cheunga"
                },
                "author": "William K. Cheunga",
                "arxiv_comment": "The final version appears in the proceedings of the 32nd\n  International Conference on Computers in Education (ICCE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05882v1",
                "updated": "2024-08-12T00:46:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    0,
                    46,
                    39,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T00:46:39Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    0,
                    46,
                    39,
                    0,
                    225,
                    0
                ],
                "title": "Creating Arabic LLM Prompts at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating Arabic LLM Prompts at Scale"
                },
                "summary": "The debut of chatGPT and BARD has popularized instruction following text\ngeneration using LLMs, where a user can interrogate an LLM using natural\nlanguage requests and obtain natural language answers that matches their\nrequests. Training LLMs to respond in this manner requires a large number of\nworked out examples of user requests (aka prompts) with corresponding gold\nresponses. In this paper, we introduce two methods for creating such prompts\nfor Arabic cheaply and quickly. The first methods entails automatically\ntranslating existing prompt datasets from English, such as PromptSource and\nSuper-NaturalInstructions, and then using machine translation quality\nestimation to retain high quality translations only. The second method involves\ncreating natural language prompts on top of existing Arabic NLP datasets. Using\nthese two methods we were able to create more than 67.4 million Arabic prompts\nthat cover a variety of tasks including summarization, headline generation,\ngrammar checking, open/closed question answering, creative writing, etc. We\nshow that fine tuning an open 7 billion parameter large language model, namely\nbase Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter\ninstruction tuned model, namely Llama3 70B, in handling Arabic prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The debut of chatGPT and BARD has popularized instruction following text\ngeneration using LLMs, where a user can interrogate an LLM using natural\nlanguage requests and obtain natural language answers that matches their\nrequests. Training LLMs to respond in this manner requires a large number of\nworked out examples of user requests (aka prompts) with corresponding gold\nresponses. In this paper, we introduce two methods for creating such prompts\nfor Arabic cheaply and quickly. The first methods entails automatically\ntranslating existing prompt datasets from English, such as PromptSource and\nSuper-NaturalInstructions, and then using machine translation quality\nestimation to retain high quality translations only. The second method involves\ncreating natural language prompts on top of existing Arabic NLP datasets. Using\nthese two methods we were able to create more than 67.4 million Arabic prompts\nthat cover a variety of tasks including summarization, headline generation,\ngrammar checking, open/closed question answering, creative writing, etc. We\nshow that fine tuning an open 7 billion parameter large language model, namely\nbase Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter\ninstruction tuned model, namely Llama3 70B, in handling Arabic prompts."
                },
                "authors": [
                    {
                        "name": "Abdelrahman El-Sheikh"
                    },
                    {
                        "name": "Ahmed Elmogtaba"
                    },
                    {
                        "name": "Kareem Darwish"
                    },
                    {
                        "name": "Muhammad Elmallah"
                    },
                    {
                        "name": "Ashraf Elneima"
                    },
                    {
                        "name": "Hassan Sawaf"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Sawaf"
                },
                "author": "Hassan Sawaf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07820v3",
                "updated": "2024-08-12T00:43:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    0,
                    43,
                    56,
                    0,
                    225,
                    0
                ],
                "published": "2023-10-11T19:01:28Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    19,
                    1,
                    28,
                    2,
                    284,
                    0
                ],
                "title": "Large Language Models Are Zero-Shot Time Series Forecasters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Zero-Shot Time Series Forecasters"
                },
                "summary": "By encoding time series as a string of numerical digits, we can frame time\nseries forecasting as next-token prediction in text. Developing this approach,\nwe find that large language models (LLMs) such as GPT-3 and LLaMA-2 can\nsurprisingly zero-shot extrapolate time series at a level comparable to or\nexceeding the performance of purpose-built time series models trained on the\ndownstream tasks. To facilitate this performance, we propose procedures for\neffectively tokenizing time series data and converting discrete distributions\nover tokens into highly flexible densities over continuous values. We argue the\nsuccess of LLMs for time series stems from their ability to naturally represent\nmultimodal distributions, in conjunction with biases for simplicity, and\nrepetition, which align with the salient features in many time series, such as\nrepeated seasonal trends. We also show how LLMs can naturally handle missing\ndata without imputation through non-numerical text, accommodate textual side\ninformation, and answer questions to help explain predictions. While we find\nthat increasing model size generally improves performance on time series, we\nshow GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers,\nand poor uncertainty calibration, which is likely the result of alignment\ninterventions such as RLHF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By encoding time series as a string of numerical digits, we can frame time\nseries forecasting as next-token prediction in text. Developing this approach,\nwe find that large language models (LLMs) such as GPT-3 and LLaMA-2 can\nsurprisingly zero-shot extrapolate time series at a level comparable to or\nexceeding the performance of purpose-built time series models trained on the\ndownstream tasks. To facilitate this performance, we propose procedures for\neffectively tokenizing time series data and converting discrete distributions\nover tokens into highly flexible densities over continuous values. We argue the\nsuccess of LLMs for time series stems from their ability to naturally represent\nmultimodal distributions, in conjunction with biases for simplicity, and\nrepetition, which align with the salient features in many time series, such as\nrepeated seasonal trends. We also show how LLMs can naturally handle missing\ndata without imputation through non-numerical text, accommodate textual side\ninformation, and answer questions to help explain predictions. While we find\nthat increasing model size generally improves performance on time series, we\nshow GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers,\nand poor uncertainty calibration, which is likely the result of alignment\ninterventions such as RLHF."
                },
                "authors": [
                    {
                        "name": "Nate Gruver"
                    },
                    {
                        "name": "Marc Finzi"
                    },
                    {
                        "name": "Shikai Qiu"
                    },
                    {
                        "name": "Andrew Gordon Wilson"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Gordon Wilson"
                },
                "author": "Andrew Gordon Wilson",
                "arxiv_comment": "NeurIPS 2023. Code available at: https://github.com/ngruver/llmtime",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12874v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12874v2",
                "updated": "2024-08-12T00:38:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    0,
                    38,
                    22,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-16T04:41:58Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    4,
                    41,
                    58,
                    1,
                    198,
                    0
                ],
                "title": "SELF-GUIDE: Better Task-Specific Instruction Following via\n  Self-Synthetic Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELF-GUIDE: Better Task-Specific Instruction Following via\n  Self-Synthetic Finetuning"
                },
                "summary": "Large language models (LLMs) hold the promise of solving diverse tasks when\nprovided with appropriate natural language prompts. However, prompting often\nleads models to make predictions with lower accuracy compared to finetuning a\nmodel with ample training data. On the other hand, while finetuning LLMs on\ntask-specific data generally improves their performance, abundant annotated\ndatasets are not available for all tasks. Previous work has explored generating\ntask-specific data from state-of-the-art LLMs and using this data to finetune\nsmaller models, but this approach requires access to a language model other\nthan the one being trained, which introduces cost, scalability challenges, and\nlegal hurdles associated with continuously relying on more powerful LLMs. In\nresponse to these, we propose SELF-GUIDE, a multi-stage mechanism in which we\nsynthesize task-specific input-output pairs from the student LLM, then use\nthese input-output pairs to finetune the student LLM itself. In our empirical\nevaluation of the Natural Instructions V2 benchmark, we find that SELF-GUIDE\nimproves the performance of LLM by a substantial margin. Specifically, we\nreport an absolute improvement of approximately 15% for classification tasks\nand 18% for generation tasks in the benchmark's metrics. This sheds light on\nthe promise of self-synthesized data guiding LLMs towards becoming\ntask-specific experts without any external learning signals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) hold the promise of solving diverse tasks when\nprovided with appropriate natural language prompts. However, prompting often\nleads models to make predictions with lower accuracy compared to finetuning a\nmodel with ample training data. On the other hand, while finetuning LLMs on\ntask-specific data generally improves their performance, abundant annotated\ndatasets are not available for all tasks. Previous work has explored generating\ntask-specific data from state-of-the-art LLMs and using this data to finetune\nsmaller models, but this approach requires access to a language model other\nthan the one being trained, which introduces cost, scalability challenges, and\nlegal hurdles associated with continuously relying on more powerful LLMs. In\nresponse to these, we propose SELF-GUIDE, a multi-stage mechanism in which we\nsynthesize task-specific input-output pairs from the student LLM, then use\nthese input-output pairs to finetune the student LLM itself. In our empirical\nevaluation of the Natural Instructions V2 benchmark, we find that SELF-GUIDE\nimproves the performance of LLM by a substantial margin. Specifically, we\nreport an absolute improvement of approximately 15% for classification tasks\nand 18% for generation tasks in the benchmark's metrics. This sheds light on\nthe promise of self-synthesized data guiding LLMs towards becoming\ntask-specific experts without any external learning signals."
                },
                "authors": [
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Xueying Jia"
                    },
                    {
                        "name": "Vijay Viswanathan"
                    },
                    {
                        "name": "Tongshuang Wu"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "arxiv_comment": "Accepted by COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12874v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12874v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05874v1",
                "updated": "2024-08-11T22:59:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    22,
                    59,
                    32,
                    6,
                    224,
                    0
                ],
                "published": "2024-08-11T22:59:32Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    22,
                    59,
                    32,
                    6,
                    224,
                    0
                ],
                "title": "LLM-Based Robust Product Classification in Commerce and Compliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Robust Product Classification in Commerce and Compliance"
                },
                "summary": "Product classification is a crucial task in international trade, as\ncompliance regulations are verified and taxes and duties are applied based on\nproduct categories. Manual classification of products is time-consuming and\nerror-prone, and the sheer volume of products imported and exported renders the\nmanual process infeasible. Consequently, e-commerce platforms and enterprises\ninvolved in international trade have turned to automatic product classification\nusing machine learning. However, current approaches do not consider the\nreal-world challenges associated with product classification, such as very\nabbreviated and incomplete product descriptions. In addition, recent\nadvancements in generative Large Language Models (LLMs) and their reasoning\ncapabilities are mainly untapped in product classification and e-commerce. In\nthis research, we explore the real-life challenges of industrial classification\nand we propose data perturbations that allow for realistic data simulation.\nFurthermore, we employ LLM-based product classification to improve the\nrobustness of the prediction in presence of incomplete data. Our research shows\nthat LLMs with in-context learning outperform the supervised approaches in the\nclean-data scenario. Additionally, we illustrate that LLMs are significantly\nmore robust than the supervised approaches when data attacks are present.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Product classification is a crucial task in international trade, as\ncompliance regulations are verified and taxes and duties are applied based on\nproduct categories. Manual classification of products is time-consuming and\nerror-prone, and the sheer volume of products imported and exported renders the\nmanual process infeasible. Consequently, e-commerce platforms and enterprises\ninvolved in international trade have turned to automatic product classification\nusing machine learning. However, current approaches do not consider the\nreal-world challenges associated with product classification, such as very\nabbreviated and incomplete product descriptions. In addition, recent\nadvancements in generative Large Language Models (LLMs) and their reasoning\ncapabilities are mainly untapped in product classification and e-commerce. In\nthis research, we explore the real-life challenges of industrial classification\nand we propose data perturbations that allow for realistic data simulation.\nFurthermore, we employ LLM-based product classification to improve the\nrobustness of the prediction in presence of incomplete data. Our research shows\nthat LLMs with in-context learning outperform the supervised approaches in the\nclean-data scenario. Additionally, we illustrate that LLMs are significantly\nmore robust than the supervised approaches when data attacks are present."
                },
                "authors": [
                    {
                        "name": "Sina Gholamian"
                    },
                    {
                        "name": "Gianfranco Romani"
                    },
                    {
                        "name": "Bartosz Rudnikowicz"
                    },
                    {
                        "name": "Laura Skylaki"
                    }
                ],
                "author_detail": {
                    "name": "Laura Skylaki"
                },
                "author": "Laura Skylaki",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2311.09755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09755v2",
                "updated": "2024-08-12T17:57:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    57,
                    0,
                    0,
                    225,
                    0
                ],
                "published": "2023-11-16T10:30:00Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    10,
                    30,
                    0,
                    3,
                    320,
                    0
                ],
                "title": "On the Impact of Calibration Data in Post-training Quantization and\n  Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Impact of Calibration Data in Post-training Quantization and\n  Pruning"
                },
                "summary": "Quantization and pruning form the foundation of compression for neural\nnetworks, enabling efficient inference for large language models (LLMs).\nRecently, various quantization and pruning techniques have demonstrated\nremarkable performance in a post-training setting. They rely upon calibration\ndata, a small set of unlabeled examples that are used to generate layer\nactivations. However, no prior work has systematically investigated how the\ncalibration data impacts the effectiveness of model compression methods. In\nthis paper, we present the first extensive empirical study on the effect of\ncalibration data upon LLM performance. We trial a variety of quantization and\npruning methods, datasets, tasks, and models. Surprisingly, we find substantial\nvariations in downstream task performance, contrasting existing work that\nsuggests a greater level of robustness to the calibration data. Finally, we\nmake a series of recommendations for the effective use of calibration data in\nLLM quantization and pruning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization and pruning form the foundation of compression for neural\nnetworks, enabling efficient inference for large language models (LLMs).\nRecently, various quantization and pruning techniques have demonstrated\nremarkable performance in a post-training setting. They rely upon calibration\ndata, a small set of unlabeled examples that are used to generate layer\nactivations. However, no prior work has systematically investigated how the\ncalibration data impacts the effectiveness of model compression methods. In\nthis paper, we present the first extensive empirical study on the effect of\ncalibration data upon LLM performance. We trial a variety of quantization and\npruning methods, datasets, tasks, and models. Surprisingly, we find substantial\nvariations in downstream task performance, contrasting existing work that\nsuggests a greater level of robustness to the calibration data. Finally, we\nmake a series of recommendations for the effective use of calibration data in\nLLM quantization and pruning."
                },
                "authors": [
                    {
                        "name": "Miles Williams"
                    },
                    {
                        "name": "Nikolaos Aletras"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Aletras"
                },
                "author": "Nikolaos Aletras",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.00798v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.00798v4",
                "updated": "2024-08-12T17:54:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    54,
                    32,
                    0,
                    225,
                    0
                ],
                "published": "2024-02-01T17:30:50Z",
                "published_parsed": [
                    2024,
                    2,
                    1,
                    17,
                    30,
                    50,
                    3,
                    32,
                    0
                ],
                "title": "Formal-LLM: Integrating Formal Language and Natural Language for\n  Controllable LLM-based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Formal-LLM: Integrating Formal Language and Natural Language for\n  Controllable LLM-based Agents"
                },
                "summary": "Recent advancements on Large Language Models (LLMs) enable AI Agents to\nautomatically generate and execute multi-step plans to solve complex tasks.\nHowever, since LLM's content generation process is hardly controllable, current\nLLM-based agents frequently generate invalid or non-executable plans, which\njeopardizes the performance of the generated plans and corrupts users' trust in\nLLM-based agents. In response, this paper proposes a novel \"Formal-LLM\"\nframework for LLM-based agents by integrating the expressiveness of natural\nlanguage and the precision of formal language. Specifically, the framework\nallows agent developers to express their requirements or constraints for the\nplanning process as an automaton. A stack-based LLM plan generation process is\nthen conducted under the supervision of the automaton to ensure that the\ngenerated plan satisfies the constraints, making the planning process\ncontrollable. We conduct experiments on both benchmark tasks and practical\nreal-life tasks, and our framework achieves over 50% overall performance\nincrease, which validates the feasibility and effectiveness of employing\nFormal-LLM to guide the plan generation of agents, preventing the agents from\ngenerating invalid and unsuccessful plans. Further, more controllable LLM-based\nagents can facilitate the broader utilization of LLM in application scenarios\nwhere high validity of planning is essential. The source code of this work is\navailable at https://github.com/agiresearch/Formal-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements on Large Language Models (LLMs) enable AI Agents to\nautomatically generate and execute multi-step plans to solve complex tasks.\nHowever, since LLM's content generation process is hardly controllable, current\nLLM-based agents frequently generate invalid or non-executable plans, which\njeopardizes the performance of the generated plans and corrupts users' trust in\nLLM-based agents. In response, this paper proposes a novel \"Formal-LLM\"\nframework for LLM-based agents by integrating the expressiveness of natural\nlanguage and the precision of formal language. Specifically, the framework\nallows agent developers to express their requirements or constraints for the\nplanning process as an automaton. A stack-based LLM plan generation process is\nthen conducted under the supervision of the automaton to ensure that the\ngenerated plan satisfies the constraints, making the planning process\ncontrollable. We conduct experiments on both benchmark tasks and practical\nreal-life tasks, and our framework achieves over 50% overall performance\nincrease, which validates the feasibility and effectiveness of employing\nFormal-LLM to guide the plan generation of agents, preventing the agents from\ngenerating invalid and unsuccessful plans. Further, more controllable LLM-based\nagents can facilitate the broader utilization of LLM in application scenarios\nwhere high validity of planning is essential. The source code of this work is\navailable at https://github.com/agiresearch/Formal-LLM."
                },
                "authors": [
                    {
                        "name": "Zelong Li"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "He Zhu"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.00798v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.00798v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.17012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.17012v2",
                "updated": "2024-08-12T17:53:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    53,
                    13,
                    0,
                    225,
                    0
                ],
                "published": "2023-09-29T06:53:10Z",
                "published_parsed": [
                    2023,
                    9,
                    29,
                    6,
                    53,
                    10,
                    4,
                    272,
                    0
                ],
                "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Cognitive Biases in Large Language Models as Evaluators"
                },
                "summary": "Large Language Models (LLMs) have recently been shown to be effective as\nautomatic evaluators with simple prompting and in-context learning. In this\nwork, we assemble 15 LLMs of four different size ranges and evaluate their\noutput responses by preference ranking from the other LLMs as evaluators, such\nas System Star is better than System Square. We then evaluate the quality of\nranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators\n(CoBBLEr), a benchmark to measure six different cognitive biases in LLM\nevaluation outputs, such as the Egocentric bias where a model prefers to rank\nits own outputs highly in evaluation. We find that LLMs are biased text quality\nevaluators, exhibiting strong indications on our bias benchmark (average of 40%\nof comparisons across all models) within each of their evaluations that\nquestion their robustness as evaluators. Furthermore, we examine the\ncorrelation between human and machine preferences and calculate the average\nRank-Biased Overlap (RBO) score to be 49.6%, indicating that machine\npreferences are misaligned with humans. According to our findings, LLMs may\nstill be unable to be utilized for automatic annotation aligned with human\npreferences. Our project page is at: https://minnesotanlp.github.io/cobbler.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently been shown to be effective as\nautomatic evaluators with simple prompting and in-context learning. In this\nwork, we assemble 15 LLMs of four different size ranges and evaluate their\noutput responses by preference ranking from the other LLMs as evaluators, such\nas System Star is better than System Square. We then evaluate the quality of\nranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators\n(CoBBLEr), a benchmark to measure six different cognitive biases in LLM\nevaluation outputs, such as the Egocentric bias where a model prefers to rank\nits own outputs highly in evaluation. We find that LLMs are biased text quality\nevaluators, exhibiting strong indications on our bias benchmark (average of 40%\nof comparisons across all models) within each of their evaluations that\nquestion their robustness as evaluators. Furthermore, we examine the\ncorrelation between human and machine preferences and calculate the average\nRank-Biased Overlap (RBO) score to be 49.6%, indicating that machine\npreferences are misaligned with humans. According to our findings, LLMs may\nstill be unable to be utilized for automatic annotation aligned with human\npreferences. Our project page is at: https://minnesotanlp.github.io/cobbler."
                },
                "authors": [
                    {
                        "name": "Ryan Koo"
                    },
                    {
                        "name": "Minhwa Lee"
                    },
                    {
                        "name": "Vipul Raheja"
                    },
                    {
                        "name": "Jong Inn Park"
                    },
                    {
                        "name": "Zae Myung Kim"
                    },
                    {
                        "name": "Dongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyeop Kang"
                },
                "author": "Dongyeop Kang",
                "arxiv_comment": "Publishsed at 2024. 29 pages, 9 figures, 14 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.17012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.17012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06332v1",
                "updated": "2024-08-12T17:48:55Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    48,
                    55,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T17:48:55Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    48,
                    55,
                    0,
                    225,
                    0
                ],
                "title": "Animate, or Inanimate, That is the Question for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Animate, or Inanimate, That is the Question for Large Language Models"
                },
                "summary": "The cognitive essence of humans is deeply intertwined with the concept of\nanimacy, which plays an essential role in shaping their memory, vision, and\nmulti-layered language understanding. Although animacy appears in language via\nnuanced constraints on verbs and adjectives, it is also learned and refined\nthrough extralinguistic information. Similarly, we assume that the LLMs'\nlimited abilities to understand natural language when processing animacy are\nmotivated by the fact that these models are trained exclusively on text.\n  Hence, the question this paper aims to answer arises: can LLMs, in their\ndigital wisdom, process animacy in a similar way to what humans would do? We\nthen propose a systematic analysis via prompting approaches. In particular, we\nprobe different LLMs by prompting them using animate, inanimate, usual, and\nstranger contexts. Results reveal that, although LLMs have been trained\npredominantly on textual data, they exhibit human-like behavior when faced with\ntypical animate and inanimate entities in alignment with earlier studies.\nHence, LLMs can adapt to understand unconventional situations by recognizing\noddities as animated without needing to interface with unspoken cognitive\ntriggers humans rely on to break down animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cognitive essence of humans is deeply intertwined with the concept of\nanimacy, which plays an essential role in shaping their memory, vision, and\nmulti-layered language understanding. Although animacy appears in language via\nnuanced constraints on verbs and adjectives, it is also learned and refined\nthrough extralinguistic information. Similarly, we assume that the LLMs'\nlimited abilities to understand natural language when processing animacy are\nmotivated by the fact that these models are trained exclusively on text.\n  Hence, the question this paper aims to answer arises: can LLMs, in their\ndigital wisdom, process animacy in a similar way to what humans would do? We\nthen propose a systematic analysis via prompting approaches. In particular, we\nprobe different LLMs by prompting them using animate, inanimate, usual, and\nstranger contexts. Results reveal that, although LLMs have been trained\npredominantly on textual data, they exhibit human-like behavior when faced with\ntypical animate and inanimate entities in alignment with earlier studies.\nHence, LLMs can adapt to understand unconventional situations by recognizing\noddities as animated without needing to interface with unspoken cognitive\ntriggers humans rely on to break down animations."
                },
                "authors": [
                    {
                        "name": "Leonardo Ranaldi"
                    },
                    {
                        "name": "Giulia Pucci"
                    },
                    {
                        "name": "Fabio Massimo Zanzotto"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Massimo Zanzotto"
                },
                "author": "Fabio Massimo Zanzotto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06331v1",
                "updated": "2024-08-12T17:47:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    47,
                    32,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T17:47:32Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    47,
                    32,
                    0,
                    225,
                    0
                ],
                "title": "Integration of blockchain in smart systems: problems and opportunities\n  for real-time sensor data storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integration of blockchain in smart systems: problems and opportunities\n  for real-time sensor data storage"
                },
                "summary": "The internet of things (IoT) and other emerging ubiquitous technologies are\nsupporting the rapid spread of smart systems, which has underlined the need for\nsafe, open, and decentralized data storage solutions. With its inherent\ndecentralization and immutability, blockchain offers itself as a potential\nsolution for these requirements. However, the practicality of incorporating\nblockchain into real-time sensor data storage systems is a topic that demands\nin-depth examination. While blockchain promises unmatched data security and\nauditability, some intrinsic qualities, namely scalability restrictions,\ntransactional delays, and escalating storage demands, impede its seamless\ndeployment in high-frequency, voluminous data contexts typical of real-time\nsensors. This essay launches a methodical investigation into these\ndifficulties, illuminating their underlying causes, potential effects, and\npotential countermeasures. In addition, we present a novel pragmatic\nexperimental setup and analysis of blockchain for smart system applications,\nwith an extended discussion of the benefits and disadvantages of deploying\nblockchain based solutions for smart system ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The internet of things (IoT) and other emerging ubiquitous technologies are\nsupporting the rapid spread of smart systems, which has underlined the need for\nsafe, open, and decentralized data storage solutions. With its inherent\ndecentralization and immutability, blockchain offers itself as a potential\nsolution for these requirements. However, the practicality of incorporating\nblockchain into real-time sensor data storage systems is a topic that demands\nin-depth examination. While blockchain promises unmatched data security and\nauditability, some intrinsic qualities, namely scalability restrictions,\ntransactional delays, and escalating storage demands, impede its seamless\ndeployment in high-frequency, voluminous data contexts typical of real-time\nsensors. This essay launches a methodical investigation into these\ndifficulties, illuminating their underlying causes, potential effects, and\npotential countermeasures. In addition, we present a novel pragmatic\nexperimental setup and analysis of blockchain for smart system applications,\nwith an extended discussion of the benefits and disadvantages of deploying\nblockchain based solutions for smart system ecosystems."
                },
                "authors": [
                    {
                        "name": "Naseem Alsadi"
                    },
                    {
                        "name": "Syed Zaidi"
                    },
                    {
                        "name": "Mankaran Rooprai"
                    },
                    {
                        "name": "Stephen A. Gadsden"
                    },
                    {
                        "name": "John Yawney"
                    }
                ],
                "author_detail": {
                    "name": "John Yawney"
                },
                "author": "John Yawney",
                "arxiv_doi": "10.1117/12.3013828",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1117/12.3013828",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.06331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06318v1",
                "updated": "2024-08-12T17:39:01Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    39,
                    1,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T17:39:01Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    39,
                    1,
                    0,
                    225,
                    0
                ],
                "title": "Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take\n  TravelPlanner as an Example",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Rely on LLM Agents to Draft Long-Horizon Plans? Let's Take\n  TravelPlanner as an Example"
                },
                "summary": "Large language models (LLMs) have brought autonomous agents closer to\nartificial general intelligence (AGI) due to their promising generalization and\nemergent capabilities. There is, however, a lack of studies on how LLM-based\nagents behave, why they could potentially fail, and how to improve them,\nparticularly in demanding real-world planning tasks. In this paper, as an\neffort to fill the gap, we present our study using a realistic benchmark,\nTravelPlanner, where an agent must meet multiple constraints to generate\naccurate plans. We leverage this benchmark to address four key research\nquestions: (1) are LLM agents robust enough to lengthy and noisy contexts when\nit comes to reasoning and planning? (2) can few-shot prompting adversely impact\nthe performance of LLM agents in scenarios with long context? (3) can we rely\non refinement to improve plans, and (4) can fine-tuning LLMs with both positive\nand negative feedback lead to further improvement? Our comprehensive\nexperiments indicate that, firstly, LLMs often fail to attend to crucial parts\nof a long context, despite their ability to handle extensive reference\ninformation and few-shot examples; secondly, they still struggle with analyzing\nthe long plans and cannot provide accurate feedback for refinement; thirdly, we\npropose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive and\nnegative feedback, resulting in substantial gains over Supervised Fine-Tuning\n(SFT). Our findings offer in-depth insights to the community on various aspects\nrelated to real-world planning applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have brought autonomous agents closer to\nartificial general intelligence (AGI) due to their promising generalization and\nemergent capabilities. There is, however, a lack of studies on how LLM-based\nagents behave, why they could potentially fail, and how to improve them,\nparticularly in demanding real-world planning tasks. In this paper, as an\neffort to fill the gap, we present our study using a realistic benchmark,\nTravelPlanner, where an agent must meet multiple constraints to generate\naccurate plans. We leverage this benchmark to address four key research\nquestions: (1) are LLM agents robust enough to lengthy and noisy contexts when\nit comes to reasoning and planning? (2) can few-shot prompting adversely impact\nthe performance of LLM agents in scenarios with long context? (3) can we rely\non refinement to improve plans, and (4) can fine-tuning LLMs with both positive\nand negative feedback lead to further improvement? Our comprehensive\nexperiments indicate that, firstly, LLMs often fail to attend to crucial parts\nof a long context, despite their ability to handle extensive reference\ninformation and few-shot examples; secondly, they still struggle with analyzing\nthe long plans and cannot provide accurate feedback for refinement; thirdly, we\npropose Feedback-Aware Fine-Tuning (FAFT), which leverages both positive and\nnegative feedback, resulting in substantial gains over Supervised Fine-Tuning\n(SFT). Our findings offer in-depth insights to the community on various aspects\nrelated to real-world planning applications."
                },
                "authors": [
                    {
                        "name": "Yanan Chen"
                    },
                    {
                        "name": "Ali Pesaranghader"
                    },
                    {
                        "name": "Tanmana Sadhu"
                    },
                    {
                        "name": "Dong Hoon Yi"
                    }
                ],
                "author_detail": {
                    "name": "Dong Hoon Yi"
                },
                "author": "Dong Hoon Yi",
                "arxiv_comment": "13 pages, 2 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06316v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06316v1",
                "updated": "2024-08-12T17:31:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    31,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T17:31:28Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    31,
                    28,
                    0,
                    225,
                    0
                ],
                "title": "Body Transformer: Leveraging Robot Embodiment for Policy Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Body Transformer: Leveraging Robot Embodiment for Policy Learning"
                },
                "summary": "In recent years, the transformer architecture has become the de facto\nstandard for machine learning algorithms applied to natural language processing\nand computer vision. Despite notable evidence of successful deployment of this\narchitecture in the context of robot learning, we claim that vanilla\ntransformers do not fully exploit the structure of the robot learning problem.\nTherefore, we propose Body Transformer (BoT), an architecture that leverages\nthe robot embodiment by providing an inductive bias that guides the learning\nprocess. We represent the robot body as a graph of sensors and actuators, and\nrely on masked attention to pool information throughout the architecture. The\nresulting architecture outperforms the vanilla transformer, as well as the\nclassical multilayer perceptron, in terms of task completion, scaling\nproperties, and computational efficiency when representing either imitation or\nreinforcement learning policies. Additional material including the open-source\ncode is available at https://sferrazza.cc/bot_site.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the transformer architecture has become the de facto\nstandard for machine learning algorithms applied to natural language processing\nand computer vision. Despite notable evidence of successful deployment of this\narchitecture in the context of robot learning, we claim that vanilla\ntransformers do not fully exploit the structure of the robot learning problem.\nTherefore, we propose Body Transformer (BoT), an architecture that leverages\nthe robot embodiment by providing an inductive bias that guides the learning\nprocess. We represent the robot body as a graph of sensors and actuators, and\nrely on masked attention to pool information throughout the architecture. The\nresulting architecture outperforms the vanilla transformer, as well as the\nclassical multilayer perceptron, in terms of task completion, scaling\nproperties, and computational efficiency when representing either imitation or\nreinforcement learning policies. Additional material including the open-source\ncode is available at https://sferrazza.cc/bot_site."
                },
                "authors": [
                    {
                        "name": "Carmelo Sferrazza"
                    },
                    {
                        "name": "Dun-Ming Huang"
                    },
                    {
                        "name": "Fangchen Liu"
                    },
                    {
                        "name": "Jongmin Lee"
                    },
                    {
                        "name": "Pieter Abbeel"
                    }
                ],
                "author_detail": {
                    "name": "Pieter Abbeel"
                },
                "author": "Pieter Abbeel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06316v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06316v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06304v1",
                "updated": "2024-08-12T17:17:16Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    17,
                    16,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T17:17:16Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    17,
                    16,
                    0,
                    225,
                    0
                ],
                "title": "Control-Flow Attestation: Concepts, Solutions, and Open Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-Flow Attestation: Concepts, Solutions, and Open Challenges"
                },
                "summary": "Control-flow attestation (CFA) unifies the worlds of control-flow integrity\nand platform attestation by measuring and reporting a target's run-time\nbehaviour to a verifier. Trust assurances in the target are provided by testing\nwhether its execution follows an authorised control-flow path. The problem has\nbeen explored in various settings, such as assessing the trustworthiness of\ncyber-physical systems, Internet of Things devices, cloud platforms, and many\nothers. Despite a significant number of proposals being made in recent years,\nthe area remains fragmented, addressing different adversarial behaviours,\nverification paradigms, and deployment challenges. In this paper, we present\nthe first survey of control-flow attestation, examining the core ideas and\nsolutions in state-of-the-art schemes. In total, we survey over 30 papers\npublished between 2016-2024, consolidate and compare their key features, and\npose several challenges and recommendations for future research in the area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Control-flow attestation (CFA) unifies the worlds of control-flow integrity\nand platform attestation by measuring and reporting a target's run-time\nbehaviour to a verifier. Trust assurances in the target are provided by testing\nwhether its execution follows an authorised control-flow path. The problem has\nbeen explored in various settings, such as assessing the trustworthiness of\ncyber-physical systems, Internet of Things devices, cloud platforms, and many\nothers. Despite a significant number of proposals being made in recent years,\nthe area remains fragmented, addressing different adversarial behaviours,\nverification paradigms, and deployment challenges. In this paper, we present\nthe first survey of control-flow attestation, examining the core ideas and\nsolutions in state-of-the-art schemes. In total, we survey over 30 papers\npublished between 2016-2024, consolidate and compare their key features, and\npose several challenges and recommendations for future research in the area."
                },
                "authors": [
                    {
                        "name": "Zhanyu Sha"
                    },
                    {
                        "name": "Carlton Shepherd"
                    },
                    {
                        "name": "Amir Rafi"
                    },
                    {
                        "name": "Konstantinos Markantonakis"
                    }
                ],
                "author_detail": {
                    "name": "Konstantinos Markantonakis"
                },
                "author": "Konstantinos Markantonakis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06302v1",
                "updated": "2024-08-12T17:14:41Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    14,
                    41,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T17:14:41Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    17,
                    14,
                    41,
                    0,
                    225,
                    0
                ],
                "title": "Finding Patterns in Ambiguity: Interpretable Stress Testing in the\n  Decision~Boundary",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Finding Patterns in Ambiguity: Interpretable Stress Testing in the\n  Decision~Boundary"
                },
                "summary": "The increasing use of deep learning across various domains highlights the\nimportance of understanding the decision-making processes of these black-box\nmodels. Recent research focusing on the decision boundaries of deep\nclassifiers, relies on generated synthetic instances in areas of low\nconfidence, uncovering samples that challenge both models and humans. We\npropose a novel approach to enhance the interpretability of deep binary\nclassifiers by selecting representative samples from the decision boundary -\nprototypes - and applying post-model explanation algorithms. We evaluate the\neffectiveness of our approach through 2D visualizations and GradientSHAP\nanalysis. Our experiments demonstrate the potential of the proposed method,\nrevealing distinct and compact clusters and diverse prototypes that capture\nessential features that lead to low-confidence decisions. By offering a more\naggregated view of deep classifiers' decision boundaries, our work contributes\nto the responsible development and deployment of reliable machine learning\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing use of deep learning across various domains highlights the\nimportance of understanding the decision-making processes of these black-box\nmodels. Recent research focusing on the decision boundaries of deep\nclassifiers, relies on generated synthetic instances in areas of low\nconfidence, uncovering samples that challenge both models and humans. We\npropose a novel approach to enhance the interpretability of deep binary\nclassifiers by selecting representative samples from the decision boundary -\nprototypes - and applying post-model explanation algorithms. We evaluate the\neffectiveness of our approach through 2D visualizations and GradientSHAP\nanalysis. Our experiments demonstrate the potential of the proposed method,\nrevealing distinct and compact clusters and diverse prototypes that capture\nessential features that lead to low-confidence decisions. By offering a more\naggregated view of deep classifiers' decision boundaries, our work contributes\nto the responsible development and deployment of reliable machine learning\nsystems."
                },
                "authors": [
                    {
                        "name": "In√™s Gomes"
                    },
                    {
                        "name": "Lu√≠s F. Teixeira"
                    },
                    {
                        "name": "Jan N. van Rijn"
                    },
                    {
                        "name": "Carlos Soares"
                    },
                    {
                        "name": "Andr√© Restivo"
                    },
                    {
                        "name": "Lu√≠s Cunha"
                    },
                    {
                        "name": "Mois√©s Santos"
                    }
                ],
                "author_detail": {
                    "name": "Mois√©s Santos"
                },
                "author": "Mois√©s Santos",
                "arxiv_comment": "To be published in the Responsible Generative AI workshop at CVPR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.00616v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.00616v5",
                "updated": "2024-08-12T16:58:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    58,
                    33,
                    0,
                    225,
                    0
                ],
                "published": "2023-09-01T17:59:56Z",
                "published_parsed": [
                    2023,
                    9,
                    1,
                    17,
                    59,
                    56,
                    4,
                    244,
                    0
                ],
                "title": "OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation"
                },
                "summary": "In this work, we introduce OpenIns3D, a new 3D-input-only framework for 3D\nopen-vocabulary scene understanding. The OpenIns3D framework employs a\n\"Mask-Snap-Lookup\" scheme. The \"Mask\" module learns class-agnostic mask\nproposals in 3D point clouds, the \"Snap\" module generates synthetic scene-level\nimages at multiple scales and leverages 2D vision-language models to extract\ninteresting objects, and the \"Lookup\" module searches through the outcomes of\n\"Snap\" to assign category names to the proposed masks. This approach, yet\nsimple, achieves state-of-the-art performance across a wide range of 3D\nopen-vocabulary tasks, including recognition, object detection, and instance\nsegmentation, on both indoor and outdoor datasets. Moreover, OpenIns3D\nfacilitates effortless switching between different 2D detectors without\nrequiring retraining. When integrated with powerful 2D open-world models, it\nachieves excellent results in scene understanding tasks. Furthermore, when\ncombined with LLM-powered 2D models, OpenIns3D exhibits an impressive\ncapability to comprehend and process highly complex text queries that demand\nintricate reasoning and real-world knowledge. Project page:\nhttps://zheninghuang.github.io/OpenIns3D/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we introduce OpenIns3D, a new 3D-input-only framework for 3D\nopen-vocabulary scene understanding. The OpenIns3D framework employs a\n\"Mask-Snap-Lookup\" scheme. The \"Mask\" module learns class-agnostic mask\nproposals in 3D point clouds, the \"Snap\" module generates synthetic scene-level\nimages at multiple scales and leverages 2D vision-language models to extract\ninteresting objects, and the \"Lookup\" module searches through the outcomes of\n\"Snap\" to assign category names to the proposed masks. This approach, yet\nsimple, achieves state-of-the-art performance across a wide range of 3D\nopen-vocabulary tasks, including recognition, object detection, and instance\nsegmentation, on both indoor and outdoor datasets. Moreover, OpenIns3D\nfacilitates effortless switching between different 2D detectors without\nrequiring retraining. When integrated with powerful 2D open-world models, it\nachieves excellent results in scene understanding tasks. Furthermore, when\ncombined with LLM-powered 2D models, OpenIns3D exhibits an impressive\ncapability to comprehend and process highly complex text queries that demand\nintricate reasoning and real-world knowledge. Project page:\nhttps://zheninghuang.github.io/OpenIns3D/"
                },
                "authors": [
                    {
                        "name": "Zhening Huang"
                    },
                    {
                        "name": "Xiaoyang Wu"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    },
                    {
                        "name": "Lei Zhu"
                    },
                    {
                        "name": "Joan Lasenby"
                    }
                ],
                "author_detail": {
                    "name": "Joan Lasenby"
                },
                "author": "Joan Lasenby",
                "arxiv_comment": "ECCV 2024. Project page: https://zheninghuang.github.io/OpenIns3D/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.00616v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.00616v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17003v2",
                "updated": "2024-08-12T16:56:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    56,
                    11,
                    0,
                    225,
                    0
                ],
                "published": "2024-02-26T20:19:14Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    20,
                    19,
                    14,
                    0,
                    57,
                    0
                ],
                "title": "Monitoring Fidelity of Online Reinforcement Learning Algorithms in\n  Clinical Trials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monitoring Fidelity of Online Reinforcement Learning Algorithms in\n  Clinical Trials"
                },
                "summary": "Online reinforcement learning (RL) algorithms offer great potential for\npersonalizing treatment for participants in clinical trials. However, deploying\nan online, autonomous algorithm in the high-stakes healthcare setting makes\nquality control and data quality especially difficult to achieve. This paper\nproposes algorithm fidelity as a critical requirement for deploying online RL\nalgorithms in clinical trials. It emphasizes the responsibility of the\nalgorithm to (1) safeguard participants and (2) preserve the scientific utility\nof the data for post-trial analyses. We also present a framework for\npre-deployment planning and real-time monitoring to help algorithm developers\nand clinical researchers ensure algorithm fidelity. To illustrate our\nframework's practical application, we present real-world examples from the\nOralytics clinical trial. Since Spring 2023, this trial successfully deployed\nan autonomous, online RL algorithm to personalize behavioral interventions for\nparticipants at risk for dental disease.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online reinforcement learning (RL) algorithms offer great potential for\npersonalizing treatment for participants in clinical trials. However, deploying\nan online, autonomous algorithm in the high-stakes healthcare setting makes\nquality control and data quality especially difficult to achieve. This paper\nproposes algorithm fidelity as a critical requirement for deploying online RL\nalgorithms in clinical trials. It emphasizes the responsibility of the\nalgorithm to (1) safeguard participants and (2) preserve the scientific utility\nof the data for post-trial analyses. We also present a framework for\npre-deployment planning and real-time monitoring to help algorithm developers\nand clinical researchers ensure algorithm fidelity. To illustrate our\nframework's practical application, we present real-world examples from the\nOralytics clinical trial. Since Spring 2023, this trial successfully deployed\nan autonomous, online RL algorithm to personalize behavioral interventions for\nparticipants at risk for dental disease."
                },
                "authors": [
                    {
                        "name": "Anna L. Trella"
                    },
                    {
                        "name": "Kelly W. Zhang"
                    },
                    {
                        "name": "Inbal Nahum-Shani"
                    },
                    {
                        "name": "Vivek Shetty"
                    },
                    {
                        "name": "Iris Yan"
                    },
                    {
                        "name": "Finale Doshi-Velez"
                    },
                    {
                        "name": "Susan A. Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Susan A. Murphy"
                },
                "author": "Susan A. Murphy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06285v1",
                "updated": "2024-08-12T16:49:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    49,
                    22,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T16:49:22Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    49,
                    22,
                    0,
                    225,
                    0
                ],
                "title": "Synthetic Patient-Physician Dialogue Generation from Clinical Notes\n  Using LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Patient-Physician Dialogue Generation from Clinical Notes\n  Using LLM"
                },
                "summary": "Medical dialogue systems (MDS) enhance patient-physician communication,\nimprove healthcare accessibility, and reduce costs. However, acquiring suitable\ndata to train these systems poses significant challenges. Privacy concerns\nprevent the use of real conversations, necessitating synthetic alternatives.\nSynthetic dialogue generation from publicly available clinical notes offers a\npromising solution to this issue, providing realistic data while safeguarding\nprivacy. Our approach, SynDial, uses a single LLM iteratively with zero-shot\nprompting and a feedback loop to generate and refine high-quality synthetic\ndialogues. The feedback consists of weighted evaluation scores for similarity\nand extractiveness. The iterative process ensures dialogues meet predefined\nthresholds, achieving superior extractiveness as a result of the feedback loop.\nAdditionally, evaluation shows that the generated dialogues excel in factuality\nmetric compared to the baselines and has comparable diversity scores with GPT4.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical dialogue systems (MDS) enhance patient-physician communication,\nimprove healthcare accessibility, and reduce costs. However, acquiring suitable\ndata to train these systems poses significant challenges. Privacy concerns\nprevent the use of real conversations, necessitating synthetic alternatives.\nSynthetic dialogue generation from publicly available clinical notes offers a\npromising solution to this issue, providing realistic data while safeguarding\nprivacy. Our approach, SynDial, uses a single LLM iteratively with zero-shot\nprompting and a feedback loop to generate and refine high-quality synthetic\ndialogues. The feedback consists of weighted evaluation scores for similarity\nand extractiveness. The iterative process ensures dialogues meet predefined\nthresholds, achieving superior extractiveness as a result of the feedback loop.\nAdditionally, evaluation shows that the generated dialogues excel in factuality\nmetric compared to the baselines and has comparable diversity scores with GPT4."
                },
                "authors": [
                    {
                        "name": "Trisha Das"
                    },
                    {
                        "name": "Dina Albassam"
                    },
                    {
                        "name": "Jimeng Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jimeng Sun"
                },
                "author": "Jimeng Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02854v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02854v3",
                "updated": "2024-08-12T16:44:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    44,
                    5,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-05T22:34:28Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    22,
                    34,
                    28,
                    0,
                    218,
                    0
                ],
                "title": "Wiping out the limitations of Large Language Models -- A Taxonomy for\n  Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wiping out the limitations of Large Language Models -- A Taxonomy for\n  Retrieval Augmented Generation"
                },
                "summary": "Current research on RAGs is distributed across various disciplines, and since\nthe technology is evolving very quickly, its unit of analysis is mostly on\ntechnological innovations, rather than applications in business contexts. Thus,\nin this research, we aim to create a taxonomy to conceptualize a comprehensive\noverview of the constituting characteristics that define RAG applications,\nfacilitating the adoption of this technology in the IS community. To the best\nof our knowledge, no RAG application taxonomies have been developed so far. We\ndescribe our methodology for developing the taxonomy, which includes the\ncriteria for selecting papers, an explanation of our rationale for employing a\nLarge Language Model (LLM)-supported approach to extract and identify initial\ncharacteristics, and a concise overview of our systematic process for\nconceptualizing the taxonomy. Our systematic taxonomy development process\nincludes four iterative phases designed to refine and enhance our understanding\nand presentation of RAG's core dimensions. We have developed a total of five\nmeta-dimensions and sixteen dimensions to comprehensively capture the concept\nof Retrieval-Augmented Generation (RAG) applications. When discussing our\nfindings, we also detail the specific research areas and pose key research\nquestions to guide future information system researchers as they explore the\nemerging topics of RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current research on RAGs is distributed across various disciplines, and since\nthe technology is evolving very quickly, its unit of analysis is mostly on\ntechnological innovations, rather than applications in business contexts. Thus,\nin this research, we aim to create a taxonomy to conceptualize a comprehensive\noverview of the constituting characteristics that define RAG applications,\nfacilitating the adoption of this technology in the IS community. To the best\nof our knowledge, no RAG application taxonomies have been developed so far. We\ndescribe our methodology for developing the taxonomy, which includes the\ncriteria for selecting papers, an explanation of our rationale for employing a\nLarge Language Model (LLM)-supported approach to extract and identify initial\ncharacteristics, and a concise overview of our systematic process for\nconceptualizing the taxonomy. Our systematic taxonomy development process\nincludes four iterative phases designed to refine and enhance our understanding\nand presentation of RAG's core dimensions. We have developed a total of five\nmeta-dimensions and sixteen dimensions to comprehensively capture the concept\nof Retrieval-Augmented Generation (RAG) applications. When discussing our\nfindings, we also detail the specific research areas and pose key research\nquestions to guide future information system researchers as they explore the\nemerging topics of RAG systems."
                },
                "authors": [
                    {
                        "name": "Mahei Manhai Li"
                    },
                    {
                        "name": "Irina Nikishina"
                    },
                    {
                        "name": "√ñzge Sevgili"
                    },
                    {
                        "name": "Martin Semmann"
                    }
                ],
                "author_detail": {
                    "name": "Martin Semmann"
                },
                "author": "Martin Semmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02854v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02854v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06276v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06276v2",
                "updated": "2024-08-13T11:05:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    11,
                    5,
                    10,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-12T16:39:03Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    39,
                    3,
                    0,
                    225,
                    0
                ],
                "title": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review-driven Personalized Preference Reasoning with Large Language\n  Models for Recommendation"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have demonstrated\nexceptional performance across a wide range of tasks, generating significant\ninterest in their application to recommendation systems. However, existing\nmethods have not fully capitalized on the potential of LLMs, often constrained\nby limited input information or failing to fully utilize their advanced\nreasoning capabilities. To address these limitations, we introduce EXP3RT, a\nnovel LLM-based recommender designed to leverage rich preference information\ncontained in user and item reviews. EXP3RT is basically fine-tuned through\ndistillation from a teacher LLM to perform three key tasks in order: EXP3RT\nfirst extracts and encapsulates essential subjective preferences from raw\nreviews, aggregates and summarizes them according to specific criteria to\ncreate user and item profiles. It then generates detailed step-by-step\nreasoning followed by predicted rating, i.e., reasoning-enhanced rating\nprediction, by considering both subjective and objective information from\nuser/item profiles and item descriptions. This personalized preference\nreasoning from EXP3RT enhances rating prediction accuracy and also provides\nfaithful and reasonable explanations for recommendation. Extensive experiments\nshow that EXP3RT outperforms existing methods on both rating prediction and\ncandidate item reranking for top-k recommendation, while significantly\nenhancing the explainability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Jieyong Kim"
                    },
                    {
                        "name": "Hyunseo Kim"
                    },
                    {
                        "name": "Hyunjin Cho"
                    },
                    {
                        "name": "SeongKu Kang"
                    },
                    {
                        "name": "Buru Chang"
                    },
                    {
                        "name": "Jinyoung Yeo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06276v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06276v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06273v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06273v2",
                "updated": "2024-08-13T14:57:25Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    14,
                    57,
                    25,
                    1,
                    226,
                    0
                ],
                "published": "2024-08-12T16:34:56Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    34,
                    56,
                    0,
                    225,
                    0
                ],
                "title": "FuxiTranyu: A Multilingual Large Language Model Trained with Balanced\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FuxiTranyu: A Multilingual Large Language Model Trained with Balanced\n  Data"
                },
                "summary": "Large language models (LLMs) have demonstrated prowess in a wide range of\ntasks. However, many LLMs exhibit significant performance discrepancies between\nhigh- and low-resource languages. To mitigate this challenge, we present\nFuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the\nneed of the research community for balanced and high-performing multilingual\ncapabilities. FuxiTranyu-8B, the base model with 8 billion parameters, is\ntrained from scratch on a meticulously balanced multilingual data repository\nthat contains 600 billion tokens covering 43 natural languages and 16\nprogramming languages. In addition to the base model, we also develop two\ninstruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse\nmultilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined\nwith DPO on a preference dataset for enhanced alignment ability. Extensive\nexperiments on a wide range of multilingual benchmarks demonstrate the\ncompetitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,\nBLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretability\nanalyses at both the neuron and representation level suggest that FuxiTranyu is\nable to learn consistent multilingual representations across different\nlanguages. To promote further research into multilingual LLMs and their working\nmechanisms, we release both the base and instruction-tuned FuxiTranyu models\ntogether with 58 pretraining checkpoints at HuggingFace and Github.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated prowess in a wide range of\ntasks. However, many LLMs exhibit significant performance discrepancies between\nhigh- and low-resource languages. To mitigate this challenge, we present\nFuxiTranyu, an open-source multilingual LLM, which is designed to satisfy the\nneed of the research community for balanced and high-performing multilingual\ncapabilities. FuxiTranyu-8B, the base model with 8 billion parameters, is\ntrained from scratch on a meticulously balanced multilingual data repository\nthat contains 600 billion tokens covering 43 natural languages and 16\nprogramming languages. In addition to the base model, we also develop two\ninstruction-tuned models: FuxiTranyu-8B-SFT that is fine-tuned on a diverse\nmultilingual instruction dataset, and FuxiTranyu-8B-DPO that is further refined\nwith DPO on a preference dataset for enhanced alignment ability. Extensive\nexperiments on a wide range of multilingual benchmarks demonstrate the\ncompetitive performance of FuxiTranyu against existing multilingual LLMs, e.g.,\nBLOOM-7B, PolyLM-13B, Llama-2-Chat-7B and Mistral-7B-Instruct. Interpretability\nanalyses at both the neuron and representation level suggest that FuxiTranyu is\nable to learn consistent multilingual representations across different\nlanguages. To promote further research into multilingual LLMs and their working\nmechanisms, we release both the base and instruction-tuned FuxiTranyu models\ntogether with 58 pretraining checkpoints at HuggingFace and Github."
                },
                "authors": [
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Renren Jin"
                    },
                    {
                        "name": "Shaoyang Xu"
                    },
                    {
                        "name": "Leiyu Pan"
                    },
                    {
                        "name": "Supryadi"
                    },
                    {
                        "name": "Menglong Cui"
                    },
                    {
                        "name": "Jiangcun Du"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Juesi Xiao"
                    },
                    {
                        "name": "Shaolin Zhu"
                    },
                    {
                        "name": "Deyi Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Deyi Xiong"
                },
                "author": "Deyi Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06273v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06273v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06272v1",
                "updated": "2024-08-12T16:33:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    33,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T16:33:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    33,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "A RAG-Based Question-Answering Solution for Cyber-Attack Investigation\n  and Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A RAG-Based Question-Answering Solution for Cyber-Attack Investigation\n  and Attribution"
                },
                "summary": "In the constantly evolving field of cybersecurity, it is imperative for\nanalysts to stay abreast of the latest attack trends and pertinent information\nthat aids in the investigation and attribution of cyber-attacks. In this work,\nwe introduce the first question-answering (QA) model and its application that\nprovides information to the cybersecurity experts about cyber-attacks\ninvestigations and attribution. Our QA model is based on Retrieval Augmented\nGeneration (RAG) techniques together with a Large Language Model (LLM) and\nprovides answers to the users' queries based on either our knowledge base (KB)\nthat contains curated information about cyber-attacks investigations and\nattribution or on outside resources provided by the users. We have tested and\nevaluated our QA model with various types of questions, including KB-based,\nmetadata-based, specific documents from the KB, and external sources-based\nquestions. We compared the answers for KB-based questions with those from\nOpenAI's GPT-3.5 and the latest GPT-4o LLMs. Our proposed QA model outperforms\nOpenAI's GPT models by providing the source of the answers and overcoming the\nhallucination limitations of the GPT models, which is critical for cyber-attack\ninvestigation and attribution. Additionally, our analysis showed that when the\nRAG QA model is given few-shot examples rather than zero-shot instructions, it\ngenerates better answers compared to cases where no examples are supplied in\naddition to the query.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the constantly evolving field of cybersecurity, it is imperative for\nanalysts to stay abreast of the latest attack trends and pertinent information\nthat aids in the investigation and attribution of cyber-attacks. In this work,\nwe introduce the first question-answering (QA) model and its application that\nprovides information to the cybersecurity experts about cyber-attacks\ninvestigations and attribution. Our QA model is based on Retrieval Augmented\nGeneration (RAG) techniques together with a Large Language Model (LLM) and\nprovides answers to the users' queries based on either our knowledge base (KB)\nthat contains curated information about cyber-attacks investigations and\nattribution or on outside resources provided by the users. We have tested and\nevaluated our QA model with various types of questions, including KB-based,\nmetadata-based, specific documents from the KB, and external sources-based\nquestions. We compared the answers for KB-based questions with those from\nOpenAI's GPT-3.5 and the latest GPT-4o LLMs. Our proposed QA model outperforms\nOpenAI's GPT models by providing the source of the answers and overcoming the\nhallucination limitations of the GPT models, which is critical for cyber-attack\ninvestigation and attribution. Additionally, our analysis showed that when the\nRAG QA model is given few-shot examples rather than zero-shot instructions, it\ngenerates better answers compared to cases where no examples are supplied in\naddition to the query."
                },
                "authors": [
                    {
                        "name": "Sampath Rajapaksha"
                    },
                    {
                        "name": "Ruby Rani"
                    },
                    {
                        "name": "Erisa Karafili"
                    }
                ],
                "author_detail": {
                    "name": "Erisa Karafili"
                },
                "author": "Erisa Karafili",
                "arxiv_comment": "Accepted at SECAI 2024 (ESORICS 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06266v1",
                "updated": "2024-08-12T16:24:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    24,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T16:24:51Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    24,
                    51,
                    0,
                    225,
                    0
                ],
                "title": "Anchored Preference Optimization and Contrastive Revisions: Addressing\n  Underspecification in Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anchored Preference Optimization and Contrastive Revisions: Addressing\n  Underspecification in Alignment"
                },
                "summary": "Large Language Models (LLMs) are often aligned using contrastive alignment\nobjectives and preference pair datasets. The interaction between model, paired\ndata, and objective makes alignment a complicated procedure, sometimes\nproducing subpar results. We study this and find that (i) preference data gives\na better learning signal when the underlying responses are contrastive, and\n(ii) alignment objectives lead to better performance when they specify more\ncontrol over the model during training. Based on these insights, we introduce\nContrastive Learning from AI Revisions (CLAIR), a data-creation method which\nleads to more contrastive preference pairs, and Anchored Preference\nOptimization (APO), a controllable and more stable alignment objective. We\nalign Llama-3-8B-Instruct using various comparable datasets and alignment\nobjectives and measure MixEval-Hard scores, which correlate highly with human\njudgments. The CLAIR preferences lead to the strongest performance out of all\ndatasets, and APO consistently outperforms less controllable objectives. Our\nbest model, trained on 32K CLAIR preferences with APO, improves\nLlama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code\nis available at https://github.com/ContextualAI/CLAIR_and_APO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are often aligned using contrastive alignment\nobjectives and preference pair datasets. The interaction between model, paired\ndata, and objective makes alignment a complicated procedure, sometimes\nproducing subpar results. We study this and find that (i) preference data gives\na better learning signal when the underlying responses are contrastive, and\n(ii) alignment objectives lead to better performance when they specify more\ncontrol over the model during training. Based on these insights, we introduce\nContrastive Learning from AI Revisions (CLAIR), a data-creation method which\nleads to more contrastive preference pairs, and Anchored Preference\nOptimization (APO), a controllable and more stable alignment objective. We\nalign Llama-3-8B-Instruct using various comparable datasets and alignment\nobjectives and measure MixEval-Hard scores, which correlate highly with human\njudgments. The CLAIR preferences lead to the strongest performance out of all\ndatasets, and APO consistently outperforms less controllable objectives. Our\nbest model, trained on 32K CLAIR preferences with APO, improves\nLlama-3-8B-Instruct by 7.65%, closing the gap with GPT4-turbo by 45%. Our code\nis available at https://github.com/ContextualAI/CLAIR_and_APO."
                },
                "authors": [
                    {
                        "name": "Karel D'Oosterlinck"
                    },
                    {
                        "name": "Winnie Xu"
                    },
                    {
                        "name": "Chris Develder"
                    },
                    {
                        "name": "Thomas Demeester"
                    },
                    {
                        "name": "Amanpreet Singh"
                    },
                    {
                        "name": "Christopher Potts"
                    },
                    {
                        "name": "Douwe Kiela"
                    },
                    {
                        "name": "Shikib Mehri"
                    }
                ],
                "author_detail": {
                    "name": "Shikib Mehri"
                },
                "author": "Shikib Mehri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06254v1",
                "updated": "2024-08-12T16:04:02Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    4,
                    2,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T16:04:02Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    16,
                    4,
                    2,
                    0,
                    225,
                    0
                ],
                "title": "Data-Efficient Prediction of Minimum Operating Voltage via Inter- and\n  Intra-Wafer Variation Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Efficient Prediction of Minimum Operating Voltage via Inter- and\n  Intra-Wafer Variation Alignment"
                },
                "summary": "Predicting the minimum operating voltage ($V_{min}$) of chips stands as a\ncrucial technique in enhancing the speed and reliability of manufacturing\ntesting flow. However, existing $V_{min}$ prediction methods often overlook\nvarious sources of variations in both training and deployment phases. Notably,\nthe neglect of wafer zone-to-zone (intra-wafer) variations and wafer-to-wafer\n(inter-wafer) variations, compounded by process variations, diminishes the\naccuracy, data efficiency, and reliability of $V_{min}$ predictors. To address\nthis gap, we introduce a novel data-efficient $V_{min}$ prediction flow, termed\nrestricted bias alignment (RBA), which incorporates a novel variation alignment\ntechnique. Our approach concurrently estimates inter- and intra-wafer\nvariations. Furthermore, we propose utilizing class probe data to model\ninter-wafer variations for the first time. We empirically demonstrate RBA's\neffectiveness and data efficiency on an industrial 16nm automotive chip\ndataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting the minimum operating voltage ($V_{min}$) of chips stands as a\ncrucial technique in enhancing the speed and reliability of manufacturing\ntesting flow. However, existing $V_{min}$ prediction methods often overlook\nvarious sources of variations in both training and deployment phases. Notably,\nthe neglect of wafer zone-to-zone (intra-wafer) variations and wafer-to-wafer\n(inter-wafer) variations, compounded by process variations, diminishes the\naccuracy, data efficiency, and reliability of $V_{min}$ predictors. To address\nthis gap, we introduce a novel data-efficient $V_{min}$ prediction flow, termed\nrestricted bias alignment (RBA), which incorporates a novel variation alignment\ntechnique. Our approach concurrently estimates inter- and intra-wafer\nvariations. Furthermore, we propose utilizing class probe data to model\ninter-wafer variations for the first time. We empirically demonstrate RBA's\neffectiveness and data efficiency on an industrial 16nm automotive chip\ndataset."
                },
                "authors": [
                    {
                        "name": "Yuxuan Yin"
                    },
                    {
                        "name": "Rebecca Chen"
                    },
                    {
                        "name": "Chen He"
                    },
                    {
                        "name": "Peng Li"
                    }
                ],
                "author_detail": {
                    "name": "Peng Li"
                },
                "author": "Peng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06223v1",
                "updated": "2024-08-12T15:24:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    15,
                    24,
                    50,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T15:24:50Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    15,
                    24,
                    50,
                    0,
                    225,
                    0
                ],
                "title": "On Effects of Steering Latent Representation for Large Language Model\n  Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Effects of Steering Latent Representation for Large Language Model\n  Unlearning"
                },
                "summary": "Representation Misdirection for Unlearning (RMU), which steers model\nrepresentation in the intermediate layer to a target random representation, is\nan effective method for large language model (LLM) unlearning. Despite its high\nperformance, the underlying cause and explanation remain underexplored. In this\npaper, we first theoretically demonstrate that steering forget representations\nin the intermediate layer reduces token confidence, causing LLMs to generate\nwrong or nonsense responses. Second, we investigate how the coefficient\ninfluences the alignment of forget-sample representations with the random\ndirection and hint at the optimal coefficient values for effective unlearning\nacross different network layers. Third, we show that RMU unlearned models are\nrobust against adversarial jailbreak attacks. Last, our empirical analysis\nshows that RMU is less effective when applied to the middle and later layers in\nLLMs. To resolve this drawback, we propose Adaptive RMU -- a simple yet\neffective alternative method that makes unlearning effective with most layers.\nExtensive experiments demonstrate that Adaptive RMU significantly improves the\nunlearning performance compared to prior art while incurring no additional\ncomputational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Misdirection for Unlearning (RMU), which steers model\nrepresentation in the intermediate layer to a target random representation, is\nan effective method for large language model (LLM) unlearning. Despite its high\nperformance, the underlying cause and explanation remain underexplored. In this\npaper, we first theoretically demonstrate that steering forget representations\nin the intermediate layer reduces token confidence, causing LLMs to generate\nwrong or nonsense responses. Second, we investigate how the coefficient\ninfluences the alignment of forget-sample representations with the random\ndirection and hint at the optimal coefficient values for effective unlearning\nacross different network layers. Third, we show that RMU unlearned models are\nrobust against adversarial jailbreak attacks. Last, our empirical analysis\nshows that RMU is less effective when applied to the middle and later layers in\nLLMs. To resolve this drawback, we propose Adaptive RMU -- a simple yet\neffective alternative method that makes unlearning effective with most layers.\nExtensive experiments demonstrate that Adaptive RMU significantly improves the\nunlearning performance compared to prior art while incurring no additional\ncomputational cost."
                },
                "authors": [
                    {
                        "name": "Dang Huu-Tien"
                    },
                    {
                        "name": "Trung-Tin Pham"
                    },
                    {
                        "name": "Hoang Thanh-Tung"
                    },
                    {
                        "name": "Naoya Inoue"
                    }
                ],
                "author_detail": {
                    "name": "Naoya Inoue"
                },
                "author": "Naoya Inoue",
                "arxiv_comment": "15 pages, 5 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06210v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06210v1",
                "updated": "2024-08-12T15:01:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    15,
                    1,
                    3,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T15:01:03Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    15,
                    1,
                    3,
                    0,
                    225,
                    0
                ],
                "title": "Certified Safe: A Schematic for Approval Regulation of Frontier AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Certified Safe: A Schematic for Approval Regulation of Frontier AI"
                },
                "summary": "Recent and unremitting capability advances have been accompanied by calls for\ncomprehensive, rather than patchwork, regulation of frontier artificial\nintelligence (AI). Approval regulation is emerging as a promising candidate. An\napproval regulation scheme is one in which a firm cannot legally market, or in\nsome cases develop, a product without explicit approval from a regulator on the\nbasis of experiments performed upon the product that demonstrate its safety.\nThis approach is used successfully by the FDA and FAA. Further, its application\nto frontier AI has been publicly supported by many prominent stakeholders. This\nreport proposes an approval regulation schematic for only the largest AI\nprojects in which scrutiny begins before training and continues through to\npost-deployment monitoring. The centerpieces of the schematic are two major\napproval gates, the first requiring approval for large-scale training and the\nsecond for deployment. Five main challenges make implementation difficult:\nnoncompliance through unsanctioned deployment, specification of deployment\nreadiness requirements, reliable model experimentation, filtering out safe\nmodels before the process, and minimizing regulatory overhead. This report\nmakes a number of crucial recommendations to increase the feasibility of\napproval regulation, some of which must be followed urgently if such a regime\nis to succeed in the near future. Further recommendations, produced by this\nreport's analysis, may improve the effectiveness of any regulatory regime for\nfrontier AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent and unremitting capability advances have been accompanied by calls for\ncomprehensive, rather than patchwork, regulation of frontier artificial\nintelligence (AI). Approval regulation is emerging as a promising candidate. An\napproval regulation scheme is one in which a firm cannot legally market, or in\nsome cases develop, a product without explicit approval from a regulator on the\nbasis of experiments performed upon the product that demonstrate its safety.\nThis approach is used successfully by the FDA and FAA. Further, its application\nto frontier AI has been publicly supported by many prominent stakeholders. This\nreport proposes an approval regulation schematic for only the largest AI\nprojects in which scrutiny begins before training and continues through to\npost-deployment monitoring. The centerpieces of the schematic are two major\napproval gates, the first requiring approval for large-scale training and the\nsecond for deployment. Five main challenges make implementation difficult:\nnoncompliance through unsanctioned deployment, specification of deployment\nreadiness requirements, reliable model experimentation, filtering out safe\nmodels before the process, and minimizing regulatory overhead. This report\nmakes a number of crucial recommendations to increase the feasibility of\napproval regulation, some of which must be followed urgently if such a regime\nis to succeed in the near future. Further recommendations, produced by this\nreport's analysis, may improve the effectiveness of any regulatory regime for\nfrontier AI."
                },
                "authors": [
                    {
                        "name": "Cole Salvador"
                    }
                ],
                "author_detail": {
                    "name": "Cole Salvador"
                },
                "author": "Cole Salvador",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06210v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03459v2",
                "updated": "2024-08-12T14:45:34Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    45,
                    34,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-06T22:11:00Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    22,
                    11,
                    0,
                    1,
                    219,
                    0
                ],
                "title": "On the Generalization of Preference Learning with DPO",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Generalization of Preference Learning with DPO"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities but\noften struggle to align with human preferences, leading to harmful or\nundesirable outputs. Preference learning, which trains models to distinguish\nbetween preferred and non-preferred responses based on human feedback, has\nbecome a crucial component for ensuring that LLMs align with human values.\nDespite the widespread adoption in real-world systems, a thorough theoretical\nunderstanding of the generalization guarantees for these models remain lacking.\nThis paper bridges that gap by introducing a new theoretical framework to\nanalyze the generalization guarantees of models trained with direct preference\noptimization (DPO). While existing generalization theory often focuses on\noverparameterized models achieving near-optimal loss or models independent of\nthe training process, our framework rigorously assesses how well models\ngeneralize after a finite number of gradient steps, reflecting real-world LLM\ntraining practices. By analyzing the reward margin associated with each sample\nand its trajectory throughout training, we can effectively bound the\ngeneralization error. We derive learning guarantees showing that, under\nspecific conditions, models trained with DPO can correctly discern preferred\nresponses on unseen data with high probability. These insights are empirically\nvalidated on contemporary LLMs, underscoring the practical relevance of our\ntheoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities but\noften struggle to align with human preferences, leading to harmful or\nundesirable outputs. Preference learning, which trains models to distinguish\nbetween preferred and non-preferred responses based on human feedback, has\nbecome a crucial component for ensuring that LLMs align with human values.\nDespite the widespread adoption in real-world systems, a thorough theoretical\nunderstanding of the generalization guarantees for these models remain lacking.\nThis paper bridges that gap by introducing a new theoretical framework to\nanalyze the generalization guarantees of models trained with direct preference\noptimization (DPO). While existing generalization theory often focuses on\noverparameterized models achieving near-optimal loss or models independent of\nthe training process, our framework rigorously assesses how well models\ngeneralize after a finite number of gradient steps, reflecting real-world LLM\ntraining practices. By analyzing the reward margin associated with each sample\nand its trajectory throughout training, we can effectively bound the\ngeneralization error. We derive learning guarantees showing that, under\nspecific conditions, models trained with DPO can correctly discern preferred\nresponses on unseen data with high probability. These insights are empirically\nvalidated on contemporary LLMs, underscoring the practical relevance of our\ntheoretical findings."
                },
                "authors": [
                    {
                        "name": "Shawn Im"
                    },
                    {
                        "name": "Yixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Yixuan Li"
                },
                "author": "Yixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05205v2",
                "updated": "2024-08-12T14:42:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    42,
                    48,
                    0,
                    225,
                    0
                ],
                "published": "2024-04-08T05:18:39Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    5,
                    18,
                    39,
                    0,
                    99,
                    0
                ],
                "title": "A secure and private ensemble matcher using multi-vault obfuscated\n  templates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A secure and private ensemble matcher using multi-vault obfuscated\n  templates"
                },
                "summary": "Generative AI has revolutionized modern machine learning by providing\nunprecedented realism, diversity, and efficiency in data generation. This\ntechnology holds immense potential for biometrics, including for securing\nsensitive and personally identifiable information. Given the irrevocability of\nbiometric samples and mounting privacy concerns, biometric template security\nand secure matching are among the most sought-after features of modern\nbiometric systems. This paper proposes a novel obfuscation method using\nGenerative AI to enhance biometric template security. Our approach utilizes\nsynthetic facial images generated by a Generative Adversarial Network (GAN) as\n\"random chaff points\" within a secure vault system. Our method creates n\nsub-templates from the original template, each obfuscated with m GAN chaff\npoints. During verification, s closest vectors to the biometric query are\nretrieved from each vault and combined to generate hash values, which are then\ncompared with the stored hash value. Thus, our method safeguards user\nidentities during the training and deployment phases by employing the\nGAN-generated synthetic images. Our protocol was tested using the AT&T, GT, and\nLFW face datasets, achieving ROC areas under the curve of 0.99, 0.99, and 0.90,\nrespectively. Our results demonstrate that the proposed method can maintain\nhigh accuracy and reasonable computational complexity comparable to those\nunprotected template methods while significantly enhancing security and\nprivacy, underscoring the potential of Generative AI in developing proactive\ndefensive strategies for biometric systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI has revolutionized modern machine learning by providing\nunprecedented realism, diversity, and efficiency in data generation. This\ntechnology holds immense potential for biometrics, including for securing\nsensitive and personally identifiable information. Given the irrevocability of\nbiometric samples and mounting privacy concerns, biometric template security\nand secure matching are among the most sought-after features of modern\nbiometric systems. This paper proposes a novel obfuscation method using\nGenerative AI to enhance biometric template security. Our approach utilizes\nsynthetic facial images generated by a Generative Adversarial Network (GAN) as\n\"random chaff points\" within a secure vault system. Our method creates n\nsub-templates from the original template, each obfuscated with m GAN chaff\npoints. During verification, s closest vectors to the biometric query are\nretrieved from each vault and combined to generate hash values, which are then\ncompared with the stored hash value. Thus, our method safeguards user\nidentities during the training and deployment phases by employing the\nGAN-generated synthetic images. Our protocol was tested using the AT&T, GT, and\nLFW face datasets, achieving ROC areas under the curve of 0.99, 0.99, and 0.90,\nrespectively. Our results demonstrate that the proposed method can maintain\nhigh accuracy and reasonable computational complexity comparable to those\nunprotected template methods while significantly enhancing security and\nprivacy, underscoring the potential of Generative AI in developing proactive\ndefensive strategies for biometric systems."
                },
                "authors": [
                    {
                        "name": "Babak Poorebrahim Gilkalaye"
                    },
                    {
                        "name": "Shubhabrata Mukherjee"
                    },
                    {
                        "name": "Reza Derakhshani"
                    }
                ],
                "author_detail": {
                    "name": "Reza Derakhshani"
                },
                "author": "Reza Derakhshani",
                "arxiv_comment": "This paper has been accepted in IJCB 2024 Special Session, Generative\n  AI for Futuristic Biometrics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06195v1",
                "updated": "2024-08-12T14:42:13Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    42,
                    13,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T14:42:13Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    42,
                    13,
                    0,
                    225,
                    0
                ],
                "title": "Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers"
                },
                "summary": "This paper introduces rStar, a self-play mutual reasoning approach that\nsignificantly improves reasoning capabilities of small language models (SLMs)\nwithout fine-tuning or superior models. rStar decouples reasoning into a\nself-play mutual generation-discrimination process. First, a target SLM\naugments the Monte Carlo Tree Search (MCTS) with a rich set of human-like\nreasoning actions to construct higher quality reasoning trajectories. Next,\nanother SLM, with capabilities similar to the target SLM, acts as a\ndiscriminator to verify each trajectory generated by the target SLM. The\nmutually agreed reasoning trajectories are considered mutual consistent, thus\nare more likely to be correct. Extensive experiments across five SLMs\ndemonstrate rStar can effectively solve diverse reasoning problems, including\nGSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K\naccuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for\nMistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct. Code will be\navailable at https://github.com/zhentingqi/rStar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces rStar, a self-play mutual reasoning approach that\nsignificantly improves reasoning capabilities of small language models (SLMs)\nwithout fine-tuning or superior models. rStar decouples reasoning into a\nself-play mutual generation-discrimination process. First, a target SLM\naugments the Monte Carlo Tree Search (MCTS) with a rich set of human-like\nreasoning actions to construct higher quality reasoning trajectories. Next,\nanother SLM, with capabilities similar to the target SLM, acts as a\ndiscriminator to verify each trajectory generated by the target SLM. The\nmutually agreed reasoning trajectories are considered mutual consistent, thus\nare more likely to be correct. Extensive experiments across five SLMs\ndemonstrate rStar can effectively solve diverse reasoning problems, including\nGSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K\naccuracy from 12.51% to 63.91% for LLaMA2-7B, from 36.46% to 81.88% for\nMistral-7B, from 74.53% to 91.13% for LLaMA3-8B-Instruct. Code will be\navailable at https://github.com/zhentingqi/rStar."
                },
                "authors": [
                    {
                        "name": "Zhenting Qi"
                    },
                    {
                        "name": "Mingyuan Ma"
                    },
                    {
                        "name": "Jiahang Xu"
                    },
                    {
                        "name": "Li Lyna Zhang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.17442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.17442v2",
                "updated": "2024-08-12T14:36:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    36,
                    32,
                    0,
                    225,
                    0
                ],
                "published": "2024-02-27T11:57:28Z",
                "published_parsed": [
                    2024,
                    2,
                    27,
                    11,
                    57,
                    28,
                    1,
                    58,
                    0
                ],
                "title": "Insights from the Usage of the Ansible Lightspeed Code Completion\n  Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Insights from the Usage of the Ansible Lightspeed Code Completion\n  Service"
                },
                "summary": "The availability of Large Language Models (LLMs) which can generate code, has\nmade it possible to create tools that improve developer productivity.\nIntegrated development environments or IDEs which developers use to write\nsoftware are often used as an interface to interact with LLMs. Although many\nsuch tools have been released, almost all of them focus on general-purpose\nprogramming languages. Domain-specific languages, such as those crucial for\nInformation Technology (IT) automation, have not received much attention.\nAnsible is one such YAML-based IT automation-specific language. Ansible\nLightspeed is an LLM-based service designed explicitly to generate Ansible YAML\ngiven natural language prompt.\n  This paper first presents the design and implementation of the Ansible\nLightspeed service. We then evaluate its utility to developers using diverse\nindicators, including extended utilization, analysis of user rejected\nsuggestions, as well as analysis of user sentiments. The analysis is based on\ndata collected for 10,696 real users including 3,910 returning users. The code\nfor Ansible Lightspeed service and the analysis framework is made available for\nothers to use.\n  To our knowledge, our study is the first to involve thousands of users in\nevaluating code assistants for domain-specific languages. We propose an\nimproved version of user acceptance rate and we are the first code completion\ntool to present N-Day user retention figures. With our findings we provide\ninsights into the effectiveness of small, dedicated models in a domain-specific\ncontext. We hope this work serves as a reference for software engineering and\nmachine learning researchers exploring code completion services for\ndomain-specific languages in particular and programming languages in general.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The availability of Large Language Models (LLMs) which can generate code, has\nmade it possible to create tools that improve developer productivity.\nIntegrated development environments or IDEs which developers use to write\nsoftware are often used as an interface to interact with LLMs. Although many\nsuch tools have been released, almost all of them focus on general-purpose\nprogramming languages. Domain-specific languages, such as those crucial for\nInformation Technology (IT) automation, have not received much attention.\nAnsible is one such YAML-based IT automation-specific language. Ansible\nLightspeed is an LLM-based service designed explicitly to generate Ansible YAML\ngiven natural language prompt.\n  This paper first presents the design and implementation of the Ansible\nLightspeed service. We then evaluate its utility to developers using diverse\nindicators, including extended utilization, analysis of user rejected\nsuggestions, as well as analysis of user sentiments. The analysis is based on\ndata collected for 10,696 real users including 3,910 returning users. The code\nfor Ansible Lightspeed service and the analysis framework is made available for\nothers to use.\n  To our knowledge, our study is the first to involve thousands of users in\nevaluating code assistants for domain-specific languages. We propose an\nimproved version of user acceptance rate and we are the first code completion\ntool to present N-Day user retention figures. With our findings we provide\ninsights into the effectiveness of small, dedicated models in a domain-specific\ncontext. We hope this work serves as a reference for software engineering and\nmachine learning researchers exploring code completion services for\ndomain-specific languages in particular and programming languages in general."
                },
                "authors": [
                    {
                        "name": "Priyam Sahoo"
                    },
                    {
                        "name": "Saurabh Pujar"
                    },
                    {
                        "name": "Ganesh Nalawade"
                    },
                    {
                        "name": "Richard Gebhardt"
                    },
                    {
                        "name": "Louis Mandel"
                    },
                    {
                        "name": "Luca Buratti"
                    }
                ],
                "author_detail": {
                    "name": "Luca Buratti"
                },
                "author": "Luca Buratti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.17442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.17442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06186v1",
                "updated": "2024-08-12T14:34:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    34,
                    6,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T14:34:06Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    34,
                    6,
                    0,
                    225,
                    0
                ],
                "title": "Improving Structural Diversity of Blackbox LLMs via\n  Chain-of-Specification Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Structural Diversity of Blackbox LLMs via\n  Chain-of-Specification Prompting"
                },
                "summary": "The capability to generate diverse text is a key challenge facing large\nlanguage models (LLMs). Thus far, diversity has been studied via metrics such\nas $n$-gram diversity or diversity of BERT embeddings. However, for these kinds\nof diversity, the user has little control over the dimensions along which\ndiversity is considered. For example, in the poetry domain, one might desire\ndiversity in terms of rhyme and meter, whereas in the code domain, one might\ndesire diversity in terms of the kinds of expressions used to solve a problem.\nWe propose a diversity metric called structural diversity, where the user\nprovides a mapping from generated text to features capturing the kinds of\ndiversity that they care about. In addition, we propose a novel strategy called\nchain-of-specification (CoS) prompting for improving diversity by first having\nthe LLM generate a specification encoding one instance of structural features,\nand then prompting the LLM to generate text that satisfies these features;\nnotably, our strategy works with blackbox LLMs. In our experiments, we show\nthat for structural diversity in the poetry and code domains, CoS significantly\nimproves diversity compared to several baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The capability to generate diverse text is a key challenge facing large\nlanguage models (LLMs). Thus far, diversity has been studied via metrics such\nas $n$-gram diversity or diversity of BERT embeddings. However, for these kinds\nof diversity, the user has little control over the dimensions along which\ndiversity is considered. For example, in the poetry domain, one might desire\ndiversity in terms of rhyme and meter, whereas in the code domain, one might\ndesire diversity in terms of the kinds of expressions used to solve a problem.\nWe propose a diversity metric called structural diversity, where the user\nprovides a mapping from generated text to features capturing the kinds of\ndiversity that they care about. In addition, we propose a novel strategy called\nchain-of-specification (CoS) prompting for improving diversity by first having\nthe LLM generate a specification encoding one instance of structural features,\nand then prompting the LLM to generate text that satisfies these features;\nnotably, our strategy works with blackbox LLMs. In our experiments, we show\nthat for structural diversity in the poetry and code domains, CoS significantly\nimproves diversity compared to several baselines."
                },
                "authors": [
                    {
                        "name": "Halley Young"
                    },
                    {
                        "name": "Yimeng Zeng"
                    },
                    {
                        "name": "Jacob Gardner"
                    },
                    {
                        "name": "Osbert Bastani"
                    }
                ],
                "author_detail": {
                    "name": "Osbert Bastani"
                },
                "author": "Osbert Bastani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03745v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03745v3",
                "updated": "2024-08-12T14:13:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    13,
                    15,
                    0,
                    225,
                    0
                ],
                "published": "2024-04-04T18:34:32Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    18,
                    34,
                    32,
                    3,
                    95,
                    0
                ],
                "title": "Fakes of Varying Shades: How Warning Affects Human Perception and\n  Engagement Regarding LLM Hallucinations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fakes of Varying Shades: How Warning Affects Human Perception and\n  Engagement Regarding LLM Hallucinations"
                },
                "summary": "The widespread adoption and transformative effects of large language models\n(LLMs) have sparked concerns regarding their capacity to produce inaccurate and\nfictitious content, referred to as `hallucinations'. Given the potential risks\nassociated with hallucinations, humans should be able to identify them. This\nresearch aims to understand the human perception of LLM hallucinations by\nsystematically varying the degree of hallucination (genuine, minor\nhallucination, major hallucination) and examining its interaction with warning\n(i.e., a warning of potential inaccuracies: absent vs. present). Participants\n(N=419) from Prolific rated the perceived accuracy and engaged with content\n(e.g., like, dislike, share) in a Q/A format. Participants ranked content as\ntruthful in the order of genuine, minor hallucination, and major hallucination,\nand user engagement behaviors mirrored this pattern. More importantly, we\nobserved that warning improved the detection of hallucination without\nsignificantly affecting the perceived truthfulness of genuine content. We\nconclude by offering insights for future tools to aid human detection of\nhallucinations. All survey materials, demographic questions, and post-session\nquestions are available at:\nhttps://github.com/MahjabinNahar/fakes-of-varying-shades-survey-materials",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption and transformative effects of large language models\n(LLMs) have sparked concerns regarding their capacity to produce inaccurate and\nfictitious content, referred to as `hallucinations'. Given the potential risks\nassociated with hallucinations, humans should be able to identify them. This\nresearch aims to understand the human perception of LLM hallucinations by\nsystematically varying the degree of hallucination (genuine, minor\nhallucination, major hallucination) and examining its interaction with warning\n(i.e., a warning of potential inaccuracies: absent vs. present). Participants\n(N=419) from Prolific rated the perceived accuracy and engaged with content\n(e.g., like, dislike, share) in a Q/A format. Participants ranked content as\ntruthful in the order of genuine, minor hallucination, and major hallucination,\nand user engagement behaviors mirrored this pattern. More importantly, we\nobserved that warning improved the detection of hallucination without\nsignificantly affecting the perceived truthfulness of genuine content. We\nconclude by offering insights for future tools to aid human detection of\nhallucinations. All survey materials, demographic questions, and post-session\nquestions are available at:\nhttps://github.com/MahjabinNahar/fakes-of-varying-shades-survey-materials"
                },
                "authors": [
                    {
                        "name": "Mahjabin Nahar"
                    },
                    {
                        "name": "Haeseung Seo"
                    },
                    {
                        "name": "Eun-Ju Lee"
                    },
                    {
                        "name": "Aiping Xiong"
                    },
                    {
                        "name": "Dongwon Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongwon Lee"
                },
                "author": "Dongwon Lee",
                "arxiv_comment": "Accepted at COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03745v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03745v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04660v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04660v2",
                "updated": "2024-08-12T14:12:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    12,
                    23,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-05T20:01:10Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    20,
                    1,
                    10,
                    0,
                    218,
                    0
                ],
                "title": "XMainframe: A Large Language Model for Mainframe Modernization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XMainframe: A Large Language Model for Mainframe Modernization"
                },
                "summary": "Mainframe operating systems, despite their inception in the 1940s, continue\nto support critical sectors like finance and government. However, these systems\nare often viewed as outdated, requiring extensive maintenance and\nmodernization. Addressing this challenge necessitates innovative tools that can\nunderstand and interact with legacy codebases. To this end, we introduce\nXMainframe, a state-of-the-art large language model (LLM) specifically designed\nwith knowledge of mainframe legacy systems and COBOL codebases. Our solution\ninvolves the creation of an extensive data collection pipeline to produce\nhigh-quality training datasets, enhancing XMainframe's performance in this\nspecialized domain. Additionally, we present MainframeBench, a comprehensive\nbenchmark for assessing mainframe knowledge, including multiple-choice\nquestions, question answering, and COBOL code summarization. Our empirical\nevaluations demonstrate that XMainframe consistently outperforms existing\nstate-of-the-art LLMs across these tasks. Specifically, XMainframe achieves 30%\nhigher accuracy than DeepSeek-Coder on multiple-choice questions, doubles the\nBLEU score of Mixtral-Instruct 8x7B on question answering, and scores six times\nhigher than GPT-3.5 on COBOL summarization. Our work highlights the potential\nof XMainframe to drive significant advancements in managing and modernizing\nlegacy systems, thereby enhancing productivity and saving time for software\ndevelopers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mainframe operating systems, despite their inception in the 1940s, continue\nto support critical sectors like finance and government. However, these systems\nare often viewed as outdated, requiring extensive maintenance and\nmodernization. Addressing this challenge necessitates innovative tools that can\nunderstand and interact with legacy codebases. To this end, we introduce\nXMainframe, a state-of-the-art large language model (LLM) specifically designed\nwith knowledge of mainframe legacy systems and COBOL codebases. Our solution\ninvolves the creation of an extensive data collection pipeline to produce\nhigh-quality training datasets, enhancing XMainframe's performance in this\nspecialized domain. Additionally, we present MainframeBench, a comprehensive\nbenchmark for assessing mainframe knowledge, including multiple-choice\nquestions, question answering, and COBOL code summarization. Our empirical\nevaluations demonstrate that XMainframe consistently outperforms existing\nstate-of-the-art LLMs across these tasks. Specifically, XMainframe achieves 30%\nhigher accuracy than DeepSeek-Coder on multiple-choice questions, doubles the\nBLEU score of Mixtral-Instruct 8x7B on question answering, and scores six times\nhigher than GPT-3.5 on COBOL summarization. Our work highlights the potential\nof XMainframe to drive significant advancements in managing and modernizing\nlegacy systems, thereby enhancing productivity and saving time for software\ndevelopers."
                },
                "authors": [
                    {
                        "name": "Anh T. V. Dau"
                    },
                    {
                        "name": "Hieu Trung Dao"
                    },
                    {
                        "name": "Anh Tuan Nguyen"
                    },
                    {
                        "name": "Hieu Trung Tran"
                    },
                    {
                        "name": "Phong X. Nguyen"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04660v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04660v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10620v2",
                "updated": "2024-08-12T14:07:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    7,
                    32,
                    0,
                    225,
                    0
                ],
                "published": "2024-05-17T08:33:27Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    8,
                    33,
                    27,
                    4,
                    138,
                    0
                ],
                "title": "MC-GPT: Empowering Vision-and-Language Navigation with Memory Map and\n  Reasoning Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC-GPT: Empowering Vision-and-Language Navigation with Memory Map and\n  Reasoning Chains"
                },
                "summary": "In the Vision-and-Language Navigation (VLN) task, the agent is required to\nnavigate to a destination following a natural language instruction. While\nlearning-based approaches have been a major solution to the task, they suffer\nfrom high training costs and lack of interpretability. Recently, Large Language\nModels (LLMs) have emerged as a promising tool for VLN due to their strong\ngeneralization capabilities. However, existing LLM-based methods face\nlimitations in memory construction and diversity of navigation strategies. To\naddress these challenges, we propose a suite of techniques. Firstly, we\nintroduce a method to maintain a topological map that stores navigation\nhistory, retaining information about viewpoints, objects, and their spatial\nrelationships. This map also serves as a global action space. Additionally, we\npresent a Navigation Chain of Thoughts module, leveraging human navigation\nexamples to enrich navigation strategy diversity. Finally, we establish a\npipeline that integrates navigational memory and strategies with perception and\naction prediction modules. Experimental results on the REVERIE and R2R datasets\nshow that our method effectively enhances the navigation ability of the LLM and\nimproves the interpretability of navigation reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the Vision-and-Language Navigation (VLN) task, the agent is required to\nnavigate to a destination following a natural language instruction. While\nlearning-based approaches have been a major solution to the task, they suffer\nfrom high training costs and lack of interpretability. Recently, Large Language\nModels (LLMs) have emerged as a promising tool for VLN due to their strong\ngeneralization capabilities. However, existing LLM-based methods face\nlimitations in memory construction and diversity of navigation strategies. To\naddress these challenges, we propose a suite of techniques. Firstly, we\nintroduce a method to maintain a topological map that stores navigation\nhistory, retaining information about viewpoints, objects, and their spatial\nrelationships. This map also serves as a global action space. Additionally, we\npresent a Navigation Chain of Thoughts module, leveraging human navigation\nexamples to enrich navigation strategy diversity. Finally, we establish a\npipeline that integrates navigational memory and strategies with perception and\naction prediction modules. Experimental results on the REVERIE and R2R datasets\nshow that our method effectively enhances the navigation ability of the LLM and\nimproves the interpretability of navigation reasoning."
                },
                "authors": [
                    {
                        "name": "Zhaohuan Zhan"
                    },
                    {
                        "name": "Lisha Yu"
                    },
                    {
                        "name": "Sijie Yu"
                    },
                    {
                        "name": "Guang Tan"
                    }
                ],
                "author_detail": {
                    "name": "Guang Tan"
                },
                "author": "Guang Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00487v2",
                "updated": "2024-08-12T14:06:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    14,
                    6,
                    48,
                    0,
                    225,
                    0
                ],
                "published": "2024-06-29T16:34:23Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    16,
                    34,
                    23,
                    5,
                    181,
                    0
                ],
                "title": "It's Morphing Time: Unleashing the Potential of Multiple LLMs via\n  Multi-objective Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It's Morphing Time: Unleashing the Potential of Multiple LLMs via\n  Multi-objective Optimization"
                },
                "summary": "In this paper, we introduce a novel approach for large language model merging\nvia black-box multi-objective optimization algorithms. The goal of model\nmerging is to combine multiple models, each excelling in different tasks, into\na single model that outperforms any of the individual source models. However,\nmodel merging faces two significant challenges: First, existing methods rely\nheavily on human intuition and customized strategies to tackle multiple tasks.\nSecond, it's difficult to search for the great model merging configuration in\nlimited evaluations. To address these challenges, we propose a multi-objective\noptimization based model merging method named MM-MO. The proposed method can\nautomatically search merging configurations for multiple tasks with\nmulti-objective optimization algorithms. Moreover, to obtain high-quality model\nmerging configurations within a limited number of evaluation iterations, we\nhave made several improvements to multi-objective Bayesian optimization\nspecifically for model merging scenarios. First, we introduced a weak-to-strong\nmethod to improve the acquisition strategy. Second, we employed Fisher\ninformation to select configurations, further increasing the chances of\ndiscovering superior model merging configurations. Third, we designed a\nsparsity metric as an additional optimization objective to enhance the model's\ngeneralization performance across different tasks. We conducted comprehensive\nexperiments with other mainstream model merging methods, demonstrating that our\nmethod consistently outperforms them. Moreover, performance improvements are\nobserved even on the tasks not explicitly targeted as optimization objectives,\nindicating that our method enhances the overall potential of the model. ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a novel approach for large language model merging\nvia black-box multi-objective optimization algorithms. The goal of model\nmerging is to combine multiple models, each excelling in different tasks, into\na single model that outperforms any of the individual source models. However,\nmodel merging faces two significant challenges: First, existing methods rely\nheavily on human intuition and customized strategies to tackle multiple tasks.\nSecond, it's difficult to search for the great model merging configuration in\nlimited evaluations. To address these challenges, we propose a multi-objective\noptimization based model merging method named MM-MO. The proposed method can\nautomatically search merging configurations for multiple tasks with\nmulti-objective optimization algorithms. Moreover, to obtain high-quality model\nmerging configurations within a limited number of evaluation iterations, we\nhave made several improvements to multi-objective Bayesian optimization\nspecifically for model merging scenarios. First, we introduced a weak-to-strong\nmethod to improve the acquisition strategy. Second, we employed Fisher\ninformation to select configurations, further increasing the chances of\ndiscovering superior model merging configurations. Third, we designed a\nsparsity metric as an additional optimization objective to enhance the model's\ngeneralization performance across different tasks. We conducted comprehensive\nexperiments with other mainstream model merging methods, demonstrating that our\nmethod consistently outperforms them. Moreover, performance improvements are\nobserved even on the tasks not explicitly targeted as optimization objectives,\nindicating that our method enhances the overall potential of the model. ..."
                },
                "authors": [
                    {
                        "name": "Bingdong Li"
                    },
                    {
                        "name": "Zixiang Di"
                    },
                    {
                        "name": "Yanting Yang"
                    },
                    {
                        "name": "Hong Qian"
                    },
                    {
                        "name": "Peng Yang"
                    },
                    {
                        "name": "Hao Hao"
                    },
                    {
                        "name": "Ke Tang"
                    },
                    {
                        "name": "Aimin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Aimin Zhou"
                },
                "author": "Aimin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05025v2",
                "updated": "2024-08-12T13:57:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    13,
                    57,
                    42,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-09T12:26:05Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    12,
                    26,
                    5,
                    4,
                    222,
                    0
                ],
                "title": "Rag and Roll: An End-to-End Evaluation of Indirect Prompt Manipulations\n  in LLM-based Application Frameworks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rag and Roll: An End-to-End Evaluation of Indirect Prompt Manipulations\n  in LLM-based Application Frameworks"
                },
                "summary": "Retrieval Augmented Generation (RAG) is a technique commonly used to equip\nmodels with out of distribution knowledge. This process involves collecting,\nindexing, retrieving, and providing information to an LLM for generating\nresponses. Despite its growing popularity due to its flexibility and low cost,\nthe security implications of RAG have not been extensively studied. The data\nfor such systems are often collected from public sources, providing an attacker\na gateway for indirect prompt injections to manipulate the responses of the\nmodel. In this paper, we investigate the security of RAG systems against\nend-to-end indirect prompt manipulations. First, we review existing RAG\nframework pipelines, deriving a prototypical architecture and identifying\ncritical parameters. We then examine prior works searching for techniques that\nattackers can use to perform indirect prompt manipulations. Finally, we\nimplemented Rag 'n Roll, a framework to determine the effectiveness of attacks\nagainst end-to-end RAG applications. Our results show that existing attacks are\nmostly optimized to boost the ranking of malicious documents during the\nretrieval phase. However, a higher rank does not immediately translate into a\nreliable attack. Most attacks, against various configurations, settle around a\n40% success rate, which could rise to 60% when considering ambiguous answers as\nsuccessful attacks (those that include the expected benign one as well).\nAdditionally, when using unoptimized documents, attackers deploying two of them\n(or more) for a target query can achieve similar results as those using\noptimized ones. Finally, exploration of the configuration space of a RAG showed\nlimited impact in thwarting the attacks, where the most successful combination\nseverely undermines functionality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval Augmented Generation (RAG) is a technique commonly used to equip\nmodels with out of distribution knowledge. This process involves collecting,\nindexing, retrieving, and providing information to an LLM for generating\nresponses. Despite its growing popularity due to its flexibility and low cost,\nthe security implications of RAG have not been extensively studied. The data\nfor such systems are often collected from public sources, providing an attacker\na gateway for indirect prompt injections to manipulate the responses of the\nmodel. In this paper, we investigate the security of RAG systems against\nend-to-end indirect prompt manipulations. First, we review existing RAG\nframework pipelines, deriving a prototypical architecture and identifying\ncritical parameters. We then examine prior works searching for techniques that\nattackers can use to perform indirect prompt manipulations. Finally, we\nimplemented Rag 'n Roll, a framework to determine the effectiveness of attacks\nagainst end-to-end RAG applications. Our results show that existing attacks are\nmostly optimized to boost the ranking of malicious documents during the\nretrieval phase. However, a higher rank does not immediately translate into a\nreliable attack. Most attacks, against various configurations, settle around a\n40% success rate, which could rise to 60% when considering ambiguous answers as\nsuccessful attacks (those that include the expected benign one as well).\nAdditionally, when using unoptimized documents, attackers deploying two of them\n(or more) for a target query can achieve similar results as those using\noptimized ones. Finally, exploration of the configuration space of a RAG showed\nlimited impact in thwarting the attacks, where the most successful combination\nseverely undermines functionality."
                },
                "authors": [
                    {
                        "name": "Gianluca De Stefano"
                    },
                    {
                        "name": "Lea Sch√∂nherr"
                    },
                    {
                        "name": "Giancarlo Pellegrino"
                    }
                ],
                "author_detail": {
                    "name": "Giancarlo Pellegrino"
                },
                "author": "Giancarlo Pellegrino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06152v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06152v1",
                "updated": "2024-08-12T13:48:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    13,
                    48,
                    6,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T13:48:06Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    13,
                    48,
                    6,
                    0,
                    225,
                    0
                ],
                "title": "Palantir: Towards Efficient Super Resolution for Ultra-high-definition\n  Live Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Palantir: Towards Efficient Super Resolution for Ultra-high-definition\n  Live Streaming"
                },
                "summary": "Neural enhancement through super-resolution deep neural networks opens up new\npossibilities for ultra-high-definition live streaming over existing encoding\nand networking infrastructure. Yet, the heavy SR DNN inference overhead leads\nto severe deployment challenges. To reduce the overhead, existing systems\npropose to apply DNN-based SR only on selected anchor frames while upscaling\nnon-anchor frames via the lightweight reusing-based SR approach. However,\nframe-level scheduling is coarse-grained and fails to deliver optimal\nefficiency. In this work, we propose Palantir, the first neural-enhanced UHD\nlive streaming system with fine-grained patch-level scheduling. In the\npresented solutions, two novel techniques are incorporated to make good\nscheduling decisions for inference overhead optimization and reduce the\nscheduling latency. Firstly, under the guidance of our pioneering and\ntheoretical analysis, Palantir constructs a directed acyclic graph (DAG) for\nlightweight yet accurate quality estimation under any possible anchor patch\nset. Secondly, to further optimize the scheduling latency, Palantir improves\nparallelizability by refactoring the computation subprocedure of the estimation\nprocess into a sparse matrix-matrix multiplication operation. The evaluation\nresults suggest that Palantir incurs a negligible scheduling latency accounting\nfor less than 5.7% of the end-to-end latency requirement. When compared to the\nstate-of-the-art real-time frame-level scheduling strategy, Palantir reduces\nthe energy overhead of SR-integrated mobile clients by 38.1% at most (and 22.4%\non average) and the monetary costs of cloud-based SR by 80.1% at most (and\n38.4% on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural enhancement through super-resolution deep neural networks opens up new\npossibilities for ultra-high-definition live streaming over existing encoding\nand networking infrastructure. Yet, the heavy SR DNN inference overhead leads\nto severe deployment challenges. To reduce the overhead, existing systems\npropose to apply DNN-based SR only on selected anchor frames while upscaling\nnon-anchor frames via the lightweight reusing-based SR approach. However,\nframe-level scheduling is coarse-grained and fails to deliver optimal\nefficiency. In this work, we propose Palantir, the first neural-enhanced UHD\nlive streaming system with fine-grained patch-level scheduling. In the\npresented solutions, two novel techniques are incorporated to make good\nscheduling decisions for inference overhead optimization and reduce the\nscheduling latency. Firstly, under the guidance of our pioneering and\ntheoretical analysis, Palantir constructs a directed acyclic graph (DAG) for\nlightweight yet accurate quality estimation under any possible anchor patch\nset. Secondly, to further optimize the scheduling latency, Palantir improves\nparallelizability by refactoring the computation subprocedure of the estimation\nprocess into a sparse matrix-matrix multiplication operation. The evaluation\nresults suggest that Palantir incurs a negligible scheduling latency accounting\nfor less than 5.7% of the end-to-end latency requirement. When compared to the\nstate-of-the-art real-time frame-level scheduling strategy, Palantir reduces\nthe energy overhead of SR-integrated mobile clients by 38.1% at most (and 22.4%\non average) and the monetary costs of cloud-based SR by 80.1% at most (and\n38.4% on average)."
                },
                "authors": [
                    {
                        "name": "Xinqi Jin"
                    },
                    {
                        "name": "Zhui Zhu"
                    },
                    {
                        "name": "Xikai Sun"
                    },
                    {
                        "name": "Fan Dang"
                    },
                    {
                        "name": "Jiangchuan Liu"
                    },
                    {
                        "name": "Jingao Xu"
                    },
                    {
                        "name": "Kebin Liu"
                    },
                    {
                        "name": "Xinlei Chen"
                    },
                    {
                        "name": "Yunhao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yunhao Liu"
                },
                "author": "Yunhao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06152v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06152v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06142v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06142v1",
                "updated": "2024-08-12T13:37:31Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    13,
                    37,
                    31,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T13:37:31Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    13,
                    37,
                    31,
                    0,
                    225,
                    0
                ],
                "title": "Med42-v2: A Suite of Clinical LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med42-v2: A Suite of Clinical LLMs"
                },
                "summary": "Med42-v2 introduces a suite of clinical large language models (LLMs) designed\nto address the limitations of generic models in healthcare settings. These\nmodels are built on Llama3 architecture and fine-tuned using specialized\nclinical data. They underwent multi-stage preference alignment to effectively\nrespond to natural prompts. While generic models are often preference-aligned\nto avoid answering clinical queries as a precaution, Med42-v2 is specifically\ntrained to overcome this limitation, enabling its use in clinical settings.\nMed42-v2 models demonstrate superior performance compared to the original\nLlama3 models in both 8B and 70B parameter configurations and GPT-4 across\nvarious medical benchmarks. These LLMs are developed to understand clinical\nqueries, perform reasoning tasks, and provide valuable assistance in clinical\nenvironments. The models are now publicly available at\n\\href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Med42-v2 introduces a suite of clinical large language models (LLMs) designed\nto address the limitations of generic models in healthcare settings. These\nmodels are built on Llama3 architecture and fine-tuned using specialized\nclinical data. They underwent multi-stage preference alignment to effectively\nrespond to natural prompts. While generic models are often preference-aligned\nto avoid answering clinical queries as a precaution, Med42-v2 is specifically\ntrained to overcome this limitation, enabling its use in clinical settings.\nMed42-v2 models demonstrate superior performance compared to the original\nLlama3 models in both 8B and 70B parameter configurations and GPT-4 across\nvarious medical benchmarks. These LLMs are developed to understand clinical\nqueries, perform reasoning tasks, and provide valuable assistance in clinical\nenvironments. The models are now publicly available at\n\\href{https://huggingface.co/m42-health}{https://huggingface.co/m42-health}."
                },
                "authors": [
                    {
                        "name": "Cl√©ment Christophe"
                    },
                    {
                        "name": "Praveen K Kanithi"
                    },
                    {
                        "name": "Tathagata Raha"
                    },
                    {
                        "name": "Shadab Khan"
                    },
                    {
                        "name": "Marco AF Pimentel"
                    }
                ],
                "author_detail": {
                    "name": "Marco AF Pimentel"
                },
                "author": "Marco AF Pimentel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06142v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04655v2",
                "updated": "2024-08-12T13:20:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    13,
                    20,
                    36,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-05T11:27:51Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    11,
                    27,
                    51,
                    0,
                    218,
                    0
                ],
                "title": "Strong and weak alignment of large language models with human values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strong and weak alignment of large language models with human values"
                },
                "summary": "Minimizing negative impacts of Artificial Intelligent (AI) systems on human\nsocieties without human supervision requires them to be able to align with\nhuman values. However, most current work only addresses this issue from a\ntechnical point of view, e.g., improving current methods relying on\nreinforcement learning from human feedback, neglecting what it means and is\nrequired for alignment to occur. Here, we propose to distinguish strong and\nweak value alignment. Strong alignment requires cognitive abilities (either\nhuman-like or different from humans) such as understanding and reasoning about\nagents' intentions and their ability to causally produce desired effects. We\nargue that this is required for AI systems like large language models (LLMs) to\nbe able to recognize situations presenting a risk that human values may be\nflouted. To illustrate this distinction, we present a series of prompts showing\nChatGPT's, Gemini's and Copilot's failures to recognize some of these\nsituations. We moreover analyze word embeddings to show that the nearest\nneighbors of some human values in LLMs differ from humans' semantic\nrepresentations. We then propose a new thought experiment that we call \"the\nChinese room with a word transition dictionary\", in extension of John Searle's\nfamous proposal. We finally mention current promising research directions\ntowards a weak alignment, which could produce statistically satisfying answers\nin a number of common situations, however so far without ensuring any truth\nvalue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minimizing negative impacts of Artificial Intelligent (AI) systems on human\nsocieties without human supervision requires them to be able to align with\nhuman values. However, most current work only addresses this issue from a\ntechnical point of view, e.g., improving current methods relying on\nreinforcement learning from human feedback, neglecting what it means and is\nrequired for alignment to occur. Here, we propose to distinguish strong and\nweak value alignment. Strong alignment requires cognitive abilities (either\nhuman-like or different from humans) such as understanding and reasoning about\nagents' intentions and their ability to causally produce desired effects. We\nargue that this is required for AI systems like large language models (LLMs) to\nbe able to recognize situations presenting a risk that human values may be\nflouted. To illustrate this distinction, we present a series of prompts showing\nChatGPT's, Gemini's and Copilot's failures to recognize some of these\nsituations. We moreover analyze word embeddings to show that the nearest\nneighbors of some human values in LLMs differ from humans' semantic\nrepresentations. We then propose a new thought experiment that we call \"the\nChinese room with a word transition dictionary\", in extension of John Searle's\nfamous proposal. We finally mention current promising research directions\ntowards a weak alignment, which could produce statistically satisfying answers\nin a number of common situations, however so far without ensuring any truth\nvalue."
                },
                "authors": [
                    {
                        "name": "Mehdi Khamassi"
                    },
                    {
                        "name": "Marceau Nahon"
                    },
                    {
                        "name": "Raja Chatila"
                    }
                ],
                "author_detail": {
                    "name": "Raja Chatila"
                },
                "author": "Raja Chatila",
                "arxiv_comment": "Accepted for publication in Scientific Reports, special issue on AI\n  aligment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06115v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06115v1",
                "updated": "2024-08-12T12:56:24Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    56,
                    24,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T12:56:24Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    56,
                    24,
                    0,
                    225,
                    0
                ],
                "title": "Measurement Study of Programmable Network Coding in Cloud-native 5G and\n  Beyond Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement Study of Programmable Network Coding in Cloud-native 5G and\n  Beyond Networks"
                },
                "summary": "Emerging 5G/6G use cases span various industries, necessitating flexible\nsolutions that leverage emerging technologies to meet diverse and stringent\napplication requirements under changing network conditions. The standard 5G RAN\nsolution, retransmission, reduces packet loss but can increase transmission\ndelay in the process. Random Linear Network Coding (RLNC) offers an alternative\nby proactively sending combinations of original packets, thus reducing both\ndelay and packet loss. Current research often only simulates the integration of\nRLNC in 5G while we implement and evaluate our approach on real commercially\navailable hardware in a real-world deployment.\n  We introduce Flexible Network Coding (FlexNC), which enables the flexible\nfusion of several RLNC protocols by incorporating a forwarder with multiple\nRLNC nodes. Network operators can configure FlexNC based on network conditions\nand application requirements. To further boost network programmability, our\nRecoder in the Network (RecNet) leverages intermediate network nodes to join\nthe coding process. Both the proposed algorithms have been implemented on\nOpenAirInterface and extensively tested with traffic from different\napplications in a real network. While FlexNC adapts to various application\nneeds of latency and packet loss, RecNet significantly minimizes packet loss\nfor a remote user with minimal increase in delay compared to pure RLNC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging 5G/6G use cases span various industries, necessitating flexible\nsolutions that leverage emerging technologies to meet diverse and stringent\napplication requirements under changing network conditions. The standard 5G RAN\nsolution, retransmission, reduces packet loss but can increase transmission\ndelay in the process. Random Linear Network Coding (RLNC) offers an alternative\nby proactively sending combinations of original packets, thus reducing both\ndelay and packet loss. Current research often only simulates the integration of\nRLNC in 5G while we implement and evaluate our approach on real commercially\navailable hardware in a real-world deployment.\n  We introduce Flexible Network Coding (FlexNC), which enables the flexible\nfusion of several RLNC protocols by incorporating a forwarder with multiple\nRLNC nodes. Network operators can configure FlexNC based on network conditions\nand application requirements. To further boost network programmability, our\nRecoder in the Network (RecNet) leverages intermediate network nodes to join\nthe coding process. Both the proposed algorithms have been implemented on\nOpenAirInterface and extensively tested with traffic from different\napplications in a real network. While FlexNC adapts to various application\nneeds of latency and packet loss, RecNet significantly minimizes packet loss\nfor a remote user with minimal increase in delay compared to pure RLNC."
                },
                "authors": [
                    {
                        "name": "Osel Lhamo"
                    },
                    {
                        "name": "Tung V. Doan"
                    },
                    {
                        "name": "Elif Tasdemir"
                    },
                    {
                        "name": "Mahdi Attawna"
                    },
                    {
                        "name": "Giang T. Nguyen"
                    },
                    {
                        "name": "Patrick Seeling"
                    },
                    {
                        "name": "Martin Reisslein"
                    },
                    {
                        "name": "Frank H. P. Fitzek"
                    }
                ],
                "author_detail": {
                    "name": "Frank H. P. Fitzek"
                },
                "author": "Frank H. P. Fitzek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06115v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06115v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11046v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11046v3",
                "updated": "2024-08-12T12:41:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    41,
                    57,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-08T12:32:10Z",
                "published_parsed": [
                    2024,
                    7,
                    8,
                    12,
                    32,
                    10,
                    0,
                    190,
                    0
                ],
                "title": "A Survey on LoRA of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on LoRA of Large Language Models"
                },
                "summary": "Low-Rank Adaptation~(LoRA), which updates the dense neural network layers\nwith pluggable low-rank matrices, is one of the best performed parameter\nefficient fine-tuning paradigms. Furthermore, it has significant advantages in\ncross-task generalization and privacy-preserving. Hence, LoRA has gained much\nattention recently, and the number of related literature demonstrates\nexponential growth. It is necessary to conduct a comprehensive overview of the\ncurrent progress on LoRA. This survey categorizes and reviews the progress from\nthe perspectives of (1) downstream adaptation improving variants that improve\nLoRA's performance on downstream tasks; (2) cross-task generalization methods\nthat mix multiple LoRA plugins to achieve cross-task generalization; (3)\nefficiency-improving methods that boost the computation-efficiency of LoRA; (4)\ndata privacy-preserving methods that use LoRA in federated learning; (5)\napplication. Besides, this survey also discusses the future directions in this\nfield. At last, we provide a Github\npage~\\footnote{\\href{https://github.com/ZJU-LLMs/Awesome-LoRAs.git}{https://github.com/ZJU-LLMs/Awesome-LoRAs.git}}\nfor readers to check the updates and initiate discussions on this survey paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation~(LoRA), which updates the dense neural network layers\nwith pluggable low-rank matrices, is one of the best performed parameter\nefficient fine-tuning paradigms. Furthermore, it has significant advantages in\ncross-task generalization and privacy-preserving. Hence, LoRA has gained much\nattention recently, and the number of related literature demonstrates\nexponential growth. It is necessary to conduct a comprehensive overview of the\ncurrent progress on LoRA. This survey categorizes and reviews the progress from\nthe perspectives of (1) downstream adaptation improving variants that improve\nLoRA's performance on downstream tasks; (2) cross-task generalization methods\nthat mix multiple LoRA plugins to achieve cross-task generalization; (3)\nefficiency-improving methods that boost the computation-efficiency of LoRA; (4)\ndata privacy-preserving methods that use LoRA in federated learning; (5)\napplication. Besides, this survey also discusses the future directions in this\nfield. At last, we provide a Github\npage~\\footnote{\\href{https://github.com/ZJU-LLMs/Awesome-LoRAs.git}{https://github.com/ZJU-LLMs/Awesome-LoRAs.git}}\nfor readers to check the updates and initiate discussions on this survey paper."
                },
                "authors": [
                    {
                        "name": "Yuren Mao"
                    },
                    {
                        "name": "Yuhang Ge"
                    },
                    {
                        "name": "Yijiang Fan"
                    },
                    {
                        "name": "Wenyi Xu"
                    },
                    {
                        "name": "Yu Mi"
                    },
                    {
                        "name": "Zhonghao Hu"
                    },
                    {
                        "name": "Yunjun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Yunjun Gao"
                },
                "author": "Yunjun Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11046v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11046v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.07032v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.07032v2",
                "updated": "2024-08-12T12:11:52Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    11,
                    52,
                    0,
                    225,
                    0
                ],
                "published": "2023-11-13T02:31:16Z",
                "published_parsed": [
                    2023,
                    11,
                    13,
                    2,
                    31,
                    16,
                    0,
                    317,
                    0
                ],
                "title": "ExpNote: Black-box Large Language Models are Better Task Solvers with\n  Experience Notebook",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpNote: Black-box Large Language Models are Better Task Solvers with\n  Experience Notebook"
                },
                "summary": "Black-box Large Language Models (LLMs) have shown great power in solving\nvarious tasks and are considered general problem solvers. However, LLMs still\nfail in many specific tasks although understand the task instruction. In this\npaper, we focus on the problem of boosting the ability of black-box LLMs to\nsolve downstream tasks. We propose ExpNote, an automated framework to help LLMs\nbetter adapt to unfamiliar tasks through reflecting and noting experiences from\ntraining data and retrieving them from external memory during testing. We\nevaluate ExpNote on multiple tasks and the experimental results demonstrate\nthat the proposed method significantly improves the performance of black-box\nLLMs. The data and code are available at\nhttps://github.com/forangel2014/ExpNote",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Black-box Large Language Models (LLMs) have shown great power in solving\nvarious tasks and are considered general problem solvers. However, LLMs still\nfail in many specific tasks although understand the task instruction. In this\npaper, we focus on the problem of boosting the ability of black-box LLMs to\nsolve downstream tasks. We propose ExpNote, an automated framework to help LLMs\nbetter adapt to unfamiliar tasks through reflecting and noting experiences from\ntraining data and retrieving them from external memory during testing. We\nevaluate ExpNote on multiple tasks and the experimental results demonstrate\nthat the proposed method significantly improves the performance of black-box\nLLMs. The data and code are available at\nhttps://github.com/forangel2014/ExpNote"
                },
                "authors": [
                    {
                        "name": "Wangtao Sun"
                    },
                    {
                        "name": "Xuanqing Yu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "arxiv_comment": "EMNLP 2023 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.07032v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.07032v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06087v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06087v1",
                "updated": "2024-08-12T12:04:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    4,
                    14,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T12:04:14Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    12,
                    4,
                    14,
                    0,
                    225,
                    0
                ],
                "title": "Building Decision Making Models Through Language Model Regime",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building Decision Making Models Through Language Model Regime"
                },
                "summary": "We propose a novel approach for decision making problems leveraging the\ngeneralization capabilities of large language models (LLMs). Traditional\nmethods such as expert systems, planning algorithms, and reinforcement learning\noften exhibit limited generalization, typically requiring the training of new\nmodels for each unique task. In contrast, LLMs demonstrate remarkable success\nin generalizing across varied language tasks, inspiring a new strategy for\ntraining decision making models. Our approach, referred to as \"Learning then\nUsing\" (LTU), entails a two-stage process. Initially, the \\textit{learning}\nphase develops a robust foundational decision making model by integrating\ndiverse knowledge from various domains and decision making contexts. The\nsubsequent \\textit{using} phase refines this foundation model for specific\ndecision making scenarios. Distinct from other studies that employ LLMs for\ndecision making through supervised learning, our LTU method embraces a\nversatile training methodology that combines broad pre-training with targeted\nfine-tuning. Experiments in e-commerce domains such as advertising and search\noptimization have shown that LTU approach outperforms traditional supervised\nlearning regimes in decision making capabilities and generalization. The LTU\napproach is the first practical training architecture for both single-step and\nmulti-step decision making tasks combined with LLMs, which can be applied\nbeyond game and robot domains. It provides a robust and adaptable framework for\ndecision making, enhances the effectiveness and flexibility of various systems\nin tackling various challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel approach for decision making problems leveraging the\ngeneralization capabilities of large language models (LLMs). Traditional\nmethods such as expert systems, planning algorithms, and reinforcement learning\noften exhibit limited generalization, typically requiring the training of new\nmodels for each unique task. In contrast, LLMs demonstrate remarkable success\nin generalizing across varied language tasks, inspiring a new strategy for\ntraining decision making models. Our approach, referred to as \"Learning then\nUsing\" (LTU), entails a two-stage process. Initially, the \\textit{learning}\nphase develops a robust foundational decision making model by integrating\ndiverse knowledge from various domains and decision making contexts. The\nsubsequent \\textit{using} phase refines this foundation model for specific\ndecision making scenarios. Distinct from other studies that employ LLMs for\ndecision making through supervised learning, our LTU method embraces a\nversatile training methodology that combines broad pre-training with targeted\nfine-tuning. Experiments in e-commerce domains such as advertising and search\noptimization have shown that LTU approach outperforms traditional supervised\nlearning regimes in decision making capabilities and generalization. The LTU\napproach is the first practical training architecture for both single-step and\nmulti-step decision making tasks combined with LLMs, which can be applied\nbeyond game and robot domains. It provides a robust and adaptable framework for\ndecision making, enhances the effectiveness and flexibility of various systems\nin tackling various challenges."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Haoxiang Liu"
                    },
                    {
                        "name": "Feijun Jiang"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaifu Zhang"
                },
                "author": "Kaifu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06087v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06087v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.01043v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.01043v4",
                "updated": "2024-08-12T11:53:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    11,
                    53,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2023-11-02T07:23:33Z",
                "published_parsed": [
                    2023,
                    11,
                    2,
                    7,
                    23,
                    33,
                    3,
                    306,
                    0
                ],
                "title": "LLM4Drive: A Survey of Large Language Models for Autonomous Driving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Drive: A Survey of Large Language Models for Autonomous Driving"
                },
                "summary": "Autonomous driving technology, a catalyst for revolutionizing transportation\nand urban mobility, has the tend to transition from rule-based systems to\ndata-driven strategies. Traditional module-based systems are constrained by\ncumulative errors among cascaded modules and inflexible pre-set rules. In\ncontrast, end-to-end autonomous driving systems have the potential to avoid\nerror accumulation due to their fully data-driven training process, although\nthey often lack transparency due to their \"black box\" nature, complicating the\nvalidation and traceability of decisions. Recently, large language models\n(LLMs) have demonstrated abilities including understanding context, logical\nreasoning, and generating answers. A natural thought is to utilize these\nabilities to empower autonomous driving. By combining LLM with foundation\nvision models, it could open the door to open-world understanding, reasoning,\nand few-shot learning, which current autonomous driving systems are lacking. In\nthis paper, we systematically review a research line about \\textit{Large\nLanguage Models for Autonomous Driving (LLM4AD)}. This study evaluates the\ncurrent state of technological advancements, distinctly outlining the principal\nchallenges and prospective directions for the field. For the convenience of\nresearchers in academia and industry, we provide real-time updates on the\nlatest advances in the field as well as relevant open-source resources via the\ndesignated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving technology, a catalyst for revolutionizing transportation\nand urban mobility, has the tend to transition from rule-based systems to\ndata-driven strategies. Traditional module-based systems are constrained by\ncumulative errors among cascaded modules and inflexible pre-set rules. In\ncontrast, end-to-end autonomous driving systems have the potential to avoid\nerror accumulation due to their fully data-driven training process, although\nthey often lack transparency due to their \"black box\" nature, complicating the\nvalidation and traceability of decisions. Recently, large language models\n(LLMs) have demonstrated abilities including understanding context, logical\nreasoning, and generating answers. A natural thought is to utilize these\nabilities to empower autonomous driving. By combining LLM with foundation\nvision models, it could open the door to open-world understanding, reasoning,\nand few-shot learning, which current autonomous driving systems are lacking. In\nthis paper, we systematically review a research line about \\textit{Large\nLanguage Models for Autonomous Driving (LLM4AD)}. This study evaluates the\ncurrent state of technological advancements, distinctly outlining the principal\nchallenges and prospective directions for the field. For the convenience of\nresearchers in academia and industry, we provide real-time updates on the\nlatest advances in the field as well as relevant open-source resources via the\ndesignated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD."
                },
                "authors": [
                    {
                        "name": "Zhenjie Yang"
                    },
                    {
                        "name": "Xiaosong Jia"
                    },
                    {
                        "name": "Hongyang Li"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "arxiv_comment": "GitHub Repo: https://github.com/Thinklab-SJTU/Awesome-LLM4AD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.01043v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.01043v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06765v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06765v3",
                "updated": "2024-08-12T10:55:38Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    10,
                    55,
                    38,
                    0,
                    225,
                    0
                ],
                "published": "2024-03-11T14:35:45Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    14,
                    35,
                    45,
                    0,
                    71,
                    0
                ],
                "title": "ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large\n  Language Model"
                },
                "summary": "The internet has brought both benefits and harms to society. A prime example\nof the latter is misinformation, including conspiracy theories, which flood the\nweb. Recent advances in natural language processing, particularly the emergence\nof large language models (LLMs), have improved the prospects of accurate\nmisinformation detection. However, most LLM-based approaches to conspiracy\ntheory detection focus only on binary classification and fail to account for\nthe important relationship between misinformation and affective features (i.e.,\nsentiment and emotions). Driven by a comprehensive analysis of conspiracy text\nthat reveals its distinctive affective features, we propose ConspEmoLLM, the\nfirst open-source LLM that integrates affective information and is able to\nperform diverse tasks relating to conspiracy theories. These tasks include not\nonly conspiracy theory detection, but also classification of theory type and\ndetection of related discussion (e.g., opinions towards theories). ConspEmoLLM\nis fine-tuned based on an emotion-oriented LLM using our novel ConDID dataset,\nwhich includes five tasks to support LLM instruction tuning and evaluation. We\ndemonstrate that when applied to these tasks, ConspEmoLLM largely outperforms\nseveral open-source general domain LLMs and ChatGPT, as well as an LLM that has\nbeen fine-tuned using ConDID, but which does not use affective features. This\nproject will be released on https://github.com/lzw108/ConspEmoLLM/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The internet has brought both benefits and harms to society. A prime example\nof the latter is misinformation, including conspiracy theories, which flood the\nweb. Recent advances in natural language processing, particularly the emergence\nof large language models (LLMs), have improved the prospects of accurate\nmisinformation detection. However, most LLM-based approaches to conspiracy\ntheory detection focus only on binary classification and fail to account for\nthe important relationship between misinformation and affective features (i.e.,\nsentiment and emotions). Driven by a comprehensive analysis of conspiracy text\nthat reveals its distinctive affective features, we propose ConspEmoLLM, the\nfirst open-source LLM that integrates affective information and is able to\nperform diverse tasks relating to conspiracy theories. These tasks include not\nonly conspiracy theory detection, but also classification of theory type and\ndetection of related discussion (e.g., opinions towards theories). ConspEmoLLM\nis fine-tuned based on an emotion-oriented LLM using our novel ConDID dataset,\nwhich includes five tasks to support LLM instruction tuning and evaluation. We\ndemonstrate that when applied to these tasks, ConspEmoLLM largely outperforms\nseveral open-source general domain LLMs and ChatGPT, as well as an LLM that has\nbeen fine-tuned using ConDID, but which does not use affective features. This\nproject will be released on https://github.com/lzw108/ConspEmoLLM/."
                },
                "authors": [
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Boyang Liu"
                    },
                    {
                        "name": "Paul Thompson"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Ananiadou"
                },
                "author": "Sophia Ananiadou",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06765v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06765v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06037v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06037v1",
                "updated": "2024-08-12T09:59:47Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    9,
                    59,
                    47,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T09:59:47Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    9,
                    59,
                    47,
                    0,
                    225,
                    0
                ],
                "title": "Hyperion: Unveiling DApp Inconsistencies using LLM and Dataflow-Guided\n  Symbolic Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyperion: Unveiling DApp Inconsistencies using LLM and Dataflow-Guided\n  Symbolic Execution"
                },
                "summary": "The rapid advancement of blockchain platforms has significantly accelerated\nthe growth of decentralized applications (DApps). Similar to traditional\napplications, DApps integrate front-end descriptions that showcase their\nfeatures to attract users, and back-end smart contracts for executing their\nbusiness logic. However, inconsistencies between the features promoted in\nfront-end descriptions and those actually implemented in the contract can\nconfuse users and undermine DApps's trustworthiness. In this paper, we first\nconducted an empirical study to identify seven types of inconsistencies, each\nexemplified by a real-world DApp. Furthermore, we introduce HYPERION, an\napproach designed to automatically identify inconsistencies between front-end\ndescriptions and back-end code implementation in DApps. This method leverages a\nfine-tuned large language model LLaMA2 to analyze DApp descriptions and employs\ndataflow-guided symbolic execution for contract bytecode analysis. Finally,\nHYPERION reports the inconsistency based on predefined detection patterns. The\nexperiment on our ground truth dataset consisting of 54 DApps shows that\nHYPERION reaches 84.06% overall recall and 92.06% overall precision in\nreporting DApp inconsistencies. We also implement HYPERION to analyze 835\nreal-world DApps. The experimental results show that HYPERION discovers 459\nreal-world DApps containing at least one inconsistency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of blockchain platforms has significantly accelerated\nthe growth of decentralized applications (DApps). Similar to traditional\napplications, DApps integrate front-end descriptions that showcase their\nfeatures to attract users, and back-end smart contracts for executing their\nbusiness logic. However, inconsistencies between the features promoted in\nfront-end descriptions and those actually implemented in the contract can\nconfuse users and undermine DApps's trustworthiness. In this paper, we first\nconducted an empirical study to identify seven types of inconsistencies, each\nexemplified by a real-world DApp. Furthermore, we introduce HYPERION, an\napproach designed to automatically identify inconsistencies between front-end\ndescriptions and back-end code implementation in DApps. This method leverages a\nfine-tuned large language model LLaMA2 to analyze DApp descriptions and employs\ndataflow-guided symbolic execution for contract bytecode analysis. Finally,\nHYPERION reports the inconsistency based on predefined detection patterns. The\nexperiment on our ground truth dataset consisting of 54 DApps shows that\nHYPERION reaches 84.06% overall recall and 92.06% overall precision in\nreporting DApp inconsistencies. We also implement HYPERION to analyze 835\nreal-world DApps. The experimental results show that HYPERION discovers 459\nreal-world DApps containing at least one inconsistency."
                },
                "authors": [
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Xingwei Lin"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Qingyuan Zhong"
                    },
                    {
                        "name": "Lei Xiao"
                    },
                    {
                        "name": "Renke Huang"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "Accepted by ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06037v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06037v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06003v1",
                "updated": "2024-08-12T08:52:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    52,
                    14,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:52:14Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    52,
                    14,
                    0,
                    225,
                    0
                ],
                "title": "LUT Tensor Core: Lookup Table Enables Efficient Low-Bit LLM Inference\n  Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LUT Tensor Core: Lookup Table Enables Efficient Low-Bit LLM Inference\n  Acceleration"
                },
                "summary": "As large language model (LLM) inference demands ever-greater resources, there\nis a rapid growing trend of using low-bit weights to shrink memory usage and\nboost inference efficiency. However, these low-bit LLMs introduce the need for\nmixed-precision matrix multiplication (mpGEMM), which is a crucial yet\nunder-explored operation that involves multiplying lower-precision weights with\nhigher-precision activations. Unfortunately, current hardware does not natively\nsupport mpGEMM, resulting in indirect and inefficient dequantization-based\nimplementations.\n  To address the mpGEMM requirements in low-bit LLMs, we explored the lookup\ntable (LUT)-based approach for mpGEMM. However, a conventional LUT\nimplementation falls short of its potential. To fully harness the power of\nLUT-based mpGEMM, we introduce LUT Tensor Core, a software-hardware co-design\noptimized for low-bit LLM inference. Specifically, we introduce software-based\noperator fusion and table symmetrization techniques to optimize table\nprecompute and table storage, respectively. Then, LUT Tensor Core proposes the\nhardware design featuring an elongated tiling shape design to enhance table\nreuse and a bit-serial design to support various precision combinations in\nmpGEMM. Moreover, we design an end-to-end compilation stack with new\ninstructions for LUT-based mpGEMM, enabling efficient LLM compilation and\noptimizations. The evaluation on low-bit LLMs (e.g., BitNet, LLAMA) shows that\nLUT Tensor Core achieves more than a magnitude of improvements on both compute\ndensity and energy efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language model (LLM) inference demands ever-greater resources, there\nis a rapid growing trend of using low-bit weights to shrink memory usage and\nboost inference efficiency. However, these low-bit LLMs introduce the need for\nmixed-precision matrix multiplication (mpGEMM), which is a crucial yet\nunder-explored operation that involves multiplying lower-precision weights with\nhigher-precision activations. Unfortunately, current hardware does not natively\nsupport mpGEMM, resulting in indirect and inefficient dequantization-based\nimplementations.\n  To address the mpGEMM requirements in low-bit LLMs, we explored the lookup\ntable (LUT)-based approach for mpGEMM. However, a conventional LUT\nimplementation falls short of its potential. To fully harness the power of\nLUT-based mpGEMM, we introduce LUT Tensor Core, a software-hardware co-design\noptimized for low-bit LLM inference. Specifically, we introduce software-based\noperator fusion and table symmetrization techniques to optimize table\nprecompute and table storage, respectively. Then, LUT Tensor Core proposes the\nhardware design featuring an elongated tiling shape design to enhance table\nreuse and a bit-serial design to support various precision combinations in\nmpGEMM. Moreover, we design an end-to-end compilation stack with new\ninstructions for LUT-based mpGEMM, enabling efficient LLM compilation and\noptimizations. The evaluation on low-bit LLMs (e.g., BitNet, LLAMA) shows that\nLUT Tensor Core achieves more than a magnitude of improvements on both compute\ndensity and energy efficiency."
                },
                "authors": [
                    {
                        "name": "Zhiwen Mo"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Jianyu Wei"
                    },
                    {
                        "name": "Zhichen Zeng"
                    },
                    {
                        "name": "Shijie Cao"
                    },
                    {
                        "name": "Lingxiao Ma"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Jilong Xue"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03095v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03095v3",
                "updated": "2024-08-12T08:27:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    27,
                    56,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-06T10:52:41Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    10,
                    52,
                    41,
                    1,
                    219,
                    0
                ],
                "title": "TestART: Improving LLM-based Unit Test via Co-evolution of Automated\n  Generation and Repair Iteration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TestART: Improving LLM-based Unit Test via Co-evolution of Automated\n  Generation and Repair Iteration"
                },
                "summary": "Unit test is crucial for detecting bugs in individual program units but\nconsumes time and effort. The existing automated unit test generation methods\nare mainly based on search-based software testing (SBST) and language models to\nliberate developers. Recently, large language models (LLMs) have demonstrated\nremarkable reasoning and generation capabilities. However, several problems\nlimit their ability to generate high-quality test cases: (1) LLMs may generate\ninvalid test cases under insufficient context, resulting in compilation errors;\n(2) Lack of test and coverage feedback information may cause runtime errors and\nlow coverage rates. (3) The repetitive suppression problem causes LLMs to get\nstuck into the repetition loop of self-repair or re-generation attempts. In\nthis paper, we propose TestART, a novel unit test generation method that\nleverages the strengths of LLMs while overcoming the limitations mentioned.\nTestART improves LLM-based unit test via co-evolution of automated generation\nand repair iteration. TestART leverages the template-based repair technique to\nfix bugs in LLM-generated test cases, using prompt injection to guide the\nnext-step automated generation and avoid repetition suppression. Furthermore,\nTestART extracts coverage information from the passed test cases and utilizes\nit as testing feedback to enhance the sufficiency of the final test case. This\nsynergy between generation and repair elevates the quality, effectiveness, and\nreadability of the produced test cases significantly beyond previous methods.\nIn comparative experiments, the pass rate of TestART-generated test cases is\n78.55%, which is approximately 18% higher than both the ChatGPT-4.0 model and\nthe same ChatGPT-3.5-based method ChatUniTest. It also achieves an impressive\nline coverage rate of 90.96% on the focal methods that passed the test,\nexceeding EvoSuite by 3.4%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unit test is crucial for detecting bugs in individual program units but\nconsumes time and effort. The existing automated unit test generation methods\nare mainly based on search-based software testing (SBST) and language models to\nliberate developers. Recently, large language models (LLMs) have demonstrated\nremarkable reasoning and generation capabilities. However, several problems\nlimit their ability to generate high-quality test cases: (1) LLMs may generate\ninvalid test cases under insufficient context, resulting in compilation errors;\n(2) Lack of test and coverage feedback information may cause runtime errors and\nlow coverage rates. (3) The repetitive suppression problem causes LLMs to get\nstuck into the repetition loop of self-repair or re-generation attempts. In\nthis paper, we propose TestART, a novel unit test generation method that\nleverages the strengths of LLMs while overcoming the limitations mentioned.\nTestART improves LLM-based unit test via co-evolution of automated generation\nand repair iteration. TestART leverages the template-based repair technique to\nfix bugs in LLM-generated test cases, using prompt injection to guide the\nnext-step automated generation and avoid repetition suppression. Furthermore,\nTestART extracts coverage information from the passed test cases and utilizes\nit as testing feedback to enhance the sufficiency of the final test case. This\nsynergy between generation and repair elevates the quality, effectiveness, and\nreadability of the produced test cases significantly beyond previous methods.\nIn comparative experiments, the pass rate of TestART-generated test cases is\n78.55%, which is approximately 18% higher than both the ChatGPT-4.0 model and\nthe same ChatGPT-3.5-based method ChatUniTest. It also achieves an impressive\nline coverage rate of 90.96% on the focal methods that passed the test,\nexceeding EvoSuite by 3.4%."
                },
                "authors": [
                    {
                        "name": "Siqi Gu"
                    },
                    {
                        "name": "Chunrong Fang"
                    },
                    {
                        "name": "Quanjun Zhang"
                    },
                    {
                        "name": "Fangyuan Tian"
                    },
                    {
                        "name": "Jianyi Zhou"
                    },
                    {
                        "name": "Zhenyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Chen"
                },
                "author": "Zhenyu Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03095v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03095v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15740v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15740v2",
                "updated": "2024-08-12T08:21:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    21,
                    32,
                    0,
                    225,
                    0
                ],
                "published": "2024-03-23T06:36:32Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    6,
                    36,
                    32,
                    5,
                    83,
                    0
                ],
                "title": "Protecting Copyrighted Material with Unique Identifiers in Large\n  Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting Copyrighted Material with Unique Identifiers in Large\n  Language Model Training"
                },
                "summary": "A major public concern regarding the training of large language models (LLMs)\nis whether they abusing copyrighted online text. Previous membership inference\nmethods may be misled by similar examples in vast amounts of training data.\nAdditionally, these methods are often too complex for general users to\nunderstand and use, making them centralized, lacking transparency, and\ntrustworthiness. To address these issues, we propose an alternative\n\\textit{insert-and-detection} methodology, advocating that web users and\ncontent platforms employ \\textbf{\\textit{unique identifiers}} for reliable and\nindependent membership inference. Users and platforms can create their own\nidentifiers, embed them in copyrighted text, and independently detect them in\nfuture LLMs. As an initial demonstration, we introduce \\textit{ghost\nsentences}, a primitive form of unique identifiers, consisting primarily of\npassphrases made up of random words. By embedding one ghost sentences in a few\ncopyrighted texts, users can detect its membership using a perplexity test and\na \\textit{user-friendly} last-$k$ words test. The perplexity test is based on\nthe fact that LLMs trained on natural language should exhibit high perplexity\nwhen encountering unnatural passphrases. As the repetition increases, users can\nleverage the verbatim memorization ability of LLMs to perform a last-$k$ words\ntest by chatting with LLMs without writing any code. Both tests offer rigorous\nstatistical guarantees for membership inference. For LLaMA-13B, a perplexity\ntest on 30 ghost sentences with an average of 7 repetitions in 148K examples\nyields a 0.891 ROC AUC. For the last-$k$ words test with OpenLLaMA-3B, 11 out\nof 16 users, with an average of 24 examples each, successfully identify their\ndata from 1.8M examples.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major public concern regarding the training of large language models (LLMs)\nis whether they abusing copyrighted online text. Previous membership inference\nmethods may be misled by similar examples in vast amounts of training data.\nAdditionally, these methods are often too complex for general users to\nunderstand and use, making them centralized, lacking transparency, and\ntrustworthiness. To address these issues, we propose an alternative\n\\textit{insert-and-detection} methodology, advocating that web users and\ncontent platforms employ \\textbf{\\textit{unique identifiers}} for reliable and\nindependent membership inference. Users and platforms can create their own\nidentifiers, embed them in copyrighted text, and independently detect them in\nfuture LLMs. As an initial demonstration, we introduce \\textit{ghost\nsentences}, a primitive form of unique identifiers, consisting primarily of\npassphrases made up of random words. By embedding one ghost sentences in a few\ncopyrighted texts, users can detect its membership using a perplexity test and\na \\textit{user-friendly} last-$k$ words test. The perplexity test is based on\nthe fact that LLMs trained on natural language should exhibit high perplexity\nwhen encountering unnatural passphrases. As the repetition increases, users can\nleverage the verbatim memorization ability of LLMs to perform a last-$k$ words\ntest by chatting with LLMs without writing any code. Both tests offer rigorous\nstatistical guarantees for membership inference. For LLaMA-13B, a perplexity\ntest on 30 ghost sentences with an average of 7 repetitions in 148K examples\nyields a 0.891 ROC AUC. For the last-$k$ words test with OpenLLaMA-3B, 11 out\nof 16 users, with an average of 24 examples each, successfully identify their\ndata from 1.8M examples."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Linchao Zhu"
                    },
                    {
                        "name": "Ruijie Quan"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15740v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15740v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05985v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05985v1",
                "updated": "2024-08-12T08:21:04Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    21,
                    4,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T08:21:04Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    8,
                    21,
                    4,
                    0,
                    225,
                    0
                ],
                "title": "Diffuse-UDA: Addressing Unsupervised Domain Adaptation in Medical Image\n  Segmentation with Appearance and Structure Aligned Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffuse-UDA: Addressing Unsupervised Domain Adaptation in Medical Image\n  Segmentation with Appearance and Structure Aligned Diffusion Models"
                },
                "summary": "The scarcity and complexity of voxel-level annotations in 3D medical imaging\npresent significant challenges, particularly due to the domain gap between\nlabeled datasets from well-resourced centers and unlabeled datasets from\nless-resourced centers. This disparity affects the fairness of artificial\nintelligence algorithms in healthcare. We introduce Diffuse-UDA, a novel method\nleveraging diffusion models to tackle Unsupervised Domain Adaptation (UDA) in\nmedical image segmentation. Diffuse-UDA generates high-quality image-mask pairs\nwith target domain characteristics and various structures, thereby enhancing\nUDA tasks. Initially, pseudo labels for target domain samples are generated.\nSubsequently, a specially tailored diffusion model, incorporating deformable\naugmentations, is trained on image-label or image-pseudo-label pairs from both\ndomains. Finally, source domain labels guide the diffusion model to generate\nimage-label pairs for the target domain. Comprehensive evaluations on several\nbenchmarks demonstrate that Diffuse-UDA outperforms leading UDA and\nsemi-supervised strategies, achieving performance close to or even surpassing\nthe theoretical upper bound of models trained directly on target domain data.\nDiffuse-UDA offers a pathway to advance the development and deployment of AI\nsystems in medical imaging, addressing disparities between healthcare\nenvironments. This approach enables the exploration of innovative AI-driven\ndiagnostic tools, improves outcomes, saves time, and reduces human error.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity and complexity of voxel-level annotations in 3D medical imaging\npresent significant challenges, particularly due to the domain gap between\nlabeled datasets from well-resourced centers and unlabeled datasets from\nless-resourced centers. This disparity affects the fairness of artificial\nintelligence algorithms in healthcare. We introduce Diffuse-UDA, a novel method\nleveraging diffusion models to tackle Unsupervised Domain Adaptation (UDA) in\nmedical image segmentation. Diffuse-UDA generates high-quality image-mask pairs\nwith target domain characteristics and various structures, thereby enhancing\nUDA tasks. Initially, pseudo labels for target domain samples are generated.\nSubsequently, a specially tailored diffusion model, incorporating deformable\naugmentations, is trained on image-label or image-pseudo-label pairs from both\ndomains. Finally, source domain labels guide the diffusion model to generate\nimage-label pairs for the target domain. Comprehensive evaluations on several\nbenchmarks demonstrate that Diffuse-UDA outperforms leading UDA and\nsemi-supervised strategies, achieving performance close to or even surpassing\nthe theoretical upper bound of models trained directly on target domain data.\nDiffuse-UDA offers a pathway to advance the development and deployment of AI\nsystems in medical imaging, addressing disparities between healthcare\nenvironments. This approach enables the exploration of innovative AI-driven\ndiagnostic tools, improves outcomes, saves time, and reduces human error."
                },
                "authors": [
                    {
                        "name": "Haifan Gong"
                    },
                    {
                        "name": "Yitao Wang"
                    },
                    {
                        "name": "Yihan Wang"
                    },
                    {
                        "name": "Jiashun Xiao"
                    },
                    {
                        "name": "Xiang Wan"
                    },
                    {
                        "name": "Haofeng Li"
                    }
                ],
                "author_detail": {
                    "name": "Haofeng Li"
                },
                "author": "Haofeng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05985v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05985v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05968v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05968v1",
                "updated": "2024-08-12T07:49:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    49,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T07:49:28Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    49,
                    28,
                    0,
                    225,
                    0
                ],
                "title": "Nob-MIAs: Non-biased Membership Inference Attacks Assessment on Large\n  Language Models with Ex-Post Dataset Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nob-MIAs: Non-biased Membership Inference Attacks Assessment on Large\n  Language Models with Ex-Post Dataset Construction"
                },
                "summary": "The rise of Large Language Models (LLMs) has triggered legal and ethical\nconcerns, especially regarding the unauthorized use of copyrighted materials in\ntheir training datasets. This has led to lawsuits against tech companies\naccused of using protected content without permission. Membership Inference\nAttacks (MIAs) aim to detect whether specific documents were used in a given\nLLM pretraining, but their effectiveness is undermined by biases such as\ntime-shifts and n-gram overlaps.\n  This paper addresses the evaluation of MIAs on LLMs with partially inferable\ntraining sets, under the ex-post hypothesis, which acknowledges inherent\ndistributional biases between members and non-members datasets. We propose and\nvalidate algorithms to create ``non-biased'' and ``non-classifiable'' datasets\nfor fairer MIA assessment. Experiments using the Gutenberg dataset on OpenLamma\nand Pythia show that neutralizing known biases alone is insufficient. Our\nmethods produce non-biased ex-post datasets with AUC-ROC scores comparable to\nthose previously obtained on genuinely random datasets, validating our\napproach. Globally, MIAs yield results close to random, with only one being\neffective on both random and our datasets, but its performance decreases when\nbias is removed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of Large Language Models (LLMs) has triggered legal and ethical\nconcerns, especially regarding the unauthorized use of copyrighted materials in\ntheir training datasets. This has led to lawsuits against tech companies\naccused of using protected content without permission. Membership Inference\nAttacks (MIAs) aim to detect whether specific documents were used in a given\nLLM pretraining, but their effectiveness is undermined by biases such as\ntime-shifts and n-gram overlaps.\n  This paper addresses the evaluation of MIAs on LLMs with partially inferable\ntraining sets, under the ex-post hypothesis, which acknowledges inherent\ndistributional biases between members and non-members datasets. We propose and\nvalidate algorithms to create ``non-biased'' and ``non-classifiable'' datasets\nfor fairer MIA assessment. Experiments using the Gutenberg dataset on OpenLamma\nand Pythia show that neutralizing known biases alone is insufficient. Our\nmethods produce non-biased ex-post datasets with AUC-ROC scores comparable to\nthose previously obtained on genuinely random datasets, validating our\napproach. Globally, MIAs yield results close to random, with only one being\neffective on both random and our datasets, but its performance decreases when\nbias is removed."
                },
                "authors": [
                    {
                        "name": "C√©dric Eichler"
                    },
                    {
                        "name": "Nathan Champeil"
                    },
                    {
                        "name": "Nicolas Anciaux"
                    },
                    {
                        "name": "Alexandra Bensamoun"
                    },
                    {
                        "name": "Heber Hwang Arcolezi"
                    },
                    {
                        "name": "Jos√© Maria De Fuentes"
                    }
                ],
                "author_detail": {
                    "name": "Jos√© Maria De Fuentes"
                },
                "author": "Jos√© Maria De Fuentes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05968v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05968v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.18279v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.18279v2",
                "updated": "2024-08-12T07:14:00Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    14,
                    0,
                    0,
                    225,
                    0
                ],
                "published": "2023-05-29T17:50:33Z",
                "published_parsed": [
                    2023,
                    5,
                    29,
                    17,
                    50,
                    33,
                    0,
                    149,
                    0
                ],
                "title": "Contextual Object Detection with Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextual Object Detection with Multimodal Large Language Models"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) are remarkable in\nvision-language tasks, such as image captioning and question answering, but\nlack the essential perception ability, i.e., object detection. In this work, we\naddress this limitation by introducing a novel research problem of contextual\nobject detection -- understanding visible objects within different human-AI\ninteractive contexts. Three representative scenarios are investigated,\nincluding the language cloze test, visual captioning, and question answering.\nMoreover, we present ContextDET, a unified multimodal model that is capable of\nend-to-end differentiable modeling of visual-language contexts, so as to\nlocate, identify, and associate visual objects with language inputs for\nhuman-AI interaction. Our ContextDET involves three key submodels: (i) a visual\nencoder for extracting visual representations, (ii) a pre-trained LLM for\nmultimodal context decoding, and (iii) a visual decoder for predicting bounding\nboxes given contextual object words. The new generate-then-detect framework\nenables us to detect object words within human vocabulary. Extensive\nexperiments show the advantages of ContextDET on our proposed CODE benchmark,\nopen-vocabulary detection, and referring image segmentation. Github:\nhttps://github.com/yuhangzang/ContextDET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) are remarkable in\nvision-language tasks, such as image captioning and question answering, but\nlack the essential perception ability, i.e., object detection. In this work, we\naddress this limitation by introducing a novel research problem of contextual\nobject detection -- understanding visible objects within different human-AI\ninteractive contexts. Three representative scenarios are investigated,\nincluding the language cloze test, visual captioning, and question answering.\nMoreover, we present ContextDET, a unified multimodal model that is capable of\nend-to-end differentiable modeling of visual-language contexts, so as to\nlocate, identify, and associate visual objects with language inputs for\nhuman-AI interaction. Our ContextDET involves three key submodels: (i) a visual\nencoder for extracting visual representations, (ii) a pre-trained LLM for\nmultimodal context decoding, and (iii) a visual decoder for predicting bounding\nboxes given contextual object words. The new generate-then-detect framework\nenables us to detect object words within human vocabulary. Extensive\nexperiments show the advantages of ContextDET on our proposed CODE benchmark,\nopen-vocabulary detection, and referring image segmentation. Github:\nhttps://github.com/yuhangzang/ContextDET."
                },
                "authors": [
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Jun Han"
                    },
                    {
                        "name": "Kaiyang Zhou"
                    },
                    {
                        "name": "Chen Change Loy"
                    }
                ],
                "author_detail": {
                    "name": "Chen Change Loy"
                },
                "author": "Chen Change Loy",
                "arxiv_comment": "IJCV 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.18279v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.18279v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05952v1",
                "updated": "2024-08-12T07:03:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    3,
                    35,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T07:03:35Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    7,
                    3,
                    35,
                    0,
                    225,
                    0
                ],
                "title": "Optimizing Vision Transformers with Data-Free Knowledge Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Vision Transformers with Data-Free Knowledge Transfer"
                },
                "summary": "The groundbreaking performance of transformers in Natural Language Processing\n(NLP) tasks has led to their replacement of traditional Convolutional Neural\nNetworks (CNNs), owing to the efficiency and accuracy achieved through the\nself-attention mechanism. This success has inspired researchers to explore the\nuse of transformers in computer vision tasks to attain enhanced long-term\nsemantic awareness. Vision transformers (ViTs) have excelled in various\ncomputer vision tasks due to their superior ability to capture long-distance\ndependencies using the self-attention mechanism. Contemporary ViTs like Data\nEfficient Transformers (DeiT) can effectively learn both global semantic\ninformation and local texture information from images, achieving performance\ncomparable to traditional CNNs. However, their impressive performance comes\nwith a high computational cost due to very large number of parameters,\nhindering their deployment on devices with limited resources like smartphones,\ncameras, drones etc. Additionally, ViTs require a large amount of data for\ntraining to achieve performance comparable to benchmark CNN models. Therefore,\nwe identified two key challenges in deploying ViTs on smaller form factor\ndevices: the high computational requirements of large models and the need for\nextensive training data. As a solution to these challenges, we propose\ncompressing large ViT models using Knowledge Distillation (KD), which is\nimplemented data-free to circumvent limitations related to data availability.\nAdditionally, we conducted experiments on object detection within the same\nenvironment in addition to classification tasks. Based on our analysis, we\nfound that datafree knowledge distillation is an effective method to overcome\nboth issues, enabling the deployment of ViTs on less resourceconstrained\ndevices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The groundbreaking performance of transformers in Natural Language Processing\n(NLP) tasks has led to their replacement of traditional Convolutional Neural\nNetworks (CNNs), owing to the efficiency and accuracy achieved through the\nself-attention mechanism. This success has inspired researchers to explore the\nuse of transformers in computer vision tasks to attain enhanced long-term\nsemantic awareness. Vision transformers (ViTs) have excelled in various\ncomputer vision tasks due to their superior ability to capture long-distance\ndependencies using the self-attention mechanism. Contemporary ViTs like Data\nEfficient Transformers (DeiT) can effectively learn both global semantic\ninformation and local texture information from images, achieving performance\ncomparable to traditional CNNs. However, their impressive performance comes\nwith a high computational cost due to very large number of parameters,\nhindering their deployment on devices with limited resources like smartphones,\ncameras, drones etc. Additionally, ViTs require a large amount of data for\ntraining to achieve performance comparable to benchmark CNN models. Therefore,\nwe identified two key challenges in deploying ViTs on smaller form factor\ndevices: the high computational requirements of large models and the need for\nextensive training data. As a solution to these challenges, we propose\ncompressing large ViT models using Knowledge Distillation (KD), which is\nimplemented data-free to circumvent limitations related to data availability.\nAdditionally, we conducted experiments on object detection within the same\nenvironment in addition to classification tasks. Based on our analysis, we\nfound that datafree knowledge distillation is an effective method to overcome\nboth issues, enabling the deployment of ViTs on less resourceconstrained\ndevices."
                },
                "authors": [
                    {
                        "name": "Gousia Habib"
                    },
                    {
                        "name": "Damandeep Singh"
                    },
                    {
                        "name": "Ishfaq Ahmad Malik"
                    },
                    {
                        "name": "Brejesh Lall"
                    }
                ],
                "author_detail": {
                    "name": "Brejesh Lall"
                },
                "author": "Brejesh Lall",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05948v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05948v1",
                "updated": "2024-08-12T06:48:43Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    48,
                    43,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T06:48:43Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    48,
                    43,
                    0,
                    225,
                    0
                ],
                "title": "ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge\n  Graph QA datasets with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConvKGYarn: Spinning Configurable and Scalable Conversational Knowledge\n  Graph QA datasets with Large Language Models"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) and conversational\nassistants necessitates dynamic, scalable, and configurable conversational\ndatasets for training and evaluation. These datasets must accommodate diverse\nuser interaction modes, including text and voice, each presenting unique\nmodeling challenges. Knowledge Graphs (KGs), with their structured and evolving\nnature, offer an ideal foundation for current and precise knowledge. Although\nhuman-curated KG-based conversational datasets exist, they struggle to keep\npace with the rapidly changing user information needs. We present ConvKGYarn, a\nscalable method for generating up-to-date and configurable conversational KGQA\ndatasets. Qualitative psychometric analyses confirm our method can generate\nhigh-quality datasets rivaling a popular conversational KGQA dataset while\noffering it at scale and covering a wide range of human-interaction\nconfigurations. We showcase its utility by testing LLMs on diverse\nconversations - exploring model behavior on conversational KGQA sets with\ndifferent configurations grounded in the same KG fact set. Our results\nhighlight the ability of ConvKGYarn to improve KGQA foundations and evaluate\nparametric knowledge of LLMs, thus offering a robust solution to the constantly\nevolving landscape of conversational assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) and conversational\nassistants necessitates dynamic, scalable, and configurable conversational\ndatasets for training and evaluation. These datasets must accommodate diverse\nuser interaction modes, including text and voice, each presenting unique\nmodeling challenges. Knowledge Graphs (KGs), with their structured and evolving\nnature, offer an ideal foundation for current and precise knowledge. Although\nhuman-curated KG-based conversational datasets exist, they struggle to keep\npace with the rapidly changing user information needs. We present ConvKGYarn, a\nscalable method for generating up-to-date and configurable conversational KGQA\ndatasets. Qualitative psychometric analyses confirm our method can generate\nhigh-quality datasets rivaling a popular conversational KGQA dataset while\noffering it at scale and covering a wide range of human-interaction\nconfigurations. We showcase its utility by testing LLMs on diverse\nconversations - exploring model behavior on conversational KGQA sets with\ndifferent configurations grounded in the same KG fact set. Our results\nhighlight the ability of ConvKGYarn to improve KGQA foundations and evaluate\nparametric knowledge of LLMs, thus offering a robust solution to the constantly\nevolving landscape of conversational assistants."
                },
                "authors": [
                    {
                        "name": "Ronak Pradeep"
                    },
                    {
                        "name": "Daniel Lee"
                    },
                    {
                        "name": "Ali Mousavi"
                    },
                    {
                        "name": "Jeff Pound"
                    },
                    {
                        "name": "Yisi Sang"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Ihab Ilyas"
                    },
                    {
                        "name": "Saloni Potdar"
                    },
                    {
                        "name": "Mostafa Arefiyan"
                    },
                    {
                        "name": "Yunyao Li"
                    }
                ],
                "author_detail": {
                    "name": "Yunyao Li"
                },
                "author": "Yunyao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05948v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05948v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20613v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20613v2",
                "updated": "2024-08-12T06:36:10Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    36,
                    10,
                    0,
                    225,
                    0
                ],
                "published": "2024-05-31T04:05:09Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    4,
                    5,
                    9,
                    4,
                    152,
                    0
                ],
                "title": "FineRadScore: A Radiology Report Line-by-Line Evaluation Technique\n  Generating Corrections with Severity Scores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineRadScore: A Radiology Report Line-by-Line Evaluation Technique\n  Generating Corrections with Severity Scores"
                },
                "summary": "The current gold standard for evaluating generated chest x-ray (CXR) reports\nis through radiologist annotations. However, this process can be extremely\ntime-consuming and costly, especially when evaluating large numbers of reports.\nIn this work, we present FineRadScore, a Large Language Model (LLM)-based\nautomated evaluation metric for generated CXR reports. Given a candidate report\nand a ground-truth report, FineRadScore gives the minimum number of\nline-by-line corrections required to go from the candidate to the ground-truth\nreport. Additionally, FineRadScore provides an error severity rating with each\ncorrection and generates comments explaining why the correction was needed. We\ndemonstrate that FineRadScore's corrections and error severity scores align\nwith radiologist opinions. We also show that, when used to judge the quality of\nthe report as a whole, FineRadScore aligns with radiologists as well as current\nstate-of-the-art automated CXR evaluation metrics. Finally, we analyze\nFineRadScore's shortcomings to provide suggestions for future improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current gold standard for evaluating generated chest x-ray (CXR) reports\nis through radiologist annotations. However, this process can be extremely\ntime-consuming and costly, especially when evaluating large numbers of reports.\nIn this work, we present FineRadScore, a Large Language Model (LLM)-based\nautomated evaluation metric for generated CXR reports. Given a candidate report\nand a ground-truth report, FineRadScore gives the minimum number of\nline-by-line corrections required to go from the candidate to the ground-truth\nreport. Additionally, FineRadScore provides an error severity rating with each\ncorrection and generates comments explaining why the correction was needed. We\ndemonstrate that FineRadScore's corrections and error severity scores align\nwith radiologist opinions. We also show that, when used to judge the quality of\nthe report as a whole, FineRadScore aligns with radiologists as well as current\nstate-of-the-art automated CXR evaluation metrics. Finally, we analyze\nFineRadScore's shortcomings to provide suggestions for future improvements."
                },
                "authors": [
                    {
                        "name": "Alyssa Huang"
                    },
                    {
                        "name": "Oishi Banerjee"
                    },
                    {
                        "name": "Kay Wu"
                    },
                    {
                        "name": "Eduardo Pontes Reis"
                    },
                    {
                        "name": "Pranav Rajpurkar"
                    }
                ],
                "author_detail": {
                    "name": "Pranav Rajpurkar"
                },
                "author": "Pranav Rajpurkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20613v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20613v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05941v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05941v1",
                "updated": "2024-08-12T06:36:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    36,
                    8,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T06:36:08Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    36,
                    8,
                    0,
                    225,
                    0
                ],
                "title": "Multimodal Large Language Models for Phishing Webpage Detection and\n  Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models for Phishing Webpage Detection and\n  Identification"
                },
                "summary": "To address the challenging problem of detecting phishing webpages,\nresearchers have developed numerous solutions, in particular those based on\nmachine learning (ML) algorithms. Among these, brand-based phishing detection\nthat uses models from Computer Vision to detect if a given webpage is imitating\na well-known brand has received widespread attention. However, such models are\ncostly and difficult to maintain, as they need to be retrained with labeled\ndataset that has to be regularly and continuously collected. Besides, they also\nneed to maintain a good reference list of well-known websites and related\nmeta-data for effective performance.\n  In this work, we take steps to study the efficacy of large language models\n(LLMs), in particular the multimodal LLMs, in detecting phishing webpages.\nGiven that the LLMs are pretrained on a large corpus of data, we aim to make\nuse of their understanding of different aspects of a webpage (logo, theme,\nfavicon, etc.) to identify the brand of a given webpage and compare the\nidentified brand with the domain name in the URL to detect a phishing attack.\nWe propose a two-phase system employing LLMs in both phases: the first phase\nfocuses on brand identification, while the second verifies the domain. We carry\nout comprehensive evaluations on a newly collected dataset. Our experiments\nshow that the LLM-based system achieves a high detection rate at high\nprecision; importantly, it also provides interpretable evidence for the\ndecisions. Our system also performs significantly better than a\nstate-of-the-art brand-based phishing detection system while demonstrating\nrobustness against two known adversarial attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To address the challenging problem of detecting phishing webpages,\nresearchers have developed numerous solutions, in particular those based on\nmachine learning (ML) algorithms. Among these, brand-based phishing detection\nthat uses models from Computer Vision to detect if a given webpage is imitating\na well-known brand has received widespread attention. However, such models are\ncostly and difficult to maintain, as they need to be retrained with labeled\ndataset that has to be regularly and continuously collected. Besides, they also\nneed to maintain a good reference list of well-known websites and related\nmeta-data for effective performance.\n  In this work, we take steps to study the efficacy of large language models\n(LLMs), in particular the multimodal LLMs, in detecting phishing webpages.\nGiven that the LLMs are pretrained on a large corpus of data, we aim to make\nuse of their understanding of different aspects of a webpage (logo, theme,\nfavicon, etc.) to identify the brand of a given webpage and compare the\nidentified brand with the domain name in the URL to detect a phishing attack.\nWe propose a two-phase system employing LLMs in both phases: the first phase\nfocuses on brand identification, while the second verifies the domain. We carry\nout comprehensive evaluations on a newly collected dataset. Our experiments\nshow that the LLM-based system achieves a high detection rate at high\nprecision; importantly, it also provides interpretable evidence for the\ndecisions. Our system also performs significantly better than a\nstate-of-the-art brand-based phishing detection system while demonstrating\nrobustness against two known adversarial attacks."
                },
                "authors": [
                    {
                        "name": "Jehyun Lee"
                    },
                    {
                        "name": "Peiyuan Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    },
                    {
                        "name": "Dinil Mon Divakaran"
                    }
                ],
                "author_detail": {
                    "name": "Dinil Mon Divakaran"
                },
                "author": "Dinil Mon Divakaran",
                "arxiv_comment": "To appear in eCrime 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05941v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05941v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.06363v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.06363v3",
                "updated": "2024-08-12T06:17:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    17,
                    21,
                    0,
                    225,
                    0
                ],
                "published": "2023-12-11T13:11:04Z",
                "published_parsed": [
                    2023,
                    12,
                    11,
                    13,
                    11,
                    4,
                    0,
                    345,
                    0
                ],
                "title": "MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples"
                },
                "summary": "Although In-Context Learning (ICL) brings remarkable performance gains to\nLarge Language Models (LLMs), the improvements remain lower than fine-tuning on\ndownstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT),\na novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning by\nfully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). We\npropose the Multi-Modal Hub (M-Hub), a unified module that captures various\nmulti-modal features according to different inputs and objectives. Based on\nM-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textual\nfeatures and subsequently generate outputs conditioned on the textual-guided\nvisual features. Moreover, leveraging the flexibility of M-Hub, we design a\nvariety of in-context demonstrations. Extensive experiments on a diverse range\nof downstream multi-modal tasks demonstrate that MMICT significantly\noutperforms traditional fine-tuning strategy and the vanilla ICT method that\ndirectly takes the concatenation of all information from different modalities\nas input. Our implementation is available at:\nhttps://github.com/KDEGroup/MMICT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although In-Context Learning (ICL) brings remarkable performance gains to\nLarge Language Models (LLMs), the improvements remain lower than fine-tuning on\ndownstream tasks. This paper introduces Multi-Modal In-Context Tuning (MMICT),\na novel multi-modal fine-tuning paradigm that boosts multi-modal fine-tuning by\nfully leveraging the promising ICL capability of multi-modal LLMs (MM-LLMs). We\npropose the Multi-Modal Hub (M-Hub), a unified module that captures various\nmulti-modal features according to different inputs and objectives. Based on\nM-Hub, MMICT enables MM-LLMs to learn from in-context visual-guided textual\nfeatures and subsequently generate outputs conditioned on the textual-guided\nvisual features. Moreover, leveraging the flexibility of M-Hub, we design a\nvariety of in-context demonstrations. Extensive experiments on a diverse range\nof downstream multi-modal tasks demonstrate that MMICT significantly\noutperforms traditional fine-tuning strategy and the vanilla ICT method that\ndirectly takes the concatenation of all information from different modalities\nas input. Our implementation is available at:\nhttps://github.com/KDEGroup/MMICT."
                },
                "authors": [
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Enwei Zhang"
                    },
                    {
                        "name": "Yuting Gao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "arxiv_comment": "TOMM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.06363v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.06363v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05933v1",
                "updated": "2024-08-12T06:16:37Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    16,
                    37,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T06:16:37Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    16,
                    37,
                    0,
                    225,
                    0
                ],
                "title": "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case\n  Study with Locally Deployed Ollama Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing RAG Techniques for Automotive Industry PDF Chatbots: A Case\n  Study with Locally Deployed Ollama Models"
                },
                "summary": "With the growing demand for offline PDF chatbots in automotive industrial\nproduction environments, optimizing the deployment of large language models\n(LLMs) in local, low-performance settings has become increasingly important.\nThis study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques\nfor processing complex automotive industry documents using locally deployed\nOllama models. Based on the Langchain framework, we propose a multi-dimensional\noptimization approach for Ollama's local RAG implementation. Our method\naddresses key challenges in automotive document processing, including\nmulti-column layouts and technical specifications. We introduce improvements in\nPDF processing, retrieval mechanisms, and context compression, tailored to the\nunique characteristics of automotive industry documents. Additionally, we\ndesign custom classes supporting embedding pipelines and an agent supporting\nself-RAG based on LangGraph best practices. To evaluate our approach, we\nconstructed a proprietary dataset comprising typical automotive industry\ndocuments, including technical reports and corporate regulations. We compared\nour optimized RAG model and self-RAG agent against a naive RAG baseline across\nthree datasets: our automotive industry dataset, QReCC, and CoQA. Results\ndemonstrate significant improvements in context precision, context recall,\nanswer relevancy, and faithfulness, with particularly notable performance on\nthe automotive industry dataset. Our optimization scheme provides an effective\nsolution for deploying local RAG systems in the automotive sector, addressing\nthe specific needs of PDF chatbots in industrial production environments. This\nresearch has important implications for advancing information processing and\nintelligent production in the automotive industry.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing demand for offline PDF chatbots in automotive industrial\nproduction environments, optimizing the deployment of large language models\n(LLMs) in local, low-performance settings has become increasingly important.\nThis study focuses on enhancing Retrieval-Augmented Generation (RAG) techniques\nfor processing complex automotive industry documents using locally deployed\nOllama models. Based on the Langchain framework, we propose a multi-dimensional\noptimization approach for Ollama's local RAG implementation. Our method\naddresses key challenges in automotive document processing, including\nmulti-column layouts and technical specifications. We introduce improvements in\nPDF processing, retrieval mechanisms, and context compression, tailored to the\nunique characteristics of automotive industry documents. Additionally, we\ndesign custom classes supporting embedding pipelines and an agent supporting\nself-RAG based on LangGraph best practices. To evaluate our approach, we\nconstructed a proprietary dataset comprising typical automotive industry\ndocuments, including technical reports and corporate regulations. We compared\nour optimized RAG model and self-RAG agent against a naive RAG baseline across\nthree datasets: our automotive industry dataset, QReCC, and CoQA. Results\ndemonstrate significant improvements in context precision, context recall,\nanswer relevancy, and faithfulness, with particularly notable performance on\nthe automotive industry dataset. Our optimization scheme provides an effective\nsolution for deploying local RAG systems in the automotive sector, addressing\nthe specific needs of PDF chatbots in industrial production environments. This\nresearch has important implications for advancing information processing and\nintelligent production in the automotive industry."
                },
                "authors": [
                    {
                        "name": "Fei Liu"
                    },
                    {
                        "name": "Zejun Kang"
                    },
                    {
                        "name": "Xing Han"
                    }
                ],
                "author_detail": {
                    "name": "Xing Han"
                },
                "author": "Xing Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02266v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02266v2",
                "updated": "2024-08-12T06:08:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    6,
                    8,
                    35,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-05T06:47:32Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    6,
                    47,
                    32,
                    0,
                    218,
                    0
                ],
                "title": "One-Shot Collaborative Data Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Shot Collaborative Data Distillation"
                },
                "summary": "Large machine-learning training datasets can be distilled into small\ncollections of informative synthetic data samples. These synthetic sets support\nefficient model learning and reduce the communication cost of data sharing.\nThus, high-fidelity distilled data can support the efficient deployment of\nmachine learning applications in distributed network environments. A naive way\nto construct a synthetic set in a distributed environment is to allow each\nclient to perform local data distillation and to merge local distillations at a\ncentral server. However, the quality of the resulting set is impaired by\nheterogeneity in the distributions of the local data held by clients. To\novercome this challenge, we introduce the first collaborative data distillation\ntechnique, called CollabDM, which captures the global distribution of the data\nand requires only a single round of communication between client and server.\nOur method outperforms the state-of-the-art one-shot learning method on skewed\ndata in distributed learning environments. We also show the promising practical\nbenefits of our method when applied to attack detection in 5G networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large machine-learning training datasets can be distilled into small\ncollections of informative synthetic data samples. These synthetic sets support\nefficient model learning and reduce the communication cost of data sharing.\nThus, high-fidelity distilled data can support the efficient deployment of\nmachine learning applications in distributed network environments. A naive way\nto construct a synthetic set in a distributed environment is to allow each\nclient to perform local data distillation and to merge local distillations at a\ncentral server. However, the quality of the resulting set is impaired by\nheterogeneity in the distributions of the local data held by clients. To\novercome this challenge, we introduce the first collaborative data distillation\ntechnique, called CollabDM, which captures the global distribution of the data\nand requires only a single round of communication between client and server.\nOur method outperforms the state-of-the-art one-shot learning method on skewed\ndata in distributed learning environments. We also show the promising practical\nbenefits of our method when applied to attack detection in 5G networks."
                },
                "authors": [
                    {
                        "name": "William Holland"
                    },
                    {
                        "name": "Chandra Thapa"
                    },
                    {
                        "name": "Sarah Ali Siddiqui"
                    },
                    {
                        "name": "Wei Shao"
                    },
                    {
                        "name": "Seyit Camtepe"
                    }
                ],
                "author_detail": {
                    "name": "Seyit Camtepe"
                },
                "author": "Seyit Camtepe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02266v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02266v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12968v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12968v2",
                "updated": "2024-08-12T04:48:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    4,
                    48,
                    11,
                    0,
                    225,
                    0
                ],
                "published": "2024-03-19T17:59:56Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    59,
                    56,
                    1,
                    79,
                    0
                ],
                "title": "LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic\n  Prompt Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic\n  Prompt Compression"
                },
                "summary": "This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x. Our code is available at\nhttps://aka.ms/LLMLingua-2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper focuses on task-agnostic prompt compression for better\ngeneralizability and efficiency. Considering the redundancy in natural\nlanguage, existing approaches compress prompts by removing tokens or lexical\nunits according to their information entropy obtained from a causal language\nmodel such as LLaMa-7B. The challenge is that information entropy may be a\nsuboptimal compression metric: (i) it only leverages unidirectional context and\nmay fail to capture all essential information needed for prompt compression;\n(ii) it is not aligned with the prompt compression objective.\n  To address these issues, we propose a data distillation procedure to derive\nknowledge from an LLM to compress prompts without losing crucial information,\nand meantime, introduce an extractive text compression dataset. We formulate\nprompt compression as a token classification problem to guarantee the\nfaithfulness of the compressed prompt to the original one, and use a\nTransformer encoder as the base architecture to capture all essential\ninformation for prompt compression from the full bidirectional context. Our\napproach leads to lower latency by explicitly learning the compression\nobjective with smaller models such as XLM-RoBERTa-large and mBERT.\n  We evaluate our method on both in-domain and out-of-domain datasets,\nincluding MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its\nsmall size, our model shows significant performance gains over strong baselines\nand demonstrates robust generalization ability across different LLMs.\nAdditionally, our model is 3x-6x faster than existing prompt compression\nmethods, while accelerating the end-to-end latency by 1.6x-2.9x with\ncompression ratios of 2x-5x. Our code is available at\nhttps://aka.ms/LLMLingua-2."
                },
                "authors": [
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Menglin Xia"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Jue Zhang"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Victor R√ºhle"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "H. Vicky Zhao"
                    },
                    {
                        "name": "Lili Qiu"
                    },
                    {
                        "name": "Dongmei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Dongmei Zhang"
                },
                "author": "Dongmei Zhang",
                "arxiv_comment": "Accepted at Findings of ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12968v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12968v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.06839v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.06839v2",
                "updated": "2024-08-12T03:53:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    53,
                    35,
                    0,
                    225,
                    0
                ],
                "published": "2023-10-10T17:59:58Z",
                "published_parsed": [
                    2023,
                    10,
                    10,
                    17,
                    59,
                    58,
                    1,
                    283,
                    0
                ],
                "title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios\n  via Prompt Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios\n  via Prompt Compression"
                },
                "summary": "In long context scenarios, large language models (LLMs) face three main\nchallenges: higher computational cost, performance reduction, and position\nbias. Research indicates that LLM performance hinges on the density and\nposition of key information in the input prompt. Inspired by these findings, we\npropose LongLLMLingua for prompt compression towards improving LLMs' perception\nof the key information to simultaneously address the three challenges. Our\nextensive evaluation across various long context scenarios demonstrates that\nLongLLMLingua not only enhances performance but also significantly reduces\ncosts and latency. For instance, in the NaturalQuestions benchmark,\nLongLLMLingua boosts performance by up to 21.4% with around 4x fewer tokens in\nGPT-3.5-Turbo, leading to substantial cost savings. It achieves a 94.0% cost\nreduction in the LooGLE benchmark. Moreover, when compressing prompts of about\n10k tokens at ratios of 2x-6x, LongLLMLingua can accelerate end-to-end latency\nby 1.4x-2.6x. Our code is available at https://aka.ms/LongLLMLingua.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In long context scenarios, large language models (LLMs) face three main\nchallenges: higher computational cost, performance reduction, and position\nbias. Research indicates that LLM performance hinges on the density and\nposition of key information in the input prompt. Inspired by these findings, we\npropose LongLLMLingua for prompt compression towards improving LLMs' perception\nof the key information to simultaneously address the three challenges. Our\nextensive evaluation across various long context scenarios demonstrates that\nLongLLMLingua not only enhances performance but also significantly reduces\ncosts and latency. For instance, in the NaturalQuestions benchmark,\nLongLLMLingua boosts performance by up to 21.4% with around 4x fewer tokens in\nGPT-3.5-Turbo, leading to substantial cost savings. It achieves a 94.0% cost\nreduction in the LooGLE benchmark. Moreover, when compressing prompts of about\n10k tokens at ratios of 2x-6x, LongLLMLingua can accelerate end-to-end latency\nby 1.4x-2.6x. Our code is available at https://aka.ms/LongLLMLingua."
                },
                "authors": [
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Chin-Yew Lin"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "Accepted at ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.06839v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.06839v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05911v1",
                "updated": "2024-08-12T03:52:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    52,
                    11,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T03:52:11Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    52,
                    11,
                    0,
                    225,
                    0
                ],
                "title": "A New Pipeline For Generating Instruction Dataset via RAG and Self\n  Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Pipeline For Generating Instruction Dataset via RAG and Self\n  Fine-Tuning"
                },
                "summary": "With the rapid development of large language models in recent years, there\nhas been an increasing demand for domain-specific Agents that can cater to the\nunique needs of enterprises and organizations. Unlike general models, which\nstrive for broad coverage, these specialized Agents rely on focused datasets\ntailored to their intended applications. This research proposes a pipeline that\nleverages the power of LLMs and the Retrieval-Augmented Generation related\nframework to construct high-quality instruction datasets for fine-tuning on\nspecific domains using custom document collections. By ingesting\ndomain-specific documents, the pipeline generates relevant and contextually\nappropriate instructions, thus effectively creating a comprehensive dataset for\nfine-tuning LLMs on the target domain. This approach overcomes the limitations\nof traditional dataset creation methods, which often rely on manual curation or\nweb-scraping techniques that may introduce noise and irrelevant data. Notably,\nour pipeline offers a dynamic solution that can quickly adapt to updates or\nmodifications in the domain-specific document collection, eliminating the need\nfor complete retraining. Additionally, it addresses the challenge of data\nscarcity by enabling the generation of instruction datasets from a limited set\nof initial documents, rendering it suitable for unpopular or specialized\ndomains where comprehensive datasets are scarce. As a case study, we apply this\napproach to the domain of psychiatry, a field requiring specialized knowledge\nand sensitive handling of patient information. The resulting fine-tuned LLM\ndemonstrates showcases the viability of the proposed approach and underscores\nits potential for widespread adoption across various industries and domains\nwhere tailored, accurate, and contextually relevant language models are\nindispensable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models in recent years, there\nhas been an increasing demand for domain-specific Agents that can cater to the\nunique needs of enterprises and organizations. Unlike general models, which\nstrive for broad coverage, these specialized Agents rely on focused datasets\ntailored to their intended applications. This research proposes a pipeline that\nleverages the power of LLMs and the Retrieval-Augmented Generation related\nframework to construct high-quality instruction datasets for fine-tuning on\nspecific domains using custom document collections. By ingesting\ndomain-specific documents, the pipeline generates relevant and contextually\nappropriate instructions, thus effectively creating a comprehensive dataset for\nfine-tuning LLMs on the target domain. This approach overcomes the limitations\nof traditional dataset creation methods, which often rely on manual curation or\nweb-scraping techniques that may introduce noise and irrelevant data. Notably,\nour pipeline offers a dynamic solution that can quickly adapt to updates or\nmodifications in the domain-specific document collection, eliminating the need\nfor complete retraining. Additionally, it addresses the challenge of data\nscarcity by enabling the generation of instruction datasets from a limited set\nof initial documents, rendering it suitable for unpopular or specialized\ndomains where comprehensive datasets are scarce. As a case study, we apply this\napproach to the domain of psychiatry, a field requiring specialized knowledge\nand sensitive handling of patient information. The resulting fine-tuned LLM\ndemonstrates showcases the viability of the proposed approach and underscores\nits potential for widespread adoption across various industries and domains\nwhere tailored, accurate, and contextually relevant language models are\nindispensable."
                },
                "authors": [
                    {
                        "name": "Chih-Wei Song"
                    },
                    {
                        "name": "Yu-Kai Lee"
                    },
                    {
                        "name": "Yin-Te Tsai"
                    }
                ],
                "author_detail": {
                    "name": "Yin-Te Tsai"
                },
                "author": "Yin-Te Tsai",
                "arxiv_comment": "5 pages, SCA 2024: The 7th IEEE International Workshop on Smart\n  Computing & Applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.05803v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.05803v2",
                "updated": "2024-08-12T03:29:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    3,
                    29,
                    51,
                    0,
                    225,
                    0
                ],
                "published": "2024-05-09T14:38:53Z",
                "published_parsed": [
                    2024,
                    5,
                    9,
                    14,
                    38,
                    53,
                    3,
                    130,
                    0
                ],
                "title": "Boosting Multimodal Large Language Models with Visual Tokens Withdrawal\n  for Rapid Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Multimodal Large Language Models with Visual Tokens Withdrawal\n  for Rapid Inference"
                },
                "summary": "Multimodal large language models (MLLMs) demand considerable computations for\ninference due to the extensive parameters and the additional input tokens\nneeded for visual information representation. Herein, we introduce Visual\nTokens Withdrawal (VTW), a plug-and-play module to boost MLLMs for rapid\ninference. Our approach is inspired by two intriguing phenomena we have\nobserved: (1) the attention sink phenomenon that is prevalent in LLMs also\npersists in MLLMs, suggesting that initial tokens and nearest tokens receive\nthe majority of attention, while middle vision tokens garner minimal attention\nin deep layers; (2) the presence of information migration, which implies that\nvisual information is transferred to subsequent text tokens within the first\nfew layers of MLLMs. As per our findings, we conclude that vision tokens are\nunnecessary in the deep layers of MLLMs. Thus, we strategically withdraw them\nat a certain layer, enabling only text tokens to engage in subsequent layers.\nTo pinpoint the ideal layer for VTW, we initially analyze a limited set of tiny\ndatasets and choose the first layer that meets the Kullback-Leibler divergence\ncriterion. Our VTW approach can cut computational overhead by over 40\\% across\ndiverse multimodal tasks while maintaining performance. Our code is released at\n\\url{https://github.com/lzhxmu/VTW}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) demand considerable computations for\ninference due to the extensive parameters and the additional input tokens\nneeded for visual information representation. Herein, we introduce Visual\nTokens Withdrawal (VTW), a plug-and-play module to boost MLLMs for rapid\ninference. Our approach is inspired by two intriguing phenomena we have\nobserved: (1) the attention sink phenomenon that is prevalent in LLMs also\npersists in MLLMs, suggesting that initial tokens and nearest tokens receive\nthe majority of attention, while middle vision tokens garner minimal attention\nin deep layers; (2) the presence of information migration, which implies that\nvisual information is transferred to subsequent text tokens within the first\nfew layers of MLLMs. As per our findings, we conclude that vision tokens are\nunnecessary in the deep layers of MLLMs. Thus, we strategically withdraw them\nat a certain layer, enabling only text tokens to engage in subsequent layers.\nTo pinpoint the ideal layer for VTW, we initially analyze a limited set of tiny\ndatasets and choose the first layer that meets the Kullback-Leibler divergence\ncriterion. Our VTW approach can cut computational overhead by over 40\\% across\ndiverse multimodal tasks while maintaining performance. Our code is released at\n\\url{https://github.com/lzhxmu/VTW}."
                },
                "authors": [
                    {
                        "name": "Zhihang Lin"
                    },
                    {
                        "name": "Mingbao Lin"
                    },
                    {
                        "name": "Luxi Lin"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.05803v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.05803v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07089v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07089v3",
                "updated": "2024-08-12T02:44:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    2,
                    44,
                    33,
                    0,
                    225,
                    0
                ],
                "published": "2024-05-11T20:20:58Z",
                "published_parsed": [
                    2024,
                    5,
                    11,
                    20,
                    20,
                    58,
                    5,
                    132,
                    0
                ],
                "title": "SonifyAR: Context-Aware Sound Generation in Augmented Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SonifyAR: Context-Aware Sound Generation in Augmented Reality"
                },
                "summary": "Sound plays a crucial role in enhancing user experience and immersiveness in\nAugmented Reality (AR). However, current platforms lack support for AR sound\nauthoring due to limited interaction types, challenges in collecting and\nspecifying context information, and difficulty in acquiring matching sound\nassets. We present SonifyAR, an LLM-based AR sound authoring system that\ngenerates context-aware sound effects for AR experiences. SonifyAR expands the\ncurrent design space of AR sound and implements a Programming by Demonstration\n(PbD) pipeline to automatically collect contextual information of AR events,\nincluding virtual content semantics and real world context. This context\ninformation is then processed by a large language model to acquire sound\neffects with Recommendation, Retrieval, Generation, and Transfer methods. To\nevaluate the usability and performance of our system, we conducted a user study\nwith eight participants and created five example applications, including an\nAR-based science experiment, an improving case for AR headset safety, and an\nassisting example for low vision AR users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sound plays a crucial role in enhancing user experience and immersiveness in\nAugmented Reality (AR). However, current platforms lack support for AR sound\nauthoring due to limited interaction types, challenges in collecting and\nspecifying context information, and difficulty in acquiring matching sound\nassets. We present SonifyAR, an LLM-based AR sound authoring system that\ngenerates context-aware sound effects for AR experiences. SonifyAR expands the\ncurrent design space of AR sound and implements a Programming by Demonstration\n(PbD) pipeline to automatically collect contextual information of AR events,\nincluding virtual content semantics and real world context. This context\ninformation is then processed by a large language model to acquire sound\neffects with Recommendation, Retrieval, Generation, and Transfer methods. To\nevaluate the usability and performance of our system, we conducted a user study\nwith eight participants and created five example applications, including an\nAR-based science experiment, an improving case for AR headset safety, and an\nassisting example for low vision AR users."
                },
                "authors": [
                    {
                        "name": "Xia Su"
                    },
                    {
                        "name": "Jon E. Froehlich"
                    },
                    {
                        "name": "Eunyee Koh"
                    },
                    {
                        "name": "Chang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xiao"
                },
                "author": "Chang Xiao",
                "arxiv_comment": "To appear in UIST2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07089v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07089v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18312v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18312v3",
                "updated": "2024-08-12T02:43:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    2,
                    43,
                    32,
                    0,
                    225,
                    0
                ],
                "published": "2024-06-26T12:51:37Z",
                "published_parsed": [
                    2024,
                    6,
                    26,
                    12,
                    51,
                    37,
                    2,
                    178,
                    0
                ],
                "title": "AI-native Memory: A Pathway from LLMs Towards AGI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-native Memory: A Pathway from LLMs Towards AGI"
                },
                "summary": "Large language models (LLMs) have demonstrated the world with the sparks of\nartificial general intelligence (AGI). One opinion, especially from some\nstartups working on LLMs, argues that an LLM with nearly unlimited context\nlength can realize AGI. However, they might be too optimistic about the\nlong-context capability of (existing) LLMs -- (1) Recent literature has shown\nthat their effective context length is significantly smaller than their claimed\ncontext length; and (2) Our reasoning-in-a-haystack experiments further\ndemonstrate that simultaneously finding the relevant information from a long\ncontext and conducting (simple) reasoning is nearly impossible. In this paper,\nwe envision a pathway from LLMs to AGI through the integration of\n\\emph{memory}. We believe that AGI should be a system where LLMs serve as core\nprocessors. In addition to raw data, the memory in this system would store a\nlarge number of important conclusions derived from reasoning processes.\nCompared with retrieval-augmented generation (RAG) that merely processing raw\ndata, this approach not only connects semantically related information closer,\nbut also simplifies complex inferences at the time of querying. As an\nintermediate stage, the memory will likely be in the form of natural language\ndescriptions, which can be directly consumed by users too. Ultimately, every\nagent/person should have its own large personal model, a deep neural network\nmodel (thus \\emph{AI-native}) that parameterizes and compresses all types of\nmemory, even the ones cannot be described by natural languages. Finally, we\ndiscuss the significant potential of AI-native memory as the transformative\ninfrastructure for (proactive) engagement, personalization, distribution, and\nsocial in the AGI era, as well as the incurred privacy and security challenges\nwith preliminary solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated the world with the sparks of\nartificial general intelligence (AGI). One opinion, especially from some\nstartups working on LLMs, argues that an LLM with nearly unlimited context\nlength can realize AGI. However, they might be too optimistic about the\nlong-context capability of (existing) LLMs -- (1) Recent literature has shown\nthat their effective context length is significantly smaller than their claimed\ncontext length; and (2) Our reasoning-in-a-haystack experiments further\ndemonstrate that simultaneously finding the relevant information from a long\ncontext and conducting (simple) reasoning is nearly impossible. In this paper,\nwe envision a pathway from LLMs to AGI through the integration of\n\\emph{memory}. We believe that AGI should be a system where LLMs serve as core\nprocessors. In addition to raw data, the memory in this system would store a\nlarge number of important conclusions derived from reasoning processes.\nCompared with retrieval-augmented generation (RAG) that merely processing raw\ndata, this approach not only connects semantically related information closer,\nbut also simplifies complex inferences at the time of querying. As an\nintermediate stage, the memory will likely be in the form of natural language\ndescriptions, which can be directly consumed by users too. Ultimately, every\nagent/person should have its own large personal model, a deep neural network\nmodel (thus \\emph{AI-native}) that parameterizes and compresses all types of\nmemory, even the ones cannot be described by natural languages. Finally, we\ndiscuss the significant potential of AI-native memory as the transformative\ninfrastructure for (proactive) engagement, personalization, distribution, and\nsocial in the AGI era, as well as the incurred privacy and security challenges\nwith preliminary solutions."
                },
                "authors": [
                    {
                        "name": "Jingbo Shang"
                    },
                    {
                        "name": "Zai Zheng"
                    },
                    {
                        "name": "Jiale Wei"
                    },
                    {
                        "name": "Xiang Ying"
                    },
                    {
                        "name": "Felix Tao"
                    },
                    {
                        "name": "Mindverse Team"
                    }
                ],
                "author_detail": {
                    "name": "Mindverse Team"
                },
                "author": "Mindverse Team",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18312v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18312v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.04325v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.04325v5",
                "updated": "2024-08-12T02:38:11Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    2,
                    38,
                    11,
                    0,
                    225,
                    0
                ],
                "published": "2023-06-07T10:45:02Z",
                "published_parsed": [
                    2023,
                    6,
                    7,
                    10,
                    45,
                    2,
                    2,
                    158,
                    0
                ],
                "title": "Last Week with ChatGPT: A Weibo Study on Social Perspective Regarding\n  ChatGPT for Education and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Last Week with ChatGPT: A Weibo Study on Social Perspective Regarding\n  ChatGPT for Education and Beyond"
                },
                "summary": "The application of AI-powered tools has piqued the interest of many fields,\nparticularly in the academic community. This study uses ChatGPT, currently the\nmost powerful and popular AI tool, as a representative example to analyze how\nthe Chinese public perceives the potential of large language models (LLMs) for\neducational and general purposes. Although facing accessibility challenges, we\nfound that the number of discussions on ChatGPT per month is 16 times that of\nErnie Bot developed by Baidu, the most popular alternative product to ChatGPT\nin the mainland, making ChatGPT a more suitable subject for our analysis. The\nstudy also serves as the first effort to investigate the changes in public\nopinion as AI technologies become more advanced and intelligent. The analysis\nreveals that, upon first encounters with advanced AI that was not yet highly\ncapable, some social media users believed that AI advancements would benefit\neducation and society, while others feared that advanced AI, like ChatGPT,\nwould make humans feel inferior and lead to problems such as cheating and a\ndecline in moral principles. The majority of users remained neutral.\nInterestingly, with the rapid development and improvement of AI capabilities,\npublic attitudes have tended to shift in a positive direction. We present a\nthorough analysis of the trending shift and a roadmap to ensure the ethical\napplication of ChatGPT-like models in education and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of AI-powered tools has piqued the interest of many fields,\nparticularly in the academic community. This study uses ChatGPT, currently the\nmost powerful and popular AI tool, as a representative example to analyze how\nthe Chinese public perceives the potential of large language models (LLMs) for\neducational and general purposes. Although facing accessibility challenges, we\nfound that the number of discussions on ChatGPT per month is 16 times that of\nErnie Bot developed by Baidu, the most popular alternative product to ChatGPT\nin the mainland, making ChatGPT a more suitable subject for our analysis. The\nstudy also serves as the first effort to investigate the changes in public\nopinion as AI technologies become more advanced and intelligent. The analysis\nreveals that, upon first encounters with advanced AI that was not yet highly\ncapable, some social media users believed that AI advancements would benefit\neducation and society, while others feared that advanced AI, like ChatGPT,\nwould make humans feel inferior and lead to problems such as cheating and a\ndecline in moral principles. The majority of users remained neutral.\nInterestingly, with the rapid development and improvement of AI capabilities,\npublic attitudes have tended to shift in a positive direction. We present a\nthorough analysis of the trending shift and a roadmap to ensure the ethical\napplication of ChatGPT-like models in education and beyond."
                },
                "authors": [
                    {
                        "name": "Yao Tian"
                    },
                    {
                        "name": "Chengwei Tong"
                    },
                    {
                        "name": "Lik-Hang Lee"
                    },
                    {
                        "name": "Reza Hadi Mogavi"
                    },
                    {
                        "name": "Yong Liao"
                    },
                    {
                        "name": "Pengyuan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Pengyuan Zhou"
                },
                "author": "Pengyuan Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.04325v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.04325v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05897v1",
                "updated": "2024-08-12T02:32:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    2,
                    32,
                    45,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T02:32:45Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    2,
                    32,
                    45,
                    0,
                    225,
                    0
                ],
                "title": "TRIZ-GPT: An LLM-augmented method for problem-solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIZ-GPT: An LLM-augmented method for problem-solving"
                },
                "summary": "TRIZ, the Theory of Inventive Problem Solving, is derived from a\ncomprehensive analysis of patents across various domains, offering a framework\nand practical tools for problem-solving. Despite its potential to foster\ninnovative solutions, the complexity and abstractness of TRIZ methodology often\nmake its acquisition and application challenging. This often requires users to\nhave a deep understanding of the theory, as well as substantial practical\nexperience and knowledge across various disciplines. The advent of Large\nLanguage Models (LLMs) presents an opportunity to address these challenges by\nleveraging their extensive knowledge bases and reasoning capabilities for\ninnovative solution generation within TRIZ-based problem-solving process. This\nstudy explores and evaluates the application of LLMs within the TRIZ-based\nproblem-solving process. The construction of TRIZ case collections establishes\na solid empirical foundation for our experiments and offers valuable resources\nto the TRIZ community. A specifically designed workflow, utilizing step-by-step\nreasoning and evaluation-validated prompt strategies, effectively transforms\nconcrete problems into TRIZ problems and finally generates inventive solutions.\nFinally, we present a case study in mechanical engineering field that\nhighlights the practical application of this LLM-augmented method. It showcases\nGPT-4's ability to generate solutions that closely resonate with original\nsolutions and suggests more implementation mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TRIZ, the Theory of Inventive Problem Solving, is derived from a\ncomprehensive analysis of patents across various domains, offering a framework\nand practical tools for problem-solving. Despite its potential to foster\ninnovative solutions, the complexity and abstractness of TRIZ methodology often\nmake its acquisition and application challenging. This often requires users to\nhave a deep understanding of the theory, as well as substantial practical\nexperience and knowledge across various disciplines. The advent of Large\nLanguage Models (LLMs) presents an opportunity to address these challenges by\nleveraging their extensive knowledge bases and reasoning capabilities for\ninnovative solution generation within TRIZ-based problem-solving process. This\nstudy explores and evaluates the application of LLMs within the TRIZ-based\nproblem-solving process. The construction of TRIZ case collections establishes\na solid empirical foundation for our experiments and offers valuable resources\nto the TRIZ community. A specifically designed workflow, utilizing step-by-step\nreasoning and evaluation-validated prompt strategies, effectively transforms\nconcrete problems into TRIZ problems and finally generates inventive solutions.\nFinally, we present a case study in mechanical engineering field that\nhighlights the practical application of this LLM-augmented method. It showcases\nGPT-4's ability to generate solutions that closely resonate with original\nsolutions and suggests more implementation mechanisms."
                },
                "authors": [
                    {
                        "name": "Liuqing Chen"
                    },
                    {
                        "name": "Yaxuan Song"
                    },
                    {
                        "name": "Shixian Ding"
                    },
                    {
                        "name": "Lingyun Sun"
                    },
                    {
                        "name": "Peter Childs"
                    },
                    {
                        "name": "Haoyu Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Zuo"
                },
                "author": "Haoyu Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00958v2",
                "updated": "2024-08-12T02:08:03Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    2,
                    8,
                    3,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-01T04:29:35Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    4,
                    29,
                    35,
                    0,
                    183,
                    0
                ],
                "title": "Universal Approximation Theory: The basic theory for large language\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Approximation Theory: The basic theory for large language\n  models"
                },
                "summary": "Language models have emerged as a critical area of focus in artificial\nintelligence, particularly with the introduction of groundbreaking innovations\nlike ChatGPT. Large-scale Transformer networks have quickly become the leading\napproach for advancing natural language processing algorithms. Built on the\nTransformer architecture, these models enable interactions that closely mimic\nhuman communication and, equipped with extensive knowledge, can even assist in\nguiding human tasks. Despite their impressive capabilities and growing\ncomplexity, a key question remains-the theoretical foundations of large\nlanguage models (LLMs). What makes Transformer so effective for powering\nintelligent language applications, such as translation and coding? What\nunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme\nenhance the fine-tuning of LLMs? And what supports the practicality of pruning\nLLMs? To address these critical questions and explore the technological\nstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) to\noffer a theoretical backdrop, shedding light on the mechanisms that underpin\nthese advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models have emerged as a critical area of focus in artificial\nintelligence, particularly with the introduction of groundbreaking innovations\nlike ChatGPT. Large-scale Transformer networks have quickly become the leading\napproach for advancing natural language processing algorithms. Built on the\nTransformer architecture, these models enable interactions that closely mimic\nhuman communication and, equipped with extensive knowledge, can even assist in\nguiding human tasks. Despite their impressive capabilities and growing\ncomplexity, a key question remains-the theoretical foundations of large\nlanguage models (LLMs). What makes Transformer so effective for powering\nintelligent language applications, such as translation and coding? What\nunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme\nenhance the fine-tuning of LLMs? And what supports the practicality of pruning\nLLMs? To address these critical questions and explore the technological\nstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) to\noffer a theoretical backdrop, shedding light on the mechanisms that underpin\nthese advancements."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05002v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05002v2",
                "updated": "2024-08-12T01:48:42Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    1,
                    48,
                    42,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-06T05:46:28Z",
                "published_parsed": [
                    2024,
                    8,
                    6,
                    5,
                    46,
                    28,
                    1,
                    219,
                    0
                ],
                "title": "An Empirical Study on Challenges for LLM Developers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on Challenges for LLM Developers"
                },
                "summary": "In recent years, large language models (LLMs) have seen rapid advancements,\nsignificantly impacting various fields such as natural language processing, and\nsoftware engineering. These LLMs, exemplified by OpenAI's ChatGPT, have\nrevolutionized the way we approach language understanding and generation tasks.\nHowever, in contrast to traditional software development practices, LLM\ndevelopment introduces new challenges for AI developers in design,\nimplementation, and deployment. These challenges span different areas (such as\nprompts, APIs, and plugins), requiring developers to navigate unique\nmethodologies and considerations specific to LLM development.\n  Despite the profound influence of LLMs, to the best of our knowledge, these\nchallenges have not been thoroughly investigated in previous empirical studies.\nTo fill this gap, we present the first comprehensive study on understanding the\nchallenges faced by LLM developers. Specifically, we crawl and analyze 29,057\nrelevant questions from a popular OpenAI developer forum. We first examine\ntheir popularity and difficulty. After manually analyzing 2,364 sampled\nquestions, we construct a taxonomy of challenges faced by LLM developers. Based\non this taxonomy, we summarize a set of findings and actionable implications\nfor LLM-related stakeholders, including developers and providers (especially\nthe OpenAI organization).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have seen rapid advancements,\nsignificantly impacting various fields such as natural language processing, and\nsoftware engineering. These LLMs, exemplified by OpenAI's ChatGPT, have\nrevolutionized the way we approach language understanding and generation tasks.\nHowever, in contrast to traditional software development practices, LLM\ndevelopment introduces new challenges for AI developers in design,\nimplementation, and deployment. These challenges span different areas (such as\nprompts, APIs, and plugins), requiring developers to navigate unique\nmethodologies and considerations specific to LLM development.\n  Despite the profound influence of LLMs, to the best of our knowledge, these\nchallenges have not been thoroughly investigated in previous empirical studies.\nTo fill this gap, we present the first comprehensive study on understanding the\nchallenges faced by LLM developers. Specifically, we crawl and analyze 29,057\nrelevant questions from a popular OpenAI developer forum. We first examine\ntheir popularity and difficulty. After manually analyzing 2,364 sampled\nquestions, we construct a taxonomy of challenges faced by LLM developers. Based\non this taxonomy, we summarize a set of findings and actionable implications\nfor LLM-related stakeholders, including developers and providers (especially\nthe OpenAI organization)."
                },
                "authors": [
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Chaoyang Gao"
                    },
                    {
                        "name": "Chunyang Chen"
                    },
                    {
                        "name": "Guangbei Zhang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "29 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05002v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05002v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01528v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01528v3",
                "updated": "2024-08-12T01:44:26Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    1,
                    44,
                    26,
                    0,
                    225,
                    0
                ],
                "published": "2024-02-02T16:15:24Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    16,
                    15,
                    24,
                    4,
                    33,
                    0
                ],
                "title": "Decoding Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Speculative Decoding"
                },
                "summary": "Speculative Decoding is a widely used technique to speed up inference for\nLarge Language Models (LLMs) without sacrificing quality. When performing\ninference, speculative decoding uses a smaller draft model to generate\nspeculative tokens and then uses the target LLM to verify those draft tokens.\nThe speedup provided by speculative decoding heavily depends on the choice of\nthe draft model. In this work, we perform a detailed study comprising over 350\nexperiments with LLaMA-65B and OPT-66B using speculative decoding and delineate\nthe factors that affect the performance gain provided by speculative decoding.\nOur experiments indicate that the performance of speculative decoding depends\nheavily on the latency of the draft model, and the draft model's capability in\nlanguage modeling does not correlate strongly with its performance in\nspeculative decoding. Based on these insights we explore a new design space for\ndraft models and design hardware-efficient draft models for speculative\ndecoding. Our newly designed draft model for LLaMA-65B can provide 111% higher\nthroughput than existing draft models and can generalize further to the LLaMA-2\nmodel family and supervised fine-tuned models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative Decoding is a widely used technique to speed up inference for\nLarge Language Models (LLMs) without sacrificing quality. When performing\ninference, speculative decoding uses a smaller draft model to generate\nspeculative tokens and then uses the target LLM to verify those draft tokens.\nThe speedup provided by speculative decoding heavily depends on the choice of\nthe draft model. In this work, we perform a detailed study comprising over 350\nexperiments with LLaMA-65B and OPT-66B using speculative decoding and delineate\nthe factors that affect the performance gain provided by speculative decoding.\nOur experiments indicate that the performance of speculative decoding depends\nheavily on the latency of the draft model, and the draft model's capability in\nlanguage modeling does not correlate strongly with its performance in\nspeculative decoding. Based on these insights we explore a new design space for\ndraft models and design hardware-efficient draft models for speculative\ndecoding. Our newly designed draft model for LLaMA-65B can provide 111% higher\nthroughput than existing draft models and can generalize further to the LLaMA-2\nmodel family and supervised fine-tuned models."
                },
                "authors": [
                    {
                        "name": "Minghao Yan"
                    },
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01528v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01528v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10885v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10885v3",
                "updated": "2024-08-12T01:24:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    1,
                    24,
                    33,
                    0,
                    225,
                    0
                ],
                "published": "2024-05-17T16:22:52Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    16,
                    22,
                    52,
                    4,
                    138,
                    0
                ],
                "title": "FA-Depth: Toward Fast and Accurate Self-supervised Monocular Depth\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FA-Depth: Toward Fast and Accurate Self-supervised Monocular Depth\n  Estimation"
                },
                "summary": "Most existing methods often rely on complex models to predict scene depth\nwith high accuracy, resulting in slow inference that is not conducive to\ndeployment. To better balance precision and speed, we first designed SmallDepth\nbased on sparsity. Second, to enhance the feature representation ability of\nSmallDepth during training under the condition of equal complexity during\ninference, we propose an equivalent transformation module(ETM). Third, to\nimprove the ability of each layer in the case of a fixed SmallDepth to perceive\ndifferent context information and improve the robustness of SmallDepth to the\nleft-right direction and illumination changes, we propose pyramid loss. Fourth,\nto further improve the accuracy of SmallDepth, we utilized the proposed\nfunction approximation loss (APX) to transfer knowledge in the pretrained\nHQDecv2, obtained by optimizing the previous HQDec to address grid artifacts in\nsome regions, to SmallDepth. Extensive experiments demonstrate that each\nproposed component improves the precision of SmallDepth without changing the\ncomplexity of SmallDepth during inference, and the developed approach achieves\nstate-of-the-art results on KITTI at an inference speed of more than 500 frames\nper second and with approximately 2 M parameters. The code and models will be\npublicly available at https://github.com/fwucas/FA-Depth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most existing methods often rely on complex models to predict scene depth\nwith high accuracy, resulting in slow inference that is not conducive to\ndeployment. To better balance precision and speed, we first designed SmallDepth\nbased on sparsity. Second, to enhance the feature representation ability of\nSmallDepth during training under the condition of equal complexity during\ninference, we propose an equivalent transformation module(ETM). Third, to\nimprove the ability of each layer in the case of a fixed SmallDepth to perceive\ndifferent context information and improve the robustness of SmallDepth to the\nleft-right direction and illumination changes, we propose pyramid loss. Fourth,\nto further improve the accuracy of SmallDepth, we utilized the proposed\nfunction approximation loss (APX) to transfer knowledge in the pretrained\nHQDecv2, obtained by optimizing the previous HQDec to address grid artifacts in\nsome regions, to SmallDepth. Extensive experiments demonstrate that each\nproposed component improves the precision of SmallDepth without changing the\ncomplexity of SmallDepth during inference, and the developed approach achieves\nstate-of-the-art results on KITTI at an inference speed of more than 500 frames\nper second and with approximately 2 M parameters. The code and models will be\npublicly available at https://github.com/fwucas/FA-Depth."
                },
                "authors": [
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Jun Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Jun Cheng"
                },
                "author": "Jun Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10885v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10885v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14755v2",
                "updated": "2024-08-12T01:09:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    1,
                    9,
                    36,
                    0,
                    225,
                    0
                ],
                "published": "2024-05-23T16:21:57Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    16,
                    21,
                    57,
                    3,
                    144,
                    0
                ],
                "title": "Large language models can be zero-shot anomaly detectors for time\n  series?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can be zero-shot anomaly detectors for time\n  series?"
                },
                "summary": "Recent studies have shown the ability of large language models to perform a\nvariety of tasks, including time series forecasting. The flexible nature of\nthese models allows them to be used for many applications. In this paper, we\npresent a novel study of large language models used for the challenging task of\ntime series anomaly detection. This problem entails two aspects novel for LLMs:\nthe need for the model to identify part of the input sequence (or multiple\nparts) as anomalous; and the need for it to work with time series data rather\nthan the traditional text input. We introduce sigllm, a framework for time\nseries anomaly detection using large language models. Our framework includes a\ntime-series-to-text conversion module, as well as end-to-end pipelines that\nprompt language models to perform time series anomaly detection. We investigate\ntwo paradigms for testing the abilities of large language models to perform the\ndetection task. First, we present a prompt-based detection method that directly\nasks a language model to indicate which elements of the input are anomalies.\nSecond, we leverage the forecasting capability of a large language model to\nguide the anomaly detection process. We evaluated our framework on 11 datasets\nspanning various sources and 10 pipelines. We show that the forecasting method\nsignificantly outperformed the prompting method in all 11 datasets with respect\nto the F1 score. Moreover, while large language models are capable of finding\nanomalies, state-of-the-art deep learning models are still superior in\nperformance, achieving results 30% better than large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have shown the ability of large language models to perform a\nvariety of tasks, including time series forecasting. The flexible nature of\nthese models allows them to be used for many applications. In this paper, we\npresent a novel study of large language models used for the challenging task of\ntime series anomaly detection. This problem entails two aspects novel for LLMs:\nthe need for the model to identify part of the input sequence (or multiple\nparts) as anomalous; and the need for it to work with time series data rather\nthan the traditional text input. We introduce sigllm, a framework for time\nseries anomaly detection using large language models. Our framework includes a\ntime-series-to-text conversion module, as well as end-to-end pipelines that\nprompt language models to perform time series anomaly detection. We investigate\ntwo paradigms for testing the abilities of large language models to perform the\ndetection task. First, we present a prompt-based detection method that directly\nasks a language model to indicate which elements of the input are anomalies.\nSecond, we leverage the forecasting capability of a large language model to\nguide the anomaly detection process. We evaluated our framework on 11 datasets\nspanning various sources and 10 pipelines. We show that the forecasting method\nsignificantly outperformed the prompting method in all 11 datasets with respect\nto the F1 score. Moreover, while large language models are capable of finding\nanomalies, state-of-the-art deep learning models are still superior in\nperformance, achieving results 30% better than large language models."
                },
                "authors": [
                    {
                        "name": "Sarah Alnegheimish"
                    },
                    {
                        "name": "Linh Nguyen"
                    },
                    {
                        "name": "Laure Berti-Equille"
                    },
                    {
                        "name": "Kalyan Veeramachaneni"
                    }
                ],
                "author_detail": {
                    "name": "Kalyan Veeramachaneni"
                },
                "author": "Kalyan Veeramachaneni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17728v2",
                "updated": "2024-08-12T00:54:28Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    0,
                    54,
                    28,
                    0,
                    225,
                    0
                ],
                "published": "2024-05-28T01:07:06Z",
                "published_parsed": [
                    2024,
                    5,
                    28,
                    1,
                    7,
                    6,
                    1,
                    149,
                    0
                ],
                "title": "Facilitating Holistic Evaluations with LLMs: Insights from\n  Scenario-Based Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facilitating Holistic Evaluations with LLMs: Insights from\n  Scenario-Based Experiments"
                },
                "summary": "Workshop courses designed to foster creativity are gaining popularity.\nHowever, even experienced faculty teams find it challenging to realize a\nholistic evaluation that accommodates diverse perspectives. Adequate\ndeliberation is essential to integrate varied assessments, but faculty often\nlack the time for such exchanges. Deriving an average score without discussion\nundermines the purpose of a holistic evaluation. Therefore, this paper explores\nthe use of a Large Language Model (LLM) as a facilitator to integrate diverse\nfaculty assessments. Scenario-based experiments were conducted to determine if\nthe LLM could integrate diverse evaluations and explain the underlying\npedagogical theories to faculty. The results were noteworthy, showing that the\nLLM can effectively facilitate faculty discussions. Additionally, the LLM\ndemonstrated the capability to create evaluation criteria by generalizing a\nsingle scenario-based experiment, leveraging its already acquired pedagogical\ndomain knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Workshop courses designed to foster creativity are gaining popularity.\nHowever, even experienced faculty teams find it challenging to realize a\nholistic evaluation that accommodates diverse perspectives. Adequate\ndeliberation is essential to integrate varied assessments, but faculty often\nlack the time for such exchanges. Deriving an average score without discussion\nundermines the purpose of a holistic evaluation. Therefore, this paper explores\nthe use of a Large Language Model (LLM) as a facilitator to integrate diverse\nfaculty assessments. Scenario-based experiments were conducted to determine if\nthe LLM could integrate diverse evaluations and explain the underlying\npedagogical theories to faculty. The results were noteworthy, showing that the\nLLM can effectively facilitate faculty discussions. Additionally, the LLM\ndemonstrated the capability to create evaluation criteria by generalizing a\nsingle scenario-based experiment, leveraging its already acquired pedagogical\ndomain knowledge."
                },
                "authors": [
                    {
                        "name": "Toru Ishida"
                    },
                    {
                        "name": "Tongxi Liu"
                    },
                    {
                        "name": "Hailong Wang"
                    },
                    {
                        "name": "William K. Cheunga"
                    }
                ],
                "author_detail": {
                    "name": "William K. Cheunga"
                },
                "author": "William K. Cheunga",
                "arxiv_comment": "The final version appears in the proceedings of the 32nd\n  International Conference on Computers in Education (ICCE 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05882v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05882v1",
                "updated": "2024-08-12T00:46:39Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    0,
                    46,
                    39,
                    0,
                    225,
                    0
                ],
                "published": "2024-08-12T00:46:39Z",
                "published_parsed": [
                    2024,
                    8,
                    12,
                    0,
                    46,
                    39,
                    0,
                    225,
                    0
                ],
                "title": "Creating Arabic LLM Prompts at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating Arabic LLM Prompts at Scale"
                },
                "summary": "The debut of chatGPT and BARD has popularized instruction following text\ngeneration using LLMs, where a user can interrogate an LLM using natural\nlanguage requests and obtain natural language answers that matches their\nrequests. Training LLMs to respond in this manner requires a large number of\nworked out examples of user requests (aka prompts) with corresponding gold\nresponses. In this paper, we introduce two methods for creating such prompts\nfor Arabic cheaply and quickly. The first methods entails automatically\ntranslating existing prompt datasets from English, such as PromptSource and\nSuper-NaturalInstructions, and then using machine translation quality\nestimation to retain high quality translations only. The second method involves\ncreating natural language prompts on top of existing Arabic NLP datasets. Using\nthese two methods we were able to create more than 67.4 million Arabic prompts\nthat cover a variety of tasks including summarization, headline generation,\ngrammar checking, open/closed question answering, creative writing, etc. We\nshow that fine tuning an open 7 billion parameter large language model, namely\nbase Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter\ninstruction tuned model, namely Llama3 70B, in handling Arabic prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The debut of chatGPT and BARD has popularized instruction following text\ngeneration using LLMs, where a user can interrogate an LLM using natural\nlanguage requests and obtain natural language answers that matches their\nrequests. Training LLMs to respond in this manner requires a large number of\nworked out examples of user requests (aka prompts) with corresponding gold\nresponses. In this paper, we introduce two methods for creating such prompts\nfor Arabic cheaply and quickly. The first methods entails automatically\ntranslating existing prompt datasets from English, such as PromptSource and\nSuper-NaturalInstructions, and then using machine translation quality\nestimation to retain high quality translations only. The second method involves\ncreating natural language prompts on top of existing Arabic NLP datasets. Using\nthese two methods we were able to create more than 67.4 million Arabic prompts\nthat cover a variety of tasks including summarization, headline generation,\ngrammar checking, open/closed question answering, creative writing, etc. We\nshow that fine tuning an open 7 billion parameter large language model, namely\nbase Qwen2 7B, enables it to outperform a state-of-the-art 70 billion parameter\ninstruction tuned model, namely Llama3 70B, in handling Arabic prompts."
                },
                "authors": [
                    {
                        "name": "Abdelrahman El-Sheikh"
                    },
                    {
                        "name": "Ahmed Elmogtaba"
                    },
                    {
                        "name": "Kareem Darwish"
                    },
                    {
                        "name": "Muhammad Elmallah"
                    },
                    {
                        "name": "Ashraf Elneima"
                    },
                    {
                        "name": "Hassan Sawaf"
                    }
                ],
                "author_detail": {
                    "name": "Hassan Sawaf"
                },
                "author": "Hassan Sawaf",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05882v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05882v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07820v3",
                "updated": "2024-08-12T00:43:56Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    0,
                    43,
                    56,
                    0,
                    225,
                    0
                ],
                "published": "2023-10-11T19:01:28Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    19,
                    1,
                    28,
                    2,
                    284,
                    0
                ],
                "title": "Large Language Models Are Zero-Shot Time Series Forecasters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Are Zero-Shot Time Series Forecasters"
                },
                "summary": "By encoding time series as a string of numerical digits, we can frame time\nseries forecasting as next-token prediction in text. Developing this approach,\nwe find that large language models (LLMs) such as GPT-3 and LLaMA-2 can\nsurprisingly zero-shot extrapolate time series at a level comparable to or\nexceeding the performance of purpose-built time series models trained on the\ndownstream tasks. To facilitate this performance, we propose procedures for\neffectively tokenizing time series data and converting discrete distributions\nover tokens into highly flexible densities over continuous values. We argue the\nsuccess of LLMs for time series stems from their ability to naturally represent\nmultimodal distributions, in conjunction with biases for simplicity, and\nrepetition, which align with the salient features in many time series, such as\nrepeated seasonal trends. We also show how LLMs can naturally handle missing\ndata without imputation through non-numerical text, accommodate textual side\ninformation, and answer questions to help explain predictions. While we find\nthat increasing model size generally improves performance on time series, we\nshow GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers,\nand poor uncertainty calibration, which is likely the result of alignment\ninterventions such as RLHF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By encoding time series as a string of numerical digits, we can frame time\nseries forecasting as next-token prediction in text. Developing this approach,\nwe find that large language models (LLMs) such as GPT-3 and LLaMA-2 can\nsurprisingly zero-shot extrapolate time series at a level comparable to or\nexceeding the performance of purpose-built time series models trained on the\ndownstream tasks. To facilitate this performance, we propose procedures for\neffectively tokenizing time series data and converting discrete distributions\nover tokens into highly flexible densities over continuous values. We argue the\nsuccess of LLMs for time series stems from their ability to naturally represent\nmultimodal distributions, in conjunction with biases for simplicity, and\nrepetition, which align with the salient features in many time series, such as\nrepeated seasonal trends. We also show how LLMs can naturally handle missing\ndata without imputation through non-numerical text, accommodate textual side\ninformation, and answer questions to help explain predictions. While we find\nthat increasing model size generally improves performance on time series, we\nshow GPT-4 can perform worse than GPT-3 because of how it tokenizes numbers,\nand poor uncertainty calibration, which is likely the result of alignment\ninterventions such as RLHF."
                },
                "authors": [
                    {
                        "name": "Nate Gruver"
                    },
                    {
                        "name": "Marc Finzi"
                    },
                    {
                        "name": "Shikai Qiu"
                    },
                    {
                        "name": "Andrew Gordon Wilson"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Gordon Wilson"
                },
                "author": "Andrew Gordon Wilson",
                "arxiv_comment": "NeurIPS 2023. Code available at: https://github.com/ngruver/llmtime",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.07820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12874v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12874v2",
                "updated": "2024-08-12T00:38:22Z",
                "updated_parsed": [
                    2024,
                    8,
                    12,
                    0,
                    38,
                    22,
                    0,
                    225,
                    0
                ],
                "published": "2024-07-16T04:41:58Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    4,
                    41,
                    58,
                    1,
                    198,
                    0
                ],
                "title": "SELF-GUIDE: Better Task-Specific Instruction Following via\n  Self-Synthetic Finetuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELF-GUIDE: Better Task-Specific Instruction Following via\n  Self-Synthetic Finetuning"
                },
                "summary": "Large language models (LLMs) hold the promise of solving diverse tasks when\nprovided with appropriate natural language prompts. However, prompting often\nleads models to make predictions with lower accuracy compared to finetuning a\nmodel with ample training data. On the other hand, while finetuning LLMs on\ntask-specific data generally improves their performance, abundant annotated\ndatasets are not available for all tasks. Previous work has explored generating\ntask-specific data from state-of-the-art LLMs and using this data to finetune\nsmaller models, but this approach requires access to a language model other\nthan the one being trained, which introduces cost, scalability challenges, and\nlegal hurdles associated with continuously relying on more powerful LLMs. In\nresponse to these, we propose SELF-GUIDE, a multi-stage mechanism in which we\nsynthesize task-specific input-output pairs from the student LLM, then use\nthese input-output pairs to finetune the student LLM itself. In our empirical\nevaluation of the Natural Instructions V2 benchmark, we find that SELF-GUIDE\nimproves the performance of LLM by a substantial margin. Specifically, we\nreport an absolute improvement of approximately 15% for classification tasks\nand 18% for generation tasks in the benchmark's metrics. This sheds light on\nthe promise of self-synthesized data guiding LLMs towards becoming\ntask-specific experts without any external learning signals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) hold the promise of solving diverse tasks when\nprovided with appropriate natural language prompts. However, prompting often\nleads models to make predictions with lower accuracy compared to finetuning a\nmodel with ample training data. On the other hand, while finetuning LLMs on\ntask-specific data generally improves their performance, abundant annotated\ndatasets are not available for all tasks. Previous work has explored generating\ntask-specific data from state-of-the-art LLMs and using this data to finetune\nsmaller models, but this approach requires access to a language model other\nthan the one being trained, which introduces cost, scalability challenges, and\nlegal hurdles associated with continuously relying on more powerful LLMs. In\nresponse to these, we propose SELF-GUIDE, a multi-stage mechanism in which we\nsynthesize task-specific input-output pairs from the student LLM, then use\nthese input-output pairs to finetune the student LLM itself. In our empirical\nevaluation of the Natural Instructions V2 benchmark, we find that SELF-GUIDE\nimproves the performance of LLM by a substantial margin. Specifically, we\nreport an absolute improvement of approximately 15% for classification tasks\nand 18% for generation tasks in the benchmark's metrics. This sheds light on\nthe promise of self-synthesized data guiding LLMs towards becoming\ntask-specific experts without any external learning signals."
                },
                "authors": [
                    {
                        "name": "Chenyang Zhao"
                    },
                    {
                        "name": "Xueying Jia"
                    },
                    {
                        "name": "Vijay Viswanathan"
                    },
                    {
                        "name": "Tongshuang Wu"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "arxiv_comment": "Accepted by COLM 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12874v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12874v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03528v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03528v2",
                "updated": "2024-08-11T23:25:49Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    23,
                    25,
                    49,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-05T21:34:21Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    21,
                    34,
                    21,
                    0,
                    36,
                    0
                ],
                "title": "Efficient Generation of Grids and Traversal Graphs in Compositional\n  Spaces towards Exploration and Path Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Generation of Grids and Traversal Graphs in Compositional\n  Spaces towards Exploration and Path Planning"
                },
                "summary": "Many disciplines of science and engineering deal with problems related to\ncompositions, ranging from chemical compositions in materials science to\nportfolio compositions in economics. They exist in non-Euclidean simplex\nspaces, causing many standard tools to be incorrect or inefficient, which is\nsignificant in combinatorically or structurally challenging spaces exemplified\nby Compositionally Complex Materials (CCMs) and Functionally Graded Materials\n(FGMs). Here, we explore them conceptually in terms of problem spaces and\nquantitatively in terms of computational feasibility.\n  This work implements several essential methods specific to the compositional\n(simplex) spaces through a high-performance open-source library nimplex. Most\nsignificantly, we derive and implement an algorithm for constructing a novel\nn-dimensional simplex graph data structure, which contains all discretized\ncompositions and all possible neighbor-to-neighbor transitions as pointer\narrays. Critically, no distance or neighborhood calculations are performed,\ninstead leveraging pure combinatorics and the ordering in procedurally\ngenerated simplex grids, keeping the algorithm $\\mathcal{O}(N)$, so that graphs\nwith billions of transitions take seconds to construct on a laptop.\nFurthermore, we demonstrate how such graph representations can be combined to\nexpress path-planning problem spaces and to incorporate prior knowledge while\nkeeping the problem space homogeneous. This allows for efficient deployment of\nexisting high-performance gradient descent, graph traversal search, and other\npath optimization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many disciplines of science and engineering deal with problems related to\ncompositions, ranging from chemical compositions in materials science to\nportfolio compositions in economics. They exist in non-Euclidean simplex\nspaces, causing many standard tools to be incorrect or inefficient, which is\nsignificant in combinatorically or structurally challenging spaces exemplified\nby Compositionally Complex Materials (CCMs) and Functionally Graded Materials\n(FGMs). Here, we explore them conceptually in terms of problem spaces and\nquantitatively in terms of computational feasibility.\n  This work implements several essential methods specific to the compositional\n(simplex) spaces through a high-performance open-source library nimplex. Most\nsignificantly, we derive and implement an algorithm for constructing a novel\nn-dimensional simplex graph data structure, which contains all discretized\ncompositions and all possible neighbor-to-neighbor transitions as pointer\narrays. Critically, no distance or neighborhood calculations are performed,\ninstead leveraging pure combinatorics and the ordering in procedurally\ngenerated simplex grids, keeping the algorithm $\\mathcal{O}(N)$, so that graphs\nwith billions of transitions take seconds to construct on a laptop.\nFurthermore, we demonstrate how such graph representations can be combined to\nexpress path-planning problem spaces and to incorporate prior knowledge while\nkeeping the problem space homogeneous. This allows for efficient deployment of\nexisting high-performance gradient descent, graph traversal search, and other\npath optimization algorithms."
                },
                "authors": [
                    {
                        "name": "Adam M. Krajewski"
                    },
                    {
                        "name": "Allison M. Beese"
                    },
                    {
                        "name": "Wesley F. Reinhart"
                    },
                    {
                        "name": "Zi-Kui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zi-Kui Liu"
                },
                "author": "Zi-Kui Liu",
                "arxiv_comment": "18 pages; 11 figures; software source code at\n  https://github.com/amkrajewski/nimplex and documentation at\n  https://nimplex.phaseslab.org; minor updates in V2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03528v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03528v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.1; F.2; G.2.1; I.1.2; J.2; J.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05568v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05568v3",
                "updated": "2024-08-11T23:19:48Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    23,
                    19,
                    48,
                    6,
                    224,
                    0
                ],
                "published": "2024-06-08T20:19:35Z",
                "published_parsed": [
                    2024,
                    6,
                    8,
                    20,
                    19,
                    35,
                    5,
                    160,
                    0
                ],
                "title": "SAMM: Sharded Automated Market Maker",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAMM: Sharded Automated Market Maker"
                },
                "summary": "Automated Market Makers (AMMs) are a cornerstone of decentralized finance\n(DeFi) blockchain-based platforms. They enable direct exchange of virtual\ntokens: Traders exchange tokens with the AMM, paying a fee; liquidity comes\nfrom liquidity providers, paid by those fees. Despite growing demand, the\nperformance of AMMs is limited. State-of-the-art blockchain platforms allow for\nparallel execution of transactions. However, we show that AMMs do not enjoy\nthese gains since their operations are not parallelizable.\n  We present SAMM, an AMM comprising multiple independent shards. All shards\noperate in the same chain, but they allow for parallel execution as each is\nindependent. The challenge is that traders are incentivized to split each trade\namong all AMMs in existing designs, leading to lower throughput. SAMM addresses\nthis issue with a novel design of the trading fees. Traders are incentivized to\nuse only a single smallest shard. We show that all Subgame-Perfect Nash\nEquilibria (SPNE) fit the desired behavior: Liquidity providers balance the\nliquidity among all shards, so the system converges to the state where trades\nare evenly distributed, overcoming destabilization attacks.\n  Evaluation in the Sui and Solana blockchains shows that SAMM improves\nthroughput by 5 and by 16, respectively, approaching their limit. SAMM is\ndirectly deployable, allowing trading at scale for individuals and DeFi\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Market Makers (AMMs) are a cornerstone of decentralized finance\n(DeFi) blockchain-based platforms. They enable direct exchange of virtual\ntokens: Traders exchange tokens with the AMM, paying a fee; liquidity comes\nfrom liquidity providers, paid by those fees. Despite growing demand, the\nperformance of AMMs is limited. State-of-the-art blockchain platforms allow for\nparallel execution of transactions. However, we show that AMMs do not enjoy\nthese gains since their operations are not parallelizable.\n  We present SAMM, an AMM comprising multiple independent shards. All shards\noperate in the same chain, but they allow for parallel execution as each is\nindependent. The challenge is that traders are incentivized to split each trade\namong all AMMs in existing designs, leading to lower throughput. SAMM addresses\nthis issue with a novel design of the trading fees. Traders are incentivized to\nuse only a single smallest shard. We show that all Subgame-Perfect Nash\nEquilibria (SPNE) fit the desired behavior: Liquidity providers balance the\nliquidity among all shards, so the system converges to the state where trades\nare evenly distributed, overcoming destabilization attacks.\n  Evaluation in the Sui and Solana blockchains shows that SAMM improves\nthroughput by 5 and by 16, respectively, approaching their limit. SAMM is\ndirectly deployable, allowing trading at scale for individuals and DeFi\napplications."
                },
                "authors": [
                    {
                        "name": "Hongyin Chen"
                    },
                    {
                        "name": "Amit Vaisman"
                    },
                    {
                        "name": "Ittay Eyal"
                    }
                ],
                "author_detail": {
                    "name": "Ittay Eyal"
                },
                "author": "Ittay Eyal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05568v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05568v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05874v1",
                "updated": "2024-08-11T22:59:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    22,
                    59,
                    32,
                    6,
                    224,
                    0
                ],
                "published": "2024-08-11T22:59:32Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    22,
                    59,
                    32,
                    6,
                    224,
                    0
                ],
                "title": "LLM-Based Robust Product Classification in Commerce and Compliance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Robust Product Classification in Commerce and Compliance"
                },
                "summary": "Product classification is a crucial task in international trade, as\ncompliance regulations are verified and taxes and duties are applied based on\nproduct categories. Manual classification of products is time-consuming and\nerror-prone, and the sheer volume of products imported and exported renders the\nmanual process infeasible. Consequently, e-commerce platforms and enterprises\ninvolved in international trade have turned to automatic product classification\nusing machine learning. However, current approaches do not consider the\nreal-world challenges associated with product classification, such as very\nabbreviated and incomplete product descriptions. In addition, recent\nadvancements in generative Large Language Models (LLMs) and their reasoning\ncapabilities are mainly untapped in product classification and e-commerce. In\nthis research, we explore the real-life challenges of industrial classification\nand we propose data perturbations that allow for realistic data simulation.\nFurthermore, we employ LLM-based product classification to improve the\nrobustness of the prediction in presence of incomplete data. Our research shows\nthat LLMs with in-context learning outperform the supervised approaches in the\nclean-data scenario. Additionally, we illustrate that LLMs are significantly\nmore robust than the supervised approaches when data attacks are present.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Product classification is a crucial task in international trade, as\ncompliance regulations are verified and taxes and duties are applied based on\nproduct categories. Manual classification of products is time-consuming and\nerror-prone, and the sheer volume of products imported and exported renders the\nmanual process infeasible. Consequently, e-commerce platforms and enterprises\ninvolved in international trade have turned to automatic product classification\nusing machine learning. However, current approaches do not consider the\nreal-world challenges associated with product classification, such as very\nabbreviated and incomplete product descriptions. In addition, recent\nadvancements in generative Large Language Models (LLMs) and their reasoning\ncapabilities are mainly untapped in product classification and e-commerce. In\nthis research, we explore the real-life challenges of industrial classification\nand we propose data perturbations that allow for realistic data simulation.\nFurthermore, we employ LLM-based product classification to improve the\nrobustness of the prediction in presence of incomplete data. Our research shows\nthat LLMs with in-context learning outperform the supervised approaches in the\nclean-data scenario. Additionally, we illustrate that LLMs are significantly\nmore robust than the supervised approaches when data attacks are present."
                },
                "authors": [
                    {
                        "name": "Sina Gholamian"
                    },
                    {
                        "name": "Gianfranco Romani"
                    },
                    {
                        "name": "Bartosz Rudnikowicz"
                    },
                    {
                        "name": "Laura Skylaki"
                    }
                ],
                "author_detail": {
                    "name": "Laura Skylaki"
                },
                "author": "Laura Skylaki",
                "arxiv_comment": "11 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05873v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05873v1",
                "updated": "2024-08-11T22:58:23Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    22,
                    58,
                    23,
                    6,
                    224,
                    0
                ],
                "published": "2024-08-11T22:58:23Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    22,
                    58,
                    23,
                    6,
                    224,
                    0
                ],
                "title": "Defining Boundaries: A Spectrum of Task Feasibility for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defining Boundaries: A Spectrum of Task Feasibility for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have shown remarkable performance in various\ntasks but often fail to handle queries that exceed their knowledge and\ncapabilities, leading to incorrect or fabricated responses. This paper\naddresses the need for LLMs to recognize and refuse infeasible tasks due to the\nrequired skills surpassing their capabilities. We first systematically\nconceptualize infeasible tasks for LLMs, providing formal definitions and\ncategorizations that cover a spectrum of related hallucinations. We develop and\nbenchmark a new dataset comprising diverse infeasible and feasible tasks to\ntest multiple LLMs' abilities on task feasibility. Furthermore, we explore the\npotential of training enhancements to increase LLMs' refusal capabilities with\nfine-tuning. Experiments validate the effectiveness of our methods, offering\npromising directions for refining the operational boundaries of LLMs in real\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown remarkable performance in various\ntasks but often fail to handle queries that exceed their knowledge and\ncapabilities, leading to incorrect or fabricated responses. This paper\naddresses the need for LLMs to recognize and refuse infeasible tasks due to the\nrequired skills surpassing their capabilities. We first systematically\nconceptualize infeasible tasks for LLMs, providing formal definitions and\ncategorizations that cover a spectrum of related hallucinations. We develop and\nbenchmark a new dataset comprising diverse infeasible and feasible tasks to\ntest multiple LLMs' abilities on task feasibility. Furthermore, we explore the\npotential of training enhancements to increase LLMs' refusal capabilities with\nfine-tuning. Experiments validate the effectiveness of our methods, offering\npromising directions for refining the operational boundaries of LLMs in real\napplications."
                },
                "authors": [
                    {
                        "name": "Wenbo Zhang"
                    },
                    {
                        "name": "Zihang Xu"
                    },
                    {
                        "name": "Hengrui Cai"
                    }
                ],
                "author_detail": {
                    "name": "Hengrui Cai"
                },
                "author": "Hengrui Cai",
                "arxiv_comment": "20 pages, 9 tables, 15 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05873v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05873v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.05221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.05221v2",
                "updated": "2024-08-11T22:20:19Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    22,
                    20,
                    19,
                    6,
                    224,
                    0
                ],
                "published": "2024-04-08T06:35:09Z",
                "published_parsed": [
                    2024,
                    4,
                    8,
                    6,
                    35,
                    9,
                    0,
                    99,
                    0
                ],
                "title": "LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step\n  Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Reasoners: New Evaluation, Library, and Analysis of Step-by-Step\n  Reasoning with Large Language Models"
                },
                "summary": "Generating accurate step-by-step reasoning is essential for Large Language\nModels (LLMs) to address complex problems and enhance robustness and\ninterpretability. Despite the flux of research on developing advanced reasoning\napproaches, systematically analyzing the diverse LLMs and reasoning strategies\nin generating reasoning chains remains a significant challenge. The\ndifficulties stem from the lack of two key elements: (1) an automatic method\nfor evaluating the generated reasoning chains on different tasks, and (2) a\nunified formalism and implementation of the diverse reasoning approaches for\nsystematic comparison. This paper aims to close the gap: (1) We introduce\nAutoRace for fully automated reasoning chain evaluation. Existing metrics rely\non expensive human annotations or pre-defined LLM prompts not adaptable to\ndifferent tasks. In contrast, AutoRace automatically creates detailed\nevaluation criteria tailored for each task, and uses GPT-4 for accurate\nevaluation following the criteria. (2) We develop LLM Reasoners, a library for\nstandardized modular implementation of existing and new reasoning algorithms,\nunder a unified formulation of the search, reward, and world model components.\nWith the new evaluation and library, (3) we conduct extensive study of\ndifferent reasoning approaches (e.g., CoT, ToT, RAP). The analysis reveals\ninteresting findings about different factors contributing to reasoning,\nincluding the reward-guidance, breadth-vs-depth in search, world model, and\nprompt formats, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating accurate step-by-step reasoning is essential for Large Language\nModels (LLMs) to address complex problems and enhance robustness and\ninterpretability. Despite the flux of research on developing advanced reasoning\napproaches, systematically analyzing the diverse LLMs and reasoning strategies\nin generating reasoning chains remains a significant challenge. The\ndifficulties stem from the lack of two key elements: (1) an automatic method\nfor evaluating the generated reasoning chains on different tasks, and (2) a\nunified formalism and implementation of the diverse reasoning approaches for\nsystematic comparison. This paper aims to close the gap: (1) We introduce\nAutoRace for fully automated reasoning chain evaluation. Existing metrics rely\non expensive human annotations or pre-defined LLM prompts not adaptable to\ndifferent tasks. In contrast, AutoRace automatically creates detailed\nevaluation criteria tailored for each task, and uses GPT-4 for accurate\nevaluation following the criteria. (2) We develop LLM Reasoners, a library for\nstandardized modular implementation of existing and new reasoning algorithms,\nunder a unified formulation of the search, reward, and world model components.\nWith the new evaluation and library, (3) we conduct extensive study of\ndifferent reasoning approaches (e.g., CoT, ToT, RAP). The analysis reveals\ninteresting findings about different factors contributing to reasoning,\nincluding the reward-guidance, breadth-vs-depth in search, world model, and\nprompt formats, etc."
                },
                "authors": [
                    {
                        "name": "Shibo Hao"
                    },
                    {
                        "name": "Yi Gu"
                    },
                    {
                        "name": "Haotian Luo"
                    },
                    {
                        "name": "Tianyang Liu"
                    },
                    {
                        "name": "Xiyan Shao"
                    },
                    {
                        "name": "Xinyuan Wang"
                    },
                    {
                        "name": "Shuhua Xie"
                    },
                    {
                        "name": "Haodi Ma"
                    },
                    {
                        "name": "Adithya Samavedhi"
                    },
                    {
                        "name": "Qiyue Gao"
                    },
                    {
                        "name": "Zhen Wang"
                    },
                    {
                        "name": "Zhiting Hu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiting Hu"
                },
                "author": "Zhiting Hu",
                "arxiv_comment": "Project website: https://www.llm-reasoners.net/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.05221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.05221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07867v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07867v3",
                "updated": "2024-08-13T01:55:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    13,
                    1,
                    55,
                    6,
                    1,
                    226,
                    0
                ],
                "published": "2024-02-12T18:28:36Z",
                "published_parsed": [
                    2024,
                    2,
                    12,
                    18,
                    28,
                    36,
                    0,
                    43,
                    0
                ],
                "title": "PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented\n  Generation of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented\n  Generation of Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success due to their\nexceptional generative capabilities. Despite their success, they also have\ninherent limitations such as a lack of up-to-date knowledge and hallucination.\nRetrieval-Augmented Generation (RAG) is a state-of-the-art technique to\nmitigate these limitations. The key idea of RAG is to ground the answer\ngeneration of an LLM on external knowledge retrieved from a knowledge database.\nExisting studies mainly focus on improving the accuracy or efficiency of RAG,\nleaving its security largely unexplored. We aim to bridge the gap in this work.\nWe find that the knowledge database in a RAG system introduces a new and\npractical attack surface. Based on this attack surface, we propose PoisonedRAG,\nthe first knowledge corruption attack to RAG, where an attacker could inject a\nfew malicious texts into the knowledge database of a RAG system to induce an\nLLM to generate an attacker-chosen target answer for an attacker-chosen target\nquestion. We formulate knowledge corruption attacks as an optimization problem,\nwhose solution is a set of malicious texts. Depending on the background\nknowledge (e.g., black-box and white-box settings) of an attacker on a RAG\nsystem, we propose two solutions to solve the optimization problem,\nrespectively. Our results show PoisonedRAG could achieve a 90% attack success\nrate when injecting five malicious texts for each target question into a\nknowledge database with millions of texts. We also evaluate several defenses\nand our results show they are insufficient to defend against PoisonedRAG,\nhighlighting the need for new defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success due to their\nexceptional generative capabilities. Despite their success, they also have\ninherent limitations such as a lack of up-to-date knowledge and hallucination.\nRetrieval-Augmented Generation (RAG) is a state-of-the-art technique to\nmitigate these limitations. The key idea of RAG is to ground the answer\ngeneration of an LLM on external knowledge retrieved from a knowledge database.\nExisting studies mainly focus on improving the accuracy or efficiency of RAG,\nleaving its security largely unexplored. We aim to bridge the gap in this work.\nWe find that the knowledge database in a RAG system introduces a new and\npractical attack surface. Based on this attack surface, we propose PoisonedRAG,\nthe first knowledge corruption attack to RAG, where an attacker could inject a\nfew malicious texts into the knowledge database of a RAG system to induce an\nLLM to generate an attacker-chosen target answer for an attacker-chosen target\nquestion. We formulate knowledge corruption attacks as an optimization problem,\nwhose solution is a set of malicious texts. Depending on the background\nknowledge (e.g., black-box and white-box settings) of an attacker on a RAG\nsystem, we propose two solutions to solve the optimization problem,\nrespectively. Our results show PoisonedRAG could achieve a 90% attack success\nrate when injecting five malicious texts for each target question into a\nknowledge database with millions of texts. We also evaluate several defenses\nand our results show they are insufficient to defend against PoisonedRAG,\nhighlighting the need for new defenses."
                },
                "authors": [
                    {
                        "name": "Wei Zou"
                    },
                    {
                        "name": "Runpeng Geng"
                    },
                    {
                        "name": "Binghui Wang"
                    },
                    {
                        "name": "Jinyuan Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jinyuan Jia"
                },
                "author": "Jinyuan Jia",
                "arxiv_comment": "To appear in USENIX Security Symposium 2025. The code is available at\n  https://github.com/sleeepeer/PoisonedRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07867v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07867v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.17100v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.17100v4",
                "updated": "2024-08-11T20:03:12Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    20,
                    3,
                    12,
                    6,
                    224,
                    0
                ],
                "published": "2023-05-26T17:14:43Z",
                "published_parsed": [
                    2023,
                    5,
                    26,
                    17,
                    14,
                    43,
                    4,
                    146,
                    0
                ],
                "title": "BiomedGPT: A Generalist Vision-Language Foundation Model for Diverse\n  Biomedical Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BiomedGPT: A Generalist Vision-Language Foundation Model for Diverse\n  Biomedical Tasks"
                },
                "summary": "Traditional biomedical artificial intelligence (AI) models, designed for\nspecific tasks or modalities, often exhibit limited flexibility in real-world\ndeployment and struggle to utilize holistic information. Generalist AI holds\nthe potential to address these limitations due to its versatility in\ninterpreting different data types and generating tailored outputs for diverse\nneeds. However, existing biomedical generalist AI solutions are typically\nheavyweight and closed source to researchers, practitioners, and patients.\nHere, we propose BiomedGPT, the first open-source and lightweight\nvision-language foundation model, designed as a generalist capable of\nperforming various biomedical tasks. BiomedGPT achieved state-of-the-art\nresults in 16 out of 25 experiments while maintaining a computing-friendly\nmodel scale. We also conducted human evaluations to assess the capabilities of\nBiomedGPT in radiology visual question answering, report generation, and\nsummarization. BiomedGPT exhibits robust prediction ability with a low error\nrate of 3.8% in question answering, satisfactory performance with an error rate\nof 8.3% in writing complex radiology reports, and competitive summarization\nability with a nearly equivalent preference score to human experts. Our method\ndemonstrates that effective training with diverse data can lead to more\npractical biomedical AI for improving diagnosis and workflow efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional biomedical artificial intelligence (AI) models, designed for\nspecific tasks or modalities, often exhibit limited flexibility in real-world\ndeployment and struggle to utilize holistic information. Generalist AI holds\nthe potential to address these limitations due to its versatility in\ninterpreting different data types and generating tailored outputs for diverse\nneeds. However, existing biomedical generalist AI solutions are typically\nheavyweight and closed source to researchers, practitioners, and patients.\nHere, we propose BiomedGPT, the first open-source and lightweight\nvision-language foundation model, designed as a generalist capable of\nperforming various biomedical tasks. BiomedGPT achieved state-of-the-art\nresults in 16 out of 25 experiments while maintaining a computing-friendly\nmodel scale. We also conducted human evaluations to assess the capabilities of\nBiomedGPT in radiology visual question answering, report generation, and\nsummarization. BiomedGPT exhibits robust prediction ability with a low error\nrate of 3.8% in question answering, satisfactory performance with an error rate\nof 8.3% in writing complex radiology reports, and competitive summarization\nability with a nearly equivalent preference score to human experts. Our method\ndemonstrates that effective training with diverse data can lead to more\npractical biomedical AI for improving diagnosis and workflow efficiency."
                },
                "authors": [
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Rong Zhou"
                    },
                    {
                        "name": "Eashan Adhikarla"
                    },
                    {
                        "name": "Zhiling Yan"
                    },
                    {
                        "name": "Yixin Liu"
                    },
                    {
                        "name": "Jun Yu"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Xun Chen"
                    },
                    {
                        "name": "Brian D. Davison"
                    },
                    {
                        "name": "Hui Ren"
                    },
                    {
                        "name": "Jing Huang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Yuyin Zhou"
                    },
                    {
                        "name": "Sunyang Fu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Yong Chen"
                    },
                    {
                        "name": "Lifang He"
                    },
                    {
                        "name": "James Zou"
                    },
                    {
                        "name": "Quanzheng Li"
                    },
                    {
                        "name": "Hongfang Liu"
                    },
                    {
                        "name": "Lichao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lichao Sun"
                },
                "author": "Lichao Sun",
                "arxiv_doi": "10.1038/s41591-024-03185-2",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41591-024-03185-2",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.17100v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.17100v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Fix incorrect citations and add journal reference for the published\n  version. Nat Med (2024)",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05855v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05855v1",
                "updated": "2024-08-11T19:59:08Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    19,
                    59,
                    8,
                    6,
                    224,
                    0
                ],
                "published": "2024-08-11T19:59:08Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    19,
                    59,
                    8,
                    6,
                    224,
                    0
                ],
                "title": "Using Retriever Augmented Large Language Models for Attack Graph\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Retriever Augmented Large Language Models for Attack Graph\n  Generation"
                },
                "summary": "As the complexity of modern systems increases, so does the importance of\nassessing their security posture through effective vulnerability management and\nthreat modeling techniques. One powerful tool in the arsenal of cybersecurity\nprofessionals is the attack graph, a representation of all potential attack\npaths within a system that an adversary might exploit to achieve a certain\nobjective. Traditional methods of generating attack graphs involve expert\nknowledge, manual curation, and computational algorithms that might not cover\nthe entire threat landscape due to the ever-evolving nature of vulnerabilities\nand exploits. This paper explores the approach of leveraging large language\nmodels (LLMs), such as ChatGPT, to automate the generation of attack graphs by\nintelligently chaining Common Vulnerabilities and Exposures (CVEs) based on\ntheir preconditions and effects. It also shows how to utilize LLMs to create\nattack graphs from threat reports.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the complexity of modern systems increases, so does the importance of\nassessing their security posture through effective vulnerability management and\nthreat modeling techniques. One powerful tool in the arsenal of cybersecurity\nprofessionals is the attack graph, a representation of all potential attack\npaths within a system that an adversary might exploit to achieve a certain\nobjective. Traditional methods of generating attack graphs involve expert\nknowledge, manual curation, and computational algorithms that might not cover\nthe entire threat landscape due to the ever-evolving nature of vulnerabilities\nand exploits. This paper explores the approach of leveraging large language\nmodels (LLMs), such as ChatGPT, to automate the generation of attack graphs by\nintelligently chaining Common Vulnerabilities and Exposures (CVEs) based on\ntheir preconditions and effects. It also shows how to utilize LLMs to create\nattack graphs from threat reports."
                },
                "authors": [
                    {
                        "name": "Renascence Tarafder Prapty"
                    },
                    {
                        "name": "Ashish Kundu"
                    },
                    {
                        "name": "Arun Iyengar"
                    }
                ],
                "author_detail": {
                    "name": "Arun Iyengar"
                },
                "author": "Arun Iyengar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05855v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05855v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01365v3",
                "updated": "2024-08-11T19:43:36Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    19,
                    43,
                    36,
                    6,
                    224,
                    0
                ],
                "published": "2024-04-01T17:56:06Z",
                "published_parsed": [
                    2024,
                    4,
                    1,
                    17,
                    56,
                    6,
                    0,
                    92,
                    0
                ],
                "title": "Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation"
                },
                "summary": "With the development of transformer-based large language models (LLMs), they\nhave been applied to many fields due to their remarkable utility, but this\ncomes at a considerable computational cost at deployment. Fortunately, some\nmethods such as pruning or constructing a mixture of experts (MoE) aim at\nexploiting sparsity in transformer feedforward (FF) blocks to gain boosts in\nspeed and reduction in memory requirements. However, these techniques can be\nvery costly and inflexible in practice, as they often require training or are\nrestricted to specific types of architectures. To address this, we introduce\nGRIFFIN, a novel training-free and calibration-free method that selects unique\nFF experts at the sequence level for efficient generation across a plethora of\nLLMs with different non-ReLU activation functions. This is possible due to a\ncritical observation that many trained LLMs naturally produce highly structured\nFF activation patterns within a sequence, which we call flocking. Despite our\nmethod's simplicity, we show with 50% of the FF parameters, GRIFFIN maintains\nthe original model's performance with little to no degradation on a variety of\nclassification and generation tasks, all while improving latency (e.g.\n1.29$\\times$ and 1.25$\\times$ speed-ups in Gemma 7B and Llama 2 13B,\nrespectively, on an NVIDIA L40). Code is available at\nhttps://github.com/hdong920/GRIFFIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the development of transformer-based large language models (LLMs), they\nhave been applied to many fields due to their remarkable utility, but this\ncomes at a considerable computational cost at deployment. Fortunately, some\nmethods such as pruning or constructing a mixture of experts (MoE) aim at\nexploiting sparsity in transformer feedforward (FF) blocks to gain boosts in\nspeed and reduction in memory requirements. However, these techniques can be\nvery costly and inflexible in practice, as they often require training or are\nrestricted to specific types of architectures. To address this, we introduce\nGRIFFIN, a novel training-free and calibration-free method that selects unique\nFF experts at the sequence level for efficient generation across a plethora of\nLLMs with different non-ReLU activation functions. This is possible due to a\ncritical observation that many trained LLMs naturally produce highly structured\nFF activation patterns within a sequence, which we call flocking. Despite our\nmethod's simplicity, we show with 50% of the FF parameters, GRIFFIN maintains\nthe original model's performance with little to no degradation on a variety of\nclassification and generation tasks, all while improving latency (e.g.\n1.29$\\times$ and 1.25$\\times$ speed-ups in Gemma 7B and Llama 2 13B,\nrespectively, on an NVIDIA L40). Code is available at\nhttps://github.com/hdong920/GRIFFIN."
                },
                "authors": [
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Yuejie Chi"
                    }
                ],
                "author_detail": {
                    "name": "Yuejie Chi"
                },
                "author": "Yuejie Chi",
                "arxiv_comment": "Revision 1: Updated abstract with code link; re-ran top-k + sampling\n  rows in Table 4, conclusions unchanged Revision 2: Reframing and new\n  experiments, conclusions unchanged",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.06424v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.06424v4",
                "updated": "2024-08-11T18:56:50Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    18,
                    56,
                    50,
                    6,
                    224,
                    0
                ],
                "published": "2023-05-10T19:09:24Z",
                "published_parsed": [
                    2023,
                    5,
                    10,
                    19,
                    9,
                    24,
                    2,
                    130,
                    0
                ],
                "title": "Bot or Human? Detecting ChatGPT Imposters with A Single Question",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bot or Human? Detecting ChatGPT Imposters with A Single Question"
                },
                "summary": "Large language models (LLMs) like GPT-4 have recently demonstrated impressive\ncapabilities in natural language understanding and generation. However, there\nis a concern that they can be misused for malicious purposes, such as fraud or\ndenial-of-service attacks. Therefore, it is crucial to develop methods for\ndetecting whether the party involved in a conversation is a bot or a human. In\nthis paper, we propose a framework named FLAIR, Finding Large Language Model\nAuthenticity via a Single Inquiry and Response, to detect conversational bots\nin an online manner. Specifically, we target a single question scenario that\ncan effectively differentiate human users from bots. The questions are divided\ninto two categories: those that are easy for humans but difficult for bots\n(e.g., counting, substitution, searching, and ASCII art reasoning), and those\nthat are easy for bots but difficult for humans (e.g., memorization and\ncomputation). Our approach shows different strengths of these questions in\ntheir effectiveness, providing a new way for online service providers to\nprotect themselves against nefarious activities. Our code and question set are\navailable at https://github.com/hongwang600/FLAIR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) like GPT-4 have recently demonstrated impressive\ncapabilities in natural language understanding and generation. However, there\nis a concern that they can be misused for malicious purposes, such as fraud or\ndenial-of-service attacks. Therefore, it is crucial to develop methods for\ndetecting whether the party involved in a conversation is a bot or a human. In\nthis paper, we propose a framework named FLAIR, Finding Large Language Model\nAuthenticity via a Single Inquiry and Response, to detect conversational bots\nin an online manner. Specifically, we target a single question scenario that\ncan effectively differentiate human users from bots. The questions are divided\ninto two categories: those that are easy for humans but difficult for bots\n(e.g., counting, substitution, searching, and ASCII art reasoning), and those\nthat are easy for bots but difficult for humans (e.g., memorization and\ncomputation). Our approach shows different strengths of these questions in\ntheir effectiveness, providing a new way for online service providers to\nprotect themselves against nefarious activities. Our code and question set are\navailable at https://github.com/hongwang600/FLAIR."
                },
                "authors": [
                    {
                        "name": "Hong Wang"
                    },
                    {
                        "name": "Xuan Luo"
                    },
                    {
                        "name": "Weizhi Wang"
                    },
                    {
                        "name": "Xifeng Yan"
                    }
                ],
                "author_detail": {
                    "name": "Xifeng Yan"
                },
                "author": "Xifeng Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.06424v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.06424v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18243v2",
                "updated": "2024-08-11T17:18:30Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    17,
                    18,
                    30,
                    6,
                    224,
                    0
                ],
                "published": "2024-04-28T16:50:12Z",
                "published_parsed": [
                    2024,
                    4,
                    28,
                    16,
                    50,
                    12,
                    6,
                    119,
                    0
                ],
                "title": "LEGENT: Open Platform for Embodied Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEGENT: Open Platform for Embodied Agents"
                },
                "summary": "Despite advancements in Large Language Models (LLMs) and Large Multimodal\nModels (LMMs), their integration into language-grounded, human-like embodied\nagents remains incomplete, hindering complex real-life task performance in\nphysical environments. Existing integrations often feature limited open\nsourcing, challenging collective progress in this field. We introduce LEGENT,\nan open, scalable platform for developing embodied agents using LLMs and LMMs.\nLEGENT offers a dual approach: a rich, interactive 3D environment with\ncommunicable and actionable agents, paired with a user-friendly interface, and\na sophisticated data generation pipeline utilizing advanced algorithms to\nexploit supervision from simulated worlds at scale. In our experiments, an\nembryonic vision-language-action model trained on LEGENT-generated data\nsurpasses GPT-4V in embodied tasks, showcasing promising generalization\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advancements in Large Language Models (LLMs) and Large Multimodal\nModels (LMMs), their integration into language-grounded, human-like embodied\nagents remains incomplete, hindering complex real-life task performance in\nphysical environments. Existing integrations often feature limited open\nsourcing, challenging collective progress in this field. We introduce LEGENT,\nan open, scalable platform for developing embodied agents using LLMs and LMMs.\nLEGENT offers a dual approach: a rich, interactive 3D environment with\ncommunicable and actionable agents, paired with a user-friendly interface, and\na sophisticated data generation pipeline utilizing advanced algorithms to\nexploit supervision from simulated worlds at scale. In our experiments, an\nembryonic vision-language-action model trained on LEGENT-generated data\nsurpasses GPT-4V in embodied tasks, showcasing promising generalization\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Zhili Cheng"
                    },
                    {
                        "name": "Zhitong Wang"
                    },
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "An Liu"
                    },
                    {
                        "name": "Yuge Tu"
                    },
                    {
                        "name": "Pengkai Li"
                    },
                    {
                        "name": "Lei Shi"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "ACL 2024 System Demonstration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18243v3",
                "updated": "2024-08-11T17:15:06Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    17,
                    15,
                    6,
                    6,
                    224,
                    0
                ],
                "published": "2024-02-28T11:16:00Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    11,
                    16,
                    0,
                    2,
                    59,
                    0
                ],
                "title": "Learning or Self-aligning? Rethinking Instruction Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning or Self-aligning? Rethinking Instruction Fine-tuning"
                },
                "summary": "Instruction Fine-tuning~(IFT) is a critical phase in building large language\nmodels~(LLMs). Previous works mainly focus on the IFT's role in the transfer of\nbehavioral norms and the learning of additional world knowledge. However, the\nunderstanding of the underlying mechanisms of IFT remains significantly\nlimited. In this paper, we design a knowledge intervention framework to\ndecouple the potential underlying factors of IFT, thereby enabling individual\nanalysis of different factors. Surprisingly, our experiments reveal that\nattempting to learn additional world knowledge through IFT often struggles to\nyield positive impacts and can even lead to markedly negative effects. Further,\nwe discover that maintaining internal knowledge consistency before and after\nIFT is a critical factor for achieving successful IFT. Our findings reveal the\nunderlying mechanisms of IFT and provide robust support for some very recent\nand potential future works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction Fine-tuning~(IFT) is a critical phase in building large language\nmodels~(LLMs). Previous works mainly focus on the IFT's role in the transfer of\nbehavioral norms and the learning of additional world knowledge. However, the\nunderstanding of the underlying mechanisms of IFT remains significantly\nlimited. In this paper, we design a knowledge intervention framework to\ndecouple the potential underlying factors of IFT, thereby enabling individual\nanalysis of different factors. Surprisingly, our experiments reveal that\nattempting to learn additional world knowledge through IFT often struggles to\nyield positive impacts and can even lead to markedly negative effects. Further,\nwe discover that maintaining internal knowledge consistency before and after\nIFT is a critical factor for achieving successful IFT. Our findings reveal the\nunderlying mechanisms of IFT and provide robust support for some very recent\nand potential future works."
                },
                "authors": [
                    {
                        "name": "Mengjie Ren"
                    },
                    {
                        "name": "Boxi Cao"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Cao Liu"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Ke Zeng"
                    },
                    {
                        "name": "Guanglu Wan"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "arxiv_comment": "Camera Ready for ACL2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05829v1",
                "updated": "2024-08-11T17:11:14Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    17,
                    11,
                    14,
                    6,
                    224,
                    0
                ],
                "published": "2024-08-11T17:11:14Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    17,
                    11,
                    14,
                    6,
                    224,
                    0
                ],
                "title": "Supporting Software Maintenance with Dynamically Generated Document\n  Hierarchies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supporting Software Maintenance with Dynamically Generated Document\n  Hierarchies"
                },
                "summary": "Software documentation supports a broad set of software maintenance tasks;\nhowever, creating and maintaining high-quality, multi-level software\ndocumentation can be incredibly time-consuming and therefore many code bases\nsuffer from a lack of adequate documentation. We address this problem through\npresenting HGEN, a fully automated pipeline that leverages LLMs to transform\nsource code through a series of six stages into a well-organized hierarchy of\nformatted documents. We evaluate HGEN both quantitatively and qualitatively.\nFirst, we use it to generate documentation for three diverse projects, and\nengage key developers in comparing the quality of the generated documentation\nagainst their own previously produced manually-crafted documentation. We then\npilot HGEN in nine different industrial projects using diverse datasets\nprovided by each project. We collect feedback from project stakeholders, and\nanalyze it using an inductive approach to identify recurring themes. Results\nshow that HGEN produces artifact hierarchies similar in quality to manually\nconstructed documentation, with much higher coverage of the core concepts than\nthe baseline approach. Stakeholder feedback highlights HGEN's commercial impact\npotential as a tool for accelerating code comprehension and maintenance tasks.\nResults and associated supplemental materials can be found at\nhttps://zenodo.org/records/11403244",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software documentation supports a broad set of software maintenance tasks;\nhowever, creating and maintaining high-quality, multi-level software\ndocumentation can be incredibly time-consuming and therefore many code bases\nsuffer from a lack of adequate documentation. We address this problem through\npresenting HGEN, a fully automated pipeline that leverages LLMs to transform\nsource code through a series of six stages into a well-organized hierarchy of\nformatted documents. We evaluate HGEN both quantitatively and qualitatively.\nFirst, we use it to generate documentation for three diverse projects, and\nengage key developers in comparing the quality of the generated documentation\nagainst their own previously produced manually-crafted documentation. We then\npilot HGEN in nine different industrial projects using diverse datasets\nprovided by each project. We collect feedback from project stakeholders, and\nanalyze it using an inductive approach to identify recurring themes. Results\nshow that HGEN produces artifact hierarchies similar in quality to manually\nconstructed documentation, with much higher coverage of the core concepts than\nthe baseline approach. Stakeholder feedback highlights HGEN's commercial impact\npotential as a tool for accelerating code comprehension and maintenance tasks.\nResults and associated supplemental materials can be found at\nhttps://zenodo.org/records/11403244"
                },
                "authors": [
                    {
                        "name": "Katherine R. Dearstyne"
                    },
                    {
                        "name": "Alberto D. Rodriguez"
                    },
                    {
                        "name": "Jane Cleland-Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jane Cleland-Huang"
                },
                "author": "Jane Cleland-Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13408v3",
                "updated": "2024-08-11T16:30:44Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    30,
                    44,
                    6,
                    224,
                    0
                ],
                "published": "2024-06-19T09:57:19Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    9,
                    57,
                    19,
                    2,
                    171,
                    0
                ],
                "title": "SQLFixAgent: Towards Semantic-Accurate Text-to-SQL Parsing via\n  Consistency-Enhanced Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQLFixAgent: Towards Semantic-Accurate Text-to-SQL Parsing via\n  Consistency-Enhanced Multi-Agent Collaboration"
                },
                "summary": "While fine-tuned large language models (LLMs) excel in generating\ngrammatically valid SQL in Text-to-SQL parsing, they often struggle to ensure\nsemantic accuracy in queries, leading to user confusion and diminished system\nusability. To tackle this challenge, we introduce SQLFixAgent, a new\nconsistency-enhanced multi-agent collaborative framework designed for detecting\nand repairing erroneous SQL. Our framework comprises a core agent, SQLRefiner,\nalongside two auxiliary agents: SQLReviewer and QueryCrafter. The SQLReviewer\nagent employs the rubber duck debugging method to identify potential semantic\nmismatches between SQL and user query. If the error is detected, the\nQueryCrafter agent generates multiple SQL as candidate repairs using a\nfine-tuned SQLTool. Subsequently, leveraging similar repair retrieval and\nfailure memory reflection, the SQLRefiner agent selects the most fitting SQL\nstatement from the candidates as the final repair. We evaluated our proposed\nframework on five Text-to-SQL benchmarks. The experimental results show that\nour method consistently enhances the performance of the baseline model,\nspecifically achieving an execution accuracy improvement of over 3\\% on the\nBird benchmark. Our framework also has a higher token efficiency compared to\nother advanced methods, making it more competitive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While fine-tuned large language models (LLMs) excel in generating\ngrammatically valid SQL in Text-to-SQL parsing, they often struggle to ensure\nsemantic accuracy in queries, leading to user confusion and diminished system\nusability. To tackle this challenge, we introduce SQLFixAgent, a new\nconsistency-enhanced multi-agent collaborative framework designed for detecting\nand repairing erroneous SQL. Our framework comprises a core agent, SQLRefiner,\nalongside two auxiliary agents: SQLReviewer and QueryCrafter. The SQLReviewer\nagent employs the rubber duck debugging method to identify potential semantic\nmismatches between SQL and user query. If the error is detected, the\nQueryCrafter agent generates multiple SQL as candidate repairs using a\nfine-tuned SQLTool. Subsequently, leveraging similar repair retrieval and\nfailure memory reflection, the SQLRefiner agent selects the most fitting SQL\nstatement from the candidates as the final repair. We evaluated our proposed\nframework on five Text-to-SQL benchmarks. The experimental results show that\nour method consistently enhances the performance of the baseline model,\nspecifically achieving an execution accuracy improvement of over 3\\% on the\nBird benchmark. Our framework also has a higher token efficiency compared to\nother advanced methods, making it more competitive."
                },
                "authors": [
                    {
                        "name": "Jipeng Cen"
                    },
                    {
                        "name": "Jiaxin Liu"
                    },
                    {
                        "name": "Zhixu Li"
                    },
                    {
                        "name": "Jingjing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jingjing Wang"
                },
                "author": "Jingjing Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03910v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03910v2",
                "updated": "2024-08-11T16:23:57Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    16,
                    23,
                    57,
                    6,
                    224,
                    0
                ],
                "published": "2024-08-07T17:13:59Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    17,
                    13,
                    59,
                    2,
                    220,
                    0
                ],
                "title": "CodexGraph: Bridging Large Language Models and Code Repositories via\n  Code Graph Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodexGraph: Bridging Large Language Models and Code Repositories via\n  Code Graph Databases"
                },
                "summary": "Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval\nand MBPP, but struggle with handling entire code repositories. This challenge\nhas prompted research on enhancing LLM-codebase interaction at a repository\nscale. Current solutions rely on similarity-based retrieval or manual tools and\nAPIs, each with notable drawbacks. Similarity-based retrieval often has low\nrecall in complex tasks, while manual tools and APIs are typically\ntask-specific and require expert knowledge, reducing their generalizability\nacross diverse code tasks and real-world applications. To mitigate these\nlimitations, we introduce CodexGraph, a system that integrates LLM agents with\ngraph database interfaces extracted from code repositories. By leveraging the\nstructural properties of graph databases and the flexibility of the graph query\nlanguage, CodexGraph enables the LLM agent to construct and execute queries,\nallowing for precise, code structure-aware context retrieval and code\nnavigation. We assess CodexGraph using three benchmarks: CrossCodeEval,\nSWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding\napplications. With a unified graph database schema, CodexGraph demonstrates\ncompetitive performance and potential in both academic and real-world\nenvironments, showcasing its versatility and efficacy in software engineering.\nOur application demo:\nhttps://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval\nand MBPP, but struggle with handling entire code repositories. This challenge\nhas prompted research on enhancing LLM-codebase interaction at a repository\nscale. Current solutions rely on similarity-based retrieval or manual tools and\nAPIs, each with notable drawbacks. Similarity-based retrieval often has low\nrecall in complex tasks, while manual tools and APIs are typically\ntask-specific and require expert knowledge, reducing their generalizability\nacross diverse code tasks and real-world applications. To mitigate these\nlimitations, we introduce CodexGraph, a system that integrates LLM agents with\ngraph database interfaces extracted from code repositories. By leveraging the\nstructural properties of graph databases and the flexibility of the graph query\nlanguage, CodexGraph enables the LLM agent to construct and execute queries,\nallowing for precise, code structure-aware context retrieval and code\nnavigation. We assess CodexGraph using three benchmarks: CrossCodeEval,\nSWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding\napplications. With a unified graph database schema, CodexGraph demonstrates\ncompetitive performance and potential in both academic and real-world\nenvironments, showcasing its versatility and efficacy in software engineering.\nOur application demo:\nhttps://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent."
                },
                "authors": [
                    {
                        "name": "Xiangyan Liu"
                    },
                    {
                        "name": "Bo Lan"
                    },
                    {
                        "name": "Zhiyuan Hu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zhicheng Zhang"
                    },
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Michael Shieh"
                    },
                    {
                        "name": "Wenmeng Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Wenmeng Zhou"
                },
                "author": "Wenmeng Zhou",
                "arxiv_comment": "work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03910v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03910v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18069v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18069v2",
                "updated": "2024-08-11T15:45:07Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    15,
                    45,
                    7,
                    6,
                    224,
                    0
                ],
                "published": "2024-07-25T14:24:57Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    14,
                    24,
                    57,
                    3,
                    207,
                    0
                ],
                "title": "C2P: Featuring Large Language Models with Causal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "C2P: Featuring Large Language Models with Causal Reasoning"
                },
                "summary": "Causal reasoning is the primary bottleneck that Large Language Models (LLMs)\nmust overcome to attain human-level intelligence. To address this, we introduce\nthe Causal Chain of Prompting (C2P) as the first reasoning framework that\nequips current LLMs with causal reasoning capabilities. C2P operates\nautonomously, avoiding reliance on external tools or modules during both the\ncausal learning and reasoning phases, and can be seamlessly implemented during\nthe training or fine-tuning of LLMs. Experimental results across various\nbenchmark datasets demonstrate a significant improvement in causal learning and\nsubsequent reasoning accuracy of LLMs. We illustrate how C2P enhances LLMs'\nability to causally reason in real-world scenarios, addressing complex problems\nin fields such as healthcare, medicine, economics, education, social sciences,\nenvironmental science, and marketing. With few-shot learning, GPT-4 Turbo using\nC2P with as few as six examples achieves significant performance improvements,\nboasting over a 33% increase in reasoning accuracy over the most\nstate-of-the-art LLMs, which perform nearly randomly in similar circumstances.\nThis demonstrates the transformative potential of integrating C2P into LLM\ntraining or fine-tuning processes, thereby empowering these models with\nadvanced causal reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal reasoning is the primary bottleneck that Large Language Models (LLMs)\nmust overcome to attain human-level intelligence. To address this, we introduce\nthe Causal Chain of Prompting (C2P) as the first reasoning framework that\nequips current LLMs with causal reasoning capabilities. C2P operates\nautonomously, avoiding reliance on external tools or modules during both the\ncausal learning and reasoning phases, and can be seamlessly implemented during\nthe training or fine-tuning of LLMs. Experimental results across various\nbenchmark datasets demonstrate a significant improvement in causal learning and\nsubsequent reasoning accuracy of LLMs. We illustrate how C2P enhances LLMs'\nability to causally reason in real-world scenarios, addressing complex problems\nin fields such as healthcare, medicine, economics, education, social sciences,\nenvironmental science, and marketing. With few-shot learning, GPT-4 Turbo using\nC2P with as few as six examples achieves significant performance improvements,\nboasting over a 33% increase in reasoning accuracy over the most\nstate-of-the-art LLMs, which perform nearly randomly in similar circumstances.\nThis demonstrates the transformative potential of integrating C2P into LLM\ntraining or fine-tuning processes, thereby empowering these models with\nadvanced causal reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Abdolmahdi Bagheri"
                    },
                    {
                        "name": "Matin Alinejad"
                    },
                    {
                        "name": "Kevin Bello"
                    },
                    {
                        "name": "Alireza Akhondi-Asl"
                    }
                ],
                "author_detail": {
                    "name": "Alireza Akhondi-Asl"
                },
                "author": "Alireza Akhondi-Asl",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2306.05836 by other authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18069v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18069v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05780v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05780v1",
                "updated": "2024-08-11T14:11:45Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    14,
                    11,
                    45,
                    6,
                    224,
                    0
                ],
                "published": "2024-08-11T14:11:45Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    14,
                    11,
                    45,
                    6,
                    224,
                    0
                ],
                "title": "U-DECN: End-to-End Underwater Object Detection ConvNet with Improved\n  DeNoising Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "U-DECN: End-to-End Underwater Object Detection ConvNet with Improved\n  DeNoising Training"
                },
                "summary": "Underwater object detection has higher requirements of running speed and\ndeployment efficiency for the detector due to its specific environmental\nchallenges. NMS of two- or one-stage object detectors and transformer\narchitecture of query-based end-to-end object detectors are not conducive to\ndeployment on underwater embedded devices with limited processing power. As for\nthe detrimental effect of underwater color cast noise, recent underwater object\ndetectors make network architecture or training complex, which also hinders\ntheir application and deployment on underwater vehicle platforms. In this\npaper, we propose the Underwater DECO with improved deNoising training\n(U-DECN), the query-based end-to-end object detector (with ConvNet\nencoder-decoder architecture) for underwater color cast noise that addresses\nthe above problems. We integrate advanced technologies from DETR variants into\nDECO and design optimization methods specifically for the ConvNet architecture,\nincluding Separate Contrastive DeNoising Forward and Deformable Convolution in\nSIM. To address the underwater color cast noise issue, we propose an underwater\ncolor denoising query to improve the generalization of the model for the biased\nobject feature information by different color cast noise. Our U-DECN, with\nResNet-50 backbone, achieves 61.4 AP (50 epochs), 63.3 AP (72 epochs), 64.0 AP\n(100 epochs) on DUO, and 21 FPS (5 times faster than Deformable DETR and DINO 4\nFPS) on NVIDIA AGX Orin by TensorRT FP16, outperforming the other\nstate-of-the-art query-based end-to-end object detectors. The code is available\nat https://github.com/LEFTeyex/U-DECN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underwater object detection has higher requirements of running speed and\ndeployment efficiency for the detector due to its specific environmental\nchallenges. NMS of two- or one-stage object detectors and transformer\narchitecture of query-based end-to-end object detectors are not conducive to\ndeployment on underwater embedded devices with limited processing power. As for\nthe detrimental effect of underwater color cast noise, recent underwater object\ndetectors make network architecture or training complex, which also hinders\ntheir application and deployment on underwater vehicle platforms. In this\npaper, we propose the Underwater DECO with improved deNoising training\n(U-DECN), the query-based end-to-end object detector (with ConvNet\nencoder-decoder architecture) for underwater color cast noise that addresses\nthe above problems. We integrate advanced technologies from DETR variants into\nDECO and design optimization methods specifically for the ConvNet architecture,\nincluding Separate Contrastive DeNoising Forward and Deformable Convolution in\nSIM. To address the underwater color cast noise issue, we propose an underwater\ncolor denoising query to improve the generalization of the model for the biased\nobject feature information by different color cast noise. Our U-DECN, with\nResNet-50 backbone, achieves 61.4 AP (50 epochs), 63.3 AP (72 epochs), 64.0 AP\n(100 epochs) on DUO, and 21 FPS (5 times faster than Deformable DETR and DINO 4\nFPS) on NVIDIA AGX Orin by TensorRT FP16, outperforming the other\nstate-of-the-art query-based end-to-end object detectors. The code is available\nat https://github.com/LEFTeyex/U-DECN."
                },
                "authors": [
                    {
                        "name": "Zhuoyan Liu"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Ye Li"
                    }
                ],
                "author_detail": {
                    "name": "Ye Li"
                },
                "author": "Ye Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05780v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05780v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05775v1",
                "updated": "2024-08-11T13:55:58Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    13,
                    55,
                    58,
                    6,
                    224,
                    0
                ],
                "published": "2024-08-11T13:55:58Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    13,
                    55,
                    58,
                    6,
                    224,
                    0
                ],
                "title": "Efficient Test-Time Prompt Tuning for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Test-Time Prompt Tuning for Vision-Language Models"
                },
                "summary": "Vision-language models have showcased impressive zero-shot classification\ncapabilities when equipped with suitable text prompts. Previous studies have\nshown the effectiveness of test-time prompt tuning; however, these methods\ntypically require per-image prompt adaptation during inference, which incurs\nhigh computational budgets and limits scalability and practical deployment. To\novercome this issue, we introduce Self-TPT, a novel framework leveraging\nSelf-supervised learning for efficient Test-time Prompt Tuning. The key aspect\nof Self-TPT is that it turns to efficient predefined class adaptation via\nself-supervised learning, thus avoiding computation-heavy per-image adaptation\nat inference. Self-TPT begins by co-training the self-supervised and the\nclassification task using source data, then applies the self-supervised task\nexclusively for test-time new class adaptation. Specifically, we propose\nContrastive Prompt Learning (CPT) as the key task for self-supervision. CPT is\ndesigned to minimize the intra-class distances while enhancing inter-class\ndistinguishability via contrastive learning. Furthermore, empirical evidence\nsuggests that CPT could closely mimic back-propagated gradients of the\nclassification task, offering a plausible explanation for its effectiveness.\nMotivated by this finding, we further introduce a gradient matching loss to\nexplicitly enhance the gradient similarity. We evaluated Self-TPT across three\nchallenging zero-shot benchmarks. The results consistently demonstrate that\nSelf-TPT not only significantly reduces inference costs but also achieves\nstate-of-the-art performance, effectively balancing the efficiency-efficacy\ntrade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models have showcased impressive zero-shot classification\ncapabilities when equipped with suitable text prompts. Previous studies have\nshown the effectiveness of test-time prompt tuning; however, these methods\ntypically require per-image prompt adaptation during inference, which incurs\nhigh computational budgets and limits scalability and practical deployment. To\novercome this issue, we introduce Self-TPT, a novel framework leveraging\nSelf-supervised learning for efficient Test-time Prompt Tuning. The key aspect\nof Self-TPT is that it turns to efficient predefined class adaptation via\nself-supervised learning, thus avoiding computation-heavy per-image adaptation\nat inference. Self-TPT begins by co-training the self-supervised and the\nclassification task using source data, then applies the self-supervised task\nexclusively for test-time new class adaptation. Specifically, we propose\nContrastive Prompt Learning (CPT) as the key task for self-supervision. CPT is\ndesigned to minimize the intra-class distances while enhancing inter-class\ndistinguishability via contrastive learning. Furthermore, empirical evidence\nsuggests that CPT could closely mimic back-propagated gradients of the\nclassification task, offering a plausible explanation for its effectiveness.\nMotivated by this finding, we further introduce a gradient matching loss to\nexplicitly enhance the gradient similarity. We evaluated Self-TPT across three\nchallenging zero-shot benchmarks. The results consistently demonstrate that\nSelf-TPT not only significantly reduces inference costs but also achieves\nstate-of-the-art performance, effectively balancing the efficiency-efficacy\ntrade-off."
                },
                "authors": [
                    {
                        "name": "Yuhan Zhu"
                    },
                    {
                        "name": "Guozhen Zhang"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Haocheng Shen"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Gangshan Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15186v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15186v2",
                "updated": "2024-08-11T13:54:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    13,
                    54,
                    21,
                    6,
                    224,
                    0
                ],
                "published": "2024-07-21T14:48:23Z",
                "published_parsed": [
                    2024,
                    7,
                    21,
                    14,
                    48,
                    23,
                    6,
                    203,
                    0
                ],
                "title": "A Survey on Employing Large Language Models for Text-to-SQL Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Employing Large Language Models for Text-to-SQL Tasks"
                },
                "summary": "The increasing volume of data stored in relational databases has led to the\nneed for efficient querying and utilization of this data in various sectors.\nHowever, writing SQL queries requires specialized knowledge, which poses a\nchallenge for non-professional users trying to access and query databases.\nText-to-SQL parsing solves this issue by converting natural language queries\ninto SQL queries, thus making database access more accessible for non-expert\nusers. To take advantage of the recent developments in Large Language Models\n(LLMs), a range of new methods have emerged, with a primary focus on prompt\nengineering and fine-tuning. This survey provides a comprehensive overview of\nLLMs in text-to-SQL tasks, discussing benchmark datasets, prompt engineering,\nfine-tuning methods, and future research directions. We hope this review will\nenable readers to gain a broader understanding of the recent advances in this\nfield and offer some insights into its future trajectory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing volume of data stored in relational databases has led to the\nneed for efficient querying and utilization of this data in various sectors.\nHowever, writing SQL queries requires specialized knowledge, which poses a\nchallenge for non-professional users trying to access and query databases.\nText-to-SQL parsing solves this issue by converting natural language queries\ninto SQL queries, thus making database access more accessible for non-expert\nusers. To take advantage of the recent developments in Large Language Models\n(LLMs), a range of new methods have emerged, with a primary focus on prompt\nengineering and fine-tuning. This survey provides a comprehensive overview of\nLLMs in text-to-SQL tasks, discussing benchmark datasets, prompt engineering,\nfine-tuning methods, and future research directions. We hope this review will\nenable readers to gain a broader understanding of the recent advances in this\nfield and offer some insights into its future trajectory."
                },
                "authors": [
                    {
                        "name": "Liang Shi"
                    },
                    {
                        "name": "Zhengju Tang"
                    },
                    {
                        "name": "Nan Zhang"
                    },
                    {
                        "name": "Xiaotong Zhang"
                    },
                    {
                        "name": "Zhi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Yang"
                },
                "author": "Zhi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15186v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15186v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05752v1",
                "updated": "2024-08-11T11:53:29Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    11,
                    53,
                    29,
                    6,
                    224,
                    0
                ],
                "published": "2024-08-11T11:53:29Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    11,
                    53,
                    29,
                    6,
                    224,
                    0
                ],
                "title": "RTF-Q: Unsupervised domain adaptation based retraining-free quantization\n  network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RTF-Q: Unsupervised domain adaptation based retraining-free quantization\n  network"
                },
                "summary": "Performing unsupervised domain adaptation on resource-constrained edge\ndevices is a significant task. Although existing research allows edge devices\nto use subnets with different computational budgets for inference, they often\nrequire expensive pre-training and do not consider the issues of parameter\nprecision redundancy in the model, which is not conducive to the deployment of\nthe model on edge devices. In this paper, we introduce a ReTraining-Free\nQuantized (RTF-Q) network based on unsupervised domain adaptation, featuring\nquantized subnets of varying computational costs that can operate on devices\nwith dynamically changing computation budgets. Our network has three switchable\ndimensions: width (number of channels), input resolution, and quantization\nbit-width. Specifically, we choose subnet dimensions that have minimal impact\non network performance and then directly load the official weight files without\nrequiring expensive and time-consuming pre-training on Imagenet-1K. To further\nreduce the network's computational load and memory usage, we use\nquantization-aware training, reducing the BitOPs of full-precision networks by\nat least 1/16. We propose a training method called SandwichQ for multiple\nquantization bit widths, which can efficiently train multiple quantization\nsubnets. By training in multiple quantization bit-width spaces simultaneously\nand using the proposed SandwichQ rule, we achieve better network performance\ncompared to using a single quantization bit-width alone. Experimental results\nshow that our method achieves classification accuracy comparable to SOTA\nmethods on various UDA tasks, significantly reducing network size and\ncomputational overhead. Code will be available at\nhttps://github.com/dunanyang/RTF-Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performing unsupervised domain adaptation on resource-constrained edge\ndevices is a significant task. Although existing research allows edge devices\nto use subnets with different computational budgets for inference, they often\nrequire expensive pre-training and do not consider the issues of parameter\nprecision redundancy in the model, which is not conducive to the deployment of\nthe model on edge devices. In this paper, we introduce a ReTraining-Free\nQuantized (RTF-Q) network based on unsupervised domain adaptation, featuring\nquantized subnets of varying computational costs that can operate on devices\nwith dynamically changing computation budgets. Our network has three switchable\ndimensions: width (number of channels), input resolution, and quantization\nbit-width. Specifically, we choose subnet dimensions that have minimal impact\non network performance and then directly load the official weight files without\nrequiring expensive and time-consuming pre-training on Imagenet-1K. To further\nreduce the network's computational load and memory usage, we use\nquantization-aware training, reducing the BitOPs of full-precision networks by\nat least 1/16. We propose a training method called SandwichQ for multiple\nquantization bit widths, which can efficiently train multiple quantization\nsubnets. By training in multiple quantization bit-width spaces simultaneously\nand using the proposed SandwichQ rule, we achieve better network performance\ncompared to using a single quantization bit-width alone. Experimental results\nshow that our method achieves classification accuracy comparable to SOTA\nmethods on various UDA tasks, significantly reducing network size and\ncomputational overhead. Code will be available at\nhttps://github.com/dunanyang/RTF-Q."
                },
                "authors": [
                    {
                        "name": "Nanyang Du"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.05365v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.05365v2",
                "updated": "2024-08-11T11:11:32Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    11,
                    11,
                    32,
                    6,
                    224,
                    0
                ],
                "published": "2024-07-07T13:38:05Z",
                "published_parsed": [
                    2024,
                    7,
                    7,
                    13,
                    38,
                    5,
                    6,
                    189,
                    0
                ],
                "title": "ElecBench: a Power Dispatch Evaluation Benchmark for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElecBench: a Power Dispatch Evaluation Benchmark for Large Language\n  Models"
                },
                "summary": "In response to the urgent demand for grid stability and the complex\nchallenges posed by renewable energy integration and electricity market\ndynamics, the power sector increasingly seeks innovative technological\nsolutions. In this context, large language models (LLMs) have become a key\ntechnology to improve efficiency and promote intelligent progress in the power\nsector with their excellent natural language processing, logical reasoning, and\ngeneralization capabilities. Despite their potential, the absence of a\nperformance evaluation benchmark for LLM in the power sector has limited the\neffective application of these technologies. Addressing this gap, our study\nintroduces \"ElecBench\", an evaluation benchmark of LLMs within the power\nsector. ElecBench aims to overcome the shortcomings of existing evaluation\nbenchmarks by providing comprehensive coverage of sector-specific scenarios,\ndeepening the testing of professional knowledge, and enhancing decision-making\nprecision. The framework categorizes scenarios into general knowledge and\nprofessional business, further divided into six core performance metrics:\nfactuality, logicality, stability, security, fairness, and expressiveness, and\nis subdivided into 24 sub-metrics, offering profound insights into the\ncapabilities and limitations of LLM applications in the power sector. To ensure\ntransparency, we have made the complete test set public, evaluating the\nperformance of eight LLMs across various scenarios and metrics. ElecBench\naspires to serve as the standard benchmark for LLM applications in the power\nsector, supporting continuous updates of scenarios, metrics, and models to\ndrive technological progress and application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In response to the urgent demand for grid stability and the complex\nchallenges posed by renewable energy integration and electricity market\ndynamics, the power sector increasingly seeks innovative technological\nsolutions. In this context, large language models (LLMs) have become a key\ntechnology to improve efficiency and promote intelligent progress in the power\nsector with their excellent natural language processing, logical reasoning, and\ngeneralization capabilities. Despite their potential, the absence of a\nperformance evaluation benchmark for LLM in the power sector has limited the\neffective application of these technologies. Addressing this gap, our study\nintroduces \"ElecBench\", an evaluation benchmark of LLMs within the power\nsector. ElecBench aims to overcome the shortcomings of existing evaluation\nbenchmarks by providing comprehensive coverage of sector-specific scenarios,\ndeepening the testing of professional knowledge, and enhancing decision-making\nprecision. The framework categorizes scenarios into general knowledge and\nprofessional business, further divided into six core performance metrics:\nfactuality, logicality, stability, security, fairness, and expressiveness, and\nis subdivided into 24 sub-metrics, offering profound insights into the\ncapabilities and limitations of LLM applications in the power sector. To ensure\ntransparency, we have made the complete test set public, evaluating the\nperformance of eight LLMs across various scenarios and metrics. ElecBench\naspires to serve as the standard benchmark for LLM applications in the power\nsector, supporting continuous updates of scenarios, metrics, and models to\ndrive technological progress and application."
                },
                "authors": [
                    {
                        "name": "Xiyuan Zhou"
                    },
                    {
                        "name": "Huan Zhao"
                    },
                    {
                        "name": "Yuheng Cheng"
                    },
                    {
                        "name": "Yuji Cao"
                    },
                    {
                        "name": "Gaoqi Liang"
                    },
                    {
                        "name": "Guolong Liu"
                    },
                    {
                        "name": "Wenxuan Liu"
                    },
                    {
                        "name": "Yan Xu"
                    },
                    {
                        "name": "Junhua Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junhua Zhao"
                },
                "author": "Junhua Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.05365v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.05365v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.03533v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.03533v2",
                "updated": "2024-08-11T09:08:59Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    9,
                    8,
                    59,
                    6,
                    224,
                    0
                ],
                "published": "2024-08-07T04:20:28Z",
                "published_parsed": [
                    2024,
                    8,
                    7,
                    4,
                    20,
                    28,
                    2,
                    220,
                    0
                ],
                "title": "Lifelong Personalized Low-Rank Adaptation of Large Language Models for\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifelong Personalized Low-Rank Adaptation of Large Language Models for\n  Recommendation"
                },
                "summary": "We primarily focus on the field of large language models (LLMs) for\nrecommendation, which has been actively explored recently and poses a\nsignificant challenge in effectively enhancing recommender systems with logical\nreasoning abilities and open-world knowledge. Current mainstream efforts mainly\ncenter around injecting personalized information from recommendation models\ninto LLMs by customizing input templates or aligning representations between\nsemantic and recommendation spaces at the prediction layer. However, they face\nthree significant limitations: (1) LoRA is mostly used as a core component in\nexisting works, but personalization is not well established in LoRA parameters\nas the LoRA matrix shared by every user may not cater to different users'\ncharacteristics, leading to suboptimal performance. (2) Although lifelong\npersonalized behavior sequences are ideal for personalization, their use raises\neffectiveness and efficiency issues since LLMs require escalating training and\ninference time to extend text lengths. (3) Existing approaches aren't scalable\nfor large datasets due to training efficiency constraints. Thus, LLMs only see\na small fraction of the datasets (e.g., less than 10%) instead of the whole\ndatasets, limiting their exposure to the full training space. To address these\nproblems, we propose RecLoRA. This model incorporates a Personalized LoRA\nmodule that maintains independent LoRAs for different users and a Long-Short\nModality Retriever that retrieves different history lengths for different\nmodalities, significantly improving performance while adding minimal time cost.\nFurthermore, we design a Few2Many Learning Strategy, using a conventional\nrecommendation model as a lens to magnify small training spaces to full spaces.\nExtensive experiments on public datasets demonstrate the efficacy of our\nRecLoRA compared to existing baseline models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We primarily focus on the field of large language models (LLMs) for\nrecommendation, which has been actively explored recently and poses a\nsignificant challenge in effectively enhancing recommender systems with logical\nreasoning abilities and open-world knowledge. Current mainstream efforts mainly\ncenter around injecting personalized information from recommendation models\ninto LLMs by customizing input templates or aligning representations between\nsemantic and recommendation spaces at the prediction layer. However, they face\nthree significant limitations: (1) LoRA is mostly used as a core component in\nexisting works, but personalization is not well established in LoRA parameters\nas the LoRA matrix shared by every user may not cater to different users'\ncharacteristics, leading to suboptimal performance. (2) Although lifelong\npersonalized behavior sequences are ideal for personalization, their use raises\neffectiveness and efficiency issues since LLMs require escalating training and\ninference time to extend text lengths. (3) Existing approaches aren't scalable\nfor large datasets due to training efficiency constraints. Thus, LLMs only see\na small fraction of the datasets (e.g., less than 10%) instead of the whole\ndatasets, limiting their exposure to the full training space. To address these\nproblems, we propose RecLoRA. This model incorporates a Personalized LoRA\nmodule that maintains independent LoRAs for different users and a Long-Short\nModality Retriever that retrieves different history lengths for different\nmodalities, significantly improving performance while adding minimal time cost.\nFurthermore, we design a Few2Many Learning Strategy, using a conventional\nrecommendation model as a lens to magnify small training spaces to full spaces.\nExtensive experiments on public datasets demonstrate the efficacy of our\nRecLoRA compared to existing baseline models."
                },
                "authors": [
                    {
                        "name": "Jiachen Zhu"
                    },
                    {
                        "name": "Jianghao Lin"
                    },
                    {
                        "name": "Xinyi Dai"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Rong Shan"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yong Yu"
                    },
                    {
                        "name": "Weinan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weinan Zhang"
                },
                "author": "Weinan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.03533v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.03533v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05715v1",
                "updated": "2024-08-11T07:53:51Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    7,
                    53,
                    51,
                    6,
                    224,
                    0
                ],
                "published": "2024-08-11T07:53:51Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    7,
                    53,
                    51,
                    6,
                    224,
                    0
                ],
                "title": "Top Pass: Improve Code Generation by Pass@k-Maximized Code Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top Pass: Improve Code Generation by Pass@k-Maximized Code Ranking"
                },
                "summary": "Code generation has been greatly enhanced by the profound advancements in\nLarge Language Models (LLMs) recently. Nevertheless, such LLM-based code\ngeneration approaches still struggle to generate error-free code in a few tries\nwhen faced with complex problems. To address this, the prevailing strategy is\nto sample a huge number of candidate programs, with the hope of any one in them\ncould work. However, users of code generation systems usually expect to find a\ncorrect program by reviewing or testing only a small number of code candidates.\nOtherwise, the system would be unhelpful. In this paper, we propose Top Pass, a\ncode ranking approach that identifies potential correct solutions from a large\nnumber of candidates. Top Pass directly optimizes the pass@k loss function,\nenhancing the quality at the top of the candidate list. This enables the user\nto find the correct solution within as few tries as possible. Experimental\nresults on four benchmarks indicate that our Top Pass method enhances the\nusability of code generation models by producing better ranking results,\nparticularly achieving a 32.9\\% relative improvement in pass@1 on CodeContests\nwhen compared to the state-of-the-art ranking method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code generation has been greatly enhanced by the profound advancements in\nLarge Language Models (LLMs) recently. Nevertheless, such LLM-based code\ngeneration approaches still struggle to generate error-free code in a few tries\nwhen faced with complex problems. To address this, the prevailing strategy is\nto sample a huge number of candidate programs, with the hope of any one in them\ncould work. However, users of code generation systems usually expect to find a\ncorrect program by reviewing or testing only a small number of code candidates.\nOtherwise, the system would be unhelpful. In this paper, we propose Top Pass, a\ncode ranking approach that identifies potential correct solutions from a large\nnumber of candidates. Top Pass directly optimizes the pass@k loss function,\nenhancing the quality at the top of the candidate list. This enables the user\nto find the correct solution within as few tries as possible. Experimental\nresults on four benchmarks indicate that our Top Pass method enhances the\nusability of code generation models by producing better ranking results,\nparticularly achieving a 32.9\\% relative improvement in pass@1 on CodeContests\nwhen compared to the state-of-the-art ranking method."
                },
                "authors": [
                    {
                        "name": "Zhi-Cun Lyu"
                    },
                    {
                        "name": "Xin-Ye Li"
                    },
                    {
                        "name": "Zheng Xie"
                    },
                    {
                        "name": "Ming Li"
                    }
                ],
                "author_detail": {
                    "name": "Ming Li"
                },
                "author": "Ming Li",
                "arxiv_comment": "Accepted by Frontier of Computer Science",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05712v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05712v1",
                "updated": "2024-08-11T07:28:35Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    7,
                    28,
                    35,
                    6,
                    224,
                    0
                ],
                "published": "2024-08-11T07:28:35Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    7,
                    28,
                    35,
                    6,
                    224,
                    0
                ],
                "title": "DeepAir: A Multi-Agent Deep Reinforcement Learning Based Scheme for an\n  Unknown User Location Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepAir: A Multi-Agent Deep Reinforcement Learning Based Scheme for an\n  Unknown User Location Problem"
                },
                "summary": "The deployment of unmanned aerial vehicles (UAVs) in many different settings\nhas provided various solutions and strategies for networking paradigms.\nTherefore, it reduces the complexity of the developments for the existing\nproblems, which otherwise require more sophisticated approaches. One of those\nexisting problems is the unknown user locations in an infrastructure-less\nenvironment in which users cannot connect to any communication device or\ncomputation-providing server, which is essential to task offloading in order to\nachieve the required quality of service (QoS). Therefore, in this study, we\ninvestigate this problem thoroughly and propose a novel deep reinforcement\nlearning (DRL) based scheme, DeepAir. DeepAir considers all of the necessary\nsteps including sensing, localization, resource allocation, and multi-access\nedge computing (MEC) to achieve QoS requirements for the offloaded tasks\nwithout violating the maximum tolerable delay. To this end, we use two types of\nUAVs including detector UAVs, and serving UAVs. We utilize detector UAVs as DRL\nagents which ensure sensing, localization, and resource allocation. On the\nother hand, we utilize serving UAVs to provide MEC features. Our experiments\nshow that DeepAir provides a high task success rate by deploying fewer detector\nUAVs in the environment, which includes different numbers of users and user\nattraction points, compared to benchmark methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of unmanned aerial vehicles (UAVs) in many different settings\nhas provided various solutions and strategies for networking paradigms.\nTherefore, it reduces the complexity of the developments for the existing\nproblems, which otherwise require more sophisticated approaches. One of those\nexisting problems is the unknown user locations in an infrastructure-less\nenvironment in which users cannot connect to any communication device or\ncomputation-providing server, which is essential to task offloading in order to\nachieve the required quality of service (QoS). Therefore, in this study, we\ninvestigate this problem thoroughly and propose a novel deep reinforcement\nlearning (DRL) based scheme, DeepAir. DeepAir considers all of the necessary\nsteps including sensing, localization, resource allocation, and multi-access\nedge computing (MEC) to achieve QoS requirements for the offloaded tasks\nwithout violating the maximum tolerable delay. To this end, we use two types of\nUAVs including detector UAVs, and serving UAVs. We utilize detector UAVs as DRL\nagents which ensure sensing, localization, and resource allocation. On the\nother hand, we utilize serving UAVs to provide MEC features. Our experiments\nshow that DeepAir provides a high task success rate by deploying fewer detector\nUAVs in the environment, which includes different numbers of users and user\nattraction points, compared to benchmark methods."
                },
                "authors": [
                    {
                        "name": "Baris Yamansavascilar"
                    },
                    {
                        "name": "Atay Ozgovde"
                    },
                    {
                        "name": "Cem Ersoy"
                    }
                ],
                "author_detail": {
                    "name": "Cem Ersoy"
                },
                "author": "Cem Ersoy",
                "arxiv_comment": "12 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05712v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05712v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.09613v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.09613v3",
                "updated": "2024-08-11T05:46:15Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    5,
                    46,
                    15,
                    6,
                    224,
                    0
                ],
                "published": "2023-11-16T06:51:46Z",
                "published_parsed": [
                    2023,
                    11,
                    16,
                    6,
                    51,
                    46,
                    3,
                    320,
                    0
                ],
                "title": "Digital Socrates: Evaluating LLMs through Explanation Critiques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Socrates: Evaluating LLMs through Explanation Critiques"
                },
                "summary": "While LLMs can provide reasoned explanations along with their answers, the\nnature and quality of those explanations are still poorly understood. In\nresponse, our goal is to define a detailed way of characterizing the\nexplanation capabilities of modern models and to create a nuanced,\ninterpretable explanation evaluation tool that can generate such\ncharacterizations automatically, without relying on expensive API calls or\nhuman annotations. Our approach is to (a) define the new task of explanation\ncritiquing - identifying and categorizing any main flaw in an explanation and\nproviding suggestions to address the flaw, (b) create a sizeable,\nhuman-verified dataset for this task, and (c) train an open-source, automatic\ncritique model (called Digital Socrates) using this data. Through quantitative\nand qualitative analysis, we demonstrate how Digital Socrates is useful for\nrevealing insights about student models by examining their reasoning chains,\nand how it can provide high-quality, nuanced, automatic evaluation of those\nmodel explanations for the first time. Digital Socrates thus fills an important\ngap in evaluation tools for understanding and improving the explanation\nbehavior of models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs can provide reasoned explanations along with their answers, the\nnature and quality of those explanations are still poorly understood. In\nresponse, our goal is to define a detailed way of characterizing the\nexplanation capabilities of modern models and to create a nuanced,\ninterpretable explanation evaluation tool that can generate such\ncharacterizations automatically, without relying on expensive API calls or\nhuman annotations. Our approach is to (a) define the new task of explanation\ncritiquing - identifying and categorizing any main flaw in an explanation and\nproviding suggestions to address the flaw, (b) create a sizeable,\nhuman-verified dataset for this task, and (c) train an open-source, automatic\ncritique model (called Digital Socrates) using this data. Through quantitative\nand qualitative analysis, we demonstrate how Digital Socrates is useful for\nrevealing insights about student models by examining their reasoning chains,\nand how it can provide high-quality, nuanced, automatic evaluation of those\nmodel explanations for the first time. Digital Socrates thus fills an important\ngap in evaluation tools for understanding and improving the explanation\nbehavior of models."
                },
                "authors": [
                    {
                        "name": "Yuling Gu"
                    },
                    {
                        "name": "Oyvind Tafjord"
                    },
                    {
                        "name": "Peter Clark"
                    }
                ],
                "author_detail": {
                    "name": "Peter Clark"
                },
                "author": "Peter Clark",
                "arxiv_comment": "ACL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.09613v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.09613v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.20174v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.20174v2",
                "updated": "2024-08-11T05:30:05Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    5,
                    30,
                    5,
                    6,
                    224,
                    0
                ],
                "published": "2024-07-29T17:04:34Z",
                "published_parsed": [
                    2024,
                    7,
                    29,
                    17,
                    4,
                    34,
                    0,
                    211,
                    0
                ],
                "title": "Advancing Multimodal Large Language Models in Chart Question Answering\n  with Visualization-Referenced Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Multimodal Large Language Models in Chart Question Answering\n  with Visualization-Referenced Instruction Tuning"
                },
                "summary": "Emerging multimodal large language models (MLLMs) exhibit great potential for\nchart question answering (CQA). Recent efforts primarily focus on scaling up\ntraining datasets (i.e., charts, data tables, and question-answer (QA) pairs)\nthrough data collection and synthesis. However, our empirical study on existing\nMLLMs and CQA datasets reveals notable gaps. First, current data collection and\nsynthesis focus on data volume and lack consideration of fine-grained visual\nencodings and QA tasks, resulting in unbalanced data distribution divergent\nfrom practical CQA scenarios. Second, existing work follows the training recipe\nof the base MLLMs initially designed for natural images, under-exploring the\nadaptation to unique chart characteristics, such as rich text elements. To fill\nthe gap, we propose a visualization-referenced instruction tuning approach to\nguide the training dataset enhancement and model development. Specifically, we\npropose a novel data engine to effectively filter diverse and high-quality data\nfrom existing datasets and subsequently refine and augment the data using\nLLM-based generation techniques to better align with practical QA tasks and\nvisual encodings. Then, to facilitate the adaptation to chart characteristics,\nwe utilize the enriched data to train an MLLM by unfreezing the vision encoder\nand incorporating a mixture-of-resolution adaptation strategy for enhanced\nfine-grained recognition. Experimental results validate the effectiveness of\nour approach. Even with fewer training examples, our model consistently\noutperforms state-of-the-art CQA models on established benchmarks. We also\ncontribute a dataset split as a benchmark for future research. Source codes and\ndatasets of this paper are available at\nhttps://github.com/zengxingchen/ChartQA-MLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging multimodal large language models (MLLMs) exhibit great potential for\nchart question answering (CQA). Recent efforts primarily focus on scaling up\ntraining datasets (i.e., charts, data tables, and question-answer (QA) pairs)\nthrough data collection and synthesis. However, our empirical study on existing\nMLLMs and CQA datasets reveals notable gaps. First, current data collection and\nsynthesis focus on data volume and lack consideration of fine-grained visual\nencodings and QA tasks, resulting in unbalanced data distribution divergent\nfrom practical CQA scenarios. Second, existing work follows the training recipe\nof the base MLLMs initially designed for natural images, under-exploring the\nadaptation to unique chart characteristics, such as rich text elements. To fill\nthe gap, we propose a visualization-referenced instruction tuning approach to\nguide the training dataset enhancement and model development. Specifically, we\npropose a novel data engine to effectively filter diverse and high-quality data\nfrom existing datasets and subsequently refine and augment the data using\nLLM-based generation techniques to better align with practical QA tasks and\nvisual encodings. Then, to facilitate the adaptation to chart characteristics,\nwe utilize the enriched data to train an MLLM by unfreezing the vision encoder\nand incorporating a mixture-of-resolution adaptation strategy for enhanced\nfine-grained recognition. Experimental results validate the effectiveness of\nour approach. Even with fewer training examples, our model consistently\noutperforms state-of-the-art CQA models on established benchmarks. We also\ncontribute a dataset split as a benchmark for future research. Source codes and\ndatasets of this paper are available at\nhttps://github.com/zengxingchen/ChartQA-MLLM."
                },
                "authors": [
                    {
                        "name": "Xingchen Zeng"
                    },
                    {
                        "name": "Haichuan Lin"
                    },
                    {
                        "name": "Yilin Ye"
                    },
                    {
                        "name": "Wei Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zeng"
                },
                "author": "Wei Zeng",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.20174v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.20174v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17638v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17638v2",
                "updated": "2024-08-11T05:21:33Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    5,
                    21,
                    33,
                    6,
                    224,
                    0
                ],
                "published": "2024-07-24T21:06:40Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    21,
                    6,
                    40,
                    2,
                    206,
                    0
                ],
                "title": "Time Matters: Examine Temporal Effects on Biomedical Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Matters: Examine Temporal Effects on Biomedical Language Models"
                },
                "summary": "Time roots in applying language models for biomedical applications: models\nare trained on historical data and will be deployed for new or future data,\nwhich may vary from training data. While increasing biomedical tasks have\nemployed state-of-the-art language models, there are very few studies have\nexamined temporal effects on biomedical models when data usually shifts across\ndevelopment and deployment. This study fills the gap by statistically probing\nrelations between language model performance and data shifts across three\nbiomedical tasks. We deploy diverse metrics to evaluate model performance,\ndistance methods to measure data drifts, and statistical methods to quantify\ntemporal effects on biomedical language models. Our study shows that time\nmatters for deploying biomedical language models, while the degree of\nperformance degradation varies by biomedical tasks and statistical\nquantification approaches. We believe this study can establish a solid\nbenchmark to evaluate and assess temporal effects on deploying biomedical\nlanguage models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time roots in applying language models for biomedical applications: models\nare trained on historical data and will be deployed for new or future data,\nwhich may vary from training data. While increasing biomedical tasks have\nemployed state-of-the-art language models, there are very few studies have\nexamined temporal effects on biomedical models when data usually shifts across\ndevelopment and deployment. This study fills the gap by statistically probing\nrelations between language model performance and data shifts across three\nbiomedical tasks. We deploy diverse metrics to evaluate model performance,\ndistance methods to measure data drifts, and statistical methods to quantify\ntemporal effects on biomedical language models. Our study shows that time\nmatters for deploying biomedical language models, while the degree of\nperformance degradation varies by biomedical tasks and statistical\nquantification approaches. We believe this study can establish a solid\nbenchmark to evaluate and assess temporal effects on deploying biomedical\nlanguage models."
                },
                "authors": [
                    {
                        "name": "Weisi Liu"
                    },
                    {
                        "name": "Zhe He"
                    },
                    {
                        "name": "Xiaolei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaolei Huang"
                },
                "author": "Xiaolei Huang",
                "arxiv_comment": "Accepted to AMIA 2024 Annual Symposium",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17638v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17638v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.00855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.00855v2",
                "updated": "2024-08-11T05:17:21Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    5,
                    17,
                    21,
                    6,
                    224,
                    0
                ],
                "published": "2024-08-01T18:09:40Z",
                "published_parsed": [
                    2024,
                    8,
                    1,
                    18,
                    9,
                    40,
                    3,
                    214,
                    0
                ],
                "title": "HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and\n  Style Generation in Fashion Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HAIGEN: Towards Human-AI Collaboration for Facilitating Creativity and\n  Style Generation in Fashion Design"
                },
                "summary": "The process of fashion design usually involves sketching, refining, and\ncoloring, with designers drawing inspiration from various images to fuel their\ncreative endeavors. However, conventional image search methods often yield\nirrelevant results, impeding the design process. Moreover, creating and\ncoloring sketches can be time-consuming and demanding, acting as a bottleneck\nin the design workflow. In this work, we introduce HAIGEN (Human-AI\nCollaboration for GENeration), an efficient fashion design system for Human-AI\ncollaboration developed to aid designers. Specifically, HAIGEN consists of four\nmodules. T2IM, located in the cloud, generates reference inspiration images\ndirectly from text prompts. With three other modules situated locally, the I2SM\nbatch generates the image material library into a certain designer-style sketch\nmaterial library. The SRM recommends similar sketches in the generated library\nto designers for further refinement, and the STM colors the refined sketch\naccording to the styles of inspiration images. Through our system, any designer\ncan perform local personalized fine-tuning and leverage the powerful generation\ncapabilities of large models in the cloud, streamlining the entire design\ndevelopment process. Given that our approach integrates both cloud and local\nmodel deployment schemes, it effectively safeguards design privacy by avoiding\nthe need to upload personalized data from local designers. We validated the\neffectiveness of each module through extensive qualitative and quantitative\nexperiments. User surveys also confirmed that HAIGEN offers significant\nadvantages in design efficiency, positioning it as a new generation of aid-tool\nfor designers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The process of fashion design usually involves sketching, refining, and\ncoloring, with designers drawing inspiration from various images to fuel their\ncreative endeavors. However, conventional image search methods often yield\nirrelevant results, impeding the design process. Moreover, creating and\ncoloring sketches can be time-consuming and demanding, acting as a bottleneck\nin the design workflow. In this work, we introduce HAIGEN (Human-AI\nCollaboration for GENeration), an efficient fashion design system for Human-AI\ncollaboration developed to aid designers. Specifically, HAIGEN consists of four\nmodules. T2IM, located in the cloud, generates reference inspiration images\ndirectly from text prompts. With three other modules situated locally, the I2SM\nbatch generates the image material library into a certain designer-style sketch\nmaterial library. The SRM recommends similar sketches in the generated library\nto designers for further refinement, and the STM colors the refined sketch\naccording to the styles of inspiration images. Through our system, any designer\ncan perform local personalized fine-tuning and leverage the powerful generation\ncapabilities of large models in the cloud, streamlining the entire design\ndevelopment process. Given that our approach integrates both cloud and local\nmodel deployment schemes, it effectively safeguards design privacy by avoiding\nthe need to upload personalized data from local designers. We validated the\neffectiveness of each module through extensive qualitative and quantitative\nexperiments. User surveys also confirmed that HAIGEN offers significant\nadvantages in design efficiency, positioning it as a new generation of aid-tool\nfor designers."
                },
                "authors": [
                    {
                        "name": "Jianan Jiang"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Hanhui Deng"
                    },
                    {
                        "name": "Yidan Long"
                    },
                    {
                        "name": "Wenyi Tang"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Can Liu"
                    },
                    {
                        "name": "Zhanpeng Jin"
                    },
                    {
                        "name": "Wenlei Zhang"
                    },
                    {
                        "name": "Tangquan Qi"
                    }
                ],
                "author_detail": {
                    "name": "Tangquan Qi"
                },
                "author": "Tangquan Qi",
                "arxiv_doi": "10.1145/3678518",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3678518",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.00855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.00855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by Proceedings of the ACM on Interactive, Mobile, Wearable\n  and Ubiquitous Technologies (ACM IMWUT/UbiComp 2024)",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02539v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02539v2",
                "updated": "2024-08-11T05:15:46Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    5,
                    15,
                    46,
                    6,
                    224,
                    0
                ],
                "published": "2024-06-04T17:56:28Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    56,
                    28,
                    1,
                    156,
                    0
                ],
                "title": "Parrot: Multilingual Visual Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parrot: Multilingual Visual Instruction Tuning"
                },
                "summary": "The rapid development of Multimodal Large Language Models (MLLMs) like GPT-4V\nhas marked a significant step towards artificial general intelligence. Existing\nmethods mainly focus on aligning vision encoders with LLMs through supervised\nfine-tuning (SFT) to endow LLMs with multimodal abilities, making MLLMs'\ninherent ability to react to multiple languages progressively deteriorate as\nthe training process evolves. We empirically find that the imbalanced SFT\ndatasets, primarily composed of English-centric image-text pairs, lead to\nsignificantly reduced performance in non-English languages. This is due to the\nfailure of aligning the vision encoder and LLM with multilingual tokens during\nthe SFT process. In this paper, we introduce Parrot, a novel method that\nutilizes textual guidance to drive visual token alignment at the language\nlevel. Parrot makes the visual tokens condition on diverse language inputs and\nuses Mixture-of-Experts (MoE) to promote the alignment of multilingual tokens.\nSpecifically, to enhance non-English visual tokens alignment, we compute the\ncross-attention using the initial visual features and textual embeddings, the\nresult of which is then fed into the MoE router to select the most relevant\nexperts. The selected experts subsequently convert the initial visual tokens\ninto language-specific visual tokens. Moreover, considering the current lack of\nbenchmarks for evaluating multilingual capabilities within the field, we\ncollect and make available a Massive Multilingual Multimodal Benchmark which\nincludes 6 languages, 15 categories, and 12,000 questions, named as MMMB. Our\nmethod not only demonstrates state-of-the-art performance on multilingual\nMMBench and MMMB, but also excels across a broad range of multimodal tasks.\nBoth the source code and the training dataset of Parrot will be made publicly\navailable. Code is available at: https://github.com/AIDC-AI/Parrot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Multimodal Large Language Models (MLLMs) like GPT-4V\nhas marked a significant step towards artificial general intelligence. Existing\nmethods mainly focus on aligning vision encoders with LLMs through supervised\nfine-tuning (SFT) to endow LLMs with multimodal abilities, making MLLMs'\ninherent ability to react to multiple languages progressively deteriorate as\nthe training process evolves. We empirically find that the imbalanced SFT\ndatasets, primarily composed of English-centric image-text pairs, lead to\nsignificantly reduced performance in non-English languages. This is due to the\nfailure of aligning the vision encoder and LLM with multilingual tokens during\nthe SFT process. In this paper, we introduce Parrot, a novel method that\nutilizes textual guidance to drive visual token alignment at the language\nlevel. Parrot makes the visual tokens condition on diverse language inputs and\nuses Mixture-of-Experts (MoE) to promote the alignment of multilingual tokens.\nSpecifically, to enhance non-English visual tokens alignment, we compute the\ncross-attention using the initial visual features and textual embeddings, the\nresult of which is then fed into the MoE router to select the most relevant\nexperts. The selected experts subsequently convert the initial visual tokens\ninto language-specific visual tokens. Moreover, considering the current lack of\nbenchmarks for evaluating multilingual capabilities within the field, we\ncollect and make available a Massive Multilingual Multimodal Benchmark which\nincludes 6 languages, 15 categories, and 12,000 questions, named as MMMB. Our\nmethod not only demonstrates state-of-the-art performance on multilingual\nMMBench and MMMB, but also excels across a broad range of multimodal tasks.\nBoth the source code and the training dataset of Parrot will be made publicly\navailable. Code is available at: https://github.com/AIDC-AI/Parrot."
                },
                "authors": [
                    {
                        "name": "Hai-Long Sun"
                    },
                    {
                        "name": "Da-Wei Zhou"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Shiyin Lu"
                    },
                    {
                        "name": "Chao Yi"
                    },
                    {
                        "name": "Qing-Guo Chen"
                    },
                    {
                        "name": "Zhao Xu"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    },
                    {
                        "name": "De-Chuan Zhan"
                    },
                    {
                        "name": "Han-Jia Ye"
                    }
                ],
                "author_detail": {
                    "name": "Han-Jia Ye"
                },
                "author": "Han-Jia Ye",
                "arxiv_comment": "Code is available at: https://github.com/AIDC-AI/Parrot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02539v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02539v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05694v1",
                "updated": "2024-08-11T04:48:54Z",
                "updated_parsed": [
                    2024,
                    8,
                    11,
                    4,
                    48,
                    54,
                    6,
                    224,
                    0
                ],
                "published": "2024-08-11T04:48:54Z",
                "published_parsed": [
                    2024,
                    8,
                    11,
                    4,
                    48,
                    54,
                    6,
                    224,
                    0
                ],
                "title": "ICSFuzz: Collision Detector Bug Discovery in Autonomous Driving\n  Simulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICSFuzz: Collision Detector Bug Discovery in Autonomous Driving\n  Simulators"
                },
                "summary": "With the increasing adoption of autonomous vehicles, ensuring the reliability\nof autonomous driving systems (ADSs) deployed on autonomous vehicles has become\na significant concern. Driving simulators have emerged as crucial platforms for\ntesting autonomous driving systems, offering realistic, dynamic, and\nconfigurable environments. However, existing simulation-based ADS testers have\nlargely overlooked the reliability of the simulators, potentially leading to\noverlooked violation scenarios and subsequent safety security risks during\nreal-world deployment. In our investigations, we identified that collision\ndetectors in simulators could fail to detect and report collisions in certain\ncollision scenarios, referred to as ignored collision scenarios.\n  This paper aims to systematically discover ignored collision scenarios to\nimprove the reliability of autonomous driving simulators. To this end, we\npresent ICSFuzz, a black-box fuzzing approach to discover ignored collision\nscenarios efficiently. Drawing upon the fact that the ignored collision\nscenarios are a sub-type of collision scenarios, our approach starts with the\ndetermined collision scenarios. Following the guidance provided by empirically\nstudied factors contributing to collisions, we selectively mutate arbitrary\ncollision scenarios in a step-wise manner toward the ignored collision\nscenarios and effectively discover them.\n  We compare ICSFuzz with DriveFuzz, a state-of-the-art simulation-based ADS\ntesting method, by replacing its oracle with our ignored-collision-aware\noracle. The evaluation demonstrates that ICSFuzz outperforms DriveFuzz by\nfinding 10-20x more ignored collision scenarios with a 20-70x speedup. All the\ndiscovered ignored collisions have been confirmed by developers with one CVE ID\nassigned.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing adoption of autonomous vehicles, ensuring the reliability\nof autonomous driving systems (ADSs) deployed on autonomous vehicles has become\na significant concern. Driving simulators have emerged as crucial platforms for\ntesting autonomous driving systems, offering realistic, dynamic, and\nconfigurable environments. However, existing simulation-based ADS testers have\nlargely overlooked the reliability of the simulators, potentially leading to\noverlooked violation scenarios and subsequent safety security risks during\nreal-world deployment. In our investigations, we identified that collision\ndetectors in simulators could fail to detect and report collisions in certain\ncollision scenarios, referred to as ignored collision scenarios.\n  This paper aims to systematically discover ignored collision scenarios to\nimprove the reliability of autonomous driving simulators. To this end, we\npresent ICSFuzz, a black-box fuzzing approach to discover ignored collision\nscenarios efficiently. Drawing upon the fact that the ignored collision\nscenarios are a sub-type of collision scenarios, our approach starts with the\ndetermined collision scenarios. Following the guidance provided by empirically\nstudied factors contributing to collisions, we selectively mutate arbitrary\ncollision scenarios in a step-wise manner toward the ignored collision\nscenarios and effectively discover them.\n  We compare ICSFuzz with DriveFuzz, a state-of-the-art simulation-based ADS\ntesting method, by replacing its oracle with our ignored-collision-aware\noracle. The evaluation demonstrates that ICSFuzz outperforms DriveFuzz by\nfinding 10-20x more ignored collision scenarios with a 20-70x speedup. All the\ndiscovered ignored collisions have been confirmed by developers with one CVE ID\nassigned."
                },
                "authors": [
                    {
                        "name": "Weiwei Fu"
                    },
                    {
                        "name": "Heqing Huang"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Jin Huang"
                    },
                    {
                        "name": "Wei-Bin Lee"
                    },
                    {
                        "name": "Jianping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianping Wang"
                },
                "author": "Jianping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]