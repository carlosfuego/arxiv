[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.07523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v1",
                "updated": "2025-01-13T17:50:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v2",
                "updated": "2025-01-13T17:34:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    34,
                    22,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian PÃ¶ppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v4",
                "updated": "2025-01-13T09:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    33,
                    25,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel KÃ¼pper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_doi": "10.1109/PST62714.2024.10788053",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/PST62714.2024.10788053",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07533v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024)",
                "arxiv_journal_ref": "2024 21st Annual International Conference on Privacy, Security and\n  Trust (PST), 2024, pp. 1-11",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v1",
                "updated": "2025-01-13T04:31:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v5",
                "updated": "2025-01-13T03:11:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    3,
                    11,
                    28,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "StÃ©phane Pouget"
                    },
                    {
                        "name": "Louis-NoÃ«l Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v2",
                "updated": "2025-01-12T20:08:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    20,
                    8,
                    46,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06872v1",
                "updated": "2025-01-12T17:01:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T17:01:40Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    1,
                    40,
                    6,
                    12,
                    0
                ],
                "title": "On Optimizing Locality of Graph Transposition on Modern Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Optimizing Locality of Graph Transposition on Modern Architectures"
                },
                "summary": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the shared-memory Graph Transposition (GT) problem, a\nfundamental graph algorithm that is widely used in graph analytics and\nscientific computing.\n  Previous GT algorithms have significant memory requirements that are\nproportional to the number of vertices and threads which obstructs their use on\nlarge graphs. Moreover, atomic memory operations have become comparably fast on\nrecent CPU architectures, which creates new opportunities for improving the\nperformance of concurrent atomic accesses in GT.\n  We design PoTra, a GT algorithm which leverages graph structure and processor\nand memory architecture to optimize locality and performance. PoTra limits the\nsize of additional data structures close to CPU cache sizes and utilizes the\nskewed degree distribution of graph datasets to optimize locality and\nperformance. We present the performance model of PoTra to explain the\nconnection between cache and memory response times and graph locality.\n  Our evaluation of PoTra on three CPU architectures and 20 real-world and\nsynthetic graph datasets with up to 128 billion edges demonstrates that PoTra\nachieves up to 8.7 times speedup compared to previous works and if there is a\nperformance loss it remains limited to 15.7%, on average."
                },
                "authors": [
                    {
                        "name": "Mohsen Koohi Esfahani"
                    },
                    {
                        "name": "Hans Vandierendonck"
                    }
                ],
                "author_detail": {
                    "name": "Hans Vandierendonck"
                },
                "author": "Hans Vandierendonck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06807v1",
                "updated": "2025-01-12T13:18:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T13:18:04Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    13,
                    18,
                    4,
                    6,
                    12,
                    0
                ],
                "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private Large\n  Language Model Inference"
                },
                "summary": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private large language model (LLM) inference based on secure multi-party\ncomputation (MPC) offers cryptographically-secure protection for both user\nprompt and proprietary model weights. However, it suffers from large latency\noverhead especially for long input sequences. While key-value (KV) cache\neviction algorithms have been proposed to reduce the computation and memory\ncost for plaintext inference, they are not designed for MPC and cannot benefit\nprivate inference easily. In this paper, we propose an accurate and\nMPC-friendly KV cache eviction framework, dubbed MPCache. MPCache is built on\nthe observation that historical tokens in a long sequence may have different\neffects on the downstream decoding. Hence, MPCache combines a look-once static\neviction algorithm to discard unimportant tokens and a query-aware dynamic\nselection algorithm to further select a small subset of tokens for attention\ncomputation. As existing dynamic selection algorithms incur too much latency,\nwe propose a series of optimizations to drastically reduce the KV cache\nselection overhead, including MPC-friendly similarity approximation,\nhierarchical KV cache clustering, and cross-layer index sharing strategy. With\nextensive experiments, we demonstrate that MPCache consistently outperforms\nprior-art KV cache eviction baselines across different LLM generation tasks and\nachieves 1.8~2.01x and 3.39~8.37x decoding latency and communication reduction\non different sequence lengths, respectively."
                },
                "authors": [
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Ye Dong"
                    },
                    {
                        "name": "Jinjin Zhou"
                    },
                    {
                        "name": "Junming Ma"
                    },
                    {
                        "name": "Jin Tan"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.02882v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.02882v2",
                "updated": "2025-01-12T12:01:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    12,
                    1,
                    47,
                    6,
                    12,
                    0
                ],
                "published": "2024-04-03T17:33:21Z",
                "published_parsed": [
                    2024,
                    4,
                    3,
                    17,
                    33,
                    21,
                    2,
                    94,
                    0
                ],
                "title": "Linear Attention Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Linear Attention Sequence Parallelism"
                },
                "summary": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequence parallelism (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single device. However, for linear\nsequence modeling methods like linear attention, existing SP approaches do not\ntake advantage of their right-product-first feature, resulting in sub-optimal\ncommunication efficiency and usability. In this paper, we introduce Linear\nAttention Sequence Parallelism (LASP), an efficient SP approach designed for\nlinear attention-based transformer models. Specifically, we design an efficient\npoint-to-point ring-style communication mechanism to leverage the right-product\nkernel trick of linear attention, which sharply decreases the communication\noverhead, comparing with existing SP methods. We enhance the computation\nefficiency of LASP by performing kernel fusion and intermediate state caching,\nmaking the implementation of LASP hardware-friendly on GPUs. Furthermore, we\nmeticulously ensure the compatibility of sequence-level LASP with all types of\nbatch-level data parallel methods, which is vital for distributed training on\nlarge clusters with very-long sequences. We also discuss the generalization of\nLASP on other linear sequence modeling methods. Extensive experiments on linear\nattention-based models are conducted with varying sequence lengths from 2K to\n4096K. LASP scales sequence length up to 4096K on 128 GPUs, which is 8$\\times$\nlonger than existing SP methods. The code is available at\nhttps://github.com/OpenNLPLab/LASP."
                },
                "authors": [
                    {
                        "name": "Weigao Sun"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Xuyang Shen"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yiran Zhong"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Zhong"
                },
                "author": "Yiran Zhong",
                "arxiv_comment": "Technical report, 20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.02882v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.02882v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07196v3",
                "updated": "2025-01-12T11:15:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    11,
                    15,
                    41,
                    6,
                    12,
                    0
                ],
                "published": "2024-09-11T11:40:23Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    11,
                    40,
                    23,
                    2,
                    255,
                    0
                ],
                "title": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sub-cycle Nanotip Field Emission of Electrons Driven by Air Plasma\n  Generated THz Pulses"
                },
                "summary": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Terahertz pulses generated by two-color laser plasmas have reported peak\nfield strengths exceeding MV/cm, and when illuminating metal nanotips the\nnear-field enhancement at the tip apex should result in extremely high bunch\ncharges and electron energies via sub-cycle cold field emission. Here, electron\nemission from tungsten nanotips driven by THz pulses generated by a long\nfilament air-plasma are reported. Electron energies up to 1.1 keV and bunch\ncharges up to 2x$10^5$ electrons per pulse were detected, well below values\nexpected for peak field calculated via the time averaged Poynting vector.\nInvestigations revealed a failure in the use of the time-averaged Poynting\nvector when applied to long filament THz pulses, due to spatio-temporal\nrestructuring of the THz pulse in the focus. Accounting for this restructuring\nsignificantly reduces the field strength to approximately 160 ~kV/cm,\nconsistent with the observed electron bunch charges, peak energies and their\ndependence on the tip position in the THz focus. Despite these findings, our\nresults surpass previous THz plasma-driven electron generation by an order of\nmagnitude in both electron energy and bunch charge and a path to increasing\nthese by an additional order of magnitude by modification of the THz optics is\nproposed."
                },
                "authors": [
                    {
                        "name": "Benjamin Colmey"
                    },
                    {
                        "name": "Rodrigo T. Paulino"
                    },
                    {
                        "name": "Gaspard Beaufort"
                    },
                    {
                        "name": "David G. Cooke"
                    }
                ],
                "author_detail": {
                    "name": "David G. Cooke"
                },
                "author": "David G. Cooke",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v2",
                "updated": "2025-01-12T05:25:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    5,
                    25,
                    6,
                    6,
                    12,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "arxiv_comment": "Project page: https://nevsnev.github.io/FloED/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06709v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06709v1",
                "updated": "2025-01-12T04:29:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T04:29:39Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    4,
                    29,
                    39,
                    6,
                    12,
                    0
                ],
                "title": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mell: Memory-Efficient Large Language Model Serving via Multi-GPU KV\n  Cache Management"
                },
                "summary": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) for massive users is challenged by the\nsignificant memory footprint of the transient state, known as the key-value\n(KV) cache, which scales with sequence length and number of requests. Instead\nof renting or buying more expensive GPUs, the load imbalance of the KV cache\nacross GPUs, coupled with recent advances in inter-GPU communication, provides\nan opportunity to serve more requests via request migration. However, high\nmigration overhead and unpredictable request patterns make it challenging.\nTherefore, this paper proposes MELL, a memory-efficient LLM serving system via\nmulti-GPU KV cache management. It saves the number of GPUs needed in the system\nby considering the dynamic KV cache load and the costly request migration.\nSpecifically, we first develop an adaptive request migration mechanism to\nbalance the computational and communication overheads and adapt to diverse\nresource conditions. Then, we design an online algorithm tailored to a\nmulti-LLM request and multi-GPU scheduling problem with migration enabled. It\naims to minimise the required GPUs while limiting the number of migrations.\nFinally, we implement a prototype of MELL and demonstrate that it reduces the\nnumber of GPUs by 31% and increases the GPU utilization by 43% at most compared\nto existing LLM serving systems."
                },
                "authors": [
                    {
                        "name": "Liu Qianli"
                    },
                    {
                        "name": "Hong Zicong"
                    },
                    {
                        "name": "Chen Fahao"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Guo Song"
                    }
                ],
                "author_detail": {
                    "name": "Guo Song"
                },
                "author": "Guo Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06709v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06709v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17918v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17918v4",
                "updated": "2025-01-11T15:26:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    15,
                    26,
                    48,
                    5,
                    11,
                    0
                ],
                "published": "2024-06-25T20:00:32Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    20,
                    0,
                    32,
                    1,
                    177,
                    0
                ],
                "title": "GraphSnapShot: Caching Local Structure for Fast Graph Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSnapShot: Caching Local Structure for Fast Graph Learning"
                },
                "summary": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In our recent research, we have developed a framework called GraphSnapShot,\nwhich has been proven an useful tool for graph learning acceleration.\nGraphSnapShot is a framework for fast cache, storage, retrieval and computation\nfor graph learning. It can quickly store and update the local topology of graph\nstructure and allows us to track patterns in the structure of graph networks,\njust like take snapshots of the graphs. In experiments, GraphSnapShot shows\nefficiency, it can achieve up to 30% training acceleration and 73% memory\nreduction for lossless graph ML training compared to current baselines such as\ndgl.This technique is particular useful for large dynamic graph learning tasks\nsuch as social media analysis and recommendation systems to process complex\nrelationships between entities.\n  The code for GraphSnapShot is publicly available at\nhttps://github.com/NoakLiu/GraphSnapShot."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Roger Waleffe"
                    },
                    {
                        "name": "Meng Jiang"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17918v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17918v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06428v1",
                "updated": "2025-01-11T03:47:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:47:04Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    47,
                    4,
                    5,
                    11,
                    0
                ],
                "title": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing digital experiences with content delivery networks:\n  Architectures, performance strategies, and future trends"
                },
                "summary": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research investigates how CDNs (Content Delivery Networks) can improve\nthe digital experience, as consumers increasingly expect fast, efficient, and\neffortless access to online resources. CDNs play a crucial role in reducing\nlatency, enhancing scalability, and optimizing delivery mechanisms, which is\nevident across various platforms and regions. The study focuses on key CDN\nconcerns, such as foundational and modern CDN architectures, edge computing,\nhybrid CDNs, and multi-CDN strategies. It also explores performance-enhancing\ntopics, including caching, load balancing, and the novel features of HTTP/3 and\nQUIC.\n  Current trends, such as integrating CDNs with 5G networks, serverless\narchitectures, and AI-driven traffic management, are examined to demonstrate\nhow CDN technology is likely to evolve. The study also addresses challenges\nrelated to security, cost, and global regulations. Practical examples from the\ne-commerce, streaming, and gaming industries highlight how enhanced CDNs are\ntransforming these sectors.\n  The conclusions emphasize the need to evolve CDN strategies to meet growing\nuser expectations and adapt to the rapidly changing digital landscape.\nAdditionally, the research identifies future research opportunities,\nparticularly in exploring the impact of QC, the enhancement of AI services, and\nthe sustainability of CDN solutions. Overall, the study situates architectural\ndesign, performance strategies, and emerging trends to address gaps and create\na more efficient and secure approach for improving digital experiences."
                },
                "authors": [
                    {
                        "name": "Anuj Tyagi"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Tyagi"
                },
                "author": "Anuj Tyagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v1",
                "updated": "2025-01-11T03:37:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPAs memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "23 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06394v1",
                "updated": "2025-01-11T00:47:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "published": "2025-01-11T00:47:29Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    0,
                    47,
                    29,
                    5,
                    11,
                    0
                ],
                "title": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unispeaker: A Unified Approach for Multimodality-driven Speaker\n  Generation"
                },
                "summary": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in personalized speech generation have brought synthetic\nspeech increasingly close to the realism of target speakers' recordings, yet\nmultimodal speaker generation remains on the rise. This paper introduces\nUniSpeaker, a unified approach for multimodality-driven speaker generation.\nSpecifically, we propose a unified voice aggregator based on KV-Former,\napplying soft contrastive loss to map diverse voice description modalities into\na shared voice space, ensuring that the generated voice aligns more closely\nwith the input descriptions. To evaluate multimodality-driven voice control, we\nbuild the first multimodality-based voice control (MVC) benchmark, focusing on\nvoice suitability, voice diversity, and speech quality. UniSpeaker is evaluated\nacross five tasks using the MVC benchmark, and the experimental results\ndemonstrate that UniSpeaker outperforms previous modality-specific models.\nSpeech samples are available at \\url{https://UniSpeaker.github.io}."
                },
                "authors": [
                    {
                        "name": "Zhengyan Sheng"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Heng Lu"
                    },
                    {
                        "name": "Shiliang Zhang"
                    },
                    {
                        "name": "Zhen-Hua Ling"
                    }
                ],
                "author_detail": {
                    "name": "Zhen-Hua Ling"
                },
                "author": "Zhen-Hua Ling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01030v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01030v2",
                "updated": "2025-01-10T10:11:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    11,
                    45,
                    4,
                    10,
                    0
                ],
                "published": "2024-07-01T07:25:08Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    7,
                    25,
                    8,
                    0,
                    183,
                    0
                ],
                "title": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tame fields, Graded Rings and Finite Complete Sequences of Key\n  Polynomials"
                },
                "summary": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present a criterion for $(K,v)$ to be henselian and\ndefectless in terms of finite complete sequences of key polynomials. For this,\nwe use the theory of Mac Lane-Vaqui\\'e chains and abstract key polynomials. We\nthen prove that a valued field $(K,v)$ is tame if and only if $vK$ is\n$p$-divisible, $Kv$ is perfect and every simple algebraic extension of $K$\nadmits a finite complete sequence of key polynomials. The properties $vK$\n$p$-divisible and $Kv$ perfect are described by the Frobenius endomorphism on\nthe associated graded ring. We also make considerations on simply defectless\nand algebraically maximal valued fields and purely inertial and purely ramified\nextensions."
                },
                "authors": [
                    {
                        "name": "Caio Henrique Silva de Souza"
                    }
                ],
                "author_detail": {
                    "name": "Caio Henrique Silva de Souza"
                },
                "author": "Caio Henrique Silva de Souza",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01030v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01030v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "13A18",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20433v2",
                "updated": "2025-01-09T15:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    15,
                    14,
                    5,
                    3,
                    9,
                    0
                ],
                "published": "2024-09-30T15:53:36Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    15,
                    53,
                    36,
                    0,
                    274,
                    0
                ],
                "title": "Handover_Management_in_UAV_Networks_with_Blockages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handover_Management_in_UAV_Networks_with_Blockages"
                },
                "summary": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the performance of unmanned aerial vehicle (UAV)-based\nnetworks in urban environments characterized by blockages, focusing on their\ncapability to support the service demands of mobile users. The UAV-base\nstations (UAV-BSs) are modeled using a two-dimensional (2-D) marked- Poisson\npoint process (MPPP), where the marks represent the altitude of each UAV-BS.\nLeveraging stochastic geometry, we analyze the impact of blockages on network\nreliability by studying the meta distribution (MD) of the\nsignal-to-interference noise ratio (SINR) for a specific reliability threshold\nand the association probabilities for both line-of-sight (LoS) and non\nline-of-sight (NLoS) UAV-BSs. Furthermore, to enhance the performance of mobile\nusers, we propose a novel cache-based handover management strategy that\ndynamically selects the cell search time and delays the received signal\nstrength (RSS)-based base station (BS) associations. This strategy aims to\nminimize unnecessary handovers (HOs) experienced by users by leveraging caching\ncapabilities at user equipment (UE), thus reducing latency, ensuring seamless\nconnectivity, and maintaining the quality of service (QoS). This study provides\nvaluable insights into optimizing UAV network deployments to support the\nstringent requirements in the network, ensuring reliable, low-latency, and\nhigh-throughput communication for next-generation smart cities."
                },
                "authors": [
                    {
                        "name": "Neetu R R"
                    },
                    {
                        "name": "Gourab Ghatak"
                    },
                    {
                        "name": "Vivek Ashok Bohara"
                    }
                ],
                "author_detail": {
                    "name": "Vivek Ashok Bohara"
                },
                "author": "Vivek Ashok Bohara",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04993v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04993v1",
                "updated": "2025-01-09T06:18:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:18:39Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    18,
                    39,
                    3,
                    9,
                    0
                ],
                "title": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State\n  Drives"
                },
                "summary": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlike non-volatile memory that resides on the processor memory bus,\nmemory-semantic solid-state drives (SSDs) support both byte and block access\ngranularity via PCIe or CXL interconnects. They provide scalable memory\ncapacity using NAND flash at a much lower cost. In addition, they have\ndifferent performance characteristics for their dual byte/block interface\nrespectively, while offering essential memory semantics for upper-level\nsoftware. Such a byte-accessible storage device provides new implications on\nthe software system design.\n  In this paper, we develop a new file system, named ByteFS, by rethinking the\ndesign primitives of file systems and SSD firmware to exploit the advantages of\nboth byte and block-granular data accesses. ByteFS supports byte-granular data\npersistence to retain the persistence nature of SSDs. It extends the core data\nstructure of file systems by enabling dual byte/block-granular data accesses.\nTo facilitate the support for byte-granular writes, \\pname{} manages the\ninternal DRAM of SSD firmware in a log-structured manner and enables data\ncoalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also\nenables coordinated data caching between the host page cache and SSD cache for\nbest utilizing the precious memory resource. We implement ByteFS on both a real\nprogrammable SSD and an emulated memory-semantic SSD for sensitivity study.\nCompared to state-of-the-art file systems for non-volatile memory and\nconventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while\npreserving the essential properties of a file system. ByteFS also reduces the\nwrite traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes\ncaused by both metadata and data updates in file systems."
                },
                "authors": [
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "arxiv_comment": "This paper is accepted at the 30th Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04993v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04993v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v1",
                "updated": "2025-01-09T06:00:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. It consistently surpasses all baseline models\nin language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with\nshort context window to generalize to longer window with a 16x cache reduction.\nOn the Longbench benchmark, TreeKV achieves the best performance with only 6\\%\nof the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. It consistently surpasses all baseline models\nin language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with\nshort context window to generalize to longer window with a 16x cache reduction.\nOn the Longbench benchmark, TreeKV achieves the best performance with only 6\\%\nof the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04216v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04216v2",
                "updated": "2025-01-09T03:02:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    2,
                    31,
                    3,
                    9,
                    0
                ],
                "published": "2025-01-08T01:23:29Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    1,
                    23,
                    29,
                    2,
                    8,
                    0
                ],
                "title": "Optimal Oblivious Algorithms for Multi-way Joins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Oblivious Algorithms for Multi-way Joins"
                },
                "summary": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cloud databases, cloud computation over sensitive data uploaded by clients\ninevitably causes concern about data security and privacy. Even when encryption\nprimitives and trusted computing environments are integrated into query\nprocessing to safeguard the actual contents of the data, access patterns of\nalgorithms can still leak private information about the data. Oblivious Random\nAccess Memory (ORAM) and circuits are two generic approaches to address this\nissue, ensuring that access patterns of algorithms remain oblivious to the\ndata. However, deploying these methods on insecure algorithms, particularly for\nmulti-way join processing, is computationally expensive and inherently\nchallenging.\n  In this paper, we propose a novel sorting-based algorithm for multi-way join\nprocessing that operates without relying on ORAM simulations or other security\nassumptions. Our algorithm is a non-trivial, provably oblivious composition of\nbasic primitives, with time complexity matching the insecure worst-case optimal\njoin algorithm, up to a logarithmic factor. Furthermore, it is cache-agnostic,\nwith cache complexity matching the insecure lower bound, also up to a\nlogarithmic factor. This clean and straightforward approach has the potential\nto be extended to other security settings and implemented in practical database\nsystems."
                },
                "authors": [
                    {
                        "name": "Xiao Hu"
                    },
                    {
                        "name": "Zhiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiang Wu"
                },
                "author": "Zhiang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04216v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04216v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04394v1",
                "updated": "2025-01-08T10:14:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "published": "2025-01-08T10:14:19Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    10,
                    14,
                    19,
                    2,
                    8,
                    0
                ],
                "title": "Modern Hardware Security: A Review of Attacks and Countermeasures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Hardware Security: A Review of Attacks and Countermeasures"
                },
                "summary": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential rise in the use of cloud services, smart devices, and\nIoT devices, advanced cyber attacks have become increasingly sophisticated and\nubiquitous. Furthermore, the rapid evolution of computing architectures and\nmemory technologies has created an urgent need to understand and address\nhardware security vulnerabilities. In this paper, we review the current state\nof vulnerabilities and mitigation strategies in contemporary computing systems.\nWe discuss cache side-channel attacks (including Spectre and Meltdown), power\nside-channel attacks (such as Simple Power Analysis, Differential Power\nAnalysis, Correlation Power Analysis, and Template Attacks), and advanced\ntechniques like Voltage Glitching and Electromagnetic Analysis to help\nunderstand and build robust cybersecurity defense systems and guide further\nresearch. We also examine memory encryption, focusing on confidentiality,\ngranularity, key management, masking, and re-keying strategies. Additionally,\nwe cover Cryptographic Instruction Set Architectures, Secure Boot, Root of\nTrust mechanisms, Physical Unclonable Functions, and hardware fault injection\ntechniques. The paper concludes with an analysis of the RISC-V architecture's\nunique security challenges. The comprehensive analysis presented in this paper\nis essential for building resilient hardware security solutions that can\nprotect against both current and emerging threats in an increasingly\nchallenging security landscape."
                },
                "authors": [
                    {
                        "name": "Jyotiprakash Mishra"
                    },
                    {
                        "name": "Sanjay K. Sahay"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay K. Sahay"
                },
                "author": "Sanjay K. Sahay",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v2",
                "updated": "2025-01-07T17:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    32,
                    19,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Approximate Sparse Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures, corrected title, added proof of a lemma in\n  appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v1",
                "updated": "2025-01-07T17:00:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-GonzÃ¡lez"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro MartÃ­n"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09275v2",
                "updated": "2025-01-06T23:16:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    23,
                    16,
                    22,
                    0,
                    6,
                    0
                ],
                "published": "2024-11-14T08:25:31Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    8,
                    25,
                    31,
                    3,
                    319,
                    0
                ],
                "title": "Parallel $k$d-tree with Batch Updates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel $k$d-tree with Batch Updates"
                },
                "summary": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The $k$d-tree is one of the most widely used data structures to manage\nmulti-dimensional data. Due to the ever-growing data volume, it is imperative\nto consider parallelism in $k$d-trees. However, we observed challenges in\nexisting parallel kd-tree implementations, for both constructions and updates.\n  The goal of this paper is to develop efficient in-memory $k$d-trees by\nsupporting high parallelism and cache-efficiency. We propose the Pkd-tree\n(Parallel $k$d-tree), a parallel $k$d-tree that is efficient both in theory and\nin practice. The Pkd-tree supports parallel tree construction, batch update\n(insertion and deletion), and various queries including k-nearest neighbor\nsearch, range query, and range count. We proved that our algorithms have strong\ntheoretical bounds in work (sequential time complexity), span (parallelism),\nand cache complexity. Our key techniques include 1) an efficient construction\nalgorithm that optimizes work, span, and cache complexity simultaneously, and\n2) reconstruction-based update algorithms that guarantee the tree to be\nweight-balanced. With the new algorithmic insights and careful engineering\neffort, we achieved a highly optimized implementation of the Pkd-tree.\n  We tested Pkd-tree with various synthetic and real-world datasets, including\nboth uniform and highly skewed data. We compare the Pkd-tree with\nstate-of-the-art parallel $k$d-tree implementations. In all tests, with better\nor competitive query performance, Pkd-tree is much faster in construction and\nupdates consistently than all baselines. We released our code."
                },
                "authors": [
                    {
                        "name": "Ziyang Men"
                    },
                    {
                        "name": "Zheqi Shen"
                    },
                    {
                        "name": "Yan Gu"
                    },
                    {
                        "name": "Yihan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yihan Sun"
                },
                "author": "Yihan Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04052v1",
                "updated": "2025-01-06T22:40:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T22:40:40Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    22,
                    40,
                    40,
                    0,
                    6,
                    0
                ],
                "title": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Power of Negative Zero: Datatype Customization for Quantized Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance across\nvarious machine learning tasks, quickly becoming one of the most prevalent AI\nworkloads. Yet the substantial memory requirement of LLMs significantly hinders\ntheir deployment for end users. Post-training quantization (PTQ) serves as one\nof the most hardware-efficient methods to mitigate the memory and computational\ndemands of LLMs. Although the traditional integer (INT) datatype has received\nwidespread adoption in PTQ methods, floating-point (FP) quantization has\nemerged as a viable alternative thanks to its effectiveness in fitting LLM\nnumerical distributions. However, the FP datatype in sign-magnitude binary\nrepresentation contains both positive and negative zero, which constrains its\nrepresentation capability, particularly under low precision (3 and 4 bits). In\nthis paper, we extend the basic FP datatype to perform Redundant Zero Remapping\n(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined\nspecial values to maximally utilize FP quantization encodings and to better fit\nLLM numerical distributions. Through careful selection of special values, RaZeR\noutperforms conventional asymmetric INT quantization while achieving high\ncomputational efficiency. We demonstrate that RaZeR can be seamlessly\nintegrated with quantization algorithms for both weights and KV-cache,\nincluding advanced methods with clipping and transformations, and consistently\nachieve better model accuracy. Additionally, we implement a fast GEMV kernel\nwith fused dequantization that efficiently converts the 4-bit RaZeR value to\nFP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows\nthat RaZeR improves the GEMV speed by up to 7.56$\\times$ compared to the FP16\nimplementation, while achieving up to 2.72$\\times$ speedup in the LLM decoding\nthroughput."
                },
                "authors": [
                    {
                        "name": "Yuzong Chen"
                    },
                    {
                        "name": "Xilai Dai"
                    },
                    {
                        "name": "Chi-chih Chang"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "arxiv_comment": "under submission",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03322v1",
                "updated": "2025-01-06T19:00:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T19:00:03Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    19,
                    0,
                    3,
                    0,
                    6,
                    0
                ],
                "title": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twinkle: A GPU-based binary-lens microlensing code with contour\n  integration method"
                },
                "summary": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapidly increasing rate of microlensing planet detections,\nmicrolensing modeling software faces significant challenges in computation\nefficiency. Here, we develop the Twinkle code, an efficient and robust\nbinary-lens modeling software suite optimized for heterogeneous computing\ndevices, especially GPUs. Existing microlensing codes have the issue of\ncatastrophic cancellation that undermines the numerical stability and\nprecision, and Twinkle resolves them by refining the coefficients of the\nbinary-lens equation. We also devise an improved method for robustly\nidentifying ghost images, thereby enhancing computational reliability. We have\nadvanced the state of the art by optimizing Twinkle specifically for\nheterogeneous computing devices by taking into account the unique task and\ncache memory dispatching patterns of GPUs, while the compatibility with the\ntraditional computing architectures of CPUs is still maintained. Twinkle has\ndemonstrated an acceleration of approximately 2 orders of magnitude (>~100\ntimes) on contemporary GPUs. The enhancement in computational speed of Twinkle\nwill translate to the delivery of accurate and highly efficient data analysis\nfor ongoing and upcoming microlensing projects. Both GPU and CPU versions of\nTwinkle are open-source and publicly available."
                },
                "authors": [
                    {
                        "name": "Suwei Wang"
                    },
                    {
                        "name": "Lile Wang"
                    },
                    {
                        "name": "Subo Dong"
                    }
                ],
                "author_detail": {
                    "name": "Subo Dong"
                },
                "author": "Subo Dong",
                "arxiv_comment": "Accepted by ApJS, GitHub link:\n  https://github.com/AsterLight0626/Twinkle",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v2",
                "updated": "2025-01-06T15:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    15,
                    59,
                    23,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02803v1",
                "updated": "2025-01-06T06:44:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "published": "2025-01-06T06:44:13Z",
                "published_parsed": [
                    2025,
                    1,
                    6,
                    6,
                    44,
                    13,
                    0,
                    6,
                    0
                ],
                "title": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Lifelong Multi-Agent Path Finding with Cache Mechanism"
                },
                "summary": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial in autonomous warehouse operations.\nLifelong MAPF (L-MAPF), where agents are continuously reassigned new targets\nupon completing their current tasks, offers a more realistic approximation of\nreal-world warehouse scenarios. While cache storage systems can enhance\nefficiency and reduce operational costs, existing approaches primarily rely on\nexpectations and mathematical models, often without adequately addressing the\nchallenges of multi-robot planning and execution. In this paper, we introduce a\nnovel mechanism called Lifelong MAPF with Cache Mechanism (L-MAPF-CM), which\nintegrates high-level cache storage with low-level path planning. We have\ninvolved a new type of map grid called cache for temporary item storage.\nAdditionally, we involved a task assigner (TA) with a locking mechanism to\nbridge the gap between the new cache grid and L-MAPF algorithm. The TA\ndynamically allocates target locations to agents based on their status in\nvarious scenarios. We evaluated L-MAPF-CM using different cache replacement\npolicies and task distributions. L-MAPF-CM has demonstrated performance\nimprovements particularly with high cache hit rates and smooth traffic\nconditions."
                },
                "authors": [
                    {
                        "name": "Yimin Tang"
                    },
                    {
                        "name": "Zhenghong Yu"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "T. K. Satish Kumar"
                    },
                    {
                        "name": "Jiaoyang Li"
                    },
                    {
                        "name": "Sven Koenig"
                    }
                ],
                "author_detail": {
                    "name": "Sven Koenig"
                },
                "author": "Sven Koenig",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2403.13421",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v2",
                "updated": "2025-01-06T01:26:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    6,
                    1,
                    26,
                    42,
                    0,
                    6,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner. We will release the code based on an\nopen-source model in the future."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v2",
                "updated": "2025-01-05T14:11:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    14,
                    11,
                    48,
                    6,
                    5,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "arxiv_comment": "Update performance in MLVU-dev and LVBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02524v1",
                "updated": "2025-01-05T12:51:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T12:51:08Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    12,
                    51,
                    8,
                    6,
                    5,
                    0
                ],
                "title": "A Full-System Simulation Framework for CXL-Based SSD Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Full-System Simulation Framework for CXL-Based SSD Memory System"
                },
                "summary": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute eXpress Link (CXL) is a promising technology for memory\ndisaggregation and expansion. Especially, CXL makes it more effectively for\nlarge-capacity storage devices such as Solid State Drive (SSD) to be deployed\nin the memory pool. However, CXL-based SSDs are still in early stages,\nnecessitating the development of reliable simulation tools. In this paper, we\npropose CXL-SSD-Sim, the first open-source full-system simulator designed to\nsimulate CXL-based SSD memory system. Constructed on the foundation of gem5 and\nSimpleSSD, CXL-SSD-Sim extends an high fidelity SSD memory expander model along\nwith the corresponding device driver. In addition, CXL-SSD-Sim models a DRAM\nlayer as a caching mechanism for the SSD, meticulously engineered to counteract\nlatency issues inherent to CXL-based SSD memory access. Experiments are\nperformed among five different memory devices with CXL-SSD-Sim in aspect of\nlatency, bandwidth and real-world benchmark performance. These experiments\nserve to underscore the efficacy of our simulation tool in providing a\ncomprehensive analysis of CXL-based SSD memory systems. The CXL-SSD-Sim\nsimulator is available at https://github.com/WangYaohuii/CXL-SSD-Sim."
                },
                "authors": [
                    {
                        "name": "Yaohui Wang"
                    },
                    {
                        "name": "Zicong Wang"
                    },
                    {
                        "name": "Fanfeng Meng"
                    },
                    {
                        "name": "Yanjing Wang"
                    },
                    {
                        "name": "Yang Ou"
                    },
                    {
                        "name": "Lizhou Wu"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Xuran Ge"
                    },
                    {
                        "name": "Jijun Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jijun Cao"
                },
                "author": "Jijun Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02469v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02469v1",
                "updated": "2025-01-05T07:41:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "published": "2025-01-05T07:41:53Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    7,
                    41,
                    53,
                    6,
                    5,
                    0
                ],
                "title": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoRaConnect: Unlocking HTTP Potential on LoRa Backbones for Remote Areas\n  and Ad-Hoc Networks"
                },
                "summary": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The minimal infrastructure requirements of LoRa make it suitable for\ndeployments in remote and disaster-stricken areas. Concomitantly, the modern\nera is witnessing the proliferation of web applications in all aspects of human\nlife, including IoT and other network services. Contemporary IoT and network\nsolutions heavily rely on web applications to render services. However, despite\nthe recent research and development pivoted around LoRa, there is still a lack\nof studies focusing on web application access over LoRa networks. Specifically,\ntechnical challenges like payload size limitation, low data rate, and\ncontentions in multi-user setups limit the applicability of LoRa for web\napplications. Hence, we propose LoRaWeb, which enables web access over LoRa\nnetworks. The LoRaWeb hardware tethers a WiFi hotspot to which the client\ndevices connect and access the web pages using a web browser. LoRa backbone of\nthe network handles the web page transmission from the requester and receiver\ndevices. LoRaWeb implements a synchronization procedure to address the\naforementioned challenges for effective message exchange between requesters and\nresponders. The system implements a caching mechanism to reduce latency and\ncontention. Additionally, it implements a message-slicing mechanism in the\napplication layer to overcome the hardware limitations on the message length.\nThe actual hardware-based implementation results indicate seamless deployment,\nand the results indicate an average access time of ~$0.95 S$ for a $1.5 KB$ and\n~$6 S$ for a $10 KB$ size web page."
                },
                "authors": [
                    {
                        "name": "Atonu Ghosh"
                    },
                    {
                        "name": "Sudip Misra"
                    }
                ],
                "author_detail": {
                    "name": "Sudip Misra"
                },
                "author": "Sudip Misra",
                "arxiv_comment": "11 pages, 15 figures, and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02469v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02469v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v1",
                "updated": "2025-01-03T13:32:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01792v1",
                "updated": "2025-01-03T12:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:51:37Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching"
                },
                "summary": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache."
                },
                "authors": [
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Guseul Heo"
                    },
                    {
                        "name": "Minwoo Noh"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "arxiv_comment": "14 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01424v1",
                "updated": "2025-01-02T18:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Object-level Visual Prompts for Compositional Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-level Visual Prompts for Compositional Image Generation"
                },
                "summary": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Gaurav Parmar"
                    },
                    {
                        "name": "Or Patashnik"
                    },
                    {
                        "name": "Kuan-Chieh Wang"
                    },
                    {
                        "name": "Daniil Ostashev"
                    },
                    {
                        "name": "Srinivasa Narasimhan"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Daniel Cohen-Or"
                    },
                    {
                        "name": "Kfir Aberman"
                    }
                ],
                "author_detail": {
                    "name": "Kfir Aberman"
                },
                "author": "Kfir Aberman",
                "arxiv_comment": "Project: https://snap-research.github.io/visual-composer/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01039v1",
                "updated": "2025-01-02T03:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:41:32Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention"
                },
                "summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shivank Nag"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v1",
                "updated": "2025-01-02T02:02:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "code available at http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00946v1",
                "updated": "2025-01-01T20:16:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:16:27Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "title": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model"
                },
                "summary": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Omid Saghatchian"
                    },
                    {
                        "name": "Atiyeh Gh. Moghadam"
                    },
                    {
                        "name": "Ahmad Nickabadi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Nickabadi"
                },
                "author": "Ahmad Nickabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21023v2",
                "updated": "2024-12-31T20:40:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    40,
                    43,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "EdgeRAG: Online-Indexed RAG for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRAG: Online-Indexed RAG for Edge Devices"
                },
                "summary": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory."
                },
                "authors": [
                    {
                        "name": "Korakit Seemakhupt"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Samira Khan"
                    }
                ],
                "author_detail": {
                    "name": "Samira Khan"
                },
                "author": "Samira Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00375v1",
                "updated": "2024-12-31T09:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T09:56:40Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "title": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free"
                },
                "summary": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17."
                },
                "authors": [
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Bang Xiao"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v1",
                "updated": "2024-12-31T05:24:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00243v1",
                "updated": "2024-12-31T03:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T03:19:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition"
                },
                "summary": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}"
                },
                "authors": [
                    {
                        "name": "Edwin Arkel Rios"
                    },
                    {
                        "name": "Jansen Christopher Yuanda"
                    },
                    {
                        "name": "Vincent Leon Ghanz"
                    },
                    {
                        "name": "Cheng-Wei Yu"
                    },
                    {
                        "name": "Bo-Cheng Lai"
                    },
                    {
                        "name": "Min-Chun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Min-Chun Hu"
                },
                "author": "Min-Chun Hu",
                "arxiv_comment": "Accepted to ICASSP 2025. Main: 5 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20887v1",
                "updated": "2024-12-30T11:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field"
                },
                "summary": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Jianlin Luo"
                    },
                    {
                        "name": "Simone Latini"
                    },
                    {
                        "name": "Dongbin Shin"
                    },
                    {
                        "name": "Jun-Ming Liu"
                    },
                    {
                        "name": "Jing-Feng Li"
                    },
                    {
                        "name": "Angel Rubio"
                    },
                    {
                        "name": "Ce-Wen Nan"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v1",
                "updated": "2024-12-29T17:41:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20524v1",
                "updated": "2024-12-29T17:18:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:18:21Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "title": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation"
                },
                "summary": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes."
                },
                "authors": [
                    {
                        "name": "Anatolij Zubow"
                    },
                    {
                        "name": "Yannik Pilz"
                    },
                    {
                        "name": "Sascha RÃ¶sler"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20221v1",
                "updated": "2024-12-28T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "title": "Revisiting Cache Freshness for Emerging Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Cache Freshness for Emerging Real-Time Applications"
                },
                "summary": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness."
                },
                "authors": [
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Rishabh Iyer"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_doi": "10.1145/3696348.3696858",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696348.3696858",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotNets '24",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v1",
                "updated": "2024-12-28T14:38:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20105v1",
                "updated": "2024-12-28T10:17:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T10:17:29Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming"
                },
                "summary": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference."
                },
                "authors": [
                    {
                        "name": "Jiedong Zhuang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Haoji Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haoji Hu"
                },
                "author": "Haoji Hu",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19991v1",
                "updated": "2024-12-28T03:28:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T03:28:52Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "title": "A Robust Federated Learning Framework for Undependable Devices at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Federated Learning Framework for Undependable Devices at Scale"
                },
                "summary": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Huarong Deng"
                    },
                    {
                        "name": "Qiuye Zheng"
                    },
                    {
                        "name": "Jiantao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Gong"
                },
                "author": "Jiantao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v1",
                "updated": "2024-12-26T15:45:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    }
                ],
                "author_detail": {
                    "name": "Heung-Yeung Shum"
                },
                "author": "Heung-Yeung Shum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19051v1",
                "updated": "2024-12-26T04:13:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T04:13:52Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "title": "Performance Characterization and Optimizations of Traditional ML\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization and Optimizations of Traditional ML\n  Applications"
                },
                "summary": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement."
                },
                "authors": [
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "R. Govindarajan"
                    }
                ],
                "author_detail": {
                    "name": "R. Govindarajan"
                },
                "author": "R. Govindarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18960v1",
                "updated": "2024-12-25T18:36:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T18:36:21Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "title": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems"
                },
                "summary": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming."
                },
                "authors": [
                    {
                        "name": "Nader Alfares"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v1",
                "updated": "2024-12-25T14:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories"
                },
                "summary": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "23 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18911v1",
                "updated": "2024-12-25T14:00:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Dual Feature Caching"
                },
                "summary": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}"
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Haohang Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18885v1",
                "updated": "2024-12-25T11:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "title": "Aspect-oriented Programming with Julia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-oriented Programming with Julia"
                },
                "summary": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems."
                },
                "authors": [
                    {
                        "name": "Osamu Ishimura"
                    },
                    {
                        "name": "Yoshihide Yoshimoto"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihide Yoshimoto"
                },
                "author": "Yoshihide Yoshimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05460v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05460v1",
                "updated": "2024-12-25T10:11:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T10:11:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    10,
                    11,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Efficiently serving large multimedia models using EPD Disaggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently serving large multimedia models using EPD Disaggregation"
                },
                "summary": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step helps convert raw inputs into tokenized representations\nthat inflate the token sequence for the prefill phase, negatively impacting key\nService Level Objectives (SLOs) like time to first token (TTFT) and end-to-end\nthroughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel\nframework that separates the encoding, prefill, and decode stages onto\ndedicated resources. Unlike current systems, which bundle encoding and prefill\ntogether, our disaggregation approach alleviates memory bottlenecks, mitigates\nsynchronization delays, and supports flexible batching. Specifically, we employ\na new caching mechanism for multimodal tokens, enabling asynchronous transfer\nof multimodal tokens and introduce an integrated module to find optimal config\nfor EPD system and minimize resource usage while maximizing SLO-based\nperformance metric. Experimental evaluations with popular LMMs show substantial\ngains in memory efficiency (up to 15$\\times$ lesser for encoding-stage GPUs),\nthat supports upto 22$\\times$ higher batch sizes, 10$\\times$ more number of\nimages/ request, 2.2$\\times$ higher kv cache size. Further, it leads to\nsignificant improvements in end-to-end throughput (up to 57\\% better), and\nlatency metrics (TTFT up to 71\\% lower), compared to systems that do not\ndisaggregate. Our findings underscore the potential of EPD disaggregation to\nenable resource-efficient and high-performance multimodal inference at scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) extend Large Language Models (LLMs) by\nhandling diverse inputs such as images, audio, and video, but at the cost of\nadding a multimodal encoding stage that increases both computational and memory\noverhead. This step helps convert raw inputs into tokenized representations\nthat inflate the token sequence for the prefill phase, negatively impacting key\nService Level Objectives (SLOs) like time to first token (TTFT) and end-to-end\nthroughput. We introduce Encode-Prefill-Decode (EPD) Disaggregation, a novel\nframework that separates the encoding, prefill, and decode stages onto\ndedicated resources. Unlike current systems, which bundle encoding and prefill\ntogether, our disaggregation approach alleviates memory bottlenecks, mitigates\nsynchronization delays, and supports flexible batching. Specifically, we employ\na new caching mechanism for multimodal tokens, enabling asynchronous transfer\nof multimodal tokens and introduce an integrated module to find optimal config\nfor EPD system and minimize resource usage while maximizing SLO-based\nperformance metric. Experimental evaluations with popular LMMs show substantial\ngains in memory efficiency (up to 15$\\times$ lesser for encoding-stage GPUs),\nthat supports upto 22$\\times$ higher batch sizes, 10$\\times$ more number of\nimages/ request, 2.2$\\times$ higher kv cache size. Further, it leads to\nsignificant improvements in end-to-end throughput (up to 57\\% better), and\nlatency metrics (TTFT up to 71\\% lower), compared to systems that do not\ndisaggregate. Our findings underscore the potential of EPD disaggregation to\nenable resource-efficient and high-performance multimodal inference at scale."
                },
                "authors": [
                    {
                        "name": "Gursimran Singh"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Ivan Hu"
                    },
                    {
                        "name": "Timothy Yu"
                    },
                    {
                        "name": "Linzi Xing"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05460v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05460v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia FermÃ¼ller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v2",
                "updated": "2024-12-24T00:46:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    0,
                    46,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17685v1",
                "updated": "2024-12-23T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment"
                },
                "summary": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application."
                },
                "authors": [
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Evan Alexander Peters"
                    },
                    {
                        "name": "Dennies Bor"
                    },
                    {
                        "name": "Noah Rivera"
                    },
                    {
                        "name": "C. Trevor Gaunt"
                    },
                    {
                        "name": "Robert Weigel"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weigel"
                },
                "author": "Robert Weigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18919v2",
                "updated": "2024-12-23T14:40:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    40,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-29T09:22:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    22,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN"
                },
                "summary": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Weixiao Meng"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "14 pages, 13 figures. This work has been accepted by IEEE Internet of\n  Things Journal. It is expanded on our previous research presented at the IEEE\n  Globecom 2024: Q. Chen, C. Wu, S. Han, W. Meng, and T. Q. Quek, \"Exploiting\n  Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground\n  Integrated Networks,\" in Proc. GLOBECOM 2024, Cape Town, South Africa, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17464v1",
                "updated": "2024-12-23T10:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "CALLIC: Content Adaptive Learning for Lossless Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLIC: Content Adaptive Learning for Lossless Image Compression"
                },
                "summary": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression."
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v1",
                "updated": "2024-12-23T03:38:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Fast and Live Model Auto Scaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Live Model Auto Scaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v2",
                "updated": "2024-12-23T02:52:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    52,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17203v1",
                "updated": "2024-12-23T00:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T00:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "Agile TLB Prefetching and Prediction Replacement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile TLB Prefetching and Prediction Replacement Policy"
                },
                "summary": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management."
                },
                "authors": [
                    {
                        "name": "Melkamu Mersha"
                    },
                    {
                        "name": "Tsion Abay"
                    },
                    {
                        "name": "Mingziem Bitewa"
                    },
                    {
                        "name": "Gedare Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Gedare Bloom"
                },
                "author": "Gedare Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v1",
                "updated": "2024-12-22T07:14:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17565v3",
                "updated": "2024-12-21T13:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    55,
                    49,
                    5,
                    356,
                    0
                ],
                "published": "2024-06-25T14:02:08Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    14,
                    2,
                    8,
                    1,
                    177,
                    0
                ],
                "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool"
                },
                "summary": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time."
                },
                "authors": [
                    {
                        "name": "Cunchen Hu"
                    },
                    {
                        "name": "Heyang Huang"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Sa Wang"
                    },
                    {
                        "name": "Yungang Bao"
                    },
                    {
                        "name": "Ninghui Sun"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16585v1",
                "updated": "2024-12-21T11:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T11:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "title": "Parameterized Complexity of Caching in Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized Complexity of Caching in Networks"
                },
                "summary": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable."
                },
                "authors": [
                    {
                        "name": "Robert Ganian"
                    },
                    {
                        "name": "Fionn Mc Inerney"
                    },
                    {
                        "name": "Dimitra Tsigkari"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Tsigkari"
                },
                "author": "Dimitra Tsigkari",
                "arxiv_comment": "A shorter version of this paper will appear in the proceedings of\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v4",
                "updated": "2024-12-21T02:36:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    2,
                    36,
                    3,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16434v1",
                "updated": "2024-12-21T01:48:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T01:48:52Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYMPHONY: Improving Memory Management for LLM Inference Workloads"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile."
                },
                "authors": [
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Anyong Mao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16001v1",
                "updated": "2024-12-20T15:51:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:51:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Strided Access Patterns to Boost Hardware Prefetching"
                },
                "summary": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future."
                },
                "authors": [
                    {
                        "name": "Miguel O. Blom"
                    },
                    {
                        "name": "Kristian F. D. Rietveld"
                    },
                    {
                        "name": "Rob V. van Nieuwpoort"
                    }
                ],
                "author_detail": {
                    "name": "Rob V. van Nieuwpoort"
                },
                "author": "Rob V. van Nieuwpoort",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v2",
                "updated": "2024-12-20T15:18:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v3",
                "updated": "2024-12-19T23:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    23,
                    52,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v2",
                "updated": "2024-12-19T22:34:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    22,
                    34,
                    37,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. JimÃ©nez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v3",
                "updated": "2024-12-19T12:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    38,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14392v1",
                "updated": "2024-12-18T22:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:52:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems"
                },
                "summary": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies."
                },
                "authors": [
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v1",
                "updated": "2024-12-18T22:01:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v1",
                "updated": "2024-12-18T21:09:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v1",
                "updated": "2024-12-18T12:16:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v2",
                "updated": "2024-12-18T09:47:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v2",
                "updated": "2024-12-18T07:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    45,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v1",
                "updated": "2024-12-18T05:16:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation"
                },
                "summary": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v2",
                "updated": "2024-12-18T05:08:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    8,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04012v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04012v1",
                "updated": "2024-12-18T00:35:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    0,
                    35,
                    16,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T00:35:16Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    0,
                    35,
                    16,
                    2,
                    353,
                    0
                ],
                "title": "FlexCache: Flexible Approximate Cache System for Video Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexCache: Flexible Approximate Cache System for Video Diffusion"
                },
                "summary": "Text-to-Video applications receive increasing attention from the public.\nAmong these, diffusion models have emerged as the most prominent approach,\noffering impressive quality in visual content generation. However, it still\nsuffers from substantial computational complexity, often requiring several\nminutes to generate a single video. While prior research has addressed the\ncomputational overhead in text-to-image diffusion models, the techniques\ndeveloped are not directly suitable for video diffusion models due to the\nsignificantly larger cache requirements and enhanced computational demands\nassociated with video generation.\n  We present FlexCache, a flexible approximate cache system that addresses the\nchallenges in two main designs. First, we compress the caches before saving\nthem to storage. Our compression strategy can reduce 6.7 times consumption on\naverage. Then we find that the approximate cache system can achieve higher hit\nrate and computation savings by decoupling the object and background. We\nfurther design a tailored cache replacement policy to support the two\ntechniques mentioned above better. Through our evaluation, FlexCache reaches\n1.26 times higher throughput and 25% lower cost compared to the\nstate-of-the-art diffusion approximate cache system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-Video applications receive increasing attention from the public.\nAmong these, diffusion models have emerged as the most prominent approach,\noffering impressive quality in visual content generation. However, it still\nsuffers from substantial computational complexity, often requiring several\nminutes to generate a single video. While prior research has addressed the\ncomputational overhead in text-to-image diffusion models, the techniques\ndeveloped are not directly suitable for video diffusion models due to the\nsignificantly larger cache requirements and enhanced computational demands\nassociated with video generation.\n  We present FlexCache, a flexible approximate cache system that addresses the\nchallenges in two main designs. First, we compress the caches before saving\nthem to storage. Our compression strategy can reduce 6.7 times consumption on\naverage. Then we find that the approximate cache system can achieve higher hit\nrate and computation savings by decoupling the object and background. We\nfurther design a tailored cache replacement policy to support the two\ntechniques mentioned above better. Through our evaluation, FlexCache reaches\n1.26 times higher throughput and 25% lower cost compared to the\nstate-of-the-art diffusion approximate cache system."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Henry Tian"
                    },
                    {
                        "name": "Tim Lu"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04012v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04012v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v3",
                "updated": "2024-12-17T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    45,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12953v1",
                "updated": "2024-12-17T14:34:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:34:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning"
                },
                "summary": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12798v1",
                "updated": "2024-12-17T11:00:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:00:56Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "title": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation"
                },
                "summary": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI."
                },
                "authors": [
                    {
                        "name": "Shiqi Huang"
                    },
                    {
                        "name": "Shuting He"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "arxiv_comment": "AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.07572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07572v1",
                "updated": "2025-01-13T18:58:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    58,
                    7,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T18:58:07Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    58,
                    7,
                    0,
                    13,
                    0
                ],
                "title": "WebWalker: Benchmarking LLMs in Web Traversal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebWalker: Benchmarking LLMs in Web Traversal"
                },
                "summary": "Retrieval-augmented generation (RAG) demonstrates remarkable performance\nacross tasks in open-domain question-answering. However, traditional search\nengines may retrieve shallow content, limiting the ability of LLMs to handle\ncomplex, multi-layered information. To address it, we introduce WebWalkerQA, a\nbenchmark designed to assess the ability of LLMs to perform web traversal. It\nevaluates the capacity of LLMs to traverse a website's subpages to extract\nhigh-quality data systematically. We propose WebWalker, which is a multi-agent\nframework that mimics human-like web navigation through an explore-critic\nparadigm. Extensive experimental results show that WebWalkerQA is challenging\nand demonstrates the effectiveness of RAG combined with WebWalker, through the\nhorizontal and vertical integration in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) demonstrates remarkable performance\nacross tasks in open-domain question-answering. However, traditional search\nengines may retrieve shallow content, limiting the ability of LLMs to handle\ncomplex, multi-layered information. To address it, we introduce WebWalkerQA, a\nbenchmark designed to assess the ability of LLMs to perform web traversal. It\nevaluates the capacity of LLMs to traverse a website's subpages to extract\nhigh-quality data systematically. We propose WebWalker, which is a multi-agent\nframework that mimics human-like web navigation through an explore-critic\nparadigm. Extensive experimental results show that WebWalkerQA is challenging\nand demonstrates the effectiveness of RAG combined with WebWalker, through the\nhorizontal and vertical integration in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Wenbiao Yin"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Zekun Xi"
                    },
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Deyu Zhou"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07563v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07563v1",
                "updated": "2025-01-13T18:53:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    53,
                    8,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T18:53:08Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    53,
                    8,
                    0,
                    13,
                    0
                ],
                "title": "Training-Free Motion-Guided Video Generation with Enhanced Temporal\n  Consistency Using Motion Consistency Loss",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Motion-Guided Video Generation with Enhanced Temporal\n  Consistency Using Motion Consistency Loss"
                },
                "summary": "In this paper, we address the challenge of generating temporally consistent\nvideos with motion guidance. While many existing methods depend on additional\ncontrol modules or inference-time fine-tuning, recent studies suggest that\neffective motion guidance is achievable without altering the model architecture\nor requiring extra training. Such approaches offer promising compatibility with\nvarious video generation foundation models. However, existing training-free\nmethods often struggle to maintain consistent temporal coherence across frames\nor to follow guided motion accurately. In this work, we propose a simple yet\neffective solution that combines an initial-noise-based approach with a novel\nmotion consistency loss, the latter being our key innovation. Specifically, we\ncapture the inter-frame feature correlation patterns of intermediate features\nfrom a video diffusion model to represent the motion pattern of the reference\nvideo. We then design a motion consistency loss to maintain similar feature\ncorrelation patterns in the generated video, using the gradient of this loss in\nthe latent space to guide the generation process for precise motion control.\nThis approach improves temporal consistency across various motion control tasks\nwhile preserving the benefits of a training-free setup. Extensive experiments\nshow that our method sets a new standard for efficient, temporally coherent\nvideo generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we address the challenge of generating temporally consistent\nvideos with motion guidance. While many existing methods depend on additional\ncontrol modules or inference-time fine-tuning, recent studies suggest that\neffective motion guidance is achievable without altering the model architecture\nor requiring extra training. Such approaches offer promising compatibility with\nvarious video generation foundation models. However, existing training-free\nmethods often struggle to maintain consistent temporal coherence across frames\nor to follow guided motion accurately. In this work, we propose a simple yet\neffective solution that combines an initial-noise-based approach with a novel\nmotion consistency loss, the latter being our key innovation. Specifically, we\ncapture the inter-frame feature correlation patterns of intermediate features\nfrom a video diffusion model to represent the motion pattern of the reference\nvideo. We then design a motion consistency loss to maintain similar feature\ncorrelation patterns in the generated video, using the gradient of this loss in\nthe latent space to guide the generation process for precise motion control.\nThis approach improves temporal consistency across various motion control tasks\nwhile preserving the benefits of a training-free setup. Extensive experiments\nshow that our method sets a new standard for efficient, temporally coherent\nvideo generation."
                },
                "authors": [
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "Zicheng Duan"
                    },
                    {
                        "name": "Dong Gong"
                    },
                    {
                        "name": "Lingqiao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lingqiao Liu"
                },
                "author": "Lingqiao Liu",
                "arxiv_comment": "Project page:\n  https://zhangxinyu-xyz.github.io/SimulateMotion.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07563v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07563v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05451v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05451v2",
                "updated": "2025-01-13T18:45:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    45,
                    57,
                    0,
                    13,
                    0
                ],
                "published": "2024-10-07T19:34:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    19,
                    34,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "SecAlign: Defending Against Prompt Injection with Preference\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecAlign: Defending Against Prompt Injection with Preference\n  Optimization"
                },
                "summary": "Large language models (LLMs) are becoming increasingly prevalent in modern\nsoftware systems, interfacing between the user and the Internet to assist with\ntasks that require advanced language understanding. To accomplish these tasks,\nthe LLM often uses external data sources such as user documents, web retrieval,\nresults from API calls, etc. This opens up new avenues for attackers to\nmanipulate the LLM via prompt injection. Adversarial prompts can be injected\ninto external data sources to override the system's intended instruction and\ninstead execute a malicious instruction.\n  To mitigate this vulnerability, we propose a new defense called SecAlign\nbased on the technique of preference optimization. Our defense first constructs\na preference dataset with prompt-injected inputs, secure outputs (ones that\nrespond to the legitimate instruction), and insecure outputs (ones that respond\nto the injection). We then perform preference optimization on this dataset to\nteach the LLM to prefer the secure output over the insecure one. This provides\nthe first known method that reduces the success rates of various prompt\ninjections to around 0%, even against attacks much more sophisticated than ones\nseen during training. This indicates our defense generalizes well against\nunknown and yet-to-come attacks. Also, our defended models are still practical\nwith similar utility to the one before our defensive training. Our code is at\nhttps://github.com/facebookresearch/SecAlign",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are becoming increasingly prevalent in modern\nsoftware systems, interfacing between the user and the Internet to assist with\ntasks that require advanced language understanding. To accomplish these tasks,\nthe LLM often uses external data sources such as user documents, web retrieval,\nresults from API calls, etc. This opens up new avenues for attackers to\nmanipulate the LLM via prompt injection. Adversarial prompts can be injected\ninto external data sources to override the system's intended instruction and\ninstead execute a malicious instruction.\n  To mitigate this vulnerability, we propose a new defense called SecAlign\nbased on the technique of preference optimization. Our defense first constructs\na preference dataset with prompt-injected inputs, secure outputs (ones that\nrespond to the legitimate instruction), and insecure outputs (ones that respond\nto the injection). We then perform preference optimization on this dataset to\nteach the LLM to prefer the secure output over the insecure one. This provides\nthe first known method that reduces the success rates of various prompt\ninjections to around 0%, even against attacks much more sophisticated than ones\nseen during training. This indicates our defense generalizes well against\nunknown and yet-to-come attacks. Also, our defended models are still practical\nwith similar utility to the one before our defensive training. Our code is at\nhttps://github.com/facebookresearch/SecAlign"
                },
                "authors": [
                    {
                        "name": "Sizhe Chen"
                    },
                    {
                        "name": "Arman Zharmagambetov"
                    },
                    {
                        "name": "Saeed Mahloujifar"
                    },
                    {
                        "name": "Kamalika Chaudhuri"
                    },
                    {
                        "name": "David Wagner"
                    },
                    {
                        "name": "Chuan Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Guo"
                },
                "author": "Chuan Guo",
                "arxiv_comment": "Key words: prompt injection defense, LLM security, LLM-integrated\n  applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05451v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05451v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07554v1",
                "updated": "2025-01-13T18:37:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    37,
                    8,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T18:37:08Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    37,
                    8,
                    0,
                    13,
                    0
                ],
                "title": "SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal\n  Aspects in Video Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal\n  Aspects in Video Editing"
                },
                "summary": "Video editing models have advanced significantly, but evaluating their\nperformance remains challenging. Traditional metrics, such as CLIP text and\nimage scores, often fall short: text scores are limited by inadequate training\ndata and hierarchical dependencies, while image scores fail to assess temporal\nconsistency. We present SST-EM (Semantic, Spatial, and Temporal Evaluation\nMetric), a novel evaluation framework that leverages modern Vision-Language\nModels (VLMs), Object Detection, and Temporal Consistency checks. SST-EM\ncomprises four components: (1) semantic extraction from frames using a VLM, (2)\nprimary object tracking with Object Detection, (3) focused object refinement\nvia an LLM agent, and (4) temporal consistency assessment using a Vision\nTransformer (ViT). These components are integrated into a unified metric with\nweights derived from human evaluations and regression analysis. The name SST-EM\nreflects its focus on Semantic, Spatial, and Temporal aspects of video\nevaluation. SST-EM provides a comprehensive evaluation of semantic fidelity and\ntemporal smoothness in video editing. The source code is available in the\n\\textbf{\\href{https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub\nRepository}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video editing models have advanced significantly, but evaluating their\nperformance remains challenging. Traditional metrics, such as CLIP text and\nimage scores, often fall short: text scores are limited by inadequate training\ndata and hierarchical dependencies, while image scores fail to assess temporal\nconsistency. We present SST-EM (Semantic, Spatial, and Temporal Evaluation\nMetric), a novel evaluation framework that leverages modern Vision-Language\nModels (VLMs), Object Detection, and Temporal Consistency checks. SST-EM\ncomprises four components: (1) semantic extraction from frames using a VLM, (2)\nprimary object tracking with Object Detection, (3) focused object refinement\nvia an LLM agent, and (4) temporal consistency assessment using a Vision\nTransformer (ViT). These components are integrated into a unified metric with\nweights derived from human evaluations and regression analysis. The name SST-EM\nreflects its focus on Semantic, Spatial, and Temporal aspects of video\nevaluation. SST-EM provides a comprehensive evaluation of semantic fidelity and\ntemporal smoothness in video editing. The source code is available in the\n\\textbf{\\href{https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub\nRepository}}."
                },
                "authors": [
                    {
                        "name": "Varun Biyyala"
                    },
                    {
                        "name": "Bharat Chanderprakash Kathuria"
                    },
                    {
                        "name": "Jialu Li"
                    },
                    {
                        "name": "Youshan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Youshan Zhang"
                },
                "author": "Youshan Zhang",
                "arxiv_comment": "WACV workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07550v1",
                "updated": "2025-01-13T18:36:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    36,
                    38,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T18:36:38Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    36,
                    38,
                    0,
                    13,
                    0
                ],
                "title": "disco: Distributional Synthetic Controls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "disco: Distributional Synthetic Controls"
                },
                "summary": "The method of synthetic controls is widely used for evaluating causal effects\nof policy changes in settings with observational data. Often, researchers aim\nto estimate the causal impact of policy interventions on a treated unit at an\naggregate level while also possessing data at a finer granularity. In this\narticle, we introduce the new disco command, which implements the\nDistributional Synthetic Controls method introduced in Gunsilius (2023). This\ncommand allows researchers to construct entire synthetic distributions for the\ntreated unit based on an optimally weighted average of the distributions of the\ncontrol units. Several aggregation schemes are provided to facilitate clear\nreporting of the distributional effects of the treatment. The package offers\nboth quantile-based and CDF-based approaches, comprehensive inference\nprocedures via bootstrap and permutation methods, and visualization\ncapabilities. We empirically illustrate the use of the package by replicating\nthe results in Van Dijcke et al. (2024).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The method of synthetic controls is widely used for evaluating causal effects\nof policy changes in settings with observational data. Often, researchers aim\nto estimate the causal impact of policy interventions on a treated unit at an\naggregate level while also possessing data at a finer granularity. In this\narticle, we introduce the new disco command, which implements the\nDistributional Synthetic Controls method introduced in Gunsilius (2023). This\ncommand allows researchers to construct entire synthetic distributions for the\ntreated unit based on an optimally weighted average of the distributions of the\ncontrol units. Several aggregation schemes are provided to facilitate clear\nreporting of the distributional effects of the treatment. The package offers\nboth quantile-based and CDF-based approaches, comprehensive inference\nprocedures via bootstrap and permutation methods, and visualization\ncapabilities. We empirically illustrate the use of the package by replicating\nthe results in Van Dijcke et al. (2024)."
                },
                "authors": [
                    {
                        "name": "Florian Gunsilius"
                    },
                    {
                        "name": "David Van Dijcke"
                    }
                ],
                "author_detail": {
                    "name": "David Van Dijcke"
                },
                "author": "David Van Dijcke",
                "arxiv_comment": "19 pages, 4 figures, replication code available at\n  https://tinyurl.com/msz9ct2e, software available at\n  https://github.com/Davidvandijcke/DiSCos_stata",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07542v1",
                "updated": "2025-01-13T18:23:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    23,
                    57,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T18:23:57Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    23,
                    57,
                    0,
                    13,
                    0
                ],
                "title": "Imagine while Reasoning in Space: Multimodal Visualization-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imagine while Reasoning in Space: Multimodal Visualization-of-Thought"
                },
                "summary": "Chain-of-Thought (CoT) prompting has proven highly effective for enhancing\ncomplex reasoning in Large Language Models (LLMs) and Multimodal Large Language\nModels (MLLMs). Yet, it struggles in complex spatial reasoning tasks.\nNonetheless, human cognition extends beyond language alone, enabling the\nremarkable capability to think in both words and images. Inspired by this\nmechanism, we propose a new reasoning paradigm, Multimodal\nVisualization-of-Thought (MVoT). It enables visual thinking in MLLMs by\ngenerating image visualizations of their reasoning traces. To ensure\nhigh-quality visualization, we introduce token discrepancy loss into\nautoregressive MLLMs. This innovation significantly improves both visual\ncoherence and fidelity. We validate this approach through several dynamic\nspatial reasoning tasks. Experimental results reveal that MVoT demonstrates\ncompetitive performance across tasks. Moreover, it exhibits robust and reliable\nimprovements in the most challenging scenarios where CoT fails. Ultimately,\nMVoT establishes new possibilities for complex reasoning tasks where visual\nthinking can effectively complement verbal reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting has proven highly effective for enhancing\ncomplex reasoning in Large Language Models (LLMs) and Multimodal Large Language\nModels (MLLMs). Yet, it struggles in complex spatial reasoning tasks.\nNonetheless, human cognition extends beyond language alone, enabling the\nremarkable capability to think in both words and images. Inspired by this\nmechanism, we propose a new reasoning paradigm, Multimodal\nVisualization-of-Thought (MVoT). It enables visual thinking in MLLMs by\ngenerating image visualizations of their reasoning traces. To ensure\nhigh-quality visualization, we introduce token discrepancy loss into\nautoregressive MLLMs. This innovation significantly improves both visual\ncoherence and fidelity. We validate this approach through several dynamic\nspatial reasoning tasks. Experimental results reveal that MVoT demonstrates\ncompetitive performance across tasks. Moreover, it exhibits robust and reliable\nimprovements in the most challenging scenarios where CoT fails. Ultimately,\nMVoT establishes new possibilities for complex reasoning tasks where visual\nthinking can effectively complement verbal reasoning."
                },
                "authors": [
                    {
                        "name": "Chengzu Li"
                    },
                    {
                        "name": "Wenshan Wu"
                    },
                    {
                        "name": "Huanyu Zhang"
                    },
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Shaoguang Mao"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Ivan VuliÄ"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "11 pages, 6 figures, 4 tables (27 pages, 10 figures, 16 tables\n  including references and appendices)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11222v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11222v2",
                "updated": "2025-01-13T18:20:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    20,
                    35,
                    0,
                    13,
                    0
                ],
                "published": "2024-11-18T01:19:37Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    1,
                    19,
                    37,
                    0,
                    323,
                    0
                ],
                "title": "The Sound of Water: Inferring Physical Properties from Pouring Liquids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Sound of Water: Inferring Physical Properties from Pouring Liquids"
                },
                "summary": "We study the connection between audio-visual observations and the underlying\nphysics of a mundane yet intriguing everyday activity: pouring liquids. Given\nonly the sound of liquid pouring into a container, our objective is to\nautomatically infer physical properties such as the liquid level, the shape and\nsize of the container, the pouring rate and the time to fill. To this end, we:\n(i) show in theory that these properties can be determined from the fundamental\nfrequency (pitch); (ii) train a pitch detection model with supervision from\nsimulated data and visual data with a physics-inspired objective; (iii)\nintroduce a new large dataset of real pouring videos for a systematic study;\n(iv) show that the trained model can indeed infer these physical properties for\nreal data; and finally, (v) we demonstrate strong generalization to various\ncontainer shapes, other datasets, and in-the-wild YouTube videos. Our work\npresents a keen understanding of a narrow yet rich problem at the intersection\nof acoustics, physics, and learning. It opens up applications to enhance\nmultisensory perception in robotic pouring.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the connection between audio-visual observations and the underlying\nphysics of a mundane yet intriguing everyday activity: pouring liquids. Given\nonly the sound of liquid pouring into a container, our objective is to\nautomatically infer physical properties such as the liquid level, the shape and\nsize of the container, the pouring rate and the time to fill. To this end, we:\n(i) show in theory that these properties can be determined from the fundamental\nfrequency (pitch); (ii) train a pitch detection model with supervision from\nsimulated data and visual data with a physics-inspired objective; (iii)\nintroduce a new large dataset of real pouring videos for a systematic study;\n(iv) show that the trained model can indeed infer these physical properties for\nreal data; and finally, (v) we demonstrate strong generalization to various\ncontainer shapes, other datasets, and in-the-wild YouTube videos. Our work\npresents a keen understanding of a narrow yet rich problem at the intersection\nof acoustics, physics, and learning. It opens up applications to enhance\nmultisensory perception in robotic pouring."
                },
                "authors": [
                    {
                        "name": "Piyush Bagad"
                    },
                    {
                        "name": "Makarand Tapaswi"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Andrew Zisserman"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Zisserman"
                },
                "author": "Andrew Zisserman",
                "arxiv_comment": "Project page at https://bpiyush.github.io/pouring-water-website.\n  Short version accepted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11222v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11222v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07538v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07538v1",
                "updated": "2025-01-13T18:18:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    18,
                    17,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T18:18:17Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    18,
                    17,
                    0,
                    13,
                    0
                ],
                "title": "Testing $Î³Î´$CDM Model in the Redshift Bins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing $Î³Î´$CDM Model in the Redshift Bins"
                },
                "summary": "The Hubble crisis is the discrepancy in the values of the Hubble constant\ninferred from diverse observations in the late and early Universe, being of the\norder 5$\\sigma$. Instead of resolution, the conflict is getting larger with\nfurther late-time observations. A fundamental constant should be and remain\nconstant throughout the cosmological history and thus at all redshifts. The\nfact that it turns out to be a function of redshift in the $\\Lambda$CDM model\npoints out that either there is a problem with the current cosmological model,\nindicating unknown new physics, or there are unknown systematics in some of the\nobservations. In this work, we investigate the redshift dependence of the\nHubble constant in the $\\gamma\\delta$CDM cosmological model, which is a new\ncosmological model based on $f(R)$ gravity in an anisotropic background.\nThrough data analysis with the Pantheon+ type Ia supernovae, the cosmic\nchronometers Hubble, and both the old and the Dark Energy Spectroscopic\nInstrument (DESI) baryon acoustic oscillation data, we establish that the\nHubble constant in our model does not evolve with redshift. We also confirm\nthat our model fits the aforementioned data better than the $\\Lambda$CDM model\nby checking various information criteria. The value of the Hubble constant\nobtained in the $\\gamma\\delta$CDM model is in the 1$\\sigma$ bound of the late\nUniverse observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Hubble crisis is the discrepancy in the values of the Hubble constant\ninferred from diverse observations in the late and early Universe, being of the\norder 5$\\sigma$. Instead of resolution, the conflict is getting larger with\nfurther late-time observations. A fundamental constant should be and remain\nconstant throughout the cosmological history and thus at all redshifts. The\nfact that it turns out to be a function of redshift in the $\\Lambda$CDM model\npoints out that either there is a problem with the current cosmological model,\nindicating unknown new physics, or there are unknown systematics in some of the\nobservations. In this work, we investigate the redshift dependence of the\nHubble constant in the $\\gamma\\delta$CDM cosmological model, which is a new\ncosmological model based on $f(R)$ gravity in an anisotropic background.\nThrough data analysis with the Pantheon+ type Ia supernovae, the cosmic\nchronometers Hubble, and both the old and the Dark Energy Spectroscopic\nInstrument (DESI) baryon acoustic oscillation data, we establish that the\nHubble constant in our model does not evolve with redshift. We also confirm\nthat our model fits the aforementioned data better than the $\\Lambda$CDM model\nby checking various information criteria. The value of the Hubble constant\nobtained in the $\\gamma\\delta$CDM model is in the 1$\\sigma$ bound of the late\nUniverse observations."
                },
                "authors": [
                    {
                        "name": "Furkan Åakir Dilsiz"
                    },
                    {
                        "name": "Cemsinan Deliduman"
                    },
                    {
                        "name": "Selinay Sude Binici"
                    }
                ],
                "author_detail": {
                    "name": "Selinay Sude Binici"
                },
                "author": "Selinay Sude Binici",
                "arxiv_comment": "26 pages, 14 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07538v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07538v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07532v1",
                "updated": "2025-01-13T18:09:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    9,
                    58,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T18:09:58Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    9,
                    58,
                    0,
                    13,
                    0
                ],
                "title": "Investigating Large Language Models in Inferring Personality Traits from\n  User Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Large Language Models in Inferring Personality Traits from\n  User Conversations"
                },
                "summary": "Large Language Models (LLMs) are demonstrating remarkable human like\ncapabilities across diverse domains, including psychological assessment. This\nstudy evaluates whether LLMs, specifically GPT-4o and GPT-4o mini, can infer\nBig Five personality traits and generate Big Five Inventory-10 (BFI-10) item\nscores from user conversations under zero-shot prompting conditions. Our\nfindings reveal that incorporating an intermediate step--prompting for BFI-10\nitem scores before calculating traits--enhances accuracy and aligns more\nclosely with the gold standard than direct trait inference. This structured\napproach underscores the importance of leveraging psychological frameworks in\nimproving predictive precision. Additionally, a group comparison based on\ndepressive symptom presence revealed differential model performance.\nParticipants were categorized into two groups: those experiencing at least one\ndepressive symptom and those without symptoms. GPT-4o mini demonstrated\nheightened sensitivity to depression-related shifts in traits such as\nNeuroticism and Conscientiousness within the symptom-present group, whereas\nGPT-4o exhibited strengths in nuanced interpretation across groups. These\nfindings underscore the potential of LLMs to analyze real-world psychological\ndata effectively, offering a valuable foundation for interdisciplinary research\nat the intersection of artificial intelligence and psychology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are demonstrating remarkable human like\ncapabilities across diverse domains, including psychological assessment. This\nstudy evaluates whether LLMs, specifically GPT-4o and GPT-4o mini, can infer\nBig Five personality traits and generate Big Five Inventory-10 (BFI-10) item\nscores from user conversations under zero-shot prompting conditions. Our\nfindings reveal that incorporating an intermediate step--prompting for BFI-10\nitem scores before calculating traits--enhances accuracy and aligns more\nclosely with the gold standard than direct trait inference. This structured\napproach underscores the importance of leveraging psychological frameworks in\nimproving predictive precision. Additionally, a group comparison based on\ndepressive symptom presence revealed differential model performance.\nParticipants were categorized into two groups: those experiencing at least one\ndepressive symptom and those without symptoms. GPT-4o mini demonstrated\nheightened sensitivity to depression-related shifts in traits such as\nNeuroticism and Conscientiousness within the symptom-present group, whereas\nGPT-4o exhibited strengths in nuanced interpretation across groups. These\nfindings underscore the potential of LLMs to analyze real-world psychological\ndata effectively, offering a valuable foundation for interdisciplinary research\nat the intersection of artificial intelligence and psychology."
                },
                "authors": [
                    {
                        "name": "Jianfeng Zhu"
                    },
                    {
                        "name": "Ruoming Jin"
                    },
                    {
                        "name": "Karin G. Coifman"
                    }
                ],
                "author_detail": {
                    "name": "Karin G. Coifman"
                },
                "author": "Karin G. Coifman",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07531v1",
                "updated": "2025-01-13T18:09:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    9,
                    25,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T18:09:25Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    9,
                    25,
                    0,
                    13,
                    0
                ],
                "title": "Evaluating Agent-based Program Repair at Google",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Agent-based Program Repair at Google"
                },
                "summary": "Agent-based program repair offers to automatically resolve complex bugs\nend-to-end by combining the planning, tool use, and code generation abilities\nof modern LLMs. Recent work has explored the use of agent-based repair\napproaches on the popular open-source SWE-Bench, a collection of bugs from\nhighly-rated GitHub Python projects. In addition, various agentic approaches\nsuch as SWE-Agent have been proposed to solve bugs in this benchmark. This\npaper explores the viability of using an agentic approach to address bugs in an\nenterprise context. To investigate this, we curate an evaluation set of 178\nbugs drawn from Google's issue tracking system. This dataset spans both\nhuman-reported (78) and machine-reported bugs (100).\n  To establish a repair performance baseline on this benchmark, we implement\nPasserine, an agent similar in spirit to SWE-Agent that can work within\nGoogle's development environment. We show that with 20 trajectory samples and\nGemini 1.5 Pro, Passerine can produce a patch that passes bug tests (i.e.,\nplausible) for 73% of machine-reported and 25.6% of human-reported bugs in our\nevaluation set. After manual examination, we found that 43% of machine-reported\nbugs and 17.9% of human-reported bugs have at least one patch that is\nsemantically equivalent to the ground-truth patch.\n  These results establish a baseline on an industrially relevant benchmark,\nwhich as we show, contains bugs drawn from a different distribution -- in terms\nof language diversity, size, and spread of changes, etc. -- compared to those\nin the popular SWE-Bench dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-based program repair offers to automatically resolve complex bugs\nend-to-end by combining the planning, tool use, and code generation abilities\nof modern LLMs. Recent work has explored the use of agent-based repair\napproaches on the popular open-source SWE-Bench, a collection of bugs from\nhighly-rated GitHub Python projects. In addition, various agentic approaches\nsuch as SWE-Agent have been proposed to solve bugs in this benchmark. This\npaper explores the viability of using an agentic approach to address bugs in an\nenterprise context. To investigate this, we curate an evaluation set of 178\nbugs drawn from Google's issue tracking system. This dataset spans both\nhuman-reported (78) and machine-reported bugs (100).\n  To establish a repair performance baseline on this benchmark, we implement\nPasserine, an agent similar in spirit to SWE-Agent that can work within\nGoogle's development environment. We show that with 20 trajectory samples and\nGemini 1.5 Pro, Passerine can produce a patch that passes bug tests (i.e.,\nplausible) for 73% of machine-reported and 25.6% of human-reported bugs in our\nevaluation set. After manual examination, we found that 43% of machine-reported\nbugs and 17.9% of human-reported bugs have at least one patch that is\nsemantically equivalent to the ground-truth patch.\n  These results establish a baseline on an industrially relevant benchmark,\nwhich as we show, contains bugs drawn from a different distribution -- in terms\nof language diversity, size, and spread of changes, etc. -- compared to those\nin the popular SWE-Bench dataset."
                },
                "authors": [
                    {
                        "name": "Pat Rondon"
                    },
                    {
                        "name": "Renyao Wei"
                    },
                    {
                        "name": "JosÃ© Cambronero"
                    },
                    {
                        "name": "JÃ¼rgen Cito"
                    },
                    {
                        "name": "Aaron Sun"
                    },
                    {
                        "name": "Siddhant Sanyam"
                    },
                    {
                        "name": "Michele Tufano"
                    },
                    {
                        "name": "Satish Chandra"
                    }
                ],
                "author_detail": {
                    "name": "Satish Chandra"
                },
                "author": "Satish Chandra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07530v1",
                "updated": "2025-01-13T18:08:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    8,
                    27,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T18:08:27Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    8,
                    27,
                    0,
                    13,
                    0
                ],
                "title": "IP-FaceDiff: Identity-Preserving Facial Video Editing with Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IP-FaceDiff: Identity-Preserving Facial Video Editing with Diffusion"
                },
                "summary": "Facial video editing has become increasingly important for content creators,\nenabling the manipulation of facial expressions and attributes. However,\nexisting models encounter challenges such as poor editing quality, high\ncomputational costs and difficulties in preserving facial identity across\ndiverse edits. Additionally, these models are often constrained to editing\npredefined facial attributes, limiting their flexibility to diverse editing\nprompts. To address these challenges, we propose a novel facial video editing\nframework that leverages the rich latent space of pre-trained text-to-image\n(T2I) diffusion models and fine-tune them specifically for facial video editing\ntasks. Our approach introduces a targeted fine-tuning scheme that enables high\nquality, localized, text-driven edits while ensuring identity preservation\nacross video frames. Additionally, by using pre-trained T2I models during\ninference, our approach significantly reduces editing time by 80%, while\nmaintaining temporal consistency throughout the video sequence. We evaluate the\neffectiveness of our approach through extensive testing across a wide range of\nchallenging scenarios, including varying head poses, complex action sequences,\nand diverse facial expressions. Our method consistently outperforms existing\ntechniques, demonstrating superior performance across a broad set of metrics\nand benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Facial video editing has become increasingly important for content creators,\nenabling the manipulation of facial expressions and attributes. However,\nexisting models encounter challenges such as poor editing quality, high\ncomputational costs and difficulties in preserving facial identity across\ndiverse edits. Additionally, these models are often constrained to editing\npredefined facial attributes, limiting their flexibility to diverse editing\nprompts. To address these challenges, we propose a novel facial video editing\nframework that leverages the rich latent space of pre-trained text-to-image\n(T2I) diffusion models and fine-tune them specifically for facial video editing\ntasks. Our approach introduces a targeted fine-tuning scheme that enables high\nquality, localized, text-driven edits while ensuring identity preservation\nacross video frames. Additionally, by using pre-trained T2I models during\ninference, our approach significantly reduces editing time by 80%, while\nmaintaining temporal consistency throughout the video sequence. We evaluate the\neffectiveness of our approach through extensive testing across a wide range of\nchallenging scenarios, including varying head poses, complex action sequences,\nand diverse facial expressions. Our method consistently outperforms existing\ntechniques, demonstrating superior performance across a broad set of metrics\nand benchmarks."
                },
                "authors": [
                    {
                        "name": "Tharun Anand"
                    },
                    {
                        "name": "Aryan Garg"
                    },
                    {
                        "name": "Kaushik Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Mitra"
                },
                "author": "Kaushik Mitra",
                "arxiv_comment": "WACV-25 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07529v1",
                "updated": "2025-01-13T18:01:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    1,
                    38,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T18:01:38Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    1,
                    38,
                    0,
                    13,
                    0
                ],
                "title": "Determining distances and consensus between mutation trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Determining distances and consensus between mutation trees"
                },
                "summary": "The mutational heterogeneity of tumours can be described with a tree\nrepresenting the evolutionary history of the tumour. With noisy sequencing data\nthere may be uncertainty in the inferred tree structure, while we may also wish\nto study patterns in the evolution of cancers in different patients. In such\nsituations, understanding tree similarities is a key challenge, and therefore\nwe present an approach to determine distances between trees. Considering the\nbounded height of trees, we determine the distances associated with the swap\noperations over strings. While in general, by solving the {\\sc Maximum Common\nAlmost $v$-tree} problem between two trees, we describe an efficient approach\nto determine the minimum number of operations to transform one tree into\nanother. The inherent noise in current statistical methods for constructing\nmutation evolution trees of cancer cells presents a significant challenge:\nhandling such collections of trees to determine a consensus tree that\naccurately represents the set and evaluating the extent of their variability or\ndispersion. Given a set of mutation trees and the notion of distance, there are\nat least two natural ways to define the ``target'' tree, such as a min-sum\n(\\emph{median tree}) or a min-max (\\emph{closest tree}) of a set of trees.\nThus, considering a set of trees as input and dealing with the {\\sc median} and\n{\\sc closest} problems, we prove that both problems are \\NP-complete, even with\nonly three input trees. In addition, we develop algorithms to obtain upper\nbounds on the median and closest solutions, which are analysed by the\nexperiments presented on generated and on real databases. We show a fast way to\nfind consensus trees with better results than any tree in the input set while\nstill preserving all internal structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The mutational heterogeneity of tumours can be described with a tree\nrepresenting the evolutionary history of the tumour. With noisy sequencing data\nthere may be uncertainty in the inferred tree structure, while we may also wish\nto study patterns in the evolution of cancers in different patients. In such\nsituations, understanding tree similarities is a key challenge, and therefore\nwe present an approach to determine distances between trees. Considering the\nbounded height of trees, we determine the distances associated with the swap\noperations over strings. While in general, by solving the {\\sc Maximum Common\nAlmost $v$-tree} problem between two trees, we describe an efficient approach\nto determine the minimum number of operations to transform one tree into\nanother. The inherent noise in current statistical methods for constructing\nmutation evolution trees of cancer cells presents a significant challenge:\nhandling such collections of trees to determine a consensus tree that\naccurately represents the set and evaluating the extent of their variability or\ndispersion. Given a set of mutation trees and the notion of distance, there are\nat least two natural ways to define the ``target'' tree, such as a min-sum\n(\\emph{median tree}) or a min-max (\\emph{closest tree}) of a set of trees.\nThus, considering a set of trees as input and dealing with the {\\sc median} and\n{\\sc closest} problems, we prove that both problems are \\NP-complete, even with\nonly three input trees. In addition, we develop algorithms to obtain upper\nbounds on the median and closest solutions, which are analysed by the\nexperiments presented on generated and on real databases. We show a fast way to\nfind consensus trees with better results than any tree in the input set while\nstill preserving all internal structure."
                },
                "authors": [
                    {
                        "name": "LuÃ­s Cunha"
                    },
                    {
                        "name": "Jack Kuipers"
                    },
                    {
                        "name": "Thiago Lopes"
                    }
                ],
                "author_detail": {
                    "name": "Thiago Lopes"
                },
                "author": "Thiago Lopes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "05C05, 05C85",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07525v1",
                "updated": "2025-01-13T17:55:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    55,
                    32,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T17:55:32Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    55,
                    32,
                    0,
                    13,
                    0
                ],
                "title": "RadAlign: Advancing Radiology Report Generation with Vision-Language\n  Concept Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RadAlign: Advancing Radiology Report Generation with Vision-Language\n  Concept Alignment"
                },
                "summary": "Automated chest radiographs interpretation requires both accurate disease\nclassification and detailed radiology report generation, presenting a\nsignificant challenge in the clinical workflow. Current approaches either focus\non classification accuracy at the expense of interpretability or generate\ndetailed but potentially unreliable reports through image captioning\ntechniques. In this study, we present RadAlign, a novel framework that combines\nthe predictive accuracy of vision-language models (VLMs) with the reasoning\ncapabilities of large language models (LLMs). Inspired by the radiologist's\nworkflow, RadAlign first employs a specialized VLM to align visual features\nwith key medical concepts, achieving superior disease classification with an\naverage AUC of 0.885 across multiple diseases. These recognized medical\nconditions, represented as text-based concepts in the aligned visual-language\nspace, are then used to prompt LLM-based report generation. Enhanced by a\nretrieval-augmented generation mechanism that grounds outputs in similar\nhistorical cases, RadAlign delivers superior report quality with a GREEN score\nof 0.678, outperforming state-of-the-art methods' 0.634. Our framework\nmaintains strong clinical interpretability while reducing hallucinations,\nadvancing automated medical imaging and report analysis through integrated\npredictive and generative AI. Code is available at\nhttps://github.com/difeigu/RadAlign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated chest radiographs interpretation requires both accurate disease\nclassification and detailed radiology report generation, presenting a\nsignificant challenge in the clinical workflow. Current approaches either focus\non classification accuracy at the expense of interpretability or generate\ndetailed but potentially unreliable reports through image captioning\ntechniques. In this study, we present RadAlign, a novel framework that combines\nthe predictive accuracy of vision-language models (VLMs) with the reasoning\ncapabilities of large language models (LLMs). Inspired by the radiologist's\nworkflow, RadAlign first employs a specialized VLM to align visual features\nwith key medical concepts, achieving superior disease classification with an\naverage AUC of 0.885 across multiple diseases. These recognized medical\nconditions, represented as text-based concepts in the aligned visual-language\nspace, are then used to prompt LLM-based report generation. Enhanced by a\nretrieval-augmented generation mechanism that grounds outputs in similar\nhistorical cases, RadAlign delivers superior report quality with a GREEN score\nof 0.678, outperforming state-of-the-art methods' 0.634. Our framework\nmaintains strong clinical interpretability while reducing hallucinations,\nadvancing automated medical imaging and report analysis through integrated\npredictive and generative AI. Code is available at\nhttps://github.com/difeigu/RadAlign."
                },
                "authors": [
                    {
                        "name": "Difei Gu"
                    },
                    {
                        "name": "Yunhe Gao"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Mu Zhou"
                    },
                    {
                        "name": "Dimitris Metaxas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Metaxas"
                },
                "author": "Dimitris Metaxas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v1",
                "updated": "2025-01-13T17:50:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09698v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09698v5",
                "updated": "2025-01-13T17:48:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    48,
                    9,
                    0,
                    13,
                    0
                ],
                "published": "2024-08-19T04:44:32Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    44,
                    32,
                    0,
                    232,
                    0
                ],
                "title": "Harnessing Multimodal Large Language Models for Multimodal Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Multimodal Large Language Models for Multimodal Sequential\n  Recommendation"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences."
                },
                "authors": [
                    {
                        "name": "Yuyang Ye"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Yishan Shen"
                    },
                    {
                        "name": "Tianshu Wang"
                    },
                    {
                        "name": "Hengruo Zhang"
                    },
                    {
                        "name": "Peijun Zhu"
                    },
                    {
                        "name": "Runlong Yu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09698v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09698v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07493v1",
                "updated": "2025-01-13T17:12:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    12,
                    38,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T17:12:38Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    12,
                    38,
                    0,
                    13,
                    0
                ],
                "title": "Exploring and Mitigating Adversarial Manipulation of Voting-Based\n  Leaderboards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring and Mitigating Adversarial Manipulation of Voting-Based\n  Leaderboards"
                },
                "summary": "It is now common to evaluate Large Language Models (LLMs) by having humans\nmanually vote to evaluate model outputs, in contrast to typical benchmarks that\nevaluate knowledge or skill at some particular task. Chatbot Arena, the most\npopular benchmark of this type, ranks models by asking users to select the\nbetter response between two randomly selected models (without revealing which\nmodel was responsible for the generations). These platforms are widely trusted\nas a fair and accurate measure of LLM capabilities. In this paper, we show that\nif bot protection and other defenses are not implemented, these voting-based\nbenchmarks are potentially vulnerable to adversarial manipulation.\nSpecifically, we show that an attacker can alter the leaderboard (to promote\ntheir favorite model or demote competitors) at the cost of roughly a thousand\nvotes (verified in a simulated, offline version of Chatbot Arena). Our attack\nconsists of two steps: first, we show how an attacker can determine which model\nwas used to generate a given reply with more than $95\\%$ accuracy; and then,\nthe attacker can use this information to consistently vote for (or against) a\ntarget model. Working with the Chatbot Arena developers, we identify, propose,\nand implement mitigations to improve the robustness of Chatbot Arena against\nadversarial manipulation, which, based on our analysis, substantially increases\nthe cost of such attacks. Some of these defenses were present before our\ncollaboration, such as bot protection with Cloudflare, malicious user\ndetection, and rate limiting. Others, including reCAPTCHA and login are being\nintegrated to strengthen the security in Chatbot Arena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is now common to evaluate Large Language Models (LLMs) by having humans\nmanually vote to evaluate model outputs, in contrast to typical benchmarks that\nevaluate knowledge or skill at some particular task. Chatbot Arena, the most\npopular benchmark of this type, ranks models by asking users to select the\nbetter response between two randomly selected models (without revealing which\nmodel was responsible for the generations). These platforms are widely trusted\nas a fair and accurate measure of LLM capabilities. In this paper, we show that\nif bot protection and other defenses are not implemented, these voting-based\nbenchmarks are potentially vulnerable to adversarial manipulation.\nSpecifically, we show that an attacker can alter the leaderboard (to promote\ntheir favorite model or demote competitors) at the cost of roughly a thousand\nvotes (verified in a simulated, offline version of Chatbot Arena). Our attack\nconsists of two steps: first, we show how an attacker can determine which model\nwas used to generate a given reply with more than $95\\%$ accuracy; and then,\nthe attacker can use this information to consistently vote for (or against) a\ntarget model. Working with the Chatbot Arena developers, we identify, propose,\nand implement mitigations to improve the robustness of Chatbot Arena against\nadversarial manipulation, which, based on our analysis, substantially increases\nthe cost of such attacks. Some of these defenses were present before our\ncollaboration, such as bot protection with Cloudflare, malicious user\ndetection, and rate limiting. Others, including reCAPTCHA and login are being\nintegrated to strengthen the security in Chatbot Arena."
                },
                "authors": [
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Milad Nasr"
                    },
                    {
                        "name": "Anastasios Angelopoulos"
                    },
                    {
                        "name": "Nicholas Carlini"
                    },
                    {
                        "name": "Wei-Lin Chiang"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "Daphne Ippolito"
                    },
                    {
                        "name": "Matthew Jagielski"
                    },
                    {
                        "name": "Katherine Lee"
                    },
                    {
                        "name": "Ken Ziyu Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Florian Tramer"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chiyuan Zhang"
                },
                "author": "Chiyuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07487v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07487v1",
                "updated": "2025-01-13T17:04:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    4,
                    23,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T17:04:23Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    4,
                    23,
                    0,
                    13,
                    0
                ],
                "title": "Data and System Perspectives of Sustainable Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data and System Perspectives of Sustainable Artificial Intelligence"
                },
                "summary": "Sustainable AI is a subfield of AI for concerning developing and using AI\nsystems in ways of aiming to reduce environmental impact and achieve\nsustainability. Sustainable AI is increasingly important given that training of\nand inference with AI models such as large langrage models are consuming a\nlarge amount of computing power. In this article, we discuss current issues,\nopportunities and example solutions for addressing these issues, and future\nchallenges to tackle, from the data and system perspectives, related to data\nacquisition, data processing, and AI model training and inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sustainable AI is a subfield of AI for concerning developing and using AI\nsystems in ways of aiming to reduce environmental impact and achieve\nsustainability. Sustainable AI is increasingly important given that training of\nand inference with AI models such as large langrage models are consuming a\nlarge amount of computing power. In this article, we discuss current issues,\nopportunities and example solutions for addressing these issues, and future\nchallenges to tackle, from the data and system perspectives, related to data\nacquisition, data processing, and AI model training and inference."
                },
                "authors": [
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "David Harel"
                    },
                    {
                        "name": "Dezhi Ran"
                    },
                    {
                        "name": "Zhenwen Li"
                    },
                    {
                        "name": "Maoliang Li"
                    },
                    {
                        "name": "Zhi Yang"
                    },
                    {
                        "name": "Leye Wang"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Ying Zhang"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Linyi Li"
                    },
                    {
                        "name": "Assaf Marron"
                    }
                ],
                "author_detail": {
                    "name": "Assaf Marron"
                },
                "author": "Assaf Marron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07487v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.09998v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.09998v5",
                "updated": "2025-01-13T17:01:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    1,
                    23,
                    0,
                    13,
                    0
                ],
                "published": "2023-07-19T14:13:02Z",
                "published_parsed": [
                    2023,
                    7,
                    19,
                    14,
                    13,
                    2,
                    2,
                    200,
                    0
                ],
                "title": "Controlling Equational Reasoning in Large Language Models with Prompt\n  Interventions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Equational Reasoning in Large Language Models with Prompt\n  Interventions"
                },
                "summary": "This paper investigates how hallucination rates in Large Language Models\n(LLMs) may be controlled via a symbolic data generation framework, exploring a\nfundamental relationship between the rate of certain mathematical errors and\ntypes of input intervention. Specifically, we systematically generate data for\na derivation generation task using a symbolic engine, applying targeted\ninterventions to prompts to perturb features of mathematical derivations such\nas the surface forms of symbols, equational tree structures, and mathematical\ncontext. We then evaluate the effect of prompt interventions across a range of\nLLMs including fine-tuned T5 models, GPT, and LLaMa-based models. Our\nexperiments suggest that T5-Large can outperform the few-shot performance of\nGPT-4 on various evaluation sets generated via the framework. However, an\nextensive evaluation based on human analysis, template-based error detection,\nand text generation metrics reveals model weaknesses beyond what the\nreference-based metrics singularly describe. We use these results to tie\ncharacteristic distributional footprints of interventions to the human\nevaluation of LLM derivation quality, potentially leading to significant\ncontrol over fine-grained mathematical capabilities of language models with\nrespect to specific types of errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates how hallucination rates in Large Language Models\n(LLMs) may be controlled via a symbolic data generation framework, exploring a\nfundamental relationship between the rate of certain mathematical errors and\ntypes of input intervention. Specifically, we systematically generate data for\na derivation generation task using a symbolic engine, applying targeted\ninterventions to prompts to perturb features of mathematical derivations such\nas the surface forms of symbols, equational tree structures, and mathematical\ncontext. We then evaluate the effect of prompt interventions across a range of\nLLMs including fine-tuned T5 models, GPT, and LLaMa-based models. Our\nexperiments suggest that T5-Large can outperform the few-shot performance of\nGPT-4 on various evaluation sets generated via the framework. However, an\nextensive evaluation based on human analysis, template-based error detection,\nand text generation metrics reveals model weaknesses beyond what the\nreference-based metrics singularly describe. We use these results to tie\ncharacteristic distributional footprints of interventions to the human\nevaluation of LLM derivation quality, potentially leading to significant\ncontrol over fine-grained mathematical capabilities of language models with\nrespect to specific types of errors."
                },
                "authors": [
                    {
                        "name": "Jordan Meadows"
                    },
                    {
                        "name": "Marco Valentino"
                    },
                    {
                        "name": "Andre Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Andre Freitas"
                },
                "author": "Andre Freitas",
                "arxiv_comment": "AAAI 2025 (7 pages)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.09998v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.09998v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.HO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00843v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00843v3",
                "updated": "2025-01-13T16:58:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    16,
                    58,
                    43,
                    0,
                    13,
                    0
                ],
                "published": "2024-06-30T22:33:47Z",
                "published_parsed": [
                    2024,
                    6,
                    30,
                    22,
                    33,
                    47,
                    6,
                    182,
                    0
                ],
                "title": "A Unified Approach to Extract Interpretable Rules from Tree Ensembles\n  via Integer Programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Approach to Extract Interpretable Rules from Tree Ensembles\n  via Integer Programming"
                },
                "summary": "Tree ensembles are very popular machine learning models, known for their\neffectiveness in supervised classification and regression tasks. Their\nperformance derives from aggregating predictions of multiple decision trees,\nwhich are renowned for their interpretability properties. However, tree\nensemble models do not reliably exhibit interpretable output. Our work aims to\nextract an optimized list of rules from a trained tree ensemble, providing the\nuser with a condensed, interpretable model that retains most of the predictive\npower of the full model. Our approach consists of solving a set partitioning\nproblem formulated through Integer Programming. The proposed method works with\neither tabular or time series data, for both classification and regression\ntasks, and its flexible formulation can include any arbitrary loss or\nregularization functions. Our extensive computational experiments offer\nstatistically significant evidence that our method is competitive with other\nrule extraction methods in terms of predictive performance and fidelity towards\nthe tree ensemble. Moreover, we empirically show that the proposed method\neffectively extracts interpretable rules from tree ensemble that are designed\nfor time series data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree ensembles are very popular machine learning models, known for their\neffectiveness in supervised classification and regression tasks. Their\nperformance derives from aggregating predictions of multiple decision trees,\nwhich are renowned for their interpretability properties. However, tree\nensemble models do not reliably exhibit interpretable output. Our work aims to\nextract an optimized list of rules from a trained tree ensemble, providing the\nuser with a condensed, interpretable model that retains most of the predictive\npower of the full model. Our approach consists of solving a set partitioning\nproblem formulated through Integer Programming. The proposed method works with\neither tabular or time series data, for both classification and regression\ntasks, and its flexible formulation can include any arbitrary loss or\nregularization functions. Our extensive computational experiments offer\nstatistically significant evidence that our method is competitive with other\nrule extraction methods in terms of predictive performance and fidelity towards\nthe tree ensemble. Moreover, we empirically show that the proposed method\neffectively extracts interpretable rules from tree ensemble that are designed\nfor time series data."
                },
                "authors": [
                    {
                        "name": "Lorenzo Bonasera"
                    },
                    {
                        "name": "Emilio Carrizosa"
                    }
                ],
                "author_detail": {
                    "name": "Emilio Carrizosa"
                },
                "author": "Emilio Carrizosa",
                "arxiv_comment": "- Improved overall manuscript flow and clearness - Added related work\n  on explanation fidelity - Added computational results on fidelity - Fixed\n  some flaws on data inference - Optimization problem with weighted objectives\n  - Added appendix containing qualitative examples - New computational results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00843v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00843v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07482v1",
                "updated": "2025-01-13T16:58:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    16,
                    58,
                    32,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T16:58:32Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    16,
                    58,
                    32,
                    0,
                    13,
                    0
                ],
                "title": "TiEBe: A Benchmark for Assessing the Current Knowledge of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TiEBe: A Benchmark for Assessing the Current Knowledge of Large Language\n  Models"
                },
                "summary": "In a rapidly evolving knowledge landscape and the increasing adoption of\nlarge language models, a need has emerged to keep these models continuously\nupdated with current events. While existing benchmarks evaluate general factual\nrecall, they often overlook two critical aspects: the ability of models to\nintegrate evolving knowledge through continual learning and the significant\nregional disparities in their performance. To address these gaps, we introduce\nthe Timely Events Benchmark (TiEBe), a dataset containing over 11,000\nquestion-answer pairs focused on globally and regionally significant events.\nTiEBe leverages structured retrospective data from Wikipedia, enabling\ncontinuous updates to assess LLMs' knowledge of evolving global affairs and\ntheir understanding of events across different regions. Our benchmark\ndemonstrates that LLMs exhibit substantial geographic disparities in factual\nrecall, emphasizing the need for more balanced global knowledge representation.\nFurthermore, TiEBe serves as a tool for evaluating continual learning\nstrategies, providing insights into models' ability to acquire new information\nwithout forgetting past knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a rapidly evolving knowledge landscape and the increasing adoption of\nlarge language models, a need has emerged to keep these models continuously\nupdated with current events. While existing benchmarks evaluate general factual\nrecall, they often overlook two critical aspects: the ability of models to\nintegrate evolving knowledge through continual learning and the significant\nregional disparities in their performance. To address these gaps, we introduce\nthe Timely Events Benchmark (TiEBe), a dataset containing over 11,000\nquestion-answer pairs focused on globally and regionally significant events.\nTiEBe leverages structured retrospective data from Wikipedia, enabling\ncontinuous updates to assess LLMs' knowledge of evolving global affairs and\ntheir understanding of events across different regions. Our benchmark\ndemonstrates that LLMs exhibit substantial geographic disparities in factual\nrecall, emphasizing the need for more balanced global knowledge representation.\nFurthermore, TiEBe serves as a tool for evaluating continual learning\nstrategies, providing insights into models' ability to acquire new information\nwithout forgetting past knowledge."
                },
                "authors": [
                    {
                        "name": "Thales Sales Almeida"
                    },
                    {
                        "name": "Giovana Kerche BonÃ¡s"
                    },
                    {
                        "name": "JoÃ£o Guilherme Alves Santos"
                    },
                    {
                        "name": "Hugo Abonizio"
                    },
                    {
                        "name": "Rodrigo Nogueira"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Nogueira"
                },
                "author": "Rodrigo Nogueira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16314v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16314v3",
                "updated": "2025-01-13T16:53:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    16,
                    53,
                    2,
                    0,
                    13,
                    0
                ],
                "published": "2024-10-09T10:09:37Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    10,
                    9,
                    37,
                    2,
                    283,
                    0
                ],
                "title": "Steering Large Language Models using Conceptors: Improving\n  Addition-Based Activation Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Large Language Models using Conceptors: Improving\n  Addition-Based Activation Engineering"
                },
                "summary": "Large language models have transformed AI, yet reliably controlling their\noutputs remains a challenge. This paper explores activation engineering, where\noutputs of pre-trained LLMs are controlled by manipulating their activations at\ninference time. Unlike traditional methods using a single steering vector, we\nintroduce conceptors - mathematical constructs that represent sets of\nactivation vectors as ellipsoidal regions. Conceptors act as soft projection\nmatrices and offer more precise control over complex activation patterns. Our\nexperiments demonstrate that conceptors outperform traditional methods across\nmultiple steering tasks. We further use Boolean operations on conceptors for\ncombined steering goals that empirically outperform additively combining\nsteering vectors on a set of tasks. These results highlight conceptors as a\npromising tool for more effective steering of LLMs. Our code is available on\ngithub.com/jorispos/conceptorsteering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have transformed AI, yet reliably controlling their\noutputs remains a challenge. This paper explores activation engineering, where\noutputs of pre-trained LLMs are controlled by manipulating their activations at\ninference time. Unlike traditional methods using a single steering vector, we\nintroduce conceptors - mathematical constructs that represent sets of\nactivation vectors as ellipsoidal regions. Conceptors act as soft projection\nmatrices and offer more precise control over complex activation patterns. Our\nexperiments demonstrate that conceptors outperform traditional methods across\nmultiple steering tasks. We further use Boolean operations on conceptors for\ncombined steering goals that empirically outperform additively combining\nsteering vectors on a set of tasks. These results highlight conceptors as a\npromising tool for more effective steering of LLMs. Our code is available on\ngithub.com/jorispos/conceptorsteering."
                },
                "authors": [
                    {
                        "name": "Joris Postmus"
                    },
                    {
                        "name": "Steven Abreu"
                    }
                ],
                "author_detail": {
                    "name": "Steven Abreu"
                },
                "author": "Steven Abreu",
                "arxiv_comment": "Presented at the MINT workshop at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16314v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16314v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07458v1",
                "updated": "2025-01-13T16:28:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    16,
                    28,
                    1,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T16:28:01Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    16,
                    28,
                    1,
                    0,
                    13,
                    0
                ],
                "title": "Understanding and Benchmarking Artificial Intelligence: OpenAI's o3 Is\n  Not AGI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Benchmarking Artificial Intelligence: OpenAI's o3 Is\n  Not AGI"
                },
                "summary": "OpenAI's o3 achieves a high score of 87.5 % on ARC-AGI, a benchmark proposed\nto measure intelligence. This raises the question whether systems based on\nLarge Language Models (LLMs), particularly o3, demonstrate intelligence and\nprogress towards artificial general intelligence (AGI). Building on the\ndistinction between skills and intelligence made by Fran\\c{c}ois Chollet, the\ncreator of ARC-AGI, a new understanding of intelligence is introduced: an agent\nis the more intelligent, the more efficiently it can achieve the more diverse\ngoals in the more diverse worlds with the less knowledge. An analysis of the\nARC-AGI benchmark shows that its tasks represent a very specific type of\nproblem that can be solved by massive trialling of combinations of predefined\noperations. This method is also applied by o3, achieving its high score through\nthe extensive use of computing power. However, for most problems in the\nphysical world and in the human domain, solutions cannot be tested in advance\nand predefined operations are not available. Consequently, massive trialling of\npredefined operations, as o3 does, cannot be a basis for AGI - instead, new\napproaches are required that can reliably solve a wide variety of problems\nwithout existing skills. To support this development, a new benchmark for\nintelligence is outlined that covers a much higher diversity of unknown tasks\nto be solved, thus enabling a comprehensive assessment of intelligence and of\nprogress towards AGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenAI's o3 achieves a high score of 87.5 % on ARC-AGI, a benchmark proposed\nto measure intelligence. This raises the question whether systems based on\nLarge Language Models (LLMs), particularly o3, demonstrate intelligence and\nprogress towards artificial general intelligence (AGI). Building on the\ndistinction between skills and intelligence made by Fran\\c{c}ois Chollet, the\ncreator of ARC-AGI, a new understanding of intelligence is introduced: an agent\nis the more intelligent, the more efficiently it can achieve the more diverse\ngoals in the more diverse worlds with the less knowledge. An analysis of the\nARC-AGI benchmark shows that its tasks represent a very specific type of\nproblem that can be solved by massive trialling of combinations of predefined\noperations. This method is also applied by o3, achieving its high score through\nthe extensive use of computing power. However, for most problems in the\nphysical world and in the human domain, solutions cannot be tested in advance\nand predefined operations are not available. Consequently, massive trialling of\npredefined operations, as o3 does, cannot be a basis for AGI - instead, new\napproaches are required that can reliably solve a wide variety of problems\nwithout existing skills. To support this development, a new benchmark for\nintelligence is outlined that covers a much higher diversity of unknown tasks\nto be solved, thus enabling a comprehensive assessment of intelligence and of\nprogress towards AGI."
                },
                "authors": [
                    {
                        "name": "Rolf Pfister"
                    },
                    {
                        "name": "Hansueli Jud"
                    }
                ],
                "author_detail": {
                    "name": "Hansueli Jud"
                },
                "author": "Hansueli Jud",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07425v1",
                "updated": "2025-01-13T15:43:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    15,
                    43,
                    36,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T15:43:36Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    15,
                    43,
                    36,
                    0,
                    13,
                    0
                ],
                "title": "Enhancing LLM's Ability to Generate More Repository-Aware Unit Tests\n  Through Precise Contextual Information Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM's Ability to Generate More Repository-Aware Unit Tests\n  Through Precise Contextual Information Injection"
                },
                "summary": "Though many learning-based approaches have been proposed for unit test\ngeneration and achieved remarkable performance, they still have limitations in\nrelying on task-specific datasets. Recently, Large Language Models (LLMs)\nguided by prompt engineering have gained attention for their ability to handle\na broad range of tasks, including unit test generation. Despite their success,\nLLMs may exhibit hallucinations when generating unit tests for focal methods or\nfunctions due to their lack of awareness regarding the project's global\ncontext. These hallucinations may manifest as calls to non-existent methods, as\nwell as incorrect parameters or return values, such as mismatched parameter\ntypes or numbers. While many studies have explored the role of context, they\noften extract fixed patterns of context for different models and focal methods,\nwhich may not be suitable for all generation processes (e.g., excessive\nirrelevant context could lead to redundancy, preventing the model from focusing\non essential information). To overcome this limitation, we propose RATester,\nwhich enhances the LLM's ability to generate more repository-aware unit tests\nthrough global contextual information injection. To equip LLMs with global\nknowledge similar to that of human testers, we integrate the language server\ngopls, which provides essential features (e.g., definition lookup) to assist\nthe LLM. When RATester encounters an unfamiliar identifier (e.g., an unfamiliar\nstruct name), it first leverages gopls to fetch relevant definitions and\ndocumentation comments, and then uses this global knowledge to guide the LLM.\nBy utilizing gopls, RATester enriches the LLM's knowledge of the project's\nglobal context, thereby reducing hallucinations during unit test generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though many learning-based approaches have been proposed for unit test\ngeneration and achieved remarkable performance, they still have limitations in\nrelying on task-specific datasets. Recently, Large Language Models (LLMs)\nguided by prompt engineering have gained attention for their ability to handle\na broad range of tasks, including unit test generation. Despite their success,\nLLMs may exhibit hallucinations when generating unit tests for focal methods or\nfunctions due to their lack of awareness regarding the project's global\ncontext. These hallucinations may manifest as calls to non-existent methods, as\nwell as incorrect parameters or return values, such as mismatched parameter\ntypes or numbers. While many studies have explored the role of context, they\noften extract fixed patterns of context for different models and focal methods,\nwhich may not be suitable for all generation processes (e.g., excessive\nirrelevant context could lead to redundancy, preventing the model from focusing\non essential information). To overcome this limitation, we propose RATester,\nwhich enhances the LLM's ability to generate more repository-aware unit tests\nthrough global contextual information injection. To equip LLMs with global\nknowledge similar to that of human testers, we integrate the language server\ngopls, which provides essential features (e.g., definition lookup) to assist\nthe LLM. When RATester encounters an unfamiliar identifier (e.g., an unfamiliar\nstruct name), it first leverages gopls to fetch relevant definitions and\ndocumentation comments, and then uses this global knowledge to guide the LLM.\nBy utilizing gopls, RATester enriches the LLM's knowledge of the project's\nglobal context, thereby reducing hallucinations during unit test generation."
                },
                "authors": [
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Chao Ni"
                    },
                    {
                        "name": "Xinrui Li"
                    },
                    {
                        "name": "Liushan Chen"
                    },
                    {
                        "name": "Guojun Ma"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11547v2",
                "updated": "2025-01-13T15:37:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    15,
                    37,
                    3,
                    0,
                    13,
                    0
                ],
                "published": "2024-09-17T20:40:02Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    20,
                    40,
                    2,
                    1,
                    261,
                    0
                ],
                "title": "Small Language Models can Outperform Humans in Short Creative Writing: A\n  Study Comparing SLMs with Humans and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Models can Outperform Humans in Short Creative Writing: A\n  Study Comparing SLMs with Humans and LLMs"
                },
                "summary": "In this paper, we evaluate the creative fiction writing abilities of a\nfine-tuned small language model (SLM), BART-large, and compare its performance\nto human writers and two large language models (LLMs): GPT-3.5 and GPT-4o. Our\nevaluation consists of two experiments: (i) a human study in which 68\nparticipants rated short stories from humans and the SLM on grammaticality,\nrelevance, creativity, and attractiveness, and (ii) a qualitative linguistic\nanalysis examining the textual characteristics of stories produced by each\nmodel. In the first experiment, BART-large outscored average human writers\noverall (2.11 vs. 1.85), a 14% relative improvement, though the slight human\nadvantage in creativity was not statistically significant. In the second\nexperiment, qualitative analysis showed that while GPT-4o demonstrated\nnear-perfect coherence and used less cliche phrases, it tended to produce more\npredictable language, with only 3% of its synopses featuring surprising\nassociations (compared to 15% for BART). These findings highlight how model\nsize and fine-tuning influence the balance between creativity, fluency, and\ncoherence in creative writing tasks, and demonstrate that smaller models can,\nin certain contexts, rival both humans and larger models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we evaluate the creative fiction writing abilities of a\nfine-tuned small language model (SLM), BART-large, and compare its performance\nto human writers and two large language models (LLMs): GPT-3.5 and GPT-4o. Our\nevaluation consists of two experiments: (i) a human study in which 68\nparticipants rated short stories from humans and the SLM on grammaticality,\nrelevance, creativity, and attractiveness, and (ii) a qualitative linguistic\nanalysis examining the textual characteristics of stories produced by each\nmodel. In the first experiment, BART-large outscored average human writers\noverall (2.11 vs. 1.85), a 14% relative improvement, though the slight human\nadvantage in creativity was not statistically significant. In the second\nexperiment, qualitative analysis showed that while GPT-4o demonstrated\nnear-perfect coherence and used less cliche phrases, it tended to produce more\npredictable language, with only 3% of its synopses featuring surprising\nassociations (compared to 15% for BART). These findings highlight how model\nsize and fine-tuning influence the balance between creativity, fluency, and\ncoherence in creative writing tasks, and demonstrate that smaller models can,\nin certain contexts, rival both humans and larger models."
                },
                "authors": [
                    {
                        "name": "Guillermo Marco"
                    },
                    {
                        "name": "Luz Rello"
                    },
                    {
                        "name": "Julio Gonzalo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Gonzalo"
                },
                "author": "Julio Gonzalo",
                "arxiv_comment": "Accepted as Main Conference Paper at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09536v2",
                "updated": "2025-01-13T15:25:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    15,
                    25,
                    37,
                    0,
                    13,
                    0
                ],
                "published": "2024-08-18T16:44:01Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    16,
                    44,
                    1,
                    6,
                    231,
                    0
                ],
                "title": "Galapagos: Automated N-Version Programming with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galapagos: Automated N-Version Programming with LLMs"
                },
                "summary": "N-Version Programming is a well-known methodology for developing\nfault-tolerant systems. It achieves fault detection and correction at runtime\nby adding diverse redundancy into programs, minimizing fault mode overlap\nbetween redundant program variants. In this work, we propose the automated\ngeneration of program variants using large language models. We design, develop\nand evaluate Gal\\'apagos: a tool for generating program variants using LLMs,\nvalidating their correctness and equivalence, and using them to assemble\nN-Version binaries. We evaluate Gal\\'apagos by creating N-Version components of\nreal-world C code. Our original results show that Gal\\'apagos can produce\nprogram variants that are proven to be functionally equivalent, even when the\nvariants are written in a different programming language. Our systematic\ndiversity measurement indicates that functionally equivalent variants produced\nby Gal\\'apagos, are statically different after compilation, and present\ndiverging internal behavior at runtime. We demonstrate that the variants\nproduced by Gal\\'apagos can protect C code against real miscompilation bugs\nwhich affect the Clang compiler. Overall, our paper shows that producing\nN-Version software can be drastically automated by advanced usage of practical\nformal verification and generative language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N-Version Programming is a well-known methodology for developing\nfault-tolerant systems. It achieves fault detection and correction at runtime\nby adding diverse redundancy into programs, minimizing fault mode overlap\nbetween redundant program variants. In this work, we propose the automated\ngeneration of program variants using large language models. We design, develop\nand evaluate Gal\\'apagos: a tool for generating program variants using LLMs,\nvalidating their correctness and equivalence, and using them to assemble\nN-Version binaries. We evaluate Gal\\'apagos by creating N-Version components of\nreal-world C code. Our original results show that Gal\\'apagos can produce\nprogram variants that are proven to be functionally equivalent, even when the\nvariants are written in a different programming language. Our systematic\ndiversity measurement indicates that functionally equivalent variants produced\nby Gal\\'apagos, are statically different after compilation, and present\ndiverging internal behavior at runtime. We demonstrate that the variants\nproduced by Gal\\'apagos can protect C code against real miscompilation bugs\nwhich affect the Clang compiler. Overall, our paper shows that producing\nN-Version software can be drastically automated by advanced usage of practical\nformal verification and generative language models."
                },
                "authors": [
                    {
                        "name": "Javier Ron"
                    },
                    {
                        "name": "Diogo Gaspar"
                    },
                    {
                        "name": "Javier Cabrera-Arteaga"
                    },
                    {
                        "name": "Benoit Baudry"
                    },
                    {
                        "name": "Martin Monperrus"
                    }
                ],
                "author_detail": {
                    "name": "Martin Monperrus"
                },
                "author": "Martin Monperrus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07408v1",
                "updated": "2025-01-13T15:24:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    15,
                    24,
                    10,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T15:24:10Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    15,
                    24,
                    10,
                    0,
                    13,
                    0
                ],
                "title": "Initial Findings on Sensor based Open Vocabulary Activity Recognition\n  via Text Embedding Inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Initial Findings on Sensor based Open Vocabulary Activity Recognition\n  via Text Embedding Inversion"
                },
                "summary": "Conventional human activity recognition (HAR) relies on classifiers trained\nto predict discrete activity classes, inherently limiting recognition to\nactivities explicitly present in the training set. Such classifiers would\ninvariably fail, putting zero likelihood, when encountering unseen activities.\nWe propose Open Vocabulary HAR (OV-HAR), a framework that overcomes this\nlimitation by first converting each activity into natural language and breaking\nit into a sequence of elementary motions. This descriptive text is then encoded\ninto a fixed-size embedding. The model is trained to regress this embedding,\nwhich is subsequently decoded back into natural language using a pre-trained\nembedding inversion model. Unlike other works that rely on auto-regressive\nlarge language models (LLMs) at their core, OV-HAR achieves open vocabulary\nrecognition without the computational overhead of such models. The generated\ntext can be transformed into a single activity class using LLM prompt\nengineering. We have evaluated our approach on different modalities, including\nvision (pose), IMU, and pressure sensors, demonstrating robust generalization\nacross unseen activities and modalities, offering a fundamentally different\nparadigm from contemporary classifiers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional human activity recognition (HAR) relies on classifiers trained\nto predict discrete activity classes, inherently limiting recognition to\nactivities explicitly present in the training set. Such classifiers would\ninvariably fail, putting zero likelihood, when encountering unseen activities.\nWe propose Open Vocabulary HAR (OV-HAR), a framework that overcomes this\nlimitation by first converting each activity into natural language and breaking\nit into a sequence of elementary motions. This descriptive text is then encoded\ninto a fixed-size embedding. The model is trained to regress this embedding,\nwhich is subsequently decoded back into natural language using a pre-trained\nembedding inversion model. Unlike other works that rely on auto-regressive\nlarge language models (LLMs) at their core, OV-HAR achieves open vocabulary\nrecognition without the computational overhead of such models. The generated\ntext can be transformed into a single activity class using LLM prompt\nengineering. We have evaluated our approach on different modalities, including\nvision (pose), IMU, and pressure sensors, demonstrating robust generalization\nacross unseen activities and modalities, offering a fundamentally different\nparadigm from contemporary classifiers."
                },
                "authors": [
                    {
                        "name": "Lala Shakti Swarup Ray"
                    },
                    {
                        "name": "Bo Zhou"
                    },
                    {
                        "name": "Sungho Suh"
                    },
                    {
                        "name": "Paul Lukowicz"
                    }
                ],
                "author_detail": {
                    "name": "Paul Lukowicz"
                },
                "author": "Paul Lukowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07373v1",
                "updated": "2025-01-13T14:41:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    14,
                    41,
                    56,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T14:41:56Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    14,
                    41,
                    56,
                    0,
                    13,
                    0
                ],
                "title": "Dynami-CAL GraphNet: A Physics-Informed Graph Neural Network Conserving\n  Linear and Angular Momentum for Dynamical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynami-CAL GraphNet: A Physics-Informed Graph Neural Network Conserving\n  Linear and Angular Momentum for Dynamical Systems"
                },
                "summary": "Accurate, interpretable, and real-time modeling of multi-body dynamical\nsystems is essential for predicting behaviors and inferring physical properties\nin natural and engineered environments. Traditional physics-based models face\nscalability challenges and are computationally demanding, while data-driven\napproaches like Graph Neural Networks (GNNs) often lack physical consistency,\ninterpretability, and generalization. In this paper, we propose Dynami-CAL\nGraphNet, a Physics-Informed Graph Neural Network that integrates the learning\ncapabilities of GNNs with physics-based inductive biases to address these\nlimitations. Dynami-CAL GraphNet enforces pairwise conservation of linear and\nangular momentum for interacting nodes using edge-local reference frames that\nare equivariant to rotational symmetries, invariant to translations, and\nequivariant to node permutations. This design ensures physically consistent\npredictions of node dynamics while offering interpretable, edge-wise linear and\nangular impulses resulting from pairwise interactions. Evaluated on a 3D\ngranular system with inelastic collisions, Dynami-CAL GraphNet demonstrates\nstable error accumulation over extended rollouts, effective extrapolations to\nunseen configurations, and robust handling of heterogeneous interactions and\nexternal forces. Dynami-CAL GraphNet offers significant advantages in fields\nrequiring accurate, interpretable, and real-time modeling of complex multi-body\ndynamical systems, such as robotics, aerospace engineering, and materials\nscience. By providing physically consistent and scalable predictions that\nadhere to fundamental conservation laws, it enables the inference of forces and\nmoments while efficiently handling heterogeneous interactions and external\nforces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate, interpretable, and real-time modeling of multi-body dynamical\nsystems is essential for predicting behaviors and inferring physical properties\nin natural and engineered environments. Traditional physics-based models face\nscalability challenges and are computationally demanding, while data-driven\napproaches like Graph Neural Networks (GNNs) often lack physical consistency,\ninterpretability, and generalization. In this paper, we propose Dynami-CAL\nGraphNet, a Physics-Informed Graph Neural Network that integrates the learning\ncapabilities of GNNs with physics-based inductive biases to address these\nlimitations. Dynami-CAL GraphNet enforces pairwise conservation of linear and\nangular momentum for interacting nodes using edge-local reference frames that\nare equivariant to rotational symmetries, invariant to translations, and\nequivariant to node permutations. This design ensures physically consistent\npredictions of node dynamics while offering interpretable, edge-wise linear and\nangular impulses resulting from pairwise interactions. Evaluated on a 3D\ngranular system with inelastic collisions, Dynami-CAL GraphNet demonstrates\nstable error accumulation over extended rollouts, effective extrapolations to\nunseen configurations, and robust handling of heterogeneous interactions and\nexternal forces. Dynami-CAL GraphNet offers significant advantages in fields\nrequiring accurate, interpretable, and real-time modeling of complex multi-body\ndynamical systems, such as robotics, aerospace engineering, and materials\nscience. By providing physically consistent and scalable predictions that\nadhere to fundamental conservation laws, it enables the inference of forces and\nmoments while efficiently handling heterogeneous interactions and external\nforces."
                },
                "authors": [
                    {
                        "name": "Vinay Sharma"
                    },
                    {
                        "name": "Olga Fink"
                    }
                ],
                "author_detail": {
                    "name": "Olga Fink"
                },
                "author": "Olga Fink",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09718v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09718v2",
                "updated": "2025-01-13T14:37:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    14,
                    37,
                    52,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-12T20:48:06Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    20,
                    48,
                    6,
                    3,
                    347,
                    0
                ],
                "title": "BayesAdapter: enhanced uncertainty estimation in CLIP few-shot\n  adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayesAdapter: enhanced uncertainty estimation in CLIP few-shot\n  adaptation"
                },
                "summary": "The emergence of large pre-trained vision-language models (VLMs) represents a\nparadigm shift in machine learning, with unprecedented results in a broad span\nof visual recognition tasks. CLIP, one of the most popular VLMs, has exhibited\nremarkable zero-shot and transfer learning capabilities in classification. To\ntransfer CLIP to downstream tasks, adapters constitute a parameter-efficient\napproach that avoids backpropagation through the large model (unlike related\nprompt learning methods). However, CLIP adapters have been developed to target\ndiscriminative performance, and the quality of their uncertainty estimates has\nbeen overlooked. In this work we show that the discriminative performance of\nstate-of-the-art CLIP adapters does not always correlate with their uncertainty\nestimation capabilities, which are essential for a safe deployment in\nreal-world scenarios. We also demonstrate that one of such adapters is obtained\nthrough MAP inference from a more general probabilistic framework. Based on\nthis observation we introduce BayesAdapter, which leverages Bayesian inference\nto estimate a full probability distribution instead of a single point, better\ncapturing the variability inherent in the parameter space. In a comprehensive\nempirical evaluation we show that our approach obtains high quality uncertainty\nestimates in the predictions, standing out in calibration and selective\nclassification. Our code will be publicly available upon acceptance of the\npaper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large pre-trained vision-language models (VLMs) represents a\nparadigm shift in machine learning, with unprecedented results in a broad span\nof visual recognition tasks. CLIP, one of the most popular VLMs, has exhibited\nremarkable zero-shot and transfer learning capabilities in classification. To\ntransfer CLIP to downstream tasks, adapters constitute a parameter-efficient\napproach that avoids backpropagation through the large model (unlike related\nprompt learning methods). However, CLIP adapters have been developed to target\ndiscriminative performance, and the quality of their uncertainty estimates has\nbeen overlooked. In this work we show that the discriminative performance of\nstate-of-the-art CLIP adapters does not always correlate with their uncertainty\nestimation capabilities, which are essential for a safe deployment in\nreal-world scenarios. We also demonstrate that one of such adapters is obtained\nthrough MAP inference from a more general probabilistic framework. Based on\nthis observation we introduce BayesAdapter, which leverages Bayesian inference\nto estimate a full probability distribution instead of a single point, better\ncapturing the variability inherent in the parameter space. In a comprehensive\nempirical evaluation we show that our approach obtains high quality uncertainty\nestimates in the predictions, standing out in calibration and selective\nclassification. Our code will be publicly available upon acceptance of the\npaper."
                },
                "authors": [
                    {
                        "name": "Pablo Morales-Ãlvarez"
                    },
                    {
                        "name": "Stergios Christodoulidis"
                    },
                    {
                        "name": "Maria Vakalopoulou"
                    },
                    {
                        "name": "Pablo Piantanida"
                    },
                    {
                        "name": "Jose Dolz"
                    }
                ],
                "author_detail": {
                    "name": "Jose Dolz"
                },
                "author": "Jose Dolz",
                "arxiv_comment": "30 pages, 5 figures, 23 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09718v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16531v2",
                "updated": "2025-01-13T14:34:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    14,
                    34,
                    40,
                    0,
                    13,
                    0
                ],
                "published": "2024-06-24T11:10:41Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    11,
                    10,
                    41,
                    0,
                    176,
                    0
                ],
                "title": "GIM: A Million-scale Benchmark for Generative Image Manipulation\n  Detection and Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GIM: A Million-scale Benchmark for Generative Image Manipulation\n  Detection and Localization"
                },
                "summary": "The extraordinary ability of generative models emerges as a new trend in\nimage editing and generating realistic images, posing a serious threat to the\ntrustworthiness of multimedia data and driving the research of image\nmanipulation detection and location (IMDL). However, the lack of a large-scale\ndata foundation makes the IMDL task unattainable. In this paper, we build a\nlocal manipulation data generation pipeline that integrates the powerful\ncapabilities of SAM, LLM, and generative models. Upon this basis, we propose\nthe GIM dataset, which has the following advantages: 1) Large scale, GIM\nincludes over one million pairs of AI-manipulated images and real images. 2)\nRich image content, GIM encompasses a broad range of image classes. 3) Diverse\ngenerative manipulation, the images are manipulated images with\nstate-of-the-art generators and various manipulation tasks. The aforementioned\nadvantages allow for a more comprehensive evaluation of IMDL methods, extending\ntheir applicability to diverse images. We introduce the GIM benchmark with two\nsettings to evaluate existing IMDL methods. In addition, we propose a novel\nIMDL framework, termed GIMFormer, which consists of a ShadowTracer,\nFrequency-Spatial block (FSB), and a Multi-Window Anomalous Modeling (MWAM)\nmodule. Extensive experiments on the GIM demonstrate that GIMFormer surpasses\nthe previous state-of-the-art approach on two different benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extraordinary ability of generative models emerges as a new trend in\nimage editing and generating realistic images, posing a serious threat to the\ntrustworthiness of multimedia data and driving the research of image\nmanipulation detection and location (IMDL). However, the lack of a large-scale\ndata foundation makes the IMDL task unattainable. In this paper, we build a\nlocal manipulation data generation pipeline that integrates the powerful\ncapabilities of SAM, LLM, and generative models. Upon this basis, we propose\nthe GIM dataset, which has the following advantages: 1) Large scale, GIM\nincludes over one million pairs of AI-manipulated images and real images. 2)\nRich image content, GIM encompasses a broad range of image classes. 3) Diverse\ngenerative manipulation, the images are manipulated images with\nstate-of-the-art generators and various manipulation tasks. The aforementioned\nadvantages allow for a more comprehensive evaluation of IMDL methods, extending\ntheir applicability to diverse images. We introduce the GIM benchmark with two\nsettings to evaluate existing IMDL methods. In addition, we propose a novel\nIMDL framework, termed GIMFormer, which consists of a ShadowTracer,\nFrequency-Spatial block (FSB), and a Multi-Window Anomalous Modeling (MWAM)\nmodule. Extensive experiments on the GIM demonstrate that GIMFormer surpasses\nthe previous state-of-the-art approach on two different benchmarks."
                },
                "authors": [
                    {
                        "name": "Yirui Chen"
                    },
                    {
                        "name": "Xudong Huang"
                    },
                    {
                        "name": "Quan Zhang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Qiangyu Yan"
                    },
                    {
                        "name": "Simiao Li"
                    },
                    {
                        "name": "Hanting Chen"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Hu"
                },
                "author": "Jie Hu",
                "arxiv_comment": "Code page: https://github.com/chenyirui/GIM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07359v1",
                "updated": "2025-01-13T14:27:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    14,
                    27,
                    39,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T14:27:39Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    14,
                    27,
                    39,
                    0,
                    13,
                    0
                ],
                "title": "Emergent effects of scaling on the functional hierarchies within large\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent effects of scaling on the functional hierarchies within large\n  language models"
                },
                "summary": "Large language model (LLM) architectures are often described as functionally\nhierarchical: Early layers process syntax, middle layers begin to parse\nsemantics, and late layers integrate information. The present work revisits\nthese ideas. This research submits simple texts to an LLM (e.g., \"A church and\norgan\") and extracts the resulting activations. Then, for each layer, support\nvector machines and ridge regressions are fit to predict a text's label and\nthus examine whether a given layer encodes some information. Analyses using a\nsmall model (Llama-3.2-3b; 28 layers) partly bolster the common hierarchical\nperspective: Item-level semantics are most strongly represented early (layers\n2-7), then two-item relations (layers 8-12), and then four-item analogies\n(layers 10-15). Afterward, the representation of items and simple relations\ngradually decreases in deeper layers that focus on more global information.\nHowever, several findings run counter to a steady hierarchy view: First,\nalthough deep layers can represent document-wide abstractions, deep layers also\ncompress information from early portions of the context window without\nmeaningful abstraction. Second, when examining a larger model\n(Llama-3.3-70b-Instruct), stark fluctuations in abstraction level appear: As\ndepth increases, two-item relations and four-item analogies initially increase\nin their representation, then markedly decrease, and afterward increase again\nmomentarily. This peculiar pattern consistently emerges across several\nexperiments. Third, another emergent effect of scaling is coordination between\nthe attention mechanisms of adjacent layers. Across multiple experiments using\nthe larger model, adjacent layers fluctuate between what information they each\nspecialize in representing. In sum, an abstraction hierarchy often manifests\nacross layers, but large models also deviate from this structure in curious\nways.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) architectures are often described as functionally\nhierarchical: Early layers process syntax, middle layers begin to parse\nsemantics, and late layers integrate information. The present work revisits\nthese ideas. This research submits simple texts to an LLM (e.g., \"A church and\norgan\") and extracts the resulting activations. Then, for each layer, support\nvector machines and ridge regressions are fit to predict a text's label and\nthus examine whether a given layer encodes some information. Analyses using a\nsmall model (Llama-3.2-3b; 28 layers) partly bolster the common hierarchical\nperspective: Item-level semantics are most strongly represented early (layers\n2-7), then two-item relations (layers 8-12), and then four-item analogies\n(layers 10-15). Afterward, the representation of items and simple relations\ngradually decreases in deeper layers that focus on more global information.\nHowever, several findings run counter to a steady hierarchy view: First,\nalthough deep layers can represent document-wide abstractions, deep layers also\ncompress information from early portions of the context window without\nmeaningful abstraction. Second, when examining a larger model\n(Llama-3.3-70b-Instruct), stark fluctuations in abstraction level appear: As\ndepth increases, two-item relations and four-item analogies initially increase\nin their representation, then markedly decrease, and afterward increase again\nmomentarily. This peculiar pattern consistently emerges across several\nexperiments. Third, another emergent effect of scaling is coordination between\nthe attention mechanisms of adjacent layers. Across multiple experiments using\nthe larger model, adjacent layers fluctuate between what information they each\nspecialize in representing. In sum, an abstraction hierarchy often manifests\nacross layers, but large models also deviate from this structure in curious\nways."
                },
                "authors": [
                    {
                        "name": "Paul C. Bogdan"
                    }
                ],
                "author_detail": {
                    "name": "Paul C. Bogdan"
                },
                "author": "Paul C. Bogdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07348v1",
                "updated": "2025-01-13T14:14:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    14,
                    14,
                    5,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T14:14:05Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    14,
                    14,
                    5,
                    0,
                    13,
                    0
                ],
                "title": "Ultrasonic Medical Tissue Imaging Using Probabilistic Inversion:\n  Leveraging Variational Inference for Speed Reconstruction and Uncertainty\n  Quantification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrasonic Medical Tissue Imaging Using Probabilistic Inversion:\n  Leveraging Variational Inference for Speed Reconstruction and Uncertainty\n  Quantification"
                },
                "summary": "Full Waveform Inversion (FWI) is a promising technique for achieving\nhigh-resolution imaging in medical ultrasound. Traditional FWI methods suffer\nfrom issues related to computational efficiency, dependence on initial models,\nand the inability to quantify uncertainty. This study introduces the Stein\nVariational Gradient Descent (SVGD) algorithm into FWI, aiming to improve\ninversion performance and enhance uncertainty quantification. By deriving the\nposterior gradient, the study explores the integration of SVGD with FWI and\ndemonstrates its ability to approximate complex priors. In-silico experiments\nwith synthetic data and real-world breast tissue data highlight the advantages\nof the SVGD-based framework over conventional FWI. SVGD-based FWI improves\ninversion quality, provides more reliable uncertainty quantification, and\noffers a tighter bound for the prior distribution. These findings show that\nprobabilistic inversion is a promising tool for addressing the limitations of\ntraditional FWI methods in ultrasonic imaging of medical tissues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full Waveform Inversion (FWI) is a promising technique for achieving\nhigh-resolution imaging in medical ultrasound. Traditional FWI methods suffer\nfrom issues related to computational efficiency, dependence on initial models,\nand the inability to quantify uncertainty. This study introduces the Stein\nVariational Gradient Descent (SVGD) algorithm into FWI, aiming to improve\ninversion performance and enhance uncertainty quantification. By deriving the\nposterior gradient, the study explores the integration of SVGD with FWI and\ndemonstrates its ability to approximate complex priors. In-silico experiments\nwith synthetic data and real-world breast tissue data highlight the advantages\nof the SVGD-based framework over conventional FWI. SVGD-based FWI improves\ninversion quality, provides more reliable uncertainty quantification, and\noffers a tighter bound for the prior distribution. These findings show that\nprobabilistic inversion is a promising tool for addressing the limitations of\ntraditional FWI methods in ultrasonic imaging of medical tissues."
                },
                "authors": [
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Heyu Ma"
                    },
                    {
                        "name": "Chengcheng Liu"
                    },
                    {
                        "name": "Dean Ta"
                    }
                ],
                "author_detail": {
                    "name": "Dean Ta"
                },
                "author": "Dean Ta",
                "arxiv_comment": "24 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08849v2",
                "updated": "2025-01-13T14:09:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    14,
                    9,
                    55,
                    0,
                    13,
                    0
                ],
                "published": "2024-10-11T14:25:39Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    14,
                    25,
                    39,
                    4,
                    285,
                    0
                ],
                "title": "Causal inference targeting a concentration index for studies of health\n  inequalities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal inference targeting a concentration index for studies of health\n  inequalities"
                },
                "summary": "A concentration index, a standardized covariance between a health variable\nand relative income ranks, is often used to quantify income-related health\ninequalities. There is a lack of formal approach to study the effect of an\nexposure, e.g., education, on such measures of inequality. In this paper we\ncontribute by filling this gap and developing the necessary theory and method.\nThus, we define a counterfactual concentration index for different levels of an\nexposure. We give conditions for their identification, and then deduce their\nefficient influence function. This allows us to propose estimators, which are\nregular asymptotic linear under certain conditions. In particular, these\nestimators are $\\sqrt n$-consistent and asymptotically normal, as well as\nlocally efficient. The implementation of the estimators is based on the fit of\nseveral nuisance functions. The estimators proposed have rate robustness\nproperties allowing for convergence rates slower than $\\sqrt{n}$-rate for some\nof the nuisance function fits. The relevance of the asymptotic results for\nfinite samples is studied with simulation experiments. We also present a case\nstudy of the effect of education on income-related health inequalities for a\nSwedish cohort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A concentration index, a standardized covariance between a health variable\nand relative income ranks, is often used to quantify income-related health\ninequalities. There is a lack of formal approach to study the effect of an\nexposure, e.g., education, on such measures of inequality. In this paper we\ncontribute by filling this gap and developing the necessary theory and method.\nThus, we define a counterfactual concentration index for different levels of an\nexposure. We give conditions for their identification, and then deduce their\nefficient influence function. This allows us to propose estimators, which are\nregular asymptotic linear under certain conditions. In particular, these\nestimators are $\\sqrt n$-consistent and asymptotically normal, as well as\nlocally efficient. The implementation of the estimators is based on the fit of\nseveral nuisance functions. The estimators proposed have rate robustness\nproperties allowing for convergence rates slower than $\\sqrt{n}$-rate for some\nof the nuisance function fits. The relevance of the asymptotic results for\nfinite samples is studied with simulation experiments. We also present a case\nstudy of the effect of education on income-related health inequalities for a\nSwedish cohort."
                },
                "authors": [
                    {
                        "name": "Mohammad Ghasempour"
                    },
                    {
                        "name": "Xavier de Luna"
                    },
                    {
                        "name": "Per E. Gustafsson"
                    }
                ],
                "author_detail": {
                    "name": "Per E. Gustafsson"
                },
                "author": "Per E. Gustafsson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07335v1",
                "updated": "2025-01-13T13:47:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    47,
                    5,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T13:47:05Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    47,
                    5,
                    0,
                    13,
                    0
                ],
                "title": "TempoGPT: Enhancing Temporal Reasoning via Quantizing Embedding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TempoGPT: Enhancing Temporal Reasoning via Quantizing Embedding"
                },
                "summary": "Multi-modal language model has made advanced progress in vision and audio,\nbut still faces significant challenges in dealing with complex reasoning tasks\nin the time series domain. The reasons are twofold. First, labels for\nmulti-modal time series data are coarse and devoid of analysis or reasoning\nprocesses. Training with these data cannot improve the model's reasoning\ncapabilities. Second, due to the lack of precise tokenization in processing\ntime series, the representation patterns for temporal and textual information\nare inconsistent, which hampers the effectiveness of multi-modal alignment. To\naddress these challenges, we propose a multi-modal time series data\nconstruction approach and a multi-modal time series language model (TLM),\nTempoGPT. Specially, we construct multi-modal data for complex reasoning tasks\nby analyzing the variable-system relationships within a white-box system.\nAdditionally, proposed TempoGPT achieves consistent representation between\ntemporal and textual information by quantizing temporal embeddings, where\ntemporal embeddings are quantized into a series of discrete tokens using a\npredefined codebook; subsequently, a shared embedding layer processes both\ntemporal and textual tokens. Extensive experiments demonstrate that TempoGPT\naccurately perceives temporal information, logically infers conclusions, and\nachieves state-of-the-art in the constructed complex time series reasoning\ntasks. Moreover, we quantitatively demonstrate the effectiveness of quantizing\ntemporal embeddings in enhancing multi-modal alignment and the reasoning\ncapabilities of TLMs. Code and data are available at\nhttps://github.com/zhanghaochuan20/TempoGPT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal language model has made advanced progress in vision and audio,\nbut still faces significant challenges in dealing with complex reasoning tasks\nin the time series domain. The reasons are twofold. First, labels for\nmulti-modal time series data are coarse and devoid of analysis or reasoning\nprocesses. Training with these data cannot improve the model's reasoning\ncapabilities. Second, due to the lack of precise tokenization in processing\ntime series, the representation patterns for temporal and textual information\nare inconsistent, which hampers the effectiveness of multi-modal alignment. To\naddress these challenges, we propose a multi-modal time series data\nconstruction approach and a multi-modal time series language model (TLM),\nTempoGPT. Specially, we construct multi-modal data for complex reasoning tasks\nby analyzing the variable-system relationships within a white-box system.\nAdditionally, proposed TempoGPT achieves consistent representation between\ntemporal and textual information by quantizing temporal embeddings, where\ntemporal embeddings are quantized into a series of discrete tokens using a\npredefined codebook; subsequently, a shared embedding layer processes both\ntemporal and textual tokens. Extensive experiments demonstrate that TempoGPT\naccurately perceives temporal information, logically infers conclusions, and\nachieves state-of-the-art in the constructed complex time series reasoning\ntasks. Moreover, we quantitatively demonstrate the effectiveness of quantizing\ntemporal embeddings in enhancing multi-modal alignment and the reasoning\ncapabilities of TLMs. Code and data are available at\nhttps://github.com/zhanghaochuan20/TempoGPT."
                },
                "authors": [
                    {
                        "name": "Haochuan Zhang"
                    },
                    {
                        "name": "Chunhua Yang"
                    },
                    {
                        "name": "Jie Han"
                    },
                    {
                        "name": "Liyang Qin"
                    },
                    {
                        "name": "Xiaoli Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoli Wang"
                },
                "author": "Xiaoli Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07330v1",
                "updated": "2025-01-13T13:44:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    44,
                    6,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T13:44:06Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    44,
                    6,
                    0,
                    13,
                    0
                ],
                "title": "Occamy: A 432-Core Dual-Chiplet Dual-HBM2E 768-DP-GFLOP/s RISC-V System\n  for 8-to-64-bit Dense and Sparse Computing in 12nm FinFET",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Occamy: A 432-Core Dual-Chiplet Dual-HBM2E 768-DP-GFLOP/s RISC-V System\n  for 8-to-64-bit Dense and Sparse Computing in 12nm FinFET"
                },
                "summary": "ML and HPC applications increasingly combine dense and sparse memory access\ncomputations to maximize storage efficiency. However, existing CPUs and GPUs\nstruggle to flexibly handle these heterogeneous workloads with consistently\nhigh compute efficiency. We present Occamy, a 432-Core, 768-DP-GFLOP/s,\ndual-HBM2E, dual-chiplet RISC-V system with a latency-tolerant hierarchical\ninterconnect and in-core streaming units (SUs) designed to accelerate dense and\nsparse FP8-to-FP64 ML and HPC workloads. We implement Occamy's compute chiplets\nin 12 nm FinFET, and its passive interposer, Hedwig, in a 65 nm node. On dense\nlinear algebra (LA), Occamy achieves a competitive FPU utilization of 89%. On\nstencil codes, Occamy reaches an FPU utilization of 83% and a\ntechnology-node-normalized compute density of 11.1 DP-GFLOP/s/mm2,leading\nstate-of-the-art (SoA) processors by 1.7x and 1.2x, respectively. On\nsparse-dense linear algebra (LA), it achieves 42% FPU utilization and a\nnormalized compute density of 5.95 DP-GFLOP/s/mm2, surpassing the SoA by 5.2x\nand 11x, respectively. On, sparse-sparse LA, Occamy reaches a throughput of up\nto 187 GCOMP/s at 17.4 GCOMP/s/W and a compute density of 3.63 GCOMP/s/mm2.\nFinally, we reach up to 75% and 54% FPU utilization on and dense (LLM) and\ngraph-sparse (GCN) ML inference workloads. Occamy's RTL is freely available\nunder a permissive open-source license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML and HPC applications increasingly combine dense and sparse memory access\ncomputations to maximize storage efficiency. However, existing CPUs and GPUs\nstruggle to flexibly handle these heterogeneous workloads with consistently\nhigh compute efficiency. We present Occamy, a 432-Core, 768-DP-GFLOP/s,\ndual-HBM2E, dual-chiplet RISC-V system with a latency-tolerant hierarchical\ninterconnect and in-core streaming units (SUs) designed to accelerate dense and\nsparse FP8-to-FP64 ML and HPC workloads. We implement Occamy's compute chiplets\nin 12 nm FinFET, and its passive interposer, Hedwig, in a 65 nm node. On dense\nlinear algebra (LA), Occamy achieves a competitive FPU utilization of 89%. On\nstencil codes, Occamy reaches an FPU utilization of 83% and a\ntechnology-node-normalized compute density of 11.1 DP-GFLOP/s/mm2,leading\nstate-of-the-art (SoA) processors by 1.7x and 1.2x, respectively. On\nsparse-dense linear algebra (LA), it achieves 42% FPU utilization and a\nnormalized compute density of 5.95 DP-GFLOP/s/mm2, surpassing the SoA by 5.2x\nand 11x, respectively. On, sparse-sparse LA, Occamy reaches a throughput of up\nto 187 GCOMP/s at 17.4 GCOMP/s/W and a compute density of 3.63 GCOMP/s/mm2.\nFinally, we reach up to 75% and 54% FPU utilization on and dense (LLM) and\ngraph-sparse (GCN) ML inference workloads. Occamy's RTL is freely available\nunder a permissive open-source license."
                },
                "authors": [
                    {
                        "name": "Paul Scheffler"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Viviane Potocnik"
                    },
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Luca Colagrande"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Luca Bertaccini"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Manuel Eggimann"
                    },
                    {
                        "name": "Matheus Cavalcante"
                    },
                    {
                        "name": "Gianna Paulin"
                    },
                    {
                        "name": "Frank K. GÃ¼rkaynak"
                    },
                    {
                        "name": "Davide Rossi"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_doi": "10.1109/JSSC.2025.3529249",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JSSC.2025.3529249",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.07330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 13 figures, 1 table. Accepted for publication in IEEE JSSC",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07314v1",
                "updated": "2025-01-13T13:26:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    26,
                    50,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T13:26:50Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    26,
                    50,
                    0,
                    13,
                    0
                ],
                "title": "FinerWeb-10BT: Refining Web Data with LLM-Based Line-Level Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinerWeb-10BT: Refining Web Data with LLM-Based Line-Level Filtering"
                },
                "summary": "Data quality is crucial for training Large Language Models (LLMs).\nTraditional heuristic filters often miss low-quality text or mistakenly remove\nvaluable content. In this paper, we introduce an LLM-based line-level filtering\nmethod to enhance training data quality. We use GPT-4o mini to label a\n20,000-document sample from FineWeb at the line level, allowing the model to\ncreate descriptive labels for low-quality lines. These labels are grouped into\nnine main categories, and we train a DeBERTa-v3 classifier to scale the\nfiltering to a 10B-token subset of FineWeb. To test the impact of our\nfiltering, we train GPT-2 models on both the original and the filtered\ndatasets. The results show that models trained on the filtered data achieve\nhigher accuracy on the HellaSwag benchmark and reach their performance targets\nfaster, even with up to 25\\% less data. This demonstrates that LLM-based\nline-level filtering can significantly improve data quality and training\nefficiency for LLMs. We release our quality-annotated dataset, FinerWeb-10BT,\nand the codebase to support further work in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data quality is crucial for training Large Language Models (LLMs).\nTraditional heuristic filters often miss low-quality text or mistakenly remove\nvaluable content. In this paper, we introduce an LLM-based line-level filtering\nmethod to enhance training data quality. We use GPT-4o mini to label a\n20,000-document sample from FineWeb at the line level, allowing the model to\ncreate descriptive labels for low-quality lines. These labels are grouped into\nnine main categories, and we train a DeBERTa-v3 classifier to scale the\nfiltering to a 10B-token subset of FineWeb. To test the impact of our\nfiltering, we train GPT-2 models on both the original and the filtered\ndatasets. The results show that models trained on the filtered data achieve\nhigher accuracy on the HellaSwag benchmark and reach their performance targets\nfaster, even with up to 25\\% less data. This demonstrates that LLM-based\nline-level filtering can significantly improve data quality and training\nefficiency for LLMs. We release our quality-annotated dataset, FinerWeb-10BT,\nand the codebase to support further work in this area."
                },
                "authors": [
                    {
                        "name": "Erik Henriksson"
                    },
                    {
                        "name": "Otto Tarkka"
                    },
                    {
                        "name": "Filip Ginter"
                    }
                ],
                "author_detail": {
                    "name": "Filip Ginter"
                },
                "author": "Filip Ginter",
                "arxiv_comment": "11 pages, 4 figures, 4 tables. To be published in NoDaLiDa/Baltic-HLT\n  2025 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00304v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00304v3",
                "updated": "2025-01-13T13:17:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    17,
                    47,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-01T04:00:09Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    4,
                    0,
                    9,
                    2,
                    122,
                    0
                ],
                "title": "QUACK: Quantum Aligned Centroid Kernel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUACK: Quantum Aligned Centroid Kernel"
                },
                "summary": "Quantum computing (QC) seems to show potential for application in machine\nlearning (ML). In particular quantum kernel methods (QKM) exhibit promising\nproperties for use in supervised ML tasks. However, a major disadvantage of\nkernel methods is their unfavorable quadratic scaling with the number of\ntraining samples. Together with the limits imposed by currently available\nquantum hardware (NISQ devices) with their low qubit coherence times, small\nnumber of qubits, and high error rates, the use of QC in ML at an industrially\nrelevant scale is currently impossible. As a small step in improving the\npotential applications of QKMs, we introduce QUACK, a quantum kernel algorithm\nwhose time complexity scales linear with the number of samples during training,\nand independent of the number of training samples in the inference stage. In\nthe training process, only the kernel entries for the samples and the centers\nof the classes are calculated, i.e. the maximum shape of the kernel for n\nsamples and c classes is (n, c). During training, the parameters of the quantum\nkernel and the positions of the centroids are optimized iteratively. In the\ninference stage, for every new sample the circuit is only evaluated for every\ncentroid, i.e. c times. We show that the QUACK algorithm nevertheless provides\nsatisfactory results and can perform at a similar level as classical kernel\nmethods with quadratic scaling during training. In addition, our (simulated)\nalgorithm is able to handle high-dimensional datasets such as MNIST with 784\nfeatures without any dimensionality reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum computing (QC) seems to show potential for application in machine\nlearning (ML). In particular quantum kernel methods (QKM) exhibit promising\nproperties for use in supervised ML tasks. However, a major disadvantage of\nkernel methods is their unfavorable quadratic scaling with the number of\ntraining samples. Together with the limits imposed by currently available\nquantum hardware (NISQ devices) with their low qubit coherence times, small\nnumber of qubits, and high error rates, the use of QC in ML at an industrially\nrelevant scale is currently impossible. As a small step in improving the\npotential applications of QKMs, we introduce QUACK, a quantum kernel algorithm\nwhose time complexity scales linear with the number of samples during training,\nand independent of the number of training samples in the inference stage. In\nthe training process, only the kernel entries for the samples and the centers\nof the classes are calculated, i.e. the maximum shape of the kernel for n\nsamples and c classes is (n, c). During training, the parameters of the quantum\nkernel and the positions of the centroids are optimized iteratively. In the\ninference stage, for every new sample the circuit is only evaluated for every\ncentroid, i.e. c times. We show that the QUACK algorithm nevertheless provides\nsatisfactory results and can perform at a similar level as classical kernel\nmethods with quadratic scaling during training. In addition, our (simulated)\nalgorithm is able to handle high-dimensional datasets such as MNIST with 784\nfeatures without any dimensionality reduction."
                },
                "authors": [
                    {
                        "name": "Kilian Tscharke"
                    },
                    {
                        "name": "Sebastian Issel"
                    },
                    {
                        "name": "Pascal Debus"
                    }
                ],
                "author_detail": {
                    "name": "Pascal Debus"
                },
                "author": "Pascal Debus",
                "arxiv_doi": "10.1109/QCE60285.2024.00169",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/QCE60285.2024.00169",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.00304v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00304v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "2nd place Best Paper award in QML track @ IEEE International\n  Conference on Quantum Computing and Engineering (QCE) 2024",
                "arxiv_journal_ref": "2024 IEEE International Conference on Quantum Computing and\n  Engineering (QCE), Montreal, QC, Canada, 2024, pp. 1425-1435",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03387v2",
                "updated": "2025-01-13T13:13:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    13,
                    38,
                    0,
                    13,
                    0
                ],
                "published": "2024-11-05T18:14:49Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    18,
                    14,
                    49,
                    1,
                    310,
                    0
                ],
                "title": "Quantifying Aleatoric Uncertainty of the Treatment Effect: A Novel\n  Orthogonal Learner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Aleatoric Uncertainty of the Treatment Effect: A Novel\n  Orthogonal Learner"
                },
                "summary": "Estimating causal quantities from observational data is crucial for\nunderstanding the safety and effectiveness of medical treatments. However, to\nmake reliable inferences, medical practitioners require not only estimating\naveraged causal quantities, such as the conditional average treatment effect,\nbut also understanding the randomness of the treatment effect as a random\nvariable. This randomness is referred to as aleatoric uncertainty and is\nnecessary for understanding the probability of benefit from treatment or\nquantiles of the treatment effect. Yet, the aleatoric uncertainty of the\ntreatment effect has received surprisingly little attention in the causal\nmachine learning community. To fill this gap, we aim to quantify the aleatoric\nuncertainty of the treatment effect at the covariate-conditional level, namely,\nthe conditional distribution of the treatment effect (CDTE). Unlike average\ncausal quantities, the CDTE is not point identifiable without strong additional\nassumptions. As a remedy, we employ partial identification to obtain sharp\nbounds on the CDTE and thereby quantify the aleatoric uncertainty of the\ntreatment effect. We then develop a novel, orthogonal learner for the bounds on\nthe CDTE, which we call AU-learner. We further show that our AU-learner has\nseveral strengths in that it satisfies Neyman-orthogonality and, thus,\nquasi-oracle efficiency. Finally, we propose a fully-parametric deep learning\ninstantiation of our AU-learner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating causal quantities from observational data is crucial for\nunderstanding the safety and effectiveness of medical treatments. However, to\nmake reliable inferences, medical practitioners require not only estimating\naveraged causal quantities, such as the conditional average treatment effect,\nbut also understanding the randomness of the treatment effect as a random\nvariable. This randomness is referred to as aleatoric uncertainty and is\nnecessary for understanding the probability of benefit from treatment or\nquantiles of the treatment effect. Yet, the aleatoric uncertainty of the\ntreatment effect has received surprisingly little attention in the causal\nmachine learning community. To fill this gap, we aim to quantify the aleatoric\nuncertainty of the treatment effect at the covariate-conditional level, namely,\nthe conditional distribution of the treatment effect (CDTE). Unlike average\ncausal quantities, the CDTE is not point identifiable without strong additional\nassumptions. As a remedy, we employ partial identification to obtain sharp\nbounds on the CDTE and thereby quantify the aleatoric uncertainty of the\ntreatment effect. We then develop a novel, orthogonal learner for the bounds on\nthe CDTE, which we call AU-learner. We further show that our AU-learner has\nseveral strengths in that it satisfies Neyman-orthogonality and, thus,\nquasi-oracle efficiency. Finally, we propose a fully-parametric deep learning\ninstantiation of our AU-learner."
                },
                "authors": [
                    {
                        "name": "Valentyn Melnychuk"
                    },
                    {
                        "name": "Stefan Feuerriegel"
                    },
                    {
                        "name": "Mihaela van der Schaar"
                    }
                ],
                "author_detail": {
                    "name": "Mihaela van der Schaar"
                },
                "author": "Mihaela van der Schaar",
                "arxiv_journal_ref": "Proceedings of the 38th Conference on Neural Information\n  Processing Systems (NeurIPS 2024), Vancouver, Canada, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10561v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10561v3",
                "updated": "2025-01-13T13:12:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    12,
                    9,
                    0,
                    13,
                    0
                ],
                "published": "2024-09-11T14:41:44Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    41,
                    44,
                    2,
                    255,
                    0
                ],
                "title": "DrLLM: Prompt-Enhanced Distributed Denial-of-Service Resistance Method\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DrLLM: Prompt-Enhanced Distributed Denial-of-Service Resistance Method\n  with Large Language Models"
                },
                "summary": "The increasing number of Distributed Denial of Service (DDoS) attacks poses a\nmajor threat to the Internet, highlighting the importance of DDoS mitigation.\nMost existing approaches require complex training methods to learn data\nfeatures, which increases the complexity and generality of the application. In\nthis paper, we propose DrLLM, which aims to mine anomalous traffic information\nin zero-shot scenarios through Large Language Models (LLMs). To bridge the gap\nbetween DrLLM and existing approaches, we embed the global and local\ninformation of the traffic data into the reasoning paradigm and design three\nmodules, namely Knowledge Embedding, Token Embedding, and Progressive Role\nReasoning, for data representation and reasoning. In addition we explore the\ngeneralization of prompt engineering in the cybersecurity domain to improve the\nclassification capability of DrLLM. Our ablation experiments demonstrate the\napplicability of DrLLM in zero-shot scenarios and further demonstrate the\npotential of LLMs in the network domains. DrLLM implementation code has been\nopen-sourced at https://github.com/liuup/DrLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing number of Distributed Denial of Service (DDoS) attacks poses a\nmajor threat to the Internet, highlighting the importance of DDoS mitigation.\nMost existing approaches require complex training methods to learn data\nfeatures, which increases the complexity and generality of the application. In\nthis paper, we propose DrLLM, which aims to mine anomalous traffic information\nin zero-shot scenarios through Large Language Models (LLMs). To bridge the gap\nbetween DrLLM and existing approaches, we embed the global and local\ninformation of the traffic data into the reasoning paradigm and design three\nmodules, namely Knowledge Embedding, Token Embedding, and Progressive Role\nReasoning, for data representation and reasoning. In addition we explore the\ngeneralization of prompt engineering in the cybersecurity domain to improve the\nclassification capability of DrLLM. Our ablation experiments demonstrate the\napplicability of DrLLM in zero-shot scenarios and further demonstrate the\npotential of LLMs in the network domains. DrLLM implementation code has been\nopen-sourced at https://github.com/liuup/DrLLM."
                },
                "authors": [
                    {
                        "name": "Zhenyu Yin"
                    },
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Guangyuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Guangyuan Xu"
                },
                "author": "Guangyuan Xu",
                "arxiv_comment": "Accepted by ICASSP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10561v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10561v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07301v1",
                "updated": "2025-01-13T13:10:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    10,
                    16,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T13:10:16Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    10,
                    16,
                    0,
                    13,
                    0
                ],
                "title": "The Lessons of Developing Process Reward Models in Mathematical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lessons of Developing Process Reward Models in Mathematical\n  Reasoning"
                },
                "summary": "Process Reward Models (PRMs) emerge as a promising approach for process\nsupervision in mathematical reasoning of Large Language Models (LLMs), which\naim to identify and mitigate intermediate errors in the reasoning processes.\nHowever, the development of effective PRMs faces significant challenges,\nparticularly in data annotation and evaluation methodologies. In this paper,\nthrough extensive experiments, we demonstrate that commonly used Monte Carlo\n(MC) estimation-based data synthesis for PRMs typically yields inferior\nperformance and generalization compared to LLM-as-a-judge and human annotation\nmethods. MC estimation relies on completion models to evaluate current-step\ncorrectness, leading to inaccurate step verification. Furthermore, we identify\npotential biases in conventional Best-of-N (BoN) evaluation strategies for\nPRMs: (1) The unreliable policy models generate responses with correct answers\nbut flawed processes, leading to a misalignment between the evaluation criteria\nof BoN and the PRM objectives of process verification. (2) The tolerance of\nPRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a\nsignificant proportion of minimum scores concentrated on the final answer\nsteps, revealing the shift from process to outcome-based assessment in BoN\nOptimized PRMs. To address these challenges, we develop a consensus filtering\nmechanism that effectively integrates MC estimation with LLM-as-a-judge and\nadvocates a more comprehensive evaluation framework that combines\nresponse-level and step-level metrics. Based on the mechanisms, we\nsignificantly improve both model performance and data efficiency in the BoN\nevaluation and the step-wise error identification task. Finally, we release a\nnew state-of-the-art PRM that outperforms existing open-source alternatives and\nprovides practical guidelines for future research in building process\nsupervision models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models (PRMs) emerge as a promising approach for process\nsupervision in mathematical reasoning of Large Language Models (LLMs), which\naim to identify and mitigate intermediate errors in the reasoning processes.\nHowever, the development of effective PRMs faces significant challenges,\nparticularly in data annotation and evaluation methodologies. In this paper,\nthrough extensive experiments, we demonstrate that commonly used Monte Carlo\n(MC) estimation-based data synthesis for PRMs typically yields inferior\nperformance and generalization compared to LLM-as-a-judge and human annotation\nmethods. MC estimation relies on completion models to evaluate current-step\ncorrectness, leading to inaccurate step verification. Furthermore, we identify\npotential biases in conventional Best-of-N (BoN) evaluation strategies for\nPRMs: (1) The unreliable policy models generate responses with correct answers\nbut flawed processes, leading to a misalignment between the evaluation criteria\nof BoN and the PRM objectives of process verification. (2) The tolerance of\nPRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a\nsignificant proportion of minimum scores concentrated on the final answer\nsteps, revealing the shift from process to outcome-based assessment in BoN\nOptimized PRMs. To address these challenges, we develop a consensus filtering\nmechanism that effectively integrates MC estimation with LLM-as-a-judge and\nadvocates a more comprehensive evaluation framework that combines\nresponse-level and step-level metrics. Based on the mechanisms, we\nsignificantly improve both model performance and data efficiency in the BoN\nevaluation and the step-wise error identification task. Finally, we release a\nnew state-of-the-art PRM that outperforms existing open-source alternatives and\nprovides practical guidelines for future research in building process\nsupervision models."
                },
                "authors": [
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Yangzhen Wu"
                    },
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Runji Lin"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17434v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17434v2",
                "updated": "2025-01-13T13:01:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    1,
                    12,
                    0,
                    13,
                    0
                ],
                "published": "2024-06-25T10:16:30Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    10,
                    16,
                    30,
                    1,
                    177,
                    0
                ],
                "title": "Moment-based parameter inference with error guarantees for stochastic\n  reaction networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moment-based parameter inference with error guarantees for stochastic\n  reaction networks"
                },
                "summary": "Inferring parameters of models of biochemical kinetics from single-cell data\nremains challenging because of the uncertainty arising from the intractability\nof the likelihood function of stochastic reaction networks. Such uncertainty\nfalls beyond current error quantification measures, which focus on the effects\nof finite sample size and identifiability but lack theoretical guarantees when\nlikelihood approximations are needed. Here, we propose a method for the\ninference of parameters of stochastic reaction networks that works for both\nsteady-state and time-resolved data and is applicable to networks with\nnon-linear and rational propensities. Our approach provides bounds on the\nparameters via convex optimisation over sets constrained by moment equations\nand moment matrices by taking observations to form moment intervals, which are\nthen used to constrain parameters through convex sets. The bounds on the\nparameters contain the true parameters under the condition that the moment\nintervals contain the true moments, thus providing uncertainty quantification\nand error guarantees. Our approach does not need to predict moments and\ndistributions for given parameters (i.e., it avoids solving or simulating the\nforward problem), and hence circumvents intractable likelihood computations or\ncomputationally expensive simulations. We demonstrate its use for uncertainty\nquantification, data integration and prediction of latent species statistics\nthrough synthetic data from common non-linear biochemical models including the\nSchl\\\"ogl model and the toggle switch, a model of post-transcriptional\nregulation at steady state, and a birth-death model with time-dependent data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring parameters of models of biochemical kinetics from single-cell data\nremains challenging because of the uncertainty arising from the intractability\nof the likelihood function of stochastic reaction networks. Such uncertainty\nfalls beyond current error quantification measures, which focus on the effects\nof finite sample size and identifiability but lack theoretical guarantees when\nlikelihood approximations are needed. Here, we propose a method for the\ninference of parameters of stochastic reaction networks that works for both\nsteady-state and time-resolved data and is applicable to networks with\nnon-linear and rational propensities. Our approach provides bounds on the\nparameters via convex optimisation over sets constrained by moment equations\nand moment matrices by taking observations to form moment intervals, which are\nthen used to constrain parameters through convex sets. The bounds on the\nparameters contain the true parameters under the condition that the moment\nintervals contain the true moments, thus providing uncertainty quantification\nand error guarantees. Our approach does not need to predict moments and\ndistributions for given parameters (i.e., it avoids solving or simulating the\nforward problem), and hence circumvents intractable likelihood computations or\ncomputationally expensive simulations. We demonstrate its use for uncertainty\nquantification, data integration and prediction of latent species statistics\nthrough synthetic data from common non-linear biochemical models including the\nSchl\\\"ogl model and the toggle switch, a model of post-transcriptional\nregulation at steady state, and a birth-death model with time-dependent data."
                },
                "authors": [
                    {
                        "name": "Zekai Li"
                    },
                    {
                        "name": "Mauricio Barahona"
                    },
                    {
                        "name": "Philipp Thomas"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Thomas"
                },
                "author": "Philipp Thomas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17434v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17434v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.MN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07288v1",
                "updated": "2025-01-13T12:56:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    56,
                    5,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T12:56:05Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    56,
                    5,
                    0,
                    13,
                    0
                ],
                "title": "LLM-Net: Democratizing LLMs-as-a-Service through Blockchain-based Expert\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Net: Democratizing LLMs-as-a-Service through Blockchain-based Expert\n  Networks"
                },
                "summary": "The centralization of Large Language Models (LLMs) development has created\nsignificant barriers to AI advancement, limiting the democratization of these\npowerful technologies. This centralization, coupled with the scarcity of\nhigh-quality training data and mounting complexity of maintaining comprehensive\nexpertise across rapidly expanding knowledge domains, poses critical challenges\nto the continued growth of LLMs. While solutions like Retrieval-Augmented\nGeneration (RAG) offer potential remedies, maintaining up-to-date expert\nknowledge across diverse domains remains a significant challenge, particularly\ngiven the exponential growth of specialized information. This paper introduces\nLLMs Networks (LLM-Net), a blockchain-based framework that democratizes\nLLMs-as-a-Service through a decentralized network of specialized LLM providers.\nBy leveraging collective computational resources and distributed domain\nexpertise, LLM-Net incorporates fine-tuned expert models for various specific\ndomains, ensuring sustained knowledge growth while maintaining service quality\nthrough collaborative prompting mechanisms. The framework's robust design\nincludes blockchain technology for transparent transaction and performance\nvalidation, establishing an immutable record of service delivery. Our\nsimulation, built on top of state-of-the-art LLMs such as Claude 3.5 Sonnet,\nLlama 3.1, Grok-2, and GPT-4o, validates the effectiveness of the\nreputation-based mechanism in maintaining service quality by selecting\nhigh-performing respondents (LLM providers). Thereby it demonstrates the\npotential of LLM-Net to sustain AI advancement through the integration of\ndecentralized expertise and blockchain-based accountability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The centralization of Large Language Models (LLMs) development has created\nsignificant barriers to AI advancement, limiting the democratization of these\npowerful technologies. This centralization, coupled with the scarcity of\nhigh-quality training data and mounting complexity of maintaining comprehensive\nexpertise across rapidly expanding knowledge domains, poses critical challenges\nto the continued growth of LLMs. While solutions like Retrieval-Augmented\nGeneration (RAG) offer potential remedies, maintaining up-to-date expert\nknowledge across diverse domains remains a significant challenge, particularly\ngiven the exponential growth of specialized information. This paper introduces\nLLMs Networks (LLM-Net), a blockchain-based framework that democratizes\nLLMs-as-a-Service through a decentralized network of specialized LLM providers.\nBy leveraging collective computational resources and distributed domain\nexpertise, LLM-Net incorporates fine-tuned expert models for various specific\ndomains, ensuring sustained knowledge growth while maintaining service quality\nthrough collaborative prompting mechanisms. The framework's robust design\nincludes blockchain technology for transparent transaction and performance\nvalidation, establishing an immutable record of service delivery. Our\nsimulation, built on top of state-of-the-art LLMs such as Claude 3.5 Sonnet,\nLlama 3.1, Grok-2, and GPT-4o, validates the effectiveness of the\nreputation-based mechanism in maintaining service quality by selecting\nhigh-performing respondents (LLM providers). Thereby it demonstrates the\npotential of LLM-Net to sustain AI advancement through the integration of\ndecentralized expertise and blockchain-based accountability."
                },
                "authors": [
                    {
                        "name": "Zan-Kai Chong"
                    },
                    {
                        "name": "Hiroyuki Ohsaki"
                    },
                    {
                        "name": "Bryan Ng"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Ng"
                },
                "author": "Bryan Ng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07283v1",
                "updated": "2025-01-13T12:52:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    52,
                    3,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T12:52:03Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    52,
                    3,
                    0,
                    13,
                    0
                ],
                "title": "Theoretical Modelling of Gamma-Ray Burst 090510",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theoretical Modelling of Gamma-Ray Burst 090510"
                },
                "summary": "Gamma-ray bursts detected at high energies provide valuable insights into the\nemission mechanisms behind these still puzzling enigmatic events. In this\nstudy, we focus on GRB 090510, which is an unusual short GRB exhibiting plateau\nemission observed by the Fermi-LAT. Using the general relativistic\nmagnetohydrodynamic code (HARM), we aim to infer the key properties of this\nGRB, such as the jet opening angle, the energetics, the Lorentz Gamma factor,\nthe jet structure and its variability, and the progenitor parameters of the\ncompact binary system. We explored both the 2D and 3D models and estimated the\nvariability timescales. Our findings show that the predicted jet opening angle\nis within $88\\%$ of the observed upper limit from observations, and the\nenergetics are in general agreement with observed values when accounting for\nthe evolution of jet opening angle with redshift. This work establishes the\nfoundation for ongoing exploration, which will further align the theoretical\nmodel simulations with observational data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gamma-ray bursts detected at high energies provide valuable insights into the\nemission mechanisms behind these still puzzling enigmatic events. In this\nstudy, we focus on GRB 090510, which is an unusual short GRB exhibiting plateau\nemission observed by the Fermi-LAT. Using the general relativistic\nmagnetohydrodynamic code (HARM), we aim to infer the key properties of this\nGRB, such as the jet opening angle, the energetics, the Lorentz Gamma factor,\nthe jet structure and its variability, and the progenitor parameters of the\ncompact binary system. We explored both the 2D and 3D models and estimated the\nvariability timescales. Our findings show that the predicted jet opening angle\nis within $88\\%$ of the observed upper limit from observations, and the\nenergetics are in general agreement with observed values when accounting for\nthe evolution of jet opening angle with redshift. This work establishes the\nfoundation for ongoing exploration, which will further align the theoretical\nmodel simulations with observational data."
                },
                "authors": [
                    {
                        "name": "Joseph Saji"
                    },
                    {
                        "name": "Agnieszka Janiuk"
                    },
                    {
                        "name": "Maria Giovanna Dainotti"
                    },
                    {
                        "name": "Shubham Bhardwaj"
                    },
                    {
                        "name": "Gerardo Urrutia"
                    }
                ],
                "author_detail": {
                    "name": "Gerardo Urrutia"
                },
                "author": "Gerardo Urrutia",
                "arxiv_comment": "accepted in the Proceedings of the MarcellGrossmann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07278v1",
                "updated": "2025-01-13T12:42:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    42,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T12:42:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    42,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "Lifelong Learning of Large Language Model based Agents: A Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifelong Learning of Large Language Model based Agents: A Roadmap"
                },
                "summary": "Lifelong learning, also known as continual or incremental learning, is a\ncrucial component for advancing Artificial General Intelligence (AGI) by\nenabling systems to continuously adapt in dynamic environments. While large\nlanguage models (LLMs) have demonstrated impressive capabilities in natural\nlanguage processing, existing LLM agents are typically designed for static\nsystems and lack the ability to adapt over time in response to new challenges.\nThis survey is the first to systematically summarize the potential techniques\nfor incorporating lifelong learning into LLM-based agents. We categorize the\ncore components of these agents into three modules: the perception module for\nmultimodal input integration, the memory module for storing and retrieving\nevolving knowledge, and the action module for grounded interactions with the\ndynamic environment. We highlight how these pillars collectively enable\ncontinuous adaptation, mitigate catastrophic forgetting, and improve long-term\nperformance. This survey provides a roadmap for researchers and practitioners\nworking to develop lifelong learning capabilities in LLM agents, offering\ninsights into emerging trends, evaluation metrics, and application scenarios.\nRelevant literature and resources are available at \\href{this\nurl}{https://github.com/qianlima-lab/awesome-lifelong-llm-agent}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifelong learning, also known as continual or incremental learning, is a\ncrucial component for advancing Artificial General Intelligence (AGI) by\nenabling systems to continuously adapt in dynamic environments. While large\nlanguage models (LLMs) have demonstrated impressive capabilities in natural\nlanguage processing, existing LLM agents are typically designed for static\nsystems and lack the ability to adapt over time in response to new challenges.\nThis survey is the first to systematically summarize the potential techniques\nfor incorporating lifelong learning into LLM-based agents. We categorize the\ncore components of these agents into three modules: the perception module for\nmultimodal input integration, the memory module for storing and retrieving\nevolving knowledge, and the action module for grounded interactions with the\ndynamic environment. We highlight how these pillars collectively enable\ncontinuous adaptation, mitigate catastrophic forgetting, and improve long-term\nperformance. This survey provides a roadmap for researchers and practitioners\nworking to develop lifelong learning capabilities in LLM agents, offering\ninsights into emerging trends, evaluation metrics, and application scenarios.\nRelevant literature and resources are available at \\href{this\nurl}{https://github.com/qianlima-lab/awesome-lifelong-llm-agent}."
                },
                "authors": [
                    {
                        "name": "Junhao Zheng"
                    },
                    {
                        "name": "Chengming Shi"
                    },
                    {
                        "name": "Xidi Cai"
                    },
                    {
                        "name": "Qiuke Li"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Chenxing Li"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Qianli Ma"
                    }
                ],
                "author_detail": {
                    "name": "Qianli Ma"
                },
                "author": "Qianli Ma",
                "arxiv_comment": "46 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07276v1",
                "updated": "2025-01-13T12:41:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    41,
                    27,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T12:41:27Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    41,
                    27,
                    0,
                    13,
                    0
                ],
                "title": "Bridging Smart Meter Gaps: A Benchmark of Statistical, Machine Learning\n  and Time Series Foundation Models for Data Imputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Smart Meter Gaps: A Benchmark of Statistical, Machine Learning\n  and Time Series Foundation Models for Data Imputation"
                },
                "summary": "The integrity of time series data in smart grids is often compromised by\nmissing values due to sensor failures, transmission errors, or disruptions.\nGaps in smart meter data can bias consumption analyses and hinder reliable\npredictions, causing technical and economic inefficiencies. As smart meter data\ngrows in volume and complexity, conventional techniques struggle with its\nnonlinear and nonstationary patterns. In this context, Generative Artificial\nIntelligence offers promising solutions that may outperform traditional\nstatistical methods. In this paper, we evaluate two general-purpose Large\nLanguage Models and five Time Series Foundation Models for smart meter data\nimputation, comparing them with conventional Machine Learning and statistical\nmodels. We introduce artificial gaps (30 minutes to one day) into an anonymized\npublic dataset to test inference capabilities. Results show that Time Series\nFoundation Models, with their contextual understanding and pattern recognition,\ncould significantly enhance imputation accuracy in certain cases. However, the\ntrade-off between computational cost and performance gains remains a critical\nconsideration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integrity of time series data in smart grids is often compromised by\nmissing values due to sensor failures, transmission errors, or disruptions.\nGaps in smart meter data can bias consumption analyses and hinder reliable\npredictions, causing technical and economic inefficiencies. As smart meter data\ngrows in volume and complexity, conventional techniques struggle with its\nnonlinear and nonstationary patterns. In this context, Generative Artificial\nIntelligence offers promising solutions that may outperform traditional\nstatistical methods. In this paper, we evaluate two general-purpose Large\nLanguage Models and five Time Series Foundation Models for smart meter data\nimputation, comparing them with conventional Machine Learning and statistical\nmodels. We introduce artificial gaps (30 minutes to one day) into an anonymized\npublic dataset to test inference capabilities. Results show that Time Series\nFoundation Models, with their contextual understanding and pattern recognition,\ncould significantly enhance imputation accuracy in certain cases. However, the\ntrade-off between computational cost and performance gains remains a critical\nconsideration."
                },
                "authors": [
                    {
                        "name": "Amir Sartipi"
                    },
                    {
                        "name": "Joaquin Delgado Fernandez"
                    },
                    {
                        "name": "Sergio Potenciano Menci"
                    },
                    {
                        "name": "Alessio Magitteri"
                    }
                ],
                "author_detail": {
                    "name": "Alessio Magitteri"
                },
                "author": "Alessio Magitteri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17726v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17726v4",
                "updated": "2025-01-13T12:38:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    38,
                    41,
                    0,
                    13,
                    0
                ],
                "published": "2024-03-26T14:14:30Z",
                "published_parsed": [
                    2024,
                    3,
                    26,
                    14,
                    14,
                    30,
                    1,
                    86,
                    0
                ],
                "title": "Tiny Models are the Computational Saver for Large Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tiny Models are the Computational Saver for Large Models"
                },
                "summary": "This paper introduces TinySaver, an early-exit-like dynamic model compression\napproach which employs tiny models to substitute large models adaptively.\nDistinct from traditional compression techniques, dynamic methods like\nTinySaver can leverage the difficulty differences to allow certain inputs to\ncomplete their inference processes early, thereby conserving computational\nresources. Most existing early exit designs are implemented by attaching\nadditional network branches to the model's backbone. Our study, however,\nreveals that completely independent tiny models can replace a substantial\nportion of the larger models' job with minimal impact on performance. Employing\nthem as the first exit can remarkably enhance computational efficiency. By\nsearching and employing the most appropriate tiny model as the computational\nsaver for a given large model, the proposed approaches work as a novel and\ngeneric method to model compression. This finding will help the research\ncommunity in exploring new compression methods to address the escalating\ncomputational demands posed by rapidly evolving AI models. Our evaluation of\nthis approach in ImageNet-1k classification demonstrates its potential to\nreduce the number of compute operations by up to 90\\%, with only negligible\nlosses in performance, across various modern vision models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TinySaver, an early-exit-like dynamic model compression\napproach which employs tiny models to substitute large models adaptively.\nDistinct from traditional compression techniques, dynamic methods like\nTinySaver can leverage the difficulty differences to allow certain inputs to\ncomplete their inference processes early, thereby conserving computational\nresources. Most existing early exit designs are implemented by attaching\nadditional network branches to the model's backbone. Our study, however,\nreveals that completely independent tiny models can replace a substantial\nportion of the larger models' job with minimal impact on performance. Employing\nthem as the first exit can remarkably enhance computational efficiency. By\nsearching and employing the most appropriate tiny model as the computational\nsaver for a given large model, the proposed approaches work as a novel and\ngeneric method to model compression. This finding will help the research\ncommunity in exploring new compression methods to address the escalating\ncomputational demands posed by rapidly evolving AI models. Our evaluation of\nthis approach in ImageNet-1k classification demonstrates its potential to\nreduce the number of compute operations by up to 90\\%, with only negligible\nlosses in performance, across various modern vision models."
                },
                "authors": [
                    {
                        "name": "Qingyuan Wang"
                    },
                    {
                        "name": "Barry Cardiff"
                    },
                    {
                        "name": "Antoine FrappÃ©"
                    },
                    {
                        "name": "Benoit Larras"
                    },
                    {
                        "name": "Deepu John"
                    }
                ],
                "author_detail": {
                    "name": "Deepu John"
                },
                "author": "Deepu John",
                "arxiv_doi": "10.1007/978-3-031-72992-8_10",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-72992-8_10",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.17726v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17726v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07268v1",
                "updated": "2025-01-13T12:31:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    31,
                    15,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T12:31:15Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    31,
                    15,
                    0,
                    13,
                    0
                ],
                "title": "A differentiable binary microlensing model using adaptive contour\n  integration method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A differentiable binary microlensing model using adaptive contour\n  integration method"
                },
                "summary": "We present microlux, which is a Jax-based code that can compute the binary\nmicrolensing light curve and its derivatives both efficiently and accurately.\nThe key feature of microlux is the implementation of a modified version of the\nadaptive sampling algorithm that was originally proposed by V. Bozza to account\nfor the finite-source effect most efficiently. The efficiency and accuracy of\nmicrolux have been verified across the relevant parameter space for binary\nmicrolensing. As a differentiable code, microlux makes it possible to apply\ngradient-based algorithms to the search and posterior estimation of the\nmicrolensing modeling. As an example, we use microlux to model a real\nmicrolensing event and infer the model posterior via both Fisher information\nmatrix and Hamiltonian Monte Carlo, neither of which would have been possible\nwithout the access to accurate model gradients.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present microlux, which is a Jax-based code that can compute the binary\nmicrolensing light curve and its derivatives both efficiently and accurately.\nThe key feature of microlux is the implementation of a modified version of the\nadaptive sampling algorithm that was originally proposed by V. Bozza to account\nfor the finite-source effect most efficiently. The efficiency and accuracy of\nmicrolux have been verified across the relevant parameter space for binary\nmicrolensing. As a differentiable code, microlux makes it possible to apply\ngradient-based algorithms to the search and posterior estimation of the\nmicrolensing modeling. As an example, we use microlux to model a real\nmicrolensing event and infer the model posterior via both Fisher information\nmatrix and Hamiltonian Monte Carlo, neither of which would have been possible\nwithout the access to accurate model gradients."
                },
                "authors": [
                    {
                        "name": "Haibin Ren"
                    },
                    {
                        "name": "Wei Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhu"
                },
                "author": "Wei Zhu",
                "arxiv_comment": "14 pages, 7 figures. Submitted to AAS Journals",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07267v1",
                "updated": "2025-01-13T12:30:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    30,
                    8,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T12:30:08Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    30,
                    8,
                    0,
                    13,
                    0
                ],
                "title": "Transforming Role Classification in Scientific Teams Using LLMs and\n  Advanced Predictive Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming Role Classification in Scientific Teams Using LLMs and\n  Advanced Predictive Analytics"
                },
                "summary": "Scientific team dynamics are critical in determining the nature and impact of\nresearch outputs. However, existing methods for classifying author roles based\non self-reports and clustering lack comprehensive contextual analysis of\ncontributions. Thus, we present a transformative approach to classifying author\nroles in scientific teams using advanced large language models (LLMs), which\noffers a more refined analysis compared to traditional clustering methods.\nSpecifically, we seek to complement and enhance these traditional methods by\nutilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2\n70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting,\nwe categorize author roles and demonstrate that GPT-4 outperforms other models\nacross multiple categories, surpassing traditional approaches such as XGBoost\nand BERT. Our methodology also includes building a predictive deep learning\nmodel using 10 features. By training this model on a dataset derived from the\nOpenAlex database, which provides detailed metadata on academic publications --\nsuch as author-publication history, author affiliation, research topics, and\ncitation counts -- we achieve an F1 score of 0.76, demonstrating robust\nclassification of author roles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific team dynamics are critical in determining the nature and impact of\nresearch outputs. However, existing methods for classifying author roles based\non self-reports and clustering lack comprehensive contextual analysis of\ncontributions. Thus, we present a transformative approach to classifying author\nroles in scientific teams using advanced large language models (LLMs), which\noffers a more refined analysis compared to traditional clustering methods.\nSpecifically, we seek to complement and enhance these traditional methods by\nutilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2\n70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting,\nwe categorize author roles and demonstrate that GPT-4 outperforms other models\nacross multiple categories, surpassing traditional approaches such as XGBoost\nand BERT. Our methodology also includes building a predictive deep learning\nmodel using 10 features. By training this model on a dataset derived from the\nOpenAlex database, which provides detailed metadata on academic publications --\nsuch as author-publication history, author affiliation, research topics, and\ncitation counts -- we achieve an F1 score of 0.76, demonstrating robust\nclassification of author roles."
                },
                "authors": [
                    {
                        "name": "Wonduk Seo"
                    },
                    {
                        "name": "Yi Bu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Bu"
                },
                "author": "Yi Bu",
                "arxiv_comment": "14 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07264v1",
                "updated": "2025-01-13T12:24:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    24,
                    10,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T12:24:10Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    24,
                    10,
                    0,
                    13,
                    0
                ],
                "title": "Assessing the systematic errors of extreme-mass-ratio inspirals\n  waveforms for testing general relativity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the systematic errors of extreme-mass-ratio inspirals\n  waveforms for testing general relativity"
                },
                "summary": "Gravitational wave (GW) observations from extreme-mass-ratio inspirals\n(EMRIs) are powerful tools for testing general relativity (GR). However,\nsystematic errors arising from waveform models could potentially lead to\nincorrect scientific conclusions. These errors can be divided into two main\ncategories: fundamental bias (due to limitations in the validity of the\nEinstein field equations) and modeling error (due to inaccuracies in waveform\ntemplates). Using Bayesian inference, we investigate the impact of these\nsystematic errors on tests of GR. Regarding fundamental bias, we find that at\nlow signal-to-noise ratios (SNR), there is a risk of misidentifying a non-GR\nEMRI signal as a GR-EMRI one, and vice versa. However, this risk diminishes as\nthe SNR increases to around 40 or higher. Additionally, modeling errors might\nreduce the SNR of detected EMRI signals and could be misinterpreted as\ndeviations from GR, leading Bayesian inference to favor non-GR scenarios,\nespecially at high SNR. We emphasize the importance of developing sufficiently\naccurate waveform templates based on alternative gravity theories for testing\nGR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gravitational wave (GW) observations from extreme-mass-ratio inspirals\n(EMRIs) are powerful tools for testing general relativity (GR). However,\nsystematic errors arising from waveform models could potentially lead to\nincorrect scientific conclusions. These errors can be divided into two main\ncategories: fundamental bias (due to limitations in the validity of the\nEinstein field equations) and modeling error (due to inaccuracies in waveform\ntemplates). Using Bayesian inference, we investigate the impact of these\nsystematic errors on tests of GR. Regarding fundamental bias, we find that at\nlow signal-to-noise ratios (SNR), there is a risk of misidentifying a non-GR\nEMRI signal as a GR-EMRI one, and vice versa. However, this risk diminishes as\nthe SNR increases to around 40 or higher. Additionally, modeling errors might\nreduce the SNR of detected EMRI signals and could be misinterpreted as\ndeviations from GR, leading Bayesian inference to favor non-GR scenarios,\nespecially at high SNR. We emphasize the importance of developing sufficiently\naccurate waveform templates based on alternative gravity theories for testing\nGR."
                },
                "authors": [
                    {
                        "name": "Ping Shen"
                    },
                    {
                        "name": "Qiuxin Cui"
                    },
                    {
                        "name": "Wen-Biao Han"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Biao Han"
                },
                "author": "Wen-Biao Han",
                "arxiv_doi": "10.1103/PhysRevD.111.024004",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.024004",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.07264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10351v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10351v2",
                "updated": "2025-01-13T12:22:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    22,
                    52,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-13T18:47:11Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    47,
                    11,
                    4,
                    348,
                    0
                ],
                "title": "VibrantVS: A high-resolution multi-task transformer for forest canopy\n  height estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VibrantVS: A high-resolution multi-task transformer for forest canopy\n  height estimation"
                },
                "summary": "This paper explores the application of a novel multi-task vision transformer\n(ViT) model for the estimation of canopy height models (CHMs) using 4-band\nNational Agriculture Imagery Program (NAIP) imagery across the western United\nStates. We compare the effectiveness of this model in terms of accuracy and\nprecision aggregated across ecoregions and class heights versus three other\nbenchmark peer-reviewed models. Key findings suggest that, while other\nbenchmark models can provide high precision in localized areas, the VibrantVS\nmodel has substantial advantages across a broad reach of ecoregions in the\nwestern United States with higher accuracy, higher precision, the ability to\ngenerate updated inference at a cadence of three years or less, and high\nspatial resolution. The VibrantVS model provides significant value for\necological monitoring and land management decisions for wildfire mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the application of a novel multi-task vision transformer\n(ViT) model for the estimation of canopy height models (CHMs) using 4-band\nNational Agriculture Imagery Program (NAIP) imagery across the western United\nStates. We compare the effectiveness of this model in terms of accuracy and\nprecision aggregated across ecoregions and class heights versus three other\nbenchmark peer-reviewed models. Key findings suggest that, while other\nbenchmark models can provide high precision in localized areas, the VibrantVS\nmodel has substantial advantages across a broad reach of ecoregions in the\nwestern United States with higher accuracy, higher precision, the ability to\ngenerate updated inference at a cadence of three years or less, and high\nspatial resolution. The VibrantVS model provides significant value for\necological monitoring and land management decisions for wildfire mitigation."
                },
                "authors": [
                    {
                        "name": "Tony Chang"
                    },
                    {
                        "name": "Kiarie Ndegwa"
                    },
                    {
                        "name": "Andreas Gros"
                    },
                    {
                        "name": "Vincent A. Landau"
                    },
                    {
                        "name": "Luke J. Zachmann"
                    },
                    {
                        "name": "Bogdan State"
                    },
                    {
                        "name": "Mitchell A. Gritts"
                    },
                    {
                        "name": "Colton W. Miller"
                    },
                    {
                        "name": "Nathan E. Rutenbeck"
                    },
                    {
                        "name": "Scott Conway"
                    },
                    {
                        "name": "Guy Bayes"
                    }
                ],
                "author_detail": {
                    "name": "Guy Bayes"
                },
                "author": "Guy Bayes",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10351v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10351v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07261v1",
                "updated": "2025-01-13T12:21:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    21,
                    33,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T12:21:33Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    21,
                    33,
                    0,
                    13,
                    0
                ],
                "title": "Inference precision about an aircraft crash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference precision about an aircraft crash"
                },
                "summary": "Problem-based learning benefits from situations taken from real life, which\narouse student interest. The shooting of Rwanda president aircraft on April\n6th, 1994 is still unsolved. We discuss the methods to infer informations and\nconclusions about where the aircraft was shot and its trajectory during its\nfall, as well as about the place from which the missiles were launched, their\ntrajectory and type. To this goal, we compile expert reports, witness\nindications and other public sources, then translate plain language sentences\ninto quantitative equalities and inequalities applied to geometry and mechanics\nat undergraduate level. The precision of each result is discussed and\npropagated in order to ensure a proper assessment of the hypotheses and a\ntraceability of their consequences. Overall, the precision discussion can train\nthe students critical mind, and teach inference methods which are routinely\nused in several fields of physics research. In addition, it demonstrates the\nimportance and limits of scientific expertise during a judiciary process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Problem-based learning benefits from situations taken from real life, which\narouse student interest. The shooting of Rwanda president aircraft on April\n6th, 1994 is still unsolved. We discuss the methods to infer informations and\nconclusions about where the aircraft was shot and its trajectory during its\nfall, as well as about the place from which the missiles were launched, their\ntrajectory and type. To this goal, we compile expert reports, witness\nindications and other public sources, then translate plain language sentences\ninto quantitative equalities and inequalities applied to geometry and mechanics\nat undergraduate level. The precision of each result is discussed and\npropagated in order to ensure a proper assessment of the hypotheses and a\ntraceability of their consequences. Overall, the precision discussion can train\nthe students critical mind, and teach inference methods which are routinely\nused in several fields of physics research. In addition, it demonstrates the\nimportance and limits of scientific expertise during a judiciary process."
                },
                "authors": [
                    {
                        "name": "FranÃ§ois Graner"
                    },
                    {
                        "name": "Stefano Matthias Panebianco"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Matthias Panebianco"
                },
                "arxiv_affiliation": "DPHN",
                "author": "Stefano Matthias Panebianco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07256v1",
                "updated": "2025-01-13T12:11:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    11,
                    7,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T12:11:07Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    11,
                    7,
                    0,
                    13,
                    0
                ],
                "title": "EdgeTAM: On-Device Track Anything Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeTAM: On-Device Track Anything Model"
                },
                "summary": "On top of Segment Anything Model (SAM), SAM 2 further extends its capability\nfrom image to video inputs through a memory bank mechanism and obtains a\nremarkable performance compared with previous methods, making it a foundation\nmodel for video segmentation task. In this paper, we aim at making SAM 2 much\nmore efficient so that it even runs on mobile devices while maintaining a\ncomparable performance. Despite several works optimizing SAM for better\nefficiency, we find they are not sufficient for SAM 2 because they all focus on\ncompressing the image encoder, while our benchmark shows that the newly\nintroduced memory attention blocks are also the latency bottleneck. Given this\nobservation, we propose EdgeTAM, which leverages a novel 2D Spatial Perceiver\nto reduce the computational cost. In particular, the proposed 2D Spatial\nPerceiver encodes the densely stored frame-level memories with a lightweight\nTransformer that contains a fixed set of learnable queries. Given that video\nsegmentation is a dense prediction task, we find preserving the spatial\nstructure of the memories is essential so that the queries are split into\nglobal-level and patch-level groups. We also propose a distillation pipeline\nthat further improves the performance without inference overhead. As a result,\nEdgeTAM achieves 87.7, 70.0, 72.3, and 71.7 J&F on DAVIS 2017, MOSE, SA-V val,\nand SA-V test, while running at 16 FPS on iPhone 15 Pro Max.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On top of Segment Anything Model (SAM), SAM 2 further extends its capability\nfrom image to video inputs through a memory bank mechanism and obtains a\nremarkable performance compared with previous methods, making it a foundation\nmodel for video segmentation task. In this paper, we aim at making SAM 2 much\nmore efficient so that it even runs on mobile devices while maintaining a\ncomparable performance. Despite several works optimizing SAM for better\nefficiency, we find they are not sufficient for SAM 2 because they all focus on\ncompressing the image encoder, while our benchmark shows that the newly\nintroduced memory attention blocks are also the latency bottleneck. Given this\nobservation, we propose EdgeTAM, which leverages a novel 2D Spatial Perceiver\nto reduce the computational cost. In particular, the proposed 2D Spatial\nPerceiver encodes the densely stored frame-level memories with a lightweight\nTransformer that contains a fixed set of learnable queries. Given that video\nsegmentation is a dense prediction task, we find preserving the spatial\nstructure of the memories is essential so that the queries are split into\nglobal-level and patch-level groups. We also propose a distillation pipeline\nthat further improves the performance without inference overhead. As a result,\nEdgeTAM achieves 87.7, 70.0, 72.3, and 71.7 J&F on DAVIS 2017, MOSE, SA-V val,\nand SA-V test, while running at 16 FPS on iPhone 15 Pro Max."
                },
                "authors": [
                    {
                        "name": "Chong Zhou"
                    },
                    {
                        "name": "Chenchen Zhu"
                    },
                    {
                        "name": "Yunyang Xiong"
                    },
                    {
                        "name": "Saksham Suri"
                    },
                    {
                        "name": "Fanyi Xiao"
                    },
                    {
                        "name": "Lemeng Wu"
                    },
                    {
                        "name": "Raghuraman Krishnamoorthi"
                    },
                    {
                        "name": "Bo Dai"
                    },
                    {
                        "name": "Chen Change Loy"
                    },
                    {
                        "name": "Vikas Chandra"
                    },
                    {
                        "name": "Bilge Soran"
                    }
                ],
                "author_detail": {
                    "name": "Bilge Soran"
                },
                "author": "Bilge Soran",
                "arxiv_comment": "Code will be released at https://github.com/facebookresearch/EdgeTAM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05068v2",
                "updated": "2025-01-13T12:06:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    6,
                    15,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-09T08:44:06Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    8,
                    44,
                    6,
                    3,
                    9,
                    0
                ],
                "title": "D3RM: A Discrete Denoising Diffusion Refinement Model for Piano\n  Transcription",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D3RM: A Discrete Denoising Diffusion Refinement Model for Piano\n  Transcription"
                },
                "summary": "Diffusion models have been widely used in the generative domain due to their\nconvincing performance in modeling complex data distributions. Moreover, they\nhave shown competitive results on discriminative tasks, such as image\nsegmentation. While diffusion models have also been explored for automatic\nmusic transcription, their performance has yet to reach a competitive level. In\nthis paper, we focus on discrete diffusion model's refinement capabilities and\npresent a novel architecture for piano transcription. Our model utilizes\nNeighborhood Attention layers as the denoising module, gradually predicting the\ntarget high-resolution piano roll, conditioned on the finetuned features of a\npretrained acoustic model. To further enhance refinement, we devise a novel\nstrategy which applies distinct transition states during training and inference\nstage of discrete diffusion models. Experiments on the MAESTRO dataset show\nthat our approach outperforms previous diffusion-based piano transcription\nmodels and the baseline model in terms of F1 score. Our code is available in\nhttps://github.com/hanshounsu/d3rm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have been widely used in the generative domain due to their\nconvincing performance in modeling complex data distributions. Moreover, they\nhave shown competitive results on discriminative tasks, such as image\nsegmentation. While diffusion models have also been explored for automatic\nmusic transcription, their performance has yet to reach a competitive level. In\nthis paper, we focus on discrete diffusion model's refinement capabilities and\npresent a novel architecture for piano transcription. Our model utilizes\nNeighborhood Attention layers as the denoising module, gradually predicting the\ntarget high-resolution piano roll, conditioned on the finetuned features of a\npretrained acoustic model. To further enhance refinement, we devise a novel\nstrategy which applies distinct transition states during training and inference\nstage of discrete diffusion models. Experiments on the MAESTRO dataset show\nthat our approach outperforms previous diffusion-based piano transcription\nmodels and the baseline model in terms of F1 score. Our code is available in\nhttps://github.com/hanshounsu/d3rm."
                },
                "authors": [
                    {
                        "name": "Hounsu Kim"
                    },
                    {
                        "name": "Taegyun Kwon"
                    },
                    {
                        "name": "Juhan Nam"
                    }
                ],
                "author_detail": {
                    "name": "Juhan Nam"
                },
                "author": "Juhan Nam",
                "arxiv_comment": "Accepted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07247v1",
                "updated": "2025-01-13T11:55:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    55,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T11:55:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    55,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "Interpretable machine-learning for predicting molecular weight of PLA\n  based on artificial bee colony optimization algorithm and adaptive neurofuzzy\n  inference system",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable machine-learning for predicting molecular weight of PLA\n  based on artificial bee colony optimization algorithm and adaptive neurofuzzy\n  inference system"
                },
                "summary": "This article discusses the integration of the Artificial Bee Colony (ABC)\nalgorithm with two supervised learning methods, namely Artificial Neural\nNetworks (ANNs) and Adaptive Network-based Fuzzy Inference System (ANFIS), for\nfeature selection from Near-Infrared (NIR) spectra for predicting the molecular\nweight of medical-grade Polylactic Acid (PLA). During extrusion processing of\nPLA, in-line NIR spectra were captured along with extrusion process and machine\nsetting data. With a dataset comprising 63 observations and 512 input features,\nappropriate machine learning tools are essential for interpreting data and\nselecting features to improve prediction accuracy. Initially, the ABC\noptimization algorithm is coupled with ANN/ANFIS to forecast PLA molecular\nweight. The objective functions of the ABC algorithm are to minimize the root\nmean square error (RMSE) between experimental and predicted PLA molecular\nweights while also minimizing the number of input features. Results indicate\nthat employing ABC-ANFIS yields the lowest RMSE of 282 Da and identifies four\nsignificant parameters (NIR wavenumbers 6158 cm-1, 6310 cm-1, 6349 cm-1, and\nmelt temperature) for prediction. These findings demonstrate the effectiveness\nof using the ABC algorithm with ANFIS for selecting a minimal set of features\nto predict PLA molecular weight with high accuracy during processing",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This article discusses the integration of the Artificial Bee Colony (ABC)\nalgorithm with two supervised learning methods, namely Artificial Neural\nNetworks (ANNs) and Adaptive Network-based Fuzzy Inference System (ANFIS), for\nfeature selection from Near-Infrared (NIR) spectra for predicting the molecular\nweight of medical-grade Polylactic Acid (PLA). During extrusion processing of\nPLA, in-line NIR spectra were captured along with extrusion process and machine\nsetting data. With a dataset comprising 63 observations and 512 input features,\nappropriate machine learning tools are essential for interpreting data and\nselecting features to improve prediction accuracy. Initially, the ABC\noptimization algorithm is coupled with ANN/ANFIS to forecast PLA molecular\nweight. The objective functions of the ABC algorithm are to minimize the root\nmean square error (RMSE) between experimental and predicted PLA molecular\nweights while also minimizing the number of input features. Results indicate\nthat employing ABC-ANFIS yields the lowest RMSE of 282 Da and identifies four\nsignificant parameters (NIR wavenumbers 6158 cm-1, 6310 cm-1, 6349 cm-1, and\nmelt temperature) for prediction. These findings demonstrate the effectiveness\nof using the ABC algorithm with ANFIS for selecting a minimal set of features\nto predict PLA molecular weight with high accuracy during processing"
                },
                "authors": [
                    {
                        "name": "Amir Pouya Masoumi"
                    },
                    {
                        "name": "Leo Creedon"
                    },
                    {
                        "name": "Ramen Ghosh"
                    },
                    {
                        "name": "Nimra Munir"
                    },
                    {
                        "name": "Ross McMorrow"
                    },
                    {
                        "name": "Marion McAfee"
                    }
                ],
                "author_detail": {
                    "name": "Marion McAfee"
                },
                "author": "Marion McAfee",
                "arxiv_doi": "10.1109/ISSC61953.2024.10603031",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ISSC61953.2024.10603031",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.07247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07246v1",
                "updated": "2025-01-13T11:54:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    54,
                    40,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T11:54:40Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    54,
                    40,
                    0,
                    13,
                    0
                ],
                "title": "Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-CoT: Exploring Chain-of-Thought Reasoning in Large Audio Language\n  Model"
                },
                "summary": "Large Audio-Language Models (LALMs) have demonstrated remarkable performance\nin tasks involving audio perception and understanding, such as speech\nrecognition and audio captioning. However, their reasoning capabilities -\ncritical for solving complex real-world problems - remain underexplored. In\nthis work, we conduct the first exploration into integrating Chain-of-Thought\n(CoT) reasoning into LALMs to enhance their reasoning ability across auditory\nmodalities. We evaluate representative CoT methods, analyzing their performance\nin both information extraction and reasoning tasks across sound, music, and\nspeech domains. Our findings reveal that CoT methods significantly improve\nperformance on easy and medium tasks but encounter challenges with hard tasks,\nwhere reasoning chains can confuse the model rather than improve accuracy.\nAdditionally, we identify a positive correlation between reasoning path length\nand accuracy, demonstrating the potential of scaling inference for advanced\ninstruction-following and reasoning. This study not only highlights the promise\nof CoT in enhancing LALM reasoning capabilities but also identifies key\nlimitations and provides actionable directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Audio-Language Models (LALMs) have demonstrated remarkable performance\nin tasks involving audio perception and understanding, such as speech\nrecognition and audio captioning. However, their reasoning capabilities -\ncritical for solving complex real-world problems - remain underexplored. In\nthis work, we conduct the first exploration into integrating Chain-of-Thought\n(CoT) reasoning into LALMs to enhance their reasoning ability across auditory\nmodalities. We evaluate representative CoT methods, analyzing their performance\nin both information extraction and reasoning tasks across sound, music, and\nspeech domains. Our findings reveal that CoT methods significantly improve\nperformance on easy and medium tasks but encounter challenges with hard tasks,\nwhere reasoning chains can confuse the model rather than improve accuracy.\nAdditionally, we identify a positive correlation between reasoning path length\nand accuracy, demonstrating the potential of scaling inference for advanced\ninstruction-following and reasoning. This study not only highlights the promise\nof CoT in enhancing LALM reasoning capabilities but also identifies key\nlimitations and provides actionable directions for future research."
                },
                "authors": [
                    {
                        "name": "Ziyang Ma"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Eng Siong Chng"
                    },
                    {
                        "name": "Xie Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xie Chen"
                },
                "author": "Xie Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12094v2",
                "updated": "2025-01-13T11:46:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    46,
                    59,
                    0,
                    13,
                    0
                ],
                "published": "2024-03-15T06:57:08Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    6,
                    57,
                    8,
                    4,
                    75,
                    0
                ],
                "title": "Are LLMs Good Cryptic Crossword Solvers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Good Cryptic Crossword Solvers?"
                },
                "summary": "Cryptic crosswords are puzzles that rely not only on general knowledge but\nalso on the solver's ability to manipulate language on different levels and\ndeal with various types of wordplay. Previous research suggests that solving\nsuch puzzles is a challenge even for modern NLP models. However, the abilities\nof large language models (LLMs) have not yet been tested on this task. In this\npaper, we establish the benchmark results for three popular LLMs -- LLaMA2,\nMistral, and ChatGPT -- showing that their performance on this task is still\nfar from that of humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryptic crosswords are puzzles that rely not only on general knowledge but\nalso on the solver's ability to manipulate language on different levels and\ndeal with various types of wordplay. Previous research suggests that solving\nsuch puzzles is a challenge even for modern NLP models. However, the abilities\nof large language models (LLMs) have not yet been tested on this task. In this\npaper, we establish the benchmark results for three popular LLMs -- LLaMA2,\nMistral, and ChatGPT -- showing that their performance on this task is still\nfar from that of humans."
                },
                "authors": [
                    {
                        "name": "Abdelrahman Sadallah"
                    },
                    {
                        "name": "Daria Kotova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07238v1",
                "updated": "2025-01-13T11:36:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    36,
                    33,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T11:36:33Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    36,
                    33,
                    0,
                    13,
                    0
                ],
                "title": "Lessons From Red Teaming 100 Generative AI Products",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lessons From Red Teaming 100 Generative AI Products"
                },
                "summary": "In recent years, AI red teaming has emerged as a practice for probing the\nsafety and security of generative AI systems. Due to the nascency of the field,\nthere are many open questions about how red teaming operations should be\nconducted. Based on our experience red teaming over 100 generative AI products\nat Microsoft, we present our internal threat model ontology and eight main\nlessons we have learned:\n  1. Understand what the system can do and where it is applied\n  2. You don't have to compute gradients to break an AI system\n  3. AI red teaming is not safety benchmarking\n  4. Automation can help cover more of the risk landscape\n  5. The human element of AI red teaming is crucial\n  6. Responsible AI harms are pervasive but difficult to measure\n  7. LLMs amplify existing security risks and introduce new ones\n  8. The work of securing AI systems will never be complete\n  By sharing these insights alongside case studies from our operations, we\noffer practical recommendations aimed at aligning red teaming efforts with real\nworld risks. We also highlight aspects of AI red teaming that we believe are\noften misunderstood and discuss open questions for the field to consider.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, AI red teaming has emerged as a practice for probing the\nsafety and security of generative AI systems. Due to the nascency of the field,\nthere are many open questions about how red teaming operations should be\nconducted. Based on our experience red teaming over 100 generative AI products\nat Microsoft, we present our internal threat model ontology and eight main\nlessons we have learned:\n  1. Understand what the system can do and where it is applied\n  2. You don't have to compute gradients to break an AI system\n  3. AI red teaming is not safety benchmarking\n  4. Automation can help cover more of the risk landscape\n  5. The human element of AI red teaming is crucial\n  6. Responsible AI harms are pervasive but difficult to measure\n  7. LLMs amplify existing security risks and introduce new ones\n  8. The work of securing AI systems will never be complete\n  By sharing these insights alongside case studies from our operations, we\noffer practical recommendations aimed at aligning red teaming efforts with real\nworld risks. We also highlight aspects of AI red teaming that we believe are\noften misunderstood and discuss open questions for the field to consider."
                },
                "authors": [
                    {
                        "name": "Blake Bullwinkel"
                    },
                    {
                        "name": "Amanda Minnich"
                    },
                    {
                        "name": "Shiven Chawla"
                    },
                    {
                        "name": "Gary Lopez"
                    },
                    {
                        "name": "Martin Pouliot"
                    },
                    {
                        "name": "Whitney Maxwell"
                    },
                    {
                        "name": "Joris de Gruyter"
                    },
                    {
                        "name": "Katherine Pratt"
                    },
                    {
                        "name": "Saphir Qi"
                    },
                    {
                        "name": "Nina Chikanov"
                    },
                    {
                        "name": "Roman Lutz"
                    },
                    {
                        "name": "Raja Sekhar Rao Dheekonda"
                    },
                    {
                        "name": "Bolor-Erdene Jagdagdorj"
                    },
                    {
                        "name": "Eugenia Kim"
                    },
                    {
                        "name": "Justin Song"
                    },
                    {
                        "name": "Keegan Hines"
                    },
                    {
                        "name": "Daniel Jones"
                    },
                    {
                        "name": "Giorgio Severi"
                    },
                    {
                        "name": "Richard Lundeen"
                    },
                    {
                        "name": "Sam Vaughan"
                    },
                    {
                        "name": "Victoria Westerhoff"
                    },
                    {
                        "name": "Pete Bryan"
                    },
                    {
                        "name": "Ram Shankar Siva Kumar"
                    },
                    {
                        "name": "Yonatan Zunger"
                    },
                    {
                        "name": "Chang Kawaguchi"
                    },
                    {
                        "name": "Mark Russinovich"
                    }
                ],
                "author_detail": {
                    "name": "Mark Russinovich"
                },
                "author": "Mark Russinovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07237v1",
                "updated": "2025-01-13T11:35:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    35,
                    9,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T11:35:09Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    35,
                    9,
                    0,
                    13,
                    0
                ],
                "title": "Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs\n  Training"
                },
                "summary": "Large language models (LLMs) have shown impressive performance across a range\nof natural language processing tasks. However, their vast number of parameters\nintroduces significant memory challenges during training, particularly when\nusing memory-intensive optimizers like Adam. Existing memory-efficient\nalgorithms often rely on techniques such as singular value decomposition\nprojection or weight freezing. While these approaches help alleviate memory\nconstraints, they generally produce suboptimal results compared to full-rank\nupdates. In this paper, we investigate the memory-efficient method beyond\nlow-rank training, proposing a novel solution called Gradient Wavelet Transform\n(GWT), which applies wavelet transforms to gradients in order to significantly\nreduce the memory requirements for maintaining optimizer states. We demonstrate\nthat GWT can be seamlessly integrated with memory-intensive optimizers,\nenabling efficient training without sacrificing performance. Through extensive\nexperiments on both pre-training and fine-tuning tasks, we show that GWT\nachieves state-of-the-art performance compared with advanced memory-efficient\noptimizers and full-rank approaches in terms of both memory usage and training\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive performance across a range\nof natural language processing tasks. However, their vast number of parameters\nintroduces significant memory challenges during training, particularly when\nusing memory-intensive optimizers like Adam. Existing memory-efficient\nalgorithms often rely on techniques such as singular value decomposition\nprojection or weight freezing. While these approaches help alleviate memory\nconstraints, they generally produce suboptimal results compared to full-rank\nupdates. In this paper, we investigate the memory-efficient method beyond\nlow-rank training, proposing a novel solution called Gradient Wavelet Transform\n(GWT), which applies wavelet transforms to gradients in order to significantly\nreduce the memory requirements for maintaining optimizer states. We demonstrate\nthat GWT can be seamlessly integrated with memory-intensive optimizers,\nenabling efficient training without sacrificing performance. Through extensive\nexperiments on both pre-training and fine-tuning tasks, we show that GWT\nachieves state-of-the-art performance compared with advanced memory-efficient\noptimizers and full-rank approaches in terms of both memory usage and training\nperformance."
                },
                "authors": [
                    {
                        "name": "Ziqing Wen"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Jiahuan Wang"
                    },
                    {
                        "name": "Xiaoge Deng"
                    },
                    {
                        "name": "Jinping Zou"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Tao Sun"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07227v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07227v1",
                "updated": "2025-01-13T11:28:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    28,
                    49,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T11:28:49Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    28,
                    49,
                    0,
                    13,
                    0
                ],
                "title": "MECD+: Unlocking Event-Level Causal Graph Discovery for Video Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MECD+: Unlocking Event-Level Causal Graph Discovery for Video Reasoning"
                },
                "summary": "Video causal reasoning aims to achieve a high-level understanding of videos\nfrom a causal perspective. However, it exhibits limitations in its scope,\nprimarily executed in a question-answering paradigm and focusing on brief video\nsegments containing isolated events and basic causal relations, lacking\ncomprehensive and structured causality analysis for videos with multiple\ninterconnected events. To fill this gap, we introduce a new task and dataset,\nMulti-Event Causal Discovery (MECD). It aims to uncover the causal relations\nbetween events distributed chronologically across long videos. Given visual\nsegments and textual descriptions of events, MECD identifies the causal\nassociations between these events to derive a comprehensive and structured\nevent-level video causal graph explaining why and how the result event\noccurred. To address the challenges of MECD, we devise a novel framework\ninspired by the Granger Causality method, incorporating an efficient mask-based\nevent prediction model to perform an Event Granger Test. It estimates causality\nby comparing the predicted result event when premise events are masked versus\nunmasked. Furthermore, we integrate causal inference techniques such as\nfront-door adjustment and counterfactual inference to mitigate challenges in\nMECD like causality confounding and illusory causality. Additionally, context\nchain reasoning is introduced to conduct more robust and generalized reasoning.\nExperiments validate the effectiveness of our framework in reasoning complete\ncausal relations, outperforming GPT-4o and VideoChat2 by 5.77% and 2.70%,\nrespectively. Further experiments demonstrate that causal relation graphs can\nalso contribute to downstream video understanding tasks such as video question\nanswering and video event prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video causal reasoning aims to achieve a high-level understanding of videos\nfrom a causal perspective. However, it exhibits limitations in its scope,\nprimarily executed in a question-answering paradigm and focusing on brief video\nsegments containing isolated events and basic causal relations, lacking\ncomprehensive and structured causality analysis for videos with multiple\ninterconnected events. To fill this gap, we introduce a new task and dataset,\nMulti-Event Causal Discovery (MECD). It aims to uncover the causal relations\nbetween events distributed chronologically across long videos. Given visual\nsegments and textual descriptions of events, MECD identifies the causal\nassociations between these events to derive a comprehensive and structured\nevent-level video causal graph explaining why and how the result event\noccurred. To address the challenges of MECD, we devise a novel framework\ninspired by the Granger Causality method, incorporating an efficient mask-based\nevent prediction model to perform an Event Granger Test. It estimates causality\nby comparing the predicted result event when premise events are masked versus\nunmasked. Furthermore, we integrate causal inference techniques such as\nfront-door adjustment and counterfactual inference to mitigate challenges in\nMECD like causality confounding and illusory causality. Additionally, context\nchain reasoning is introduced to conduct more robust and generalized reasoning.\nExperiments validate the effectiveness of our framework in reasoning complete\ncausal relations, outperforming GPT-4o and VideoChat2 by 5.77% and 2.70%,\nrespectively. Further experiments demonstrate that causal relation graphs can\nalso contribute to downstream video understanding tasks such as video question\nanswering and video event prediction."
                },
                "authors": [
                    {
                        "name": "Tieyuan Chen"
                    },
                    {
                        "name": "Huabin Liu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Yihang Chen"
                    },
                    {
                        "name": "Tianyao He"
                    },
                    {
                        "name": "Chaofan Gan"
                    },
                    {
                        "name": "Huanyu He"
                    },
                    {
                        "name": "Weiyao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weiyao Lin"
                },
                "author": "Weiyao Lin",
                "arxiv_comment": "IEEE TPAMI Submission. arXiv admin note: substantial text overlap\n  with arXiv:2409.17647",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07227v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07227v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07224v1",
                "updated": "2025-01-13T11:22:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    22,
                    57,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T11:22:57Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    22,
                    57,
                    0,
                    13,
                    0
                ],
                "title": "Touched by ChatGPT: Using an LLM to Drive Affective Tactile Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Touched by ChatGPT: Using an LLM to Drive Affective Tactile Interaction"
                },
                "summary": "Touch is a fundamental aspect of emotion-rich communication, playing a vital\nrole in human interaction and offering significant potential in human-robot\ninteraction. Previous research has demonstrated that a sparse representation of\nhuman touch can effectively convey social tactile signals. However, advances in\nhuman-robot tactile interaction remain limited, as many humanoid robots possess\nsimplistic capabilities, such as only opening and closing their hands,\nrestricting nuanced tactile expressions. In this study, we explore how a robot\ncan use sparse representations of tactile vibrations to convey emotions to a\nperson. To achieve this, we developed a wearable sleeve integrated with a 5x5\ngrid of vibration motors, enabling the robot to communicate diverse tactile\nemotions and gestures. Using chain prompts within a Large Language Model (LLM),\nwe generated distinct 10-second vibration patterns corresponding to 10 emotions\n(e.g., happiness, sadness, fear) and 6 touch gestures (e.g., pat, rub, tap).\nParticipants (N = 32) then rated each vibration stimulus based on perceived\nvalence and arousal. People are accurate at recognising intended emotions, a\nresult which aligns with earlier findings. These results highlight the LLM's\nability to generate emotional haptic data and effectively convey emotions\nthrough tactile signals. By translating complex emotional and tactile\nexpressions into vibratory patterns, this research demonstrates how LLMs can\nenhance physical interaction between humans and robots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Touch is a fundamental aspect of emotion-rich communication, playing a vital\nrole in human interaction and offering significant potential in human-robot\ninteraction. Previous research has demonstrated that a sparse representation of\nhuman touch can effectively convey social tactile signals. However, advances in\nhuman-robot tactile interaction remain limited, as many humanoid robots possess\nsimplistic capabilities, such as only opening and closing their hands,\nrestricting nuanced tactile expressions. In this study, we explore how a robot\ncan use sparse representations of tactile vibrations to convey emotions to a\nperson. To achieve this, we developed a wearable sleeve integrated with a 5x5\ngrid of vibration motors, enabling the robot to communicate diverse tactile\nemotions and gestures. Using chain prompts within a Large Language Model (LLM),\nwe generated distinct 10-second vibration patterns corresponding to 10 emotions\n(e.g., happiness, sadness, fear) and 6 touch gestures (e.g., pat, rub, tap).\nParticipants (N = 32) then rated each vibration stimulus based on perceived\nvalence and arousal. People are accurate at recognising intended emotions, a\nresult which aligns with earlier findings. These results highlight the LLM's\nability to generate emotional haptic data and effectively convey emotions\nthrough tactile signals. By translating complex emotional and tactile\nexpressions into vibratory patterns, this research demonstrates how LLMs can\nenhance physical interaction between humans and robots."
                },
                "authors": [
                    {
                        "name": "Qiaoqiao Ren"
                    },
                    {
                        "name": "Tony Belpaeme"
                    }
                ],
                "author_detail": {
                    "name": "Tony Belpaeme"
                },
                "author": "Tony Belpaeme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07221v1",
                "updated": "2025-01-13T11:20:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    20,
                    44,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T11:20:44Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    20,
                    44,
                    0,
                    13,
                    0
                ],
                "title": "Exploring the Use of Contrastive Language-Image Pre-Training for Human\n  Posture Classification: Insights from Yoga Pose Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Use of Contrastive Language-Image Pre-Training for Human\n  Posture Classification: Insights from Yoga Pose Analysis"
                },
                "summary": "Accurate human posture classification in images and videos is crucial for\nautomated applications across various fields, including work safety, physical\nrehabilitation, sports training, or daily assisted living. Recently, multimodal\nlearning methods, such as Contrastive Language-Image Pretraining (CLIP), have\nadvanced significantly in jointly understanding images and text. This study\naims to assess the effectiveness of CLIP in classifying human postures,\nfocusing on its application in yoga. Despite the initial limitations of the\nzero-shot approach, applying transfer learning on 15,301 images (real and\nsynthetic) with 82 classes has shown promising results. The article describes\nthe full procedure for fine-tuning, including the choice for image description\nsyntax, models and hyperparameters adjustment. The fine-tuned CLIP model,\ntested on 3826 images, achieves an accuracy of over 85%, surpassing the current\nstate-of-the-art of previous works on the same dataset by approximately 6%, its\ntraining time being 3.5 times lower than what is needed to fine-tune a\nYOLOv8-based model. For more application-oriented scenarios, with smaller\ndatasets of six postures each, containing 1301 and 401 training images, the\nfine-tuned models attain an accuracy of 98.8% and 99.1%, respectively.\nFurthermore, our experiments indicate that training with as few as 20 images\nper pose can yield around 90% accuracy in a six-class dataset. This study\ndemonstrates that this multimodal technique can be effectively used for yoga\npose classification, and possibly for human posture classification, in general.\nAdditionally, CLIP inference time (around 7 ms) supports that the model can be\nintegrated into automated systems for posture evaluation, e.g., for developing\na real-time personal yoga assistant for performance assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate human posture classification in images and videos is crucial for\nautomated applications across various fields, including work safety, physical\nrehabilitation, sports training, or daily assisted living. Recently, multimodal\nlearning methods, such as Contrastive Language-Image Pretraining (CLIP), have\nadvanced significantly in jointly understanding images and text. This study\naims to assess the effectiveness of CLIP in classifying human postures,\nfocusing on its application in yoga. Despite the initial limitations of the\nzero-shot approach, applying transfer learning on 15,301 images (real and\nsynthetic) with 82 classes has shown promising results. The article describes\nthe full procedure for fine-tuning, including the choice for image description\nsyntax, models and hyperparameters adjustment. The fine-tuned CLIP model,\ntested on 3826 images, achieves an accuracy of over 85%, surpassing the current\nstate-of-the-art of previous works on the same dataset by approximately 6%, its\ntraining time being 3.5 times lower than what is needed to fine-tune a\nYOLOv8-based model. For more application-oriented scenarios, with smaller\ndatasets of six postures each, containing 1301 and 401 training images, the\nfine-tuned models attain an accuracy of 98.8% and 99.1%, respectively.\nFurthermore, our experiments indicate that training with as few as 20 images\nper pose can yield around 90% accuracy in a six-class dataset. This study\ndemonstrates that this multimodal technique can be effectively used for yoga\npose classification, and possibly for human posture classification, in general.\nAdditionally, CLIP inference time (around 7 ms) supports that the model can be\nintegrated into automated systems for posture evaluation, e.g., for developing\na real-time personal yoga assistant for performance assessment."
                },
                "authors": [
                    {
                        "name": "Andrzej D. Dobrzycki"
                    },
                    {
                        "name": "Ana M. Bernardos"
                    },
                    {
                        "name": "Luca Bergesio"
                    },
                    {
                        "name": "Andrzej Pomirski"
                    },
                    {
                        "name": "Daniel SÃ¡ez-Trigueros"
                    }
                ],
                "author_detail": {
                    "name": "Daniel SÃ¡ez-Trigueros"
                },
                "author": "Daniel SÃ¡ez-Trigueros",
                "arxiv_doi": "10.3390/math12010076",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/math12010076",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.07221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Mathematics 2024, 12(1), 76",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07212v1",
                "updated": "2025-01-13T11:12:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    12,
                    43,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T11:12:43Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    12,
                    43,
                    0,
                    13,
                    0
                ],
                "title": "Future-Conditioned Recommendations with Multi-Objective Controllable\n  Decision Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future-Conditioned Recommendations with Multi-Objective Controllable\n  Decision Transformer"
                },
                "summary": "Securing long-term success is the ultimate aim of recommender systems,\ndemanding strategies capable of foreseeing and shaping the impact of decisions\non future user satisfaction. Current recommendation strategies grapple with two\nsignificant hurdles. Firstly, the future impacts of recommendation decisions\nremain obscured, rendering it impractical to evaluate them through direct\noptimization of immediate metrics. Secondly, conflicts often emerge between\nmultiple objectives, like enhancing accuracy versus exploring diverse\nrecommendations. Existing strategies, trapped in a \"training, evaluation, and\nretraining\" loop, grow more labor-intensive as objectives evolve. To address\nthese challenges, we introduce a future-conditioned strategy for\nmulti-objective controllable recommendations, allowing for the direct\nspecification of future objectives and empowering the model to generate item\nsequences that align with these goals autoregressively. We present the\nMulti-Objective Controllable Decision Transformer (MocDT), an offline\nReinforcement Learning (RL) model capable of autonomously learning the mapping\nfrom multiple objectives to item sequences, leveraging extensive offline data.\nConsequently, it can produce recommendations tailored to any specified\nobjectives during the inference stage. Our empirical findings emphasize the\ncontrollable recommendation strategy's ability to produce item sequences\naccording to different objectives while maintaining performance that is\ncompetitive with current recommendation strategies across various objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing long-term success is the ultimate aim of recommender systems,\ndemanding strategies capable of foreseeing and shaping the impact of decisions\non future user satisfaction. Current recommendation strategies grapple with two\nsignificant hurdles. Firstly, the future impacts of recommendation decisions\nremain obscured, rendering it impractical to evaluate them through direct\noptimization of immediate metrics. Secondly, conflicts often emerge between\nmultiple objectives, like enhancing accuracy versus exploring diverse\nrecommendations. Existing strategies, trapped in a \"training, evaluation, and\nretraining\" loop, grow more labor-intensive as objectives evolve. To address\nthese challenges, we introduce a future-conditioned strategy for\nmulti-objective controllable recommendations, allowing for the direct\nspecification of future objectives and empowering the model to generate item\nsequences that align with these goals autoregressively. We present the\nMulti-Objective Controllable Decision Transformer (MocDT), an offline\nReinforcement Learning (RL) model capable of autonomously learning the mapping\nfrom multiple objectives to item sequences, leveraging extensive offline data.\nConsequently, it can produce recommendations tailored to any specified\nobjectives during the inference stage. Our empirical findings emphasize the\ncontrollable recommendation strategy's ability to produce item sequences\naccording to different objectives while maintaining performance that is\ncompetitive with current recommendation strategies across various objectives."
                },
                "authors": [
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Kexin Huang"
                    },
                    {
                        "name": "Ziang Fei"
                    },
                    {
                        "name": "Jiaju Chen"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Jianshan Sun"
                    },
                    {
                        "name": "Shuchang Liu"
                    },
                    {
                        "name": "Qingpeng Cai"
                    },
                    {
                        "name": "Peng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Jiang"
                },
                "author": "Peng Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07202v1",
                "updated": "2025-01-13T10:53:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    53,
                    48,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T10:53:48Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    53,
                    48,
                    0,
                    13,
                    0
                ],
                "title": "FaceOracle: Chat with a Face Image Oracle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaceOracle: Chat with a Face Image Oracle"
                },
                "summary": "A face image is a mandatory part of ID and travel documents. Obtaining\nhigh-quality face images when issuing such documents is crucial for both human\nexaminers and automated face recognition systems. In several international\nstandards, face image quality requirements are intricate and defined in detail.\nIdentifying and understanding non-compliance or defects in the submitted face\nimages is crucial for both issuing authorities and applicants. In this work, we\nintroduce FaceOracle, an LLM-powered AI assistant that helps its users analyze\na face image in a natural conversational manner using standard compliant\nalgorithms. Leveraging the power of LLMs, users can get explanations of various\nface image quality concepts as well as interpret the outcome of face image\nquality assessment (FIQA) algorithms. We implement a proof-of-concept that\ndemonstrates how experts at an issuing authority could integrate FaceOracle\ninto their workflow to analyze, understand, and communicate their decisions\nmore efficiently, resulting in enhanced productivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A face image is a mandatory part of ID and travel documents. Obtaining\nhigh-quality face images when issuing such documents is crucial for both human\nexaminers and automated face recognition systems. In several international\nstandards, face image quality requirements are intricate and defined in detail.\nIdentifying and understanding non-compliance or defects in the submitted face\nimages is crucial for both issuing authorities and applicants. In this work, we\nintroduce FaceOracle, an LLM-powered AI assistant that helps its users analyze\na face image in a natural conversational manner using standard compliant\nalgorithms. Leveraging the power of LLMs, users can get explanations of various\nface image quality concepts as well as interpret the outcome of face image\nquality assessment (FIQA) algorithms. We implement a proof-of-concept that\ndemonstrates how experts at an issuing authority could integrate FaceOracle\ninto their workflow to analyze, understand, and communicate their decisions\nmore efficiently, resulting in enhanced productivity."
                },
                "authors": [
                    {
                        "name": "Wassim Kabbani"
                    },
                    {
                        "name": "Kiran Raja"
                    },
                    {
                        "name": "Raghavendra Ramachandra"
                    },
                    {
                        "name": "Christoph Busch"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Busch"
                },
                "author": "Christoph Busch",
                "arxiv_journal_ref": "ECCV 2024 Workshops",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00393v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00393v3",
                "updated": "2025-01-13T10:50:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    50,
                    53,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-01T08:46:36Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    8,
                    46,
                    36,
                    2,
                    122,
                    0
                ],
                "title": "Inferring State Machine from the Protocol Implementation via Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring State Machine from the Protocol Implementation via Large\n  Language Model"
                },
                "summary": "State machines play a pivotal role in augmenting the efficacy of protocol\nanalyzing to unveil more vulnerabilities. However, inferring state machines\nfrom network protocol implementations presents significant challenges, mainly\nbecause of the complicated code syntax and semantics. Traditional methods based\non dynamic analysis often overlook crucial state transitions due to limited\ncoverage, while static analysis suffers from path explosion facing to protocol\nimplementations. To address these limitations, we propose an innovative state\nmachine inference approach powered by Large Language Models (LLMs) named\nProtocolGPT. Utilizing retrieval augmented generation technology, this method\naugments pre-trained model with specific knowledge drawn from protocol\nimplementations. Through targeted prompt engineering, we systematically\nidentify and infer the underlying state machines. Our evaluation across six\nprotocol implementations demonstrates the method's high efficacy, achieving\nprecision exceeding 90% and successfully delineating differences on state\nmachines among various implementations of the same protocol. Integrating our\napproach with protocol fuzzing significantly improves fuzzers by more than 20%\nin terms of coverage and detects two zero-day vulnerabilities compared to the\nbaseline. Our proposed method represents a major advancement in accurate state\nmachine inference and highlights the substantial potential of LLMs in enhancing\nnetwork protocol security analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State machines play a pivotal role in augmenting the efficacy of protocol\nanalyzing to unveil more vulnerabilities. However, inferring state machines\nfrom network protocol implementations presents significant challenges, mainly\nbecause of the complicated code syntax and semantics. Traditional methods based\non dynamic analysis often overlook crucial state transitions due to limited\ncoverage, while static analysis suffers from path explosion facing to protocol\nimplementations. To address these limitations, we propose an innovative state\nmachine inference approach powered by Large Language Models (LLMs) named\nProtocolGPT. Utilizing retrieval augmented generation technology, this method\naugments pre-trained model with specific knowledge drawn from protocol\nimplementations. Through targeted prompt engineering, we systematically\nidentify and infer the underlying state machines. Our evaluation across six\nprotocol implementations demonstrates the method's high efficacy, achieving\nprecision exceeding 90% and successfully delineating differences on state\nmachines among various implementations of the same protocol. Integrating our\napproach with protocol fuzzing significantly improves fuzzers by more than 20%\nin terms of coverage and detects two zero-day vulnerabilities compared to the\nbaseline. Our proposed method represents a major advancement in accurate state\nmachine inference and highlights the substantial potential of LLMs in enhancing\nnetwork protocol security analysis."
                },
                "authors": [
                    {
                        "name": "Haiyang Wei"
                    },
                    {
                        "name": "Zhengjie Du"
                    },
                    {
                        "name": "Haohui Huang"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Guang Cheng"
                    },
                    {
                        "name": "Linzhang Wang"
                    },
                    {
                        "name": "Bing Mao"
                    }
                ],
                "author_detail": {
                    "name": "Bing Mao"
                },
                "author": "Bing Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00393v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00393v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05855v2",
                "updated": "2025-01-13T10:39:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    39,
                    54,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-10T10:53:48Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    53,
                    48,
                    4,
                    10,
                    0
                ],
                "title": "ConSim: Measuring Concept-Based Explanations' Effectiveness with\n  Automated Simulatability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConSim: Measuring Concept-Based Explanations' Effectiveness with\n  Automated Simulatability"
                },
                "summary": "Concept-based explanations work by mapping complex model computations to\nhuman-understandable concepts. Evaluating such explanations is very difficult,\nas it includes not only the quality of the induced space of possible concepts\nbut also how effectively the chosen concepts are communicated to users.\nExisting evaluation metrics often focus solely on the former, neglecting the\nlatter. We introduce an evaluation framework for measuring concept explanations\nvia automated simulatability: a simulator's ability to predict the explained\nmodel's outputs based on the provided explanations. This approach accounts for\nboth the concept space and its interpretation in an end-to-end evaluation.\nHuman studies for simulatability are notoriously difficult to enact,\nparticularly at the scale of a wide, comprehensive empirical evaluation (which\nis the subject of this work). We propose using large language models (LLMs) as\nsimulators to approximate the evaluation and report various analyses to make\nsuch approximations reliable. Our method allows for scalable and consistent\nevaluation across various models and datasets. We report a comprehensive\nempirical evaluation using this framework and show that LLMs provide consistent\nrankings of explanation methods. Code available at\nhttps://github.com/AnonymousConSim/ConSim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept-based explanations work by mapping complex model computations to\nhuman-understandable concepts. Evaluating such explanations is very difficult,\nas it includes not only the quality of the induced space of possible concepts\nbut also how effectively the chosen concepts are communicated to users.\nExisting evaluation metrics often focus solely on the former, neglecting the\nlatter. We introduce an evaluation framework for measuring concept explanations\nvia automated simulatability: a simulator's ability to predict the explained\nmodel's outputs based on the provided explanations. This approach accounts for\nboth the concept space and its interpretation in an end-to-end evaluation.\nHuman studies for simulatability are notoriously difficult to enact,\nparticularly at the scale of a wide, comprehensive empirical evaluation (which\nis the subject of this work). We propose using large language models (LLMs) as\nsimulators to approximate the evaluation and report various analyses to make\nsuch approximations reliable. Our method allows for scalable and consistent\nevaluation across various models and datasets. We report a comprehensive\nempirical evaluation using this framework and show that LLMs provide consistent\nrankings of explanation methods. Code available at\nhttps://github.com/AnonymousConSim/ConSim."
                },
                "authors": [
                    {
                        "name": "Antonin PochÃ©"
                    },
                    {
                        "name": "Alon Jacovi"
                    },
                    {
                        "name": "Agustin Martin Picard"
                    },
                    {
                        "name": "Victor Boutin"
                    },
                    {
                        "name": "Fanny Jourdan"
                    }
                ],
                "author_detail": {
                    "name": "Fanny Jourdan"
                },
                "arxiv_affiliation": "CERCO UMR5549, ANITI",
                "author": "Fanny Jourdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11543v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11543v4",
                "updated": "2025-01-13T10:39:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    39,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2024-11-18T13:01:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    1,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "PSA-VLM: Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PSA-VLM: Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment"
                },
                "summary": "Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark."
                },
                "authors": [
                    {
                        "name": "Zhendong Liu"
                    },
                    {
                        "name": "Yuanbi Nie"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xiangyu Yue"
                    },
                    {
                        "name": "Qiushi Cui"
                    },
                    {
                        "name": "Chongjun Wang"
                    },
                    {
                        "name": "Xiaoyong Zhu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2405.13581",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11543v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11543v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20971v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20971v2",
                "updated": "2025-01-13T10:14:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    14,
                    27,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-31T16:18:46Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    16,
                    18,
                    46,
                    4,
                    152,
                    0
                ],
                "title": "Amortizing intractable inference in diffusion models for vision,\n  language, and control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amortizing intractable inference in diffusion models for vision,\n  language, and control"
                },
                "summary": "Diffusion models have emerged as effective distribution estimators in vision,\nlanguage, and reinforcement learning, but their use as priors in downstream\ntasks poses an intractable posterior inference problem. This paper studies\namortized sampling of the posterior over data, $\\mathbf{x}\\sim p^{\\rm\npost}(\\mathbf{x})\\propto p(\\mathbf{x})r(\\mathbf{x})$, in a model that consists\nof a diffusion generative model prior $p(\\mathbf{x})$ and a black-box\nconstraint or likelihood function $r(\\mathbf{x})$. We state and prove the\nasymptotic correctness of a data-free learning objective, relative trajectory\nbalance, for training a diffusion model that samples from this posterior, a\nproblem that existing methods solve only approximately or in restricted cases.\nRelative trajectory balance arises from the generative flow network perspective\non diffusion models, which allows the use of deep reinforcement learning\ntechniques to improve mode coverage. Experiments illustrate the broad potential\nof unbiased inference of arbitrary posteriors under diffusion priors: in vision\n(classifier guidance), language (infilling under a discrete diffusion LLM), and\nmultimodal data (text-to-image generation). Beyond generative modeling, we\napply relative trajectory balance to the problem of continuous control with a\nscore-based behavior prior, achieving state-of-the-art results on benchmarks in\noffline reinforcement learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as effective distribution estimators in vision,\nlanguage, and reinforcement learning, but their use as priors in downstream\ntasks poses an intractable posterior inference problem. This paper studies\namortized sampling of the posterior over data, $\\mathbf{x}\\sim p^{\\rm\npost}(\\mathbf{x})\\propto p(\\mathbf{x})r(\\mathbf{x})$, in a model that consists\nof a diffusion generative model prior $p(\\mathbf{x})$ and a black-box\nconstraint or likelihood function $r(\\mathbf{x})$. We state and prove the\nasymptotic correctness of a data-free learning objective, relative trajectory\nbalance, for training a diffusion model that samples from this posterior, a\nproblem that existing methods solve only approximately or in restricted cases.\nRelative trajectory balance arises from the generative flow network perspective\non diffusion models, which allows the use of deep reinforcement learning\ntechniques to improve mode coverage. Experiments illustrate the broad potential\nof unbiased inference of arbitrary posteriors under diffusion priors: in vision\n(classifier guidance), language (infilling under a discrete diffusion LLM), and\nmultimodal data (text-to-image generation). Beyond generative modeling, we\napply relative trajectory balance to the problem of continuous control with a\nscore-based behavior prior, achieving state-of-the-art results on benchmarks in\noffline reinforcement learning."
                },
                "authors": [
                    {
                        "name": "Siddarth Venkatraman"
                    },
                    {
                        "name": "Moksh Jain"
                    },
                    {
                        "name": "Luca Scimeca"
                    },
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Marcin Sendera"
                    },
                    {
                        "name": "Mohsin Hasan"
                    },
                    {
                        "name": "Luke Rowe"
                    },
                    {
                        "name": "Sarthak Mittal"
                    },
                    {
                        "name": "Pablo Lemos"
                    },
                    {
                        "name": "Emmanuel Bengio"
                    },
                    {
                        "name": "Alexandre Adam"
                    },
                    {
                        "name": "Jarrid Rector-Brooks"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Glen Berseth"
                    },
                    {
                        "name": "Nikolay Malkin"
                    }
                ],
                "author_detail": {
                    "name": "Nikolay Malkin"
                },
                "author": "Nikolay Malkin",
                "arxiv_comment": "NeurIPS 2024; code: https://github.com/GFNOrg/diffusion-finetuning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20971v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20971v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19655v2",
                "updated": "2025-01-13T10:08:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    8,
                    7,
                    0,
                    13,
                    0
                ],
                "published": "2024-09-29T11:00:41Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    11,
                    0,
                    41,
                    6,
                    273,
                    0
                ],
                "title": "Assessment and manipulation of latent constructs in pre-trained language\n  models using psychometric scales",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessment and manipulation of latent constructs in pre-trained language\n  models using psychometric scales"
                },
                "summary": "Human-like personality traits have recently been discovered in large language\nmodels, raising the hypothesis that their (known and as yet undiscovered)\nbiases conform with human latent psychological constructs. While large\nconversational models may be tricked into answering psychometric\nquestionnaires, the latent psychological constructs of thousands of simpler\ntransformers, trained for other tasks, cannot be assessed because appropriate\npsychometric methods are currently lacking. Here, we show how standard\npsychological questionnaires can be reformulated into natural language\ninference prompts, and we provide a code library to support the psychometric\nassessment of arbitrary models. We demonstrate, using a sample of 88 publicly\navailable models, the existence of human-like mental health-related constructs\n(including anxiety, depression, and Sense of Coherence) which conform with\nstandard theories in human psychology and show similar correlations and\nmitigation strategies. The ability to interpret and rectify the performance of\nlanguage models by using psychological tools can boost the development of more\nexplainable, controllable, and trustworthy models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-like personality traits have recently been discovered in large language\nmodels, raising the hypothesis that their (known and as yet undiscovered)\nbiases conform with human latent psychological constructs. While large\nconversational models may be tricked into answering psychometric\nquestionnaires, the latent psychological constructs of thousands of simpler\ntransformers, trained for other tasks, cannot be assessed because appropriate\npsychometric methods are currently lacking. Here, we show how standard\npsychological questionnaires can be reformulated into natural language\ninference prompts, and we provide a code library to support the psychometric\nassessment of arbitrary models. We demonstrate, using a sample of 88 publicly\navailable models, the existence of human-like mental health-related constructs\n(including anxiety, depression, and Sense of Coherence) which conform with\nstandard theories in human psychology and show similar correlations and\nmitigation strategies. The ability to interpret and rectify the performance of\nlanguage models by using psychological tools can boost the development of more\nexplainable, controllable, and trustworthy models."
                },
                "authors": [
                    {
                        "name": "Maor Reuben"
                    },
                    {
                        "name": "Ortal Slobodin"
                    },
                    {
                        "name": "Aviad Elyshar"
                    },
                    {
                        "name": "Idan-Chaim Cohen"
                    },
                    {
                        "name": "Orna Braun-Lewensohn"
                    },
                    {
                        "name": "Odeya Cohen"
                    },
                    {
                        "name": "Rami Puzis"
                    }
                ],
                "author_detail": {
                    "name": "Rami Puzis"
                },
                "author": "Rami Puzis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v3",
                "updated": "2025-01-13T10:02:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    2,
                    27,
                    0,
                    13,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Efficient Large Foundation Models Design: A Perspective From Model and\n  System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Large Foundation Models Design: A Perspective From Model and\n  System Co-Design"
                },
                "summary": "This paper focuses on modern efficient training and inference technologies on\nfoundation models and illustrates them from two perspectives: model and system\ndesign. Model and System Design optimize LLM training and inference from\ndifferent aspects to save computational resources, making LLMs more efficient,\naffordable, and more accessible. The paper list repository is available at\n\\url{https://github.com/NoakLiu/Efficient-Foundation-Models-Survey}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper focuses on modern efficient training and inference technologies on\nfoundation models and illustrates them from two perspectives: model and system\ndesign. Model and System Design optimize LLM training and inference from\ndifferent aspects to save computational resources, making LLMs more efficient,\naffordable, and more accessible. The paper list repository is available at\n\\url{https://github.com/NoakLiu/Efficient-Foundation-Models-Survey}"
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Zhixin Lai"
                    },
                    {
                        "name": "Yite Wang"
                    },
                    {
                        "name": "Jing Wu"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Sina Alinejad"
                    },
                    {
                        "name": "Benjamin Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15523v2",
                "updated": "2025-01-13T10:01:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    1,
                    56,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-20T03:23:26Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    3,
                    23,
                    26,
                    4,
                    355,
                    0
                ],
                "title": "InstructOCR: Instruction Boosting Scene Text Spotting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstructOCR: Instruction Boosting Scene Text Spotting"
                },
                "summary": "In the field of scene text spotting, previous OCR methods primarily relied on\nimage encoders and pre-trained text information, but they often overlooked the\nadvantages of incorporating human language instructions. To address this gap,\nwe propose InstructOCR, an innovative instruction-based scene text spotting\nmodel that leverages human language instructions to enhance the understanding\nof text within images. Our framework employs both text and image encoders\nduring training and inference, along with instructions meticulously designed\nbased on text attributes. This approach enables the model to interpret text\nmore accurately and flexibly. Extensive experiments demonstrate the\neffectiveness of our model and we achieve state-of-the-art results on widely\nused benchmarks. Furthermore, the proposed framework can be seamlessly applied\nto scene text VQA tasks. By leveraging instruction strategies during\npre-training, the performance on downstream VQA tasks can be significantly\nimproved, with a 2.6% increase on the TextVQA dataset and a 2.1% increase on\nthe ST-VQA dataset. These experimental results provide insights into the\nbenefits of incorporating human language instructions for OCR-related tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of scene text spotting, previous OCR methods primarily relied on\nimage encoders and pre-trained text information, but they often overlooked the\nadvantages of incorporating human language instructions. To address this gap,\nwe propose InstructOCR, an innovative instruction-based scene text spotting\nmodel that leverages human language instructions to enhance the understanding\nof text within images. Our framework employs both text and image encoders\nduring training and inference, along with instructions meticulously designed\nbased on text attributes. This approach enables the model to interpret text\nmore accurately and flexibly. Extensive experiments demonstrate the\neffectiveness of our model and we achieve state-of-the-art results on widely\nused benchmarks. Furthermore, the proposed framework can be seamlessly applied\nto scene text VQA tasks. By leveraging instruction strategies during\npre-training, the performance on downstream VQA tasks can be significantly\nimproved, with a 2.6% increase on the TextVQA dataset and a 2.1% increase on\nthe ST-VQA dataset. These experimental results provide insights into the\nbenefits of incorporating human language instructions for OCR-related tasks."
                },
                "authors": [
                    {
                        "name": "Chen Duan"
                    },
                    {
                        "name": "Qianyi Jiang"
                    },
                    {
                        "name": "Pei Fu"
                    },
                    {
                        "name": "Jiamin Chen"
                    },
                    {
                        "name": "Shengxi Li"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Shan Guo"
                    },
                    {
                        "name": "Junfeng Luo"
                    }
                ],
                "author_detail": {
                    "name": "Junfeng Luo"
                },
                "author": "Junfeng Luo",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06501v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06501v3",
                "updated": "2025-01-13T09:53:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    53,
                    48,
                    0,
                    13,
                    0
                ],
                "published": "2024-09-10T13:34:53Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    13,
                    34,
                    53,
                    1,
                    254,
                    0
                ],
                "title": "An Adaptive Sliding Window Estimator for Positioning of Unmanned Aerial\n  Vehicle Using a Single Anchor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Adaptive Sliding Window Estimator for Positioning of Unmanned Aerial\n  Vehicle Using a Single Anchor"
                },
                "summary": "Localization using a single range anchor combined with onboard\noptical-inertial odometry offers a lightweight solution that provides\nmultidimensional measurements for the positioning of unmanned aerial vehicles.\nUnfortunately, the performance of such lightweight sensors varies with the\ndynamic environment, and the fidelity of the dynamic model is also severely\naffected by environmental aerial flow. To address this challenge, we propose an\nadaptive sliding window estimator equipped with an estimation reliability\nevaluator, where the states, noise covariance matrices and aerial drag are\nestimated simultaneously. The aerial drag effects are first evaluated based on\nposterior states and covariance. Then, an augmented Kalman filter is designed\nto pre-process multidimensional measurements and inherit historical\ninformation. Subsequently, an inverse-Wishart smoother is employed to estimate\nposterior states and covariance matrices. To further suppress potential\ndivergence, a reliability evaluator is devised to infer estimation errors. We\nfurther determine the fidelity of each sensor based on the error propagation.\nExtensive experiments are conducted in both standard and harsh environments,\ndemonstrating the adaptability and robustness of the proposed method. The root\nmean square error reaches 0.15 m, outperforming the state-of-the-art approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localization using a single range anchor combined with onboard\noptical-inertial odometry offers a lightweight solution that provides\nmultidimensional measurements for the positioning of unmanned aerial vehicles.\nUnfortunately, the performance of such lightweight sensors varies with the\ndynamic environment, and the fidelity of the dynamic model is also severely\naffected by environmental aerial flow. To address this challenge, we propose an\nadaptive sliding window estimator equipped with an estimation reliability\nevaluator, where the states, noise covariance matrices and aerial drag are\nestimated simultaneously. The aerial drag effects are first evaluated based on\nposterior states and covariance. Then, an augmented Kalman filter is designed\nto pre-process multidimensional measurements and inherit historical\ninformation. Subsequently, an inverse-Wishart smoother is employed to estimate\nposterior states and covariance matrices. To further suppress potential\ndivergence, a reliability evaluator is devised to infer estimation errors. We\nfurther determine the fidelity of each sensor based on the error propagation.\nExtensive experiments are conducted in both standard and harsh environments,\ndemonstrating the adaptability and robustness of the proposed method. The root\nmean square error reaches 0.15 m, outperforming the state-of-the-art approach."
                },
                "authors": [
                    {
                        "name": "Kaiwen Xiong"
                    },
                    {
                        "name": "Sijia Chen"
                    },
                    {
                        "name": "Wei Dong"
                    }
                ],
                "author_detail": {
                    "name": "Wei Dong"
                },
                "author": "Wei Dong",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06501v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06501v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07165v1",
                "updated": "2025-01-13T09:51:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    51,
                    23,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T09:51:23Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    51,
                    23,
                    0,
                    13,
                    0
                ],
                "title": "Unveiling Code Clone Patterns in Open Source VR Software: An Empirical\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Code Clone Patterns in Open Source VR Software: An Empirical\n  Study"
                },
                "summary": "Code cloning is frequently observed in software development, often leading to\na variety of maintenance and security issues. While substantial research has\nbeen conducted on code cloning in traditional software, to the best of my\nknowledge, there is a lack of studies on cloning in VR software that consider\nits unique nature, particularly the presence of numerous serialized files in\nconjunction with the source code. In this paper, we conduct the first\nlarge-scale quantitative empirical analysis of software clones in 345\nopen-source VR projects, using the NiCad detector for source code clone\ndetection and large language models (LLMs) for identifying serialized file\nclones. Our study leads to a number of insights into cloning phenomena in VR\nsoftware, guided by seven carefully formulated research questions. These\nfindings, along with their implications, are anticipated to provide useful\nguidance for both researchers and software developers within the VR field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code cloning is frequently observed in software development, often leading to\na variety of maintenance and security issues. While substantial research has\nbeen conducted on code cloning in traditional software, to the best of my\nknowledge, there is a lack of studies on cloning in VR software that consider\nits unique nature, particularly the presence of numerous serialized files in\nconjunction with the source code. In this paper, we conduct the first\nlarge-scale quantitative empirical analysis of software clones in 345\nopen-source VR projects, using the NiCad detector for source code clone\ndetection and large language models (LLMs) for identifying serialized file\nclones. Our study leads to a number of insights into cloning phenomena in VR\nsoftware, guided by seven carefully formulated research questions. These\nfindings, along with their implications, are anticipated to provide useful\nguidance for both researchers and software developers within the VR field."
                },
                "authors": [
                    {
                        "name": "Huashan Chen"
                    },
                    {
                        "name": "Zisheng Huang"
                    },
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Wenjie Huang"
                    },
                    {
                        "name": "Jinfu Chen"
                    },
                    {
                        "name": "Haotang Li"
                    },
                    {
                        "name": "Kebin Peng"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Sen He"
                    }
                ],
                "author_detail": {
                    "name": "Sen He"
                },
                "author": "Sen He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07161v1",
                "updated": "2025-01-13T09:41:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    41,
                    54,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T09:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    41,
                    54,
                    0,
                    13,
                    0
                ],
                "title": "QuantuneV2: Compiler-Based Local Metric-Driven Mixed Precision\n  Quantization for Practical Embedded AI Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantuneV2: Compiler-Based Local Metric-Driven Mixed Precision\n  Quantization for Practical Embedded AI Applications"
                },
                "summary": "Mixed-precision quantization methods have been proposed to reduce model size\nwhile minimizing accuracy degradation. However, existing studies require\nretraining and do not consider the computational overhead and intermediate\nrepresentations (IR) generated during the compilation process, limiting their\napplication at the compiler level. This computational overhead refers to the\nruntime latency caused by frequent quantization and dequantization operations\nduring inference. Performing these operations at the individual operator level\ncauses significant runtime delays. To address these issues, we propose\nQuantuneV2, a compiler-based mixed-precision quantization method designed for\npractical embedded AI applications. QuantuneV2 performs inference only twice,\nonce before quantization and once after quantization, and operates with a\ncomputational complexity of O(n) that increases linearly with the number of\nmodel parameters. We also made the sensitivity analysis more stable by using\nlocal metrics like weights, activation values, the Signal to Quantization Noise\nRatio, and the Mean Squared Error. We also cut down on computational overhead\nby choosing the best IR and using operator fusion. Experimental results show\nthat QuantuneV2 achieved up to a 10.28 percent improvement in accuracy and a\n12.52 percent increase in speed compared to existing methods across five\nmodels: ResNet18v1, ResNet50v1, SqueezeNetv1, VGGNet, and MobileNetv2. This\ndemonstrates that QuantuneV2 enhances model performance while maintaining\ncomputational efficiency, making it suitable for deployment in embedded AI\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-precision quantization methods have been proposed to reduce model size\nwhile minimizing accuracy degradation. However, existing studies require\nretraining and do not consider the computational overhead and intermediate\nrepresentations (IR) generated during the compilation process, limiting their\napplication at the compiler level. This computational overhead refers to the\nruntime latency caused by frequent quantization and dequantization operations\nduring inference. Performing these operations at the individual operator level\ncauses significant runtime delays. To address these issues, we propose\nQuantuneV2, a compiler-based mixed-precision quantization method designed for\npractical embedded AI applications. QuantuneV2 performs inference only twice,\nonce before quantization and once after quantization, and operates with a\ncomputational complexity of O(n) that increases linearly with the number of\nmodel parameters. We also made the sensitivity analysis more stable by using\nlocal metrics like weights, activation values, the Signal to Quantization Noise\nRatio, and the Mean Squared Error. We also cut down on computational overhead\nby choosing the best IR and using operator fusion. Experimental results show\nthat QuantuneV2 achieved up to a 10.28 percent improvement in accuracy and a\n12.52 percent increase in speed compared to existing methods across five\nmodels: ResNet18v1, ResNet50v1, SqueezeNetv1, VGGNet, and MobileNetv2. This\ndemonstrates that QuantuneV2 enhances model performance while maintaining\ncomputational efficiency, making it suitable for deployment in embedded AI\nenvironments."
                },
                "authors": [
                    {
                        "name": "Jeongseok Kim"
                    },
                    {
                        "name": "Jemin Lee"
                    },
                    {
                        "name": "Yongin Kwon"
                    },
                    {
                        "name": "Daeyoung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Daeyoung Kim"
                },
                "author": "Daeyoung Kim",
                "arxiv_comment": "18 pages, 10 figures, Accepted in Future Generation Computer Systems\n  Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07157v1",
                "updated": "2025-01-13T09:30:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    30,
                    38,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T09:30:38Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    30,
                    38,
                    0,
                    13,
                    0
                ],
                "title": "CureGraph: Contrastive Multi-Modal Graph Representation Learning for\n  Urban Living Circle Health Profiling and Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CureGraph: Contrastive Multi-Modal Graph Representation Learning for\n  Urban Living Circle Health Profiling and Prediction"
                },
                "summary": "The early detection and prediction of health status decline among the elderly\nat the neighborhood level are of great significance for urban planning and\npublic health policymaking. While existing studies affirm the connection\nbetween living environments and health outcomes, most rely on single data\nmodalities or simplistic feature concatenation of multi-modal information,\nlimiting their ability to comprehensively profile the health-oriented urban\nenvironments. To fill this gap, we propose CureGraph, a contrastive multi-modal\nrepresentation learning framework for urban health prediction that employs\ngraph-based techniques to infer the prevalence of common chronic diseases among\nthe elderly within the urban living circles of each neighborhood. CureGraph\nleverages rich multi-modal information, including photos and textual reviews of\nresidential areas and their surrounding points of interest, to generate urban\nneighborhood embeddings. By integrating pre-trained visual and textual encoders\nwith graph modeling techniques, CureGraph captures cross-modal spatial\ndependencies, offering a comprehensive understanding of urban environments\ntailored to elderly health considerations. Extensive experiments on real-world\ndatasets demonstrate that CureGraph improves the best baseline by $28\\%$ on\naverage in terms of $R^2$ across elderly disease risk prediction tasks.\nMoreover, the model enables the identification of stage-wise chronic disease\nprogression and supports comparative public health analysis across\nneighborhoods, offering actionable insights for sustainable urban development\nand enhanced quality of life. The code is publicly available at\nhttps://github.com/jinlin2021/CureGraph.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The early detection and prediction of health status decline among the elderly\nat the neighborhood level are of great significance for urban planning and\npublic health policymaking. While existing studies affirm the connection\nbetween living environments and health outcomes, most rely on single data\nmodalities or simplistic feature concatenation of multi-modal information,\nlimiting their ability to comprehensively profile the health-oriented urban\nenvironments. To fill this gap, we propose CureGraph, a contrastive multi-modal\nrepresentation learning framework for urban health prediction that employs\ngraph-based techniques to infer the prevalence of common chronic diseases among\nthe elderly within the urban living circles of each neighborhood. CureGraph\nleverages rich multi-modal information, including photos and textual reviews of\nresidential areas and their surrounding points of interest, to generate urban\nneighborhood embeddings. By integrating pre-trained visual and textual encoders\nwith graph modeling techniques, CureGraph captures cross-modal spatial\ndependencies, offering a comprehensive understanding of urban environments\ntailored to elderly health considerations. Extensive experiments on real-world\ndatasets demonstrate that CureGraph improves the best baseline by $28\\%$ on\naverage in terms of $R^2$ across elderly disease risk prediction tasks.\nMoreover, the model enables the identification of stage-wise chronic disease\nprogression and supports comparative public health analysis across\nneighborhoods, offering actionable insights for sustainable urban development\nand enhanced quality of life. The code is publicly available at\nhttps://github.com/jinlin2021/CureGraph."
                },
                "authors": [
                    {
                        "name": "Jinlin Li"
                    },
                    {
                        "name": "Xiao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Zhou"
                },
                "author": "Xiao Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07149v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07149v1",
                "updated": "2025-01-13T09:22:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    22,
                    30,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T09:22:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    22,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Pantomime: Towards the Anonymization of Motion Data using Foundation\n  Motion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pantomime: Towards the Anonymization of Motion Data using Foundation\n  Motion Models"
                },
                "summary": "Human motion is a behavioral biometric trait that can be used to identify\nindividuals and infer private attributes such as medical conditions. This poses\na serious privacy threat as motion extraction from video and motion capture are\nincreasingly used for a variety of applications, including mixed reality,\nrobotics, medicine, and the quantified self. In order to protect the privacy of\nthe tracked individuals, anonymization techniques that preserve the utility of\nthe data are required. However, anonymizing motion data is a challenging task\nbecause there are many dependencies in motion sequences (such as physiological\nconstraints) that, if ignored, make the anonymized motion sequence appear\nunnatural. In this paper, we propose Pantomime, a full-body anonymization\ntechnique for motion data, which uses foundation motion models to generate\nmotion sequences that adhere to the dependencies in the data, thus keeping the\nutility of the anonymized data high. Our results show that Pantomime can\nmaintain the naturalness of the motion sequences while reducing the\nidentification accuracy to 10%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human motion is a behavioral biometric trait that can be used to identify\nindividuals and infer private attributes such as medical conditions. This poses\na serious privacy threat as motion extraction from video and motion capture are\nincreasingly used for a variety of applications, including mixed reality,\nrobotics, medicine, and the quantified self. In order to protect the privacy of\nthe tracked individuals, anonymization techniques that preserve the utility of\nthe data are required. However, anonymizing motion data is a challenging task\nbecause there are many dependencies in motion sequences (such as physiological\nconstraints) that, if ignored, make the anonymized motion sequence appear\nunnatural. In this paper, we propose Pantomime, a full-body anonymization\ntechnique for motion data, which uses foundation motion models to generate\nmotion sequences that adhere to the dependencies in the data, thus keeping the\nutility of the anonymized data high. Our results show that Pantomime can\nmaintain the naturalness of the motion sequences while reducing the\nidentification accuracy to 10%."
                },
                "authors": [
                    {
                        "name": "Simon Hanisch"
                    },
                    {
                        "name": "Julian Todt"
                    },
                    {
                        "name": "Thorsten Strufe"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Strufe"
                },
                "author": "Thorsten Strufe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07149v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07149v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07146v1",
                "updated": "2025-01-13T09:11:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    11,
                    33,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T09:11:33Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    11,
                    33,
                    0,
                    13,
                    0
                ],
                "title": "TIMRL: A Novel Meta-Reinforcement Learning Framework for Non-Stationary\n  and Multi-Task Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TIMRL: A Novel Meta-Reinforcement Learning Framework for Non-Stationary\n  and Multi-Task Environments"
                },
                "summary": "In recent years, meta-reinforcement learning (meta-RL) algorithm has been\nproposed to improve sample efficiency in the field of decision-making and\ncontrol, enabling agents to learn new knowledge from a small number of samples.\nHowever, most research uses the Gaussian distribution to extract task\nrepresentation, which is poorly adapted to tasks that change in non-stationary\nenvironment. To address this problem, we propose a novel meta-reinforcement\nlearning method by leveraging Gaussian mixture model and the transformer\nnetwork to construct task inference model. The Gaussian mixture model is\nutilized to extend the task representation and conduct explicit encoding of\ntasks. Specifically, the classification of tasks is encoded through transformer\nnetwork to determine the Gaussian component corresponding to the task. By\nleveraging task labels, the transformer network is trained using supervised\nlearning. We validate our method on MuJoCo benchmarks with non-stationary and\nmulti-task environments. Experimental results demonstrate that the proposed\nmethod dramatically improves sample efficiency and accurately recognizes the\nclassification of the tasks, while performing excellently in the environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, meta-reinforcement learning (meta-RL) algorithm has been\nproposed to improve sample efficiency in the field of decision-making and\ncontrol, enabling agents to learn new knowledge from a small number of samples.\nHowever, most research uses the Gaussian distribution to extract task\nrepresentation, which is poorly adapted to tasks that change in non-stationary\nenvironment. To address this problem, we propose a novel meta-reinforcement\nlearning method by leveraging Gaussian mixture model and the transformer\nnetwork to construct task inference model. The Gaussian mixture model is\nutilized to extend the task representation and conduct explicit encoding of\ntasks. Specifically, the classification of tasks is encoded through transformer\nnetwork to determine the Gaussian component corresponding to the task. By\nleveraging task labels, the transformer network is trained using supervised\nlearning. We validate our method on MuJoCo benchmarks with non-stationary and\nmulti-task environments. Experimental results demonstrate that the proposed\nmethod dramatically improves sample efficiency and accurately recognizes the\nclassification of the tasks, while performing excellently in the environment."
                },
                "authors": [
                    {
                        "name": "Chenyang Qi"
                    },
                    {
                        "name": "Huiping Li"
                    },
                    {
                        "name": "Panfeng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Panfeng Huang"
                },
                "author": "Panfeng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18873v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18873v3",
                "updated": "2025-01-13T09:11:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    11,
                    12,
                    0,
                    13,
                    0
                ],
                "published": "2024-06-27T03:57:12Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    3,
                    57,
                    12,
                    3,
                    179,
                    0
                ],
                "title": "LayoutCopilot: An LLM-powered Multi-agent Collaborative Framework for\n  Interactive Analog Layout Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayoutCopilot: An LLM-powered Multi-agent Collaborative Framework for\n  Interactive Analog Layout Design"
                },
                "summary": "Analog layout design heavily involves interactive processes between humans\nand design tools. Electronic Design Automation (EDA) tools for this task are\nusually designed to use scripting commands or visualized buttons for\nmanipulation, especially for interactive automation functionalities, which have\na steep learning curve and cumbersome user experience, making a notable barrier\nto designers' adoption. Aiming to address such a usability issue, this paper\nintroduces LayoutCopilot, a pioneering multi-agent collaborative framework\npowered by Large Language Models (LLMs) for interactive analog layout design.\nLayoutCopilot simplifies human-tool interaction by converting natural language\ninstructions into executable script commands, and it interprets high-level\ndesign intents into actionable suggestions, significantly streamlining the\ndesign process. Experimental results demonstrate the flexibility, efficiency,\nand accessibility of LayoutCopilot in handling real-world analog designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog layout design heavily involves interactive processes between humans\nand design tools. Electronic Design Automation (EDA) tools for this task are\nusually designed to use scripting commands or visualized buttons for\nmanipulation, especially for interactive automation functionalities, which have\na steep learning curve and cumbersome user experience, making a notable barrier\nto designers' adoption. Aiming to address such a usability issue, this paper\nintroduces LayoutCopilot, a pioneering multi-agent collaborative framework\npowered by Large Language Models (LLMs) for interactive analog layout design.\nLayoutCopilot simplifies human-tool interaction by converting natural language\ninstructions into executable script commands, and it interprets high-level\ndesign intents into actionable suggestions, significantly streamlining the\ndesign process. Experimental results demonstrate the flexibility, efficiency,\nand accessibility of LayoutCopilot in handling real-world analog designs."
                },
                "authors": [
                    {
                        "name": "Bingyang Liu"
                    },
                    {
                        "name": "Haoyi Zhang"
                    },
                    {
                        "name": "Xiaohan Gao"
                    },
                    {
                        "name": "Zichen Kong"
                    },
                    {
                        "name": "Xiyuan Tang"
                    },
                    {
                        "name": "Yibo Lin"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "arxiv_comment": "8pages, 8figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18873v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18873v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14047v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14047v3",
                "updated": "2025-01-13T09:01:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    1,
                    13,
                    0,
                    13,
                    0
                ],
                "published": "2024-04-22T10:03:03Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    10,
                    3,
                    3,
                    0,
                    113,
                    0
                ],
                "title": "An empirical study of LLaMA3 quantization: from LLMs to MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An empirical study of LLaMA3 quantization: from LLMs to MLLMs"
                },
                "summary": "The LLaMA family, a collection of foundation language models ranging from 7B\nto 65B parameters, has become one of the most powerful open-source large\nlanguage models (LLMs) and the popular LLM backbone of multi-modal large\nlanguage models (MLLMs), widely used in computer vision and natural language\nunderstanding tasks. In particular, LLaMA3 models have recently been released\nand have achieved impressive performance in various domains with super-large\nscale pre-training on over 15T tokens of data. Given the wide application of\nlow-bit quantization for LLMs in resource-constrained scenarios, we explore\nLLaMA3's capabilities when quantized to low bit-width. This exploration can\npotentially provide new insights and challenges for the low-bit quantization of\nLLaMA3 and other future LLMs, especially in addressing performance degradation\nissues that suffer in LLM compression. Specifically, we comprehensively\nevaluate the 10 existing post-training quantization and LoRA fine-tuning\n(LoRA-FT) methods of LLaMA3 on 1-8 bits and various datasets to reveal the\nlow-bit quantization performance of LLaMA3. To uncover the capabilities of\nlow-bit quantized MLLM, we assessed the performance of the LLaMA3-based\nLLaVA-Next-8B model under 2-4 ultra-low bits with post-training quantization\nmethods. Our experimental results indicate that LLaMA3 still suffers from\nnon-negligible degradation in linguistic and visual contexts, particularly\nunder ultra-low bit widths. This highlights the significant performance gap at\nlow bit-width that needs to be addressed in future developments. We expect that\nthis empirical study will prove valuable in advancing future models, driving\nLLMs and MLLMs to achieve higher accuracy at lower bit to enhance practicality.\nOur project is released on https://github.com/Macaronlin/LLaMA3-Quantization ,\nand quantized models are released at https://huggingface.co/Efficient-ML .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLaMA family, a collection of foundation language models ranging from 7B\nto 65B parameters, has become one of the most powerful open-source large\nlanguage models (LLMs) and the popular LLM backbone of multi-modal large\nlanguage models (MLLMs), widely used in computer vision and natural language\nunderstanding tasks. In particular, LLaMA3 models have recently been released\nand have achieved impressive performance in various domains with super-large\nscale pre-training on over 15T tokens of data. Given the wide application of\nlow-bit quantization for LLMs in resource-constrained scenarios, we explore\nLLaMA3's capabilities when quantized to low bit-width. This exploration can\npotentially provide new insights and challenges for the low-bit quantization of\nLLaMA3 and other future LLMs, especially in addressing performance degradation\nissues that suffer in LLM compression. Specifically, we comprehensively\nevaluate the 10 existing post-training quantization and LoRA fine-tuning\n(LoRA-FT) methods of LLaMA3 on 1-8 bits and various datasets to reveal the\nlow-bit quantization performance of LLaMA3. To uncover the capabilities of\nlow-bit quantized MLLM, we assessed the performance of the LLaMA3-based\nLLaVA-Next-8B model under 2-4 ultra-low bits with post-training quantization\nmethods. Our experimental results indicate that LLaMA3 still suffers from\nnon-negligible degradation in linguistic and visual contexts, particularly\nunder ultra-low bit widths. This highlights the significant performance gap at\nlow bit-width that needs to be addressed in future developments. We expect that\nthis empirical study will prove valuable in advancing future models, driving\nLLMs and MLLMs to achieve higher accuracy at lower bit to enhance practicality.\nOur project is released on https://github.com/Macaronlin/LLaMA3-Quantization ,\nand quantized models are released at https://huggingface.co/Efficient-ML ."
                },
                "authors": [
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Xingyu Zheng"
                    },
                    {
                        "name": "Xudong Ma"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Chengtao Lv"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Jie Luo"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Michele Magno"
                    }
                ],
                "author_detail": {
                    "name": "Michele Magno"
                },
                "author": "Michele Magno",
                "arxiv_doi": "10.1007/s44267-024-00070-x",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s44267-024-00070-x",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.14047v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14047v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07139v1",
                "updated": "2025-01-13T08:58:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    8,
                    58,
                    0,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T08:58:00Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    8,
                    58,
                    0,
                    0,
                    13,
                    0
                ],
                "title": "FlexQuant: Elastic Quantization Framework for Locally Hosted LLM on Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexQuant: Elastic Quantization Framework for Locally Hosted LLM on Edge\n  Devices"
                },
                "summary": "Deploying LLMs on edge devices presents serious technical challenges. Memory\nelasticity is crucial for edge devices with unified memory, where memory is\nshared and fluctuates dynamically. Existing solutions suffer from either poor\ntransition granularity or high storage costs. We propose FlexQuant, a novel\nelasticity framework that generates an ensemble of quantized models, providing\nan elastic hosting solution with 15x granularity improvement and 10x storage\nreduction compared to SoTA methods. FlexQuant works with most quantization\nmethods and creates a family of trade-off options under various storage limits\nthrough our pruning method. It brings great performance and flexibility to the\nedge deployment of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying LLMs on edge devices presents serious technical challenges. Memory\nelasticity is crucial for edge devices with unified memory, where memory is\nshared and fluctuates dynamically. Existing solutions suffer from either poor\ntransition granularity or high storage costs. We propose FlexQuant, a novel\nelasticity framework that generates an ensemble of quantized models, providing\nan elastic hosting solution with 15x granularity improvement and 10x storage\nreduction compared to SoTA methods. FlexQuant works with most quantization\nmethods and creates a family of trade-off options under various storage limits\nthrough our pruning method. It brings great performance and flexibility to the\nedge deployment of LLMs."
                },
                "authors": [
                    {
                        "name": "Yuji Chai"
                    },
                    {
                        "name": "Mujin Kwen"
                    },
                    {
                        "name": "David Brooks"
                    },
                    {
                        "name": "Gu-Yeon Wei"
                    }
                ],
                "author_detail": {
                    "name": "Gu-Yeon Wei"
                },
                "author": "Gu-Yeon Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07124v1",
                "updated": "2025-01-13T08:26:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    8,
                    26,
                    43,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T08:26:43Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    8,
                    26,
                    43,
                    0,
                    13,
                    0
                ],
                "title": "LLM360 K2: Scaling Up 360-Open-Source Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM360 K2: Scaling Up 360-Open-Source Large Language Models"
                },
                "summary": "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree\nOPEN SOURCE approach to the largest and most powerful models under project\nLLM360. While open-source LLMs continue to advance, the answer to \"How are the\nlargest LLMs trained?\" remains unclear within the community. The implementation\ndetails for such high-capacity models are often protected due to business\nconsiderations associated with their high cost. This lack of transparency\nprevents LLM researchers from leveraging valuable insights from prior\nexperience, e.g., \"What are the best practices for addressing loss spikes?\" The\nLLM360 K2 project addresses this gap by providing full transparency and access\nto resources accumulated during the training of LLMs at the largest scale. This\nreport highlights key elements of the K2 project, including our first model, K2\nDIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals\nLLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the\nimplementation steps and present a longitudinal analysis of K2 DIAMOND's\ncapabilities throughout its training process. We also outline ongoing projects\nsuch as TXT360, setting the stage for future models in the series. By offering\npreviously unavailable resources, the K2 project also resonates with the\n360-degree OPEN SOURCE principles of transparency, reproducibility, and\naccessibility, which we believe are vital in the era of resource-intensive AI\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree\nOPEN SOURCE approach to the largest and most powerful models under project\nLLM360. While open-source LLMs continue to advance, the answer to \"How are the\nlargest LLMs trained?\" remains unclear within the community. The implementation\ndetails for such high-capacity models are often protected due to business\nconsiderations associated with their high cost. This lack of transparency\nprevents LLM researchers from leveraging valuable insights from prior\nexperience, e.g., \"What are the best practices for addressing loss spikes?\" The\nLLM360 K2 project addresses this gap by providing full transparency and access\nto resources accumulated during the training of LLMs at the largest scale. This\nreport highlights key elements of the K2 project, including our first model, K2\nDIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals\nLLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the\nimplementation steps and present a longitudinal analysis of K2 DIAMOND's\ncapabilities throughout its training process. We also outline ongoing projects\nsuch as TXT360, setting the stage for future models in the series. By offering\npreviously unavailable resources, the K2 project also resonates with the\n360-degree OPEN SOURCE principles of transparency, reproducibility, and\naccessibility, which we believe are vital in the era of resource-intensive AI\nresearch."
                },
                "authors": [
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Bowen Tan"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Tianhua Tao"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Yuqi Wang"
                    },
                    {
                        "name": "Suqi Sun"
                    },
                    {
                        "name": "Omkar Pangarkar"
                    },
                    {
                        "name": "Richard Fan"
                    },
                    {
                        "name": "Yi Gu"
                    },
                    {
                        "name": "Victor Miller"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Liping Tang"
                    },
                    {
                        "name": "Nikhil Ranjan"
                    },
                    {
                        "name": "Yonghao Zhuang"
                    },
                    {
                        "name": "Guowei He"
                    },
                    {
                        "name": "Renxi Wang"
                    },
                    {
                        "name": "Mingkai Deng"
                    },
                    {
                        "name": "Robin Algayres"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Eric Xing"
                    }
                ],
                "author_detail": {
                    "name": "Eric Xing"
                },
                "author": "Eric Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07123v1",
                "updated": "2025-01-13T08:25:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    8,
                    25,
                    14,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T08:25:14Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    8,
                    25,
                    14,
                    0,
                    13,
                    0
                ],
                "title": "Inferring Interpretable Models of Fragmentation Functions using Symbolic\n  Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Interpretable Models of Fragmentation Functions using Symbolic\n  Regression"
                },
                "summary": "Machine learning is rapidly making its path into natural sciences, including\nhigh-energy physics. We present the first study that infers, directly from\nexperimental data, a functional form of fragmentation functions. The latter\nrepresent a key ingredient to describe physical observables measured in\nhigh-energy physics processes that involve hadron production, and predict their\nvalues at different energy. Fragmentation functions can not be calculated in\ntheory and have to be determined instead from data. Traditional approaches rely\non global fits of experimental data using a pre-assumed functional form\ninspired from phenomenological models to learn its parameters. This novel\napproach uses a ML technique, namely symbolic regression, to learn an\nanalytical model from measured charged hadron multiplicities. The function\nlearned by symbolic regression resembles the Lund string function and describes\nthe data well, thus representing a potential candidate for use in global FFs\nfits. This study represents an approach to follow in such QCD-related\nphenomenology studies and more generally in sciences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning is rapidly making its path into natural sciences, including\nhigh-energy physics. We present the first study that infers, directly from\nexperimental data, a functional form of fragmentation functions. The latter\nrepresent a key ingredient to describe physical observables measured in\nhigh-energy physics processes that involve hadron production, and predict their\nvalues at different energy. Fragmentation functions can not be calculated in\ntheory and have to be determined instead from data. Traditional approaches rely\non global fits of experimental data using a pre-assumed functional form\ninspired from phenomenological models to learn its parameters. This novel\napproach uses a ML technique, namely symbolic regression, to learn an\nanalytical model from measured charged hadron multiplicities. The function\nlearned by symbolic regression resembles the Lund string function and describes\nthe data well, thus representing a potential candidate for use in global FFs\nfits. This study represents an approach to follow in such QCD-related\nphenomenology studies and more generally in sciences."
                },
                "authors": [
                    {
                        "name": "Nour Makke"
                    },
                    {
                        "name": "Sanjay Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Sanjay Chawla"
                },
                "author": "Sanjay Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13973v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13973v2",
                "updated": "2025-01-13T07:50:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    7,
                    50,
                    54,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-18T15:50:50Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    15,
                    50,
                    50,
                    2,
                    353,
                    0
                ],
                "title": "Model-Agnostic Cosmological Inference with SDSS-IV eBOSS: Simultaneous\n  Probing for Background and Perturbed Universe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Agnostic Cosmological Inference with SDSS-IV eBOSS: Simultaneous\n  Probing for Background and Perturbed Universe"
                },
                "summary": "Here we explore certain subtle features imprinted in data from the completed\nSloan Digital Sky Survey IV (SDSS-IV) extended Baryon Oscillation Spectroscopic\nSurvey (eBOSS) as a combined probe for the background and perturbed Universe.\nWe reconstruct the baryon Acoustic Oscillation (BAO) and Redshift Space\nDistortion (RSD) observables as functions of redshift, using measurements from\nSDSS alone. We apply the Multi-Task Gaussian Process (MTGP) framework to model\nthe interdependencies of cosmological observables $D_M(z)/r_d$, $D_H(z)/r_d$,\nand $f\\sigma_8(z)$, and track their evolution across different redshifts.\nSubsequently, we obtain constrained three-dimensional phase space containing\n$D_M(z)/r_d$, $D_H(z)/r_d$, and $f\\sigma_8(z)$ at different redshifts probed by\nthe SDSS-IV eBOSS survey. Furthermore, assuming the $\\Lambda$CDM model, we\nobtain constraints on model parameters $\\Omega_{m}$, $H_{0}r_{d}$, $\\sigma_{8}$\nand $S_{8}$ at each redshift probed by SDSS-IV eBOSS. This indicates\nredshift-dependent trends in $H_0$, $\\Omega_m$, $\\sigma_8$ and $S_8$ in the\n$\\Lambda$CDM model, suggesting a possible inconsistency in the $\\Lambda$CDM\nmodel. Ours is a template for model-independent extraction of information for\nboth background and perturbed Universe using a single galaxy survey taking into\naccount all the existing correlations between background and perturbed\nobservables and this can be easily extended to future DESI-3YR as well as\nEuclid results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Here we explore certain subtle features imprinted in data from the completed\nSloan Digital Sky Survey IV (SDSS-IV) extended Baryon Oscillation Spectroscopic\nSurvey (eBOSS) as a combined probe for the background and perturbed Universe.\nWe reconstruct the baryon Acoustic Oscillation (BAO) and Redshift Space\nDistortion (RSD) observables as functions of redshift, using measurements from\nSDSS alone. We apply the Multi-Task Gaussian Process (MTGP) framework to model\nthe interdependencies of cosmological observables $D_M(z)/r_d$, $D_H(z)/r_d$,\nand $f\\sigma_8(z)$, and track their evolution across different redshifts.\nSubsequently, we obtain constrained three-dimensional phase space containing\n$D_M(z)/r_d$, $D_H(z)/r_d$, and $f\\sigma_8(z)$ at different redshifts probed by\nthe SDSS-IV eBOSS survey. Furthermore, assuming the $\\Lambda$CDM model, we\nobtain constraints on model parameters $\\Omega_{m}$, $H_{0}r_{d}$, $\\sigma_{8}$\nand $S_{8}$ at each redshift probed by SDSS-IV eBOSS. This indicates\nredshift-dependent trends in $H_0$, $\\Omega_m$, $\\sigma_8$ and $S_8$ in the\n$\\Lambda$CDM model, suggesting a possible inconsistency in the $\\Lambda$CDM\nmodel. Ours is a template for model-independent extraction of information for\nboth background and perturbed Universe using a single galaxy survey taking into\naccount all the existing correlations between background and perturbed\nobservables and this can be easily extended to future DESI-3YR as well as\nEuclid results."
                },
                "authors": [
                    {
                        "name": "Purba Mukherjee"
                    },
                    {
                        "name": "Anjan A. Sen"
                    }
                ],
                "author_detail": {
                    "name": "Anjan A. Sen"
                },
                "author": "Anjan A. Sen",
                "arxiv_comment": "14 pages, 7 sets of figures, 3 tables. Comments are welcome. New\n  references added",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13973v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13973v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07108v1",
                "updated": "2025-01-13T07:42:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    7,
                    42,
                    55,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T07:42:55Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    7,
                    42,
                    55,
                    0,
                    13,
                    0
                ],
                "title": "How GPT learns layer by layer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How GPT learns layer by layer"
                },
                "summary": "Large Language Models (LLMs) excel at tasks like language processing,\nstrategy games, and reasoning but struggle to build generalizable internal\nrepresentations essential for adaptive decision-making in agents. For agents to\neffectively navigate complex environments, they must construct reliable world\nmodels. While LLMs perform well on specific benchmarks, they often fail to\ngeneralize, leading to brittle representations that limit their real-world\neffectiveness. Understanding how LLMs build internal world models is key to\ndeveloping agents capable of consistent, adaptive behavior across tasks. We\nanalyze OthelloGPT, a GPT-based model trained on Othello gameplay, as a\ncontrolled testbed for studying representation learning. Despite being trained\nsolely on next-token prediction with random valid moves, OthelloGPT shows\nmeaningful layer-wise progression in understanding board state and gameplay.\nEarly layers capture static attributes like board edges, while deeper layers\nreflect dynamic tile changes. To interpret these representations, we compare\nSparse Autoencoders (SAEs) with linear probes, finding that SAEs offer more\nrobust, disentangled insights into compositional features, whereas linear\nprobes mainly detect features useful for classification. We use SAEs to decode\nfeatures related to tile color and tile stability, a previously unexamined\nfeature that reflects complex gameplay concepts like board control and\nlong-term planning. We study the progression of linear probe accuracy and tile\ncolor using both SAE's and linear probes to compare their effectiveness at\ncapturing what the model is learning. Although we begin with a smaller language\nmodel, OthelloGPT, this study establishes a framework for understanding the\ninternal representations learned by GPT models, transformers, and LLMs more\nbroadly. Our code is publicly available: https://github.com/ALT-JS/OthelloSAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at tasks like language processing,\nstrategy games, and reasoning but struggle to build generalizable internal\nrepresentations essential for adaptive decision-making in agents. For agents to\neffectively navigate complex environments, they must construct reliable world\nmodels. While LLMs perform well on specific benchmarks, they often fail to\ngeneralize, leading to brittle representations that limit their real-world\neffectiveness. Understanding how LLMs build internal world models is key to\ndeveloping agents capable of consistent, adaptive behavior across tasks. We\nanalyze OthelloGPT, a GPT-based model trained on Othello gameplay, as a\ncontrolled testbed for studying representation learning. Despite being trained\nsolely on next-token prediction with random valid moves, OthelloGPT shows\nmeaningful layer-wise progression in understanding board state and gameplay.\nEarly layers capture static attributes like board edges, while deeper layers\nreflect dynamic tile changes. To interpret these representations, we compare\nSparse Autoencoders (SAEs) with linear probes, finding that SAEs offer more\nrobust, disentangled insights into compositional features, whereas linear\nprobes mainly detect features useful for classification. We use SAEs to decode\nfeatures related to tile color and tile stability, a previously unexamined\nfeature that reflects complex gameplay concepts like board control and\nlong-term planning. We study the progression of linear probe accuracy and tile\ncolor using both SAE's and linear probes to compare their effectiveness at\ncapturing what the model is learning. Although we begin with a smaller language\nmodel, OthelloGPT, this study establishes a framework for understanding the\ninternal representations learned by GPT models, transformers, and LLMs more\nbroadly. Our code is publicly available: https://github.com/ALT-JS/OthelloSAE."
                },
                "authors": [
                    {
                        "name": "Jason Du"
                    },
                    {
                        "name": "Kelly Hong"
                    },
                    {
                        "name": "Alishba Imran"
                    },
                    {
                        "name": "Erfan Jahanparast"
                    },
                    {
                        "name": "Mehdi Khfifi"
                    },
                    {
                        "name": "Kaichun Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Kaichun Qiao"
                },
                "author": "Kaichun Qiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17692v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17692v3",
                "updated": "2025-01-13T07:41:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    7,
                    41,
                    44,
                    0,
                    13,
                    0
                ],
                "published": "2024-09-26T09:57:16Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    57,
                    16,
                    3,
                    270,
                    0
                ],
                "title": "MIO: A Foundation Model on Multimodal Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIO: A Foundation Model on Multimodal Tokens"
                },
                "summary": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc."
                },
                "authors": [
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "King Zhu"
                    },
                    {
                        "name": "Chunpu Xu"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yibo Zhang"
                    },
                    {
                        "name": "Jiashuo Wang"
                    },
                    {
                        "name": "Ning Shi"
                    },
                    {
                        "name": "Siyu Li"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "arxiv_comment": "Technical Report. Codes and models are available in\n  https://github.com/MIO-Team/MIO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17692v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17692v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14789v2",
                "updated": "2025-01-13T07:29:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    7,
                    29,
                    53,
                    0,
                    13,
                    0
                ],
                "published": "2024-11-22T08:17:46Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    17,
                    46,
                    4,
                    327,
                    0
                ],
                "title": "Simplifying CLIP: Unleashing the Power of Large-Scale Models on\n  Consumer-level Computers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplifying CLIP: Unleashing the Power of Large-Scale Models on\n  Consumer-level Computers"
                },
                "summary": "Contrastive Language-Image Pre-training (CLIP) has attracted a surge of\nattention for its superior zero-shot performance and excellent transferability\nto downstream tasks. However, training such large-scale models usually requires\nsubstantial computation and storage, which poses barriers for general users\nwith consumer-level computers. Motivated by this observation, in this paper we\ninvestigate how to achieve competitive performance on only one Nvidia RTX3090\nGPU and with one terabyte for storing dataset. On one hand, we simplify the\ntransformer block structure and combine Weight Inheritance with multi-stage\nKnowledge Distillation (WIKD), thereby reducing the parameters and improving\nthe inference speed during training along with deployment. On the other hand,\nconfronted with the convergence challenge posed by small dataset, we generate\nsynthetic captions for each sample as data augmentation, and devise a novel\nPair Matching (PM) loss to fully exploit the distinguishment among positive and\nnegative image-text pairs. Extensive experiments demonstrate that our model can\nachieve a new state-of-the-art datascale-parameter-accuracy tradeoff, which\ncould further popularize the CLIP model in the related research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Language-Image Pre-training (CLIP) has attracted a surge of\nattention for its superior zero-shot performance and excellent transferability\nto downstream tasks. However, training such large-scale models usually requires\nsubstantial computation and storage, which poses barriers for general users\nwith consumer-level computers. Motivated by this observation, in this paper we\ninvestigate how to achieve competitive performance on only one Nvidia RTX3090\nGPU and with one terabyte for storing dataset. On one hand, we simplify the\ntransformer block structure and combine Weight Inheritance with multi-stage\nKnowledge Distillation (WIKD), thereby reducing the parameters and improving\nthe inference speed during training along with deployment. On the other hand,\nconfronted with the convergence challenge posed by small dataset, we generate\nsynthetic captions for each sample as data augmentation, and devise a novel\nPair Matching (PM) loss to fully exploit the distinguishment among positive and\nnegative image-text pairs. Extensive experiments demonstrate that our model can\nachieve a new state-of-the-art datascale-parameter-accuracy tradeoff, which\ncould further popularize the CLIP model in the related research community."
                },
                "authors": [
                    {
                        "name": "Hongbo Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hongbo Liu"
                },
                "author": "Hongbo Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07102v1",
                "updated": "2025-01-13T07:27:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    7,
                    27,
                    0,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T07:27:00Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    7,
                    27,
                    0,
                    0,
                    13,
                    0
                ],
                "title": "AdaCS: Adaptive Normalization for Enhanced Code-Switching ASR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaCS: Adaptive Normalization for Enhanced Code-Switching ASR"
                },
                "summary": "Intra-sentential code-switching (CS) refers to the alternation between\nlanguages that happens within a single utterance and is a significant challenge\nfor Automatic Speech Recognition (ASR) systems. For example, when a Vietnamese\nspeaker uses foreign proper names or specialized terms within their speech. ASR\nsystems often struggle to accurately transcribe intra-sentential CS due to\ntheir training on monolingual data and the unpredictable nature of CS. This\nissue is even more pronounced for low-resource languages, where limited data\navailability hinders the development of robust models. In this study, we\npropose AdaCS, a normalization model integrates an adaptive bias attention\nmodule (BAM) into encoder-decoder network. This novel approach provides a\nrobust solution to CS ASR in unseen domains, thereby significantly enhancing\nour contribution to the field. By utilizing BAM to both identify and normalize\nCS phrases, AdaCS enhances its adaptive capabilities with a biased list of\nwords provided during inference. Our method demonstrates impressive performance\nand the ability to handle unseen CS phrases across various domains. Experiments\nshow that AdaCS outperforms previous state-of-the-art method on Vietnamese CS\nASR normalization by considerable WER reduction of 56.2% and 36.8% on the two\nproposed test sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intra-sentential code-switching (CS) refers to the alternation between\nlanguages that happens within a single utterance and is a significant challenge\nfor Automatic Speech Recognition (ASR) systems. For example, when a Vietnamese\nspeaker uses foreign proper names or specialized terms within their speech. ASR\nsystems often struggle to accurately transcribe intra-sentential CS due to\ntheir training on monolingual data and the unpredictable nature of CS. This\nissue is even more pronounced for low-resource languages, where limited data\navailability hinders the development of robust models. In this study, we\npropose AdaCS, a normalization model integrates an adaptive bias attention\nmodule (BAM) into encoder-decoder network. This novel approach provides a\nrobust solution to CS ASR in unseen domains, thereby significantly enhancing\nour contribution to the field. By utilizing BAM to both identify and normalize\nCS phrases, AdaCS enhances its adaptive capabilities with a biased list of\nwords provided during inference. Our method demonstrates impressive performance\nand the ability to handle unseen CS phrases across various domains. Experiments\nshow that AdaCS outperforms previous state-of-the-art method on Vietnamese CS\nASR normalization by considerable WER reduction of 56.2% and 36.8% on the two\nproposed test sets."
                },
                "authors": [
                    {
                        "name": "The Chuong Chu"
                    },
                    {
                        "name": "Vu Tuan Dat Pham"
                    },
                    {
                        "name": "Kien Dao"
                    },
                    {
                        "name": "Hoang Nguyen"
                    },
                    {
                        "name": "Quoc Hung Truong"
                    }
                ],
                "author_detail": {
                    "name": "Quoc Hung Truong"
                },
                "author": "Quoc Hung Truong",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07249v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07249v2",
                "updated": "2025-01-13T07:22:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    7,
                    22,
                    2,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-10T07:18:51Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    7,
                    18,
                    51,
                    1,
                    345,
                    0
                ],
                "title": "Buster: Implanting Semantic Backdoor into Text Encoder to Mitigate NSFW\n  Content Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Buster: Implanting Semantic Backdoor into Text Encoder to Mitigate NSFW\n  Content Generation"
                },
                "summary": "The rise of deep learning models in the digital era has raised substantial\nconcerns regarding the generation of Not-Safe-for-Work (NSFW) content. Existing\ndefense methods primarily involve model fine-tuning and post-hoc content\nmoderation. Nevertheless, these approaches largely lack scalability in\neliminating harmful content, degrade the quality of benign image generation, or\nincur high inference costs. To address these challenges, we propose an\ninnovative framework named \\textit{Buster}, which injects backdoors into the\ntext encoder to prevent NSFW content generation. Buster leverages deep semantic\ninformation rather than explicit prompts as triggers, redirecting NSFW prompts\ntowards targeted benign prompts. Additionally, Buster employs energy-based\ntraining data generation through Langevin dynamics for adversarial knowledge\naugmentation, thereby ensuring robustness in harmful concept definition. This\napproach demonstrates exceptional resilience and scalability in mitigating NSFW\ncontent. Particularly, Buster fine-tunes the text encoder of Text-to-Image\nmodels within merely five minutes, showcasing its efficiency. Our extensive\nexperiments denote that Buster outperforms nine state-of-the-art baselines,\nachieving a superior NSFW content removal rate of at least 91.2\\% while\npreserving the quality of harmless images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of deep learning models in the digital era has raised substantial\nconcerns regarding the generation of Not-Safe-for-Work (NSFW) content. Existing\ndefense methods primarily involve model fine-tuning and post-hoc content\nmoderation. Nevertheless, these approaches largely lack scalability in\neliminating harmful content, degrade the quality of benign image generation, or\nincur high inference costs. To address these challenges, we propose an\ninnovative framework named \\textit{Buster}, which injects backdoors into the\ntext encoder to prevent NSFW content generation. Buster leverages deep semantic\ninformation rather than explicit prompts as triggers, redirecting NSFW prompts\ntowards targeted benign prompts. Additionally, Buster employs energy-based\ntraining data generation through Langevin dynamics for adversarial knowledge\naugmentation, thereby ensuring robustness in harmful concept definition. This\napproach demonstrates exceptional resilience and scalability in mitigating NSFW\ncontent. Particularly, Buster fine-tunes the text encoder of Text-to-Image\nmodels within merely five minutes, showcasing its efficiency. Our extensive\nexperiments denote that Buster outperforms nine state-of-the-art baselines,\nachieving a superior NSFW content removal rate of at least 91.2\\% while\npreserving the quality of harmless images."
                },
                "authors": [
                    {
                        "name": "Xin Zhao"
                    },
                    {
                        "name": "Xiaojun Chen"
                    },
                    {
                        "name": "Yuexin Xuan"
                    },
                    {
                        "name": "Zhendong Zhao"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Wang"
                },
                "author": "Xiaofeng Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07249v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07249v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01933v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01933v4",
                "updated": "2025-01-13T07:13:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    7,
                    13,
                    56,
                    0,
                    13,
                    0
                ],
                "published": "2024-08-04T05:15:02Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    5,
                    15,
                    2,
                    6,
                    217,
                    0
                ],
                "title": "DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 511 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 511 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios."
                },
                "authors": [
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Jiuyang Chang"
                    },
                    {
                        "name": "Yiming Qian"
                    },
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Zhouqiang Jiang"
                    },
                    {
                        "name": "Jiahao Zhang"
                    },
                    {
                        "name": "Yuta Nakashima"
                    },
                    {
                        "name": "Hajime Nagahara"
                    }
                ],
                "author_detail": {
                    "name": "Hajime Nagahara"
                },
                "author": "Hajime Nagahara",
                "arxiv_comment": "9 pages,6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01933v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01933v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19943v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19943v3",
                "updated": "2025-01-13T06:53:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    6,
                    53,
                    56,
                    0,
                    13,
                    0
                ],
                "published": "2024-11-29T18:58:22Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    58,
                    22,
                    4,
                    334,
                    0
                ],
                "title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhances\n  LLM's Reasoning Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhances\n  LLM's Reasoning Capability"
                },
                "summary": "Mathematical reasoning tasks pose significant challenges for large language\nmodels (LLMs) because they require precise logical deduction and sequence\nanalysis. In this work, we introduce the concept of critical tokens -- elements\nwithin reasoning trajectories that significantly influence incorrect outcomes.\nWe present a novel framework for identifying these tokens through rollout\nsampling and demonstrate their substantial divergence from traditional error\ntokens. Through extensive experiments on datasets such as GSM8K and MATH500, we\nshow that identifying and replacing critical tokens significantly improves\nmodel accuracy. We propose an efficient methodology for pinpointing these\ntokens in large-scale datasets using contrastive estimation and extend this\nframework to enhance model training processes with direct preference\noptimization (DPO). Experimental results on GSM8K and MATH500 benchmarks with\nthe widely used models Llama-3 (8B and 70B) and Deepseek-math (7B) demonstrate\nthe effectiveness of the proposed approach, cDPO. Our results underscore the\npotential of leveraging critical tokens to reduce errors in reasoning tasks,\nadvancing the development of AI systems capable of robust logical deduction.\nOur code, annotated datasets, and trained models are available at\nhttps://github.com/chenzhiling9954/Critical-Tokens-Matter to support and\nencourage future research in this promising field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning tasks pose significant challenges for large language\nmodels (LLMs) because they require precise logical deduction and sequence\nanalysis. In this work, we introduce the concept of critical tokens -- elements\nwithin reasoning trajectories that significantly influence incorrect outcomes.\nWe present a novel framework for identifying these tokens through rollout\nsampling and demonstrate their substantial divergence from traditional error\ntokens. Through extensive experiments on datasets such as GSM8K and MATH500, we\nshow that identifying and replacing critical tokens significantly improves\nmodel accuracy. We propose an efficient methodology for pinpointing these\ntokens in large-scale datasets using contrastive estimation and extend this\nframework to enhance model training processes with direct preference\noptimization (DPO). Experimental results on GSM8K and MATH500 benchmarks with\nthe widely used models Llama-3 (8B and 70B) and Deepseek-math (7B) demonstrate\nthe effectiveness of the proposed approach, cDPO. Our results underscore the\npotential of leveraging critical tokens to reduce errors in reasoning tasks,\nadvancing the development of AI systems capable of robust logical deduction.\nOur code, annotated datasets, and trained models are available at\nhttps://github.com/chenzhiling9954/Critical-Tokens-Matter to support and\nencourage future research in this promising field."
                },
                "authors": [
                    {
                        "name": "Zicheng Lin"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Qiuzhi Lin"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Ruilin Luo"
                    },
                    {
                        "name": "Chufan Shi"
                    },
                    {
                        "name": "Siheng Li"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaopeng Tu"
                },
                "author": "Zhaopeng Tu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19943v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19943v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10047v2",
                "updated": "2025-01-13T06:47:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    6,
                    47,
                    21,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-13T11:19:56Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    19,
                    56,
                    4,
                    348,
                    0
                ],
                "title": "Large Action Models: From Inception to Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Action Models: From Inception to Implementation"
                },
                "summary": "As AI continues to advance, there is a growing demand for systems that go\nbeyond language-based assistance and move toward intelligent agents capable of\nperforming real-world actions. This evolution requires the transition from\ntraditional Large Language Models (LLMs), which excel at generating textual\nresponses, to Large Action Models (LAMs), designed for action generation and\nexecution within dynamic environments. Enabled by agent systems, LAMs hold the\npotential to transform AI from passive language understanding to active task\ncompletion, marking a significant milestone in the progression toward\nartificial general intelligence.\n  In this paper, we present a comprehensive framework for developing LAMs,\noffering a systematic approach to their creation, from inception to deployment.\nWe begin with an overview of LAMs, highlighting their unique characteristics\nand delineating their differences from LLMs. Using a Windows OS-based agent as\na case study, we provide a detailed, step-by-step guide on the key stages of\nLAM development, including data collection, model training, environment\nintegration, grounding, and evaluation. This generalizable workflow can serve\nas a blueprint for creating functional LAMs in various application domains. We\nconclude by identifying the current limitations of LAMs and discussing\ndirections for future research and industrial deployment, emphasizing the\nchallenges and opportunities that lie ahead in realizing the full potential of\nLAMs in real-world applications.\n  The code for the data collection process utilized in this paper is publicly\navailable at: https://github.com/microsoft/UFO/tree/main/dataflow, and\ncomprehensive documentation can be found at\nhttps://microsoft.github.io/UFO/dataflow/overview/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI continues to advance, there is a growing demand for systems that go\nbeyond language-based assistance and move toward intelligent agents capable of\nperforming real-world actions. This evolution requires the transition from\ntraditional Large Language Models (LLMs), which excel at generating textual\nresponses, to Large Action Models (LAMs), designed for action generation and\nexecution within dynamic environments. Enabled by agent systems, LAMs hold the\npotential to transform AI from passive language understanding to active task\ncompletion, marking a significant milestone in the progression toward\nartificial general intelligence.\n  In this paper, we present a comprehensive framework for developing LAMs,\noffering a systematic approach to their creation, from inception to deployment.\nWe begin with an overview of LAMs, highlighting their unique characteristics\nand delineating their differences from LLMs. Using a Windows OS-based agent as\na case study, we provide a detailed, step-by-step guide on the key stages of\nLAM development, including data collection, model training, environment\nintegration, grounding, and evaluation. This generalizable workflow can serve\nas a blueprint for creating functional LAMs in various application domains. We\nconclude by identifying the current limitations of LAMs and discussing\ndirections for future research and industrial deployment, emphasizing the\nchallenges and opportunities that lie ahead in realizing the full potential of\nLAMs in real-world applications.\n  The code for the data collection process utilized in this paper is publicly\navailable at: https://github.com/microsoft/UFO/tree/main/dataflow, and\ncomprehensive documentation can be found at\nhttps://microsoft.github.io/UFO/dataflow/overview/."
                },
                "authors": [
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Junting Lu"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Bo Qiao"
                    },
                    {
                        "name": "Ray Huang"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Qisheng Su"
                    },
                    {
                        "name": "Jiayi Ye"
                    },
                    {
                        "name": "Yudi Zhang"
                    },
                    {
                        "name": "Jian-Guang Lou"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "25pages,12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09311v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09311v2",
                "updated": "2025-01-13T06:41:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    6,
                    41,
                    16,
                    0,
                    13,
                    0
                ],
                "published": "2024-09-14T05:13:48Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    5,
                    13,
                    48,
                    5,
                    258,
                    0
                ],
                "title": "Improving Robustness of Diffusion-Based Zero-Shot Speech Synthesis via\n  Stable Formant Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Robustness of Diffusion-Based Zero-Shot Speech Synthesis via\n  Stable Formant Generation"
                },
                "summary": "Diffusion models have achieved remarkable success in text-to-speech (TTS),\neven in zero-shot scenarios. Recent efforts aim to address the trade-off\nbetween inference speed and sound quality, often considered the primary\ndrawback of diffusion models. However, we find a critical mispronunciation\nissue is being overlooked. Our preliminary study reveals the unstable\npronunciation resulting from the diffusion process. Based on this observation,\nwe introduce StableForm-TTS, a novel zero-shot speech synthesis framework\ndesigned to produce robust pronunciation while maintaining the advantages of\ndiffusion modeling. By pioneering the adoption of source-filter theory in\ndiffusion TTS, we propose an elaborate architecture for stable formant\ngeneration. Experimental results on unseen speakers show that our model\noutperforms the state-of-the-art method in terms of pronunciation accuracy and\nnaturalness, with comparable speaker similarity. Moreover, our model\ndemonstrates effective scalability as both data and model sizes increase. Audio\nsamples are available online:\nhttps://deepbrainai-research.github.io/stableformtts/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved remarkable success in text-to-speech (TTS),\neven in zero-shot scenarios. Recent efforts aim to address the trade-off\nbetween inference speed and sound quality, often considered the primary\ndrawback of diffusion models. However, we find a critical mispronunciation\nissue is being overlooked. Our preliminary study reveals the unstable\npronunciation resulting from the diffusion process. Based on this observation,\nwe introduce StableForm-TTS, a novel zero-shot speech synthesis framework\ndesigned to produce robust pronunciation while maintaining the advantages of\ndiffusion modeling. By pioneering the adoption of source-filter theory in\ndiffusion TTS, we propose an elaborate architecture for stable formant\ngeneration. Experimental results on unseen speakers show that our model\noutperforms the state-of-the-art method in terms of pronunciation accuracy and\nnaturalness, with comparable speaker similarity. Moreover, our model\ndemonstrates effective scalability as both data and model sizes increase. Audio\nsamples are available online:\nhttps://deepbrainai-research.github.io/stableformtts/."
                },
                "authors": [
                    {
                        "name": "Changjin Han"
                    },
                    {
                        "name": "Seokgi Lee"
                    },
                    {
                        "name": "Gyuhyeon Nam"
                    },
                    {
                        "name": "Gyeongsu Chae"
                    }
                ],
                "author_detail": {
                    "name": "Gyeongsu Chae"
                },
                "author": "Gyeongsu Chae",
                "arxiv_comment": "Accepted to ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09311v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09311v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07083v1",
                "updated": "2025-01-13T06:35:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    6,
                    35,
                    42,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T06:35:42Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    6,
                    35,
                    42,
                    0,
                    13,
                    0
                ],
                "title": "Stochastic reconstruction of multiphase composite microstructures using\n  statistics-encoded neural network for poro/micro-mechanical modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic reconstruction of multiphase composite microstructures using\n  statistics-encoded neural network for poro/micro-mechanical modelling"
                },
                "summary": "Understanding microstructure-property relationships (MPRs) is essential for\noptimising the performance of multiphase composites. Image-based\nporo/micro-mechanical modelling provides a non-invasive approach to exploring\nMPRs, but the randomness of multiphase composites often necessitates extensive\n3D microstructure datasets for statistical reliability. This study introduces a\ncost-effective machine learning framework to reconstruct numerous virtual 3D\nmicrostructures from limited 2D exemplars, circumventing the high costs of\nvolumetric microscopy. Using feedforward neural networks, termed the\nstatistics-encoded neural network (SENN), the framework encodes 2D\nmorphological statistics and infers 3D morphological statistics via a 2D-to-3D\nintegration scheme. Statistically equivalent 3D microstructures are synthesised\nusing Gibbs sampling. Hierarchical characterisation enables seamless capture of\nfeatures across multiple scales. Validation on three composites demonstrates\nstrong statistical equivalence between reconstructed and reference\nmicrostructures, confirmed by morphological descriptors and simulated\nmacroscopic properties (e.g., stiffness, permeability). The SENN-based\nframework is a high-fidelity tool for efficiently and accurately reconstructing\nmultiphase microstructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding microstructure-property relationships (MPRs) is essential for\noptimising the performance of multiphase composites. Image-based\nporo/micro-mechanical modelling provides a non-invasive approach to exploring\nMPRs, but the randomness of multiphase composites often necessitates extensive\n3D microstructure datasets for statistical reliability. This study introduces a\ncost-effective machine learning framework to reconstruct numerous virtual 3D\nmicrostructures from limited 2D exemplars, circumventing the high costs of\nvolumetric microscopy. Using feedforward neural networks, termed the\nstatistics-encoded neural network (SENN), the framework encodes 2D\nmorphological statistics and infers 3D morphological statistics via a 2D-to-3D\nintegration scheme. Statistically equivalent 3D microstructures are synthesised\nusing Gibbs sampling. Hierarchical characterisation enables seamless capture of\nfeatures across multiple scales. Validation on three composites demonstrates\nstrong statistical equivalence between reconstructed and reference\nmicrostructures, confirmed by morphological descriptors and simulated\nmacroscopic properties (e.g., stiffness, permeability). The SENN-based\nframework is a high-fidelity tool for efficiently and accurately reconstructing\nmultiphase microstructures."
                },
                "authors": [
                    {
                        "name": "Jinlong Fu"
                    },
                    {
                        "name": "Wei Tan"
                    }
                ],
                "author_detail": {
                    "name": "Wei Tan"
                },
                "author": "Wei Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07078v1",
                "updated": "2025-01-13T06:22:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    6,
                    22,
                    52,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T06:22:52Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    6,
                    22,
                    52,
                    0,
                    13,
                    0
                ],
                "title": "ADKGD: Anomaly Detection in Knowledge Graphs with Dual-Channel Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADKGD: Anomaly Detection in Knowledge Graphs with Dual-Channel Training"
                },
                "summary": "In the current development of large language models (LLMs), it is important\nto ensure the accuracy and reliability of the underlying data sources. LLMs are\ncritical for various applications, but they often suffer from hallucinations\nand inaccuracies due to knowledge gaps in the training data. Knowledge graphs\n(KGs), as a powerful structural tool, could serve as a vital external\ninformation source to mitigate the aforementioned issues. By providing a\nstructured and comprehensive understanding of real-world data, KGs enhance the\nperformance and reliability of LLMs. However, it is common that errors exist in\nKGs while extracting triplets from unstructured data to construct KGs. This\ncould lead to degraded performance in downstream tasks such as\nquestion-answering and recommender systems. Therefore, anomaly detection in KGs\nis essential to identify and correct these errors. This paper presents an\nanomaly detection algorithm in knowledge graphs with dual-channel learning\n(ADKGD). ADKGD leverages a dual-channel learning approach to enhance\nrepresentation learning from both the entity-view and triplet-view\nperspectives. Furthermore, using a cross-layer approach, our framework\nintegrates internal information aggregation and context information\naggregation. We introduce a kullback-leibler (KL)-loss component to improve the\naccuracy of the scoring function between the dual channels. To evaluate ADKGD's\nperformance, we conduct empirical studies on three real-world KGs: WN18RR,\nFB15K, and NELL-995. Experimental results demonstrate that ADKGD outperforms\nthe state-of-the-art anomaly detection algorithms. The source code and datasets\nare publicly available at https://github.com/csjywu1/ADKGD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the current development of large language models (LLMs), it is important\nto ensure the accuracy and reliability of the underlying data sources. LLMs are\ncritical for various applications, but they often suffer from hallucinations\nand inaccuracies due to knowledge gaps in the training data. Knowledge graphs\n(KGs), as a powerful structural tool, could serve as a vital external\ninformation source to mitigate the aforementioned issues. By providing a\nstructured and comprehensive understanding of real-world data, KGs enhance the\nperformance and reliability of LLMs. However, it is common that errors exist in\nKGs while extracting triplets from unstructured data to construct KGs. This\ncould lead to degraded performance in downstream tasks such as\nquestion-answering and recommender systems. Therefore, anomaly detection in KGs\nis essential to identify and correct these errors. This paper presents an\nanomaly detection algorithm in knowledge graphs with dual-channel learning\n(ADKGD). ADKGD leverages a dual-channel learning approach to enhance\nrepresentation learning from both the entity-view and triplet-view\nperspectives. Furthermore, using a cross-layer approach, our framework\nintegrates internal information aggregation and context information\naggregation. We introduce a kullback-leibler (KL)-loss component to improve the\naccuracy of the scoring function between the dual channels. To evaluate ADKGD's\nperformance, we conduct empirical studies on three real-world KGs: WN18RR,\nFB15K, and NELL-995. Experimental results demonstrate that ADKGD outperforms\nthe state-of-the-art anomaly detection algorithms. The source code and datasets\nare publicly available at https://github.com/csjywu1/ADKGD."
                },
                "authors": [
                    {
                        "name": "Jiayang Wu"
                    },
                    {
                        "name": "Wensheng Gan"
                    },
                    {
                        "name": "Jiahao Zhang"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "Preprint. 11 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20163v2",
                "updated": "2025-01-13T06:17:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    6,
                    17,
                    38,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-28T14:27:45Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    27,
                    45,
                    5,
                    363,
                    0
                ],
                "title": "Topic-Aware Knowledge Graph with Large Language Models for\n  Interoperability in Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic-Aware Knowledge Graph with Large Language Models for\n  Interoperability in Recommender Systems"
                },
                "summary": "The use of knowledge graphs in recommender systems has become one of the\ncommon approaches to addressing data sparsity and cold start problems. Recent\nadvances in large language models (LLMs) offer new possibilities for processing\nside and context information within knowledge graphs. However, consistent\nintegration across various systems remains challenging due to the need for\ndomain expert intervention and differences in system characteristics. To\naddress these issues, we propose a consistent approach that extracts both\ngeneral and specific topics from both side and context information using LLMs.\nFirst, general topics are iteratively extracted and updated from side\ninformation. Then, specific topics are extracted using context information.\nFinally, to address synonymous topics generated during the specific topic\nextraction process, a refining algorithm processes and resolves these issues\neffectively. This approach allows general topics to capture broad knowledge\nacross diverse item characteristics, while specific topics emphasize detailed\nattributes, providing a more comprehensive understanding of the semantic\nfeatures of items and the preferences of users. Experimental results\ndemonstrate significant improvements in recommendation performance across\ndiverse knowledge graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of knowledge graphs in recommender systems has become one of the\ncommon approaches to addressing data sparsity and cold start problems. Recent\nadvances in large language models (LLMs) offer new possibilities for processing\nside and context information within knowledge graphs. However, consistent\nintegration across various systems remains challenging due to the need for\ndomain expert intervention and differences in system characteristics. To\naddress these issues, we propose a consistent approach that extracts both\ngeneral and specific topics from both side and context information using LLMs.\nFirst, general topics are iteratively extracted and updated from side\ninformation. Then, specific topics are extracted using context information.\nFinally, to address synonymous topics generated during the specific topic\nextraction process, a refining algorithm processes and resolves these issues\neffectively. This approach allows general topics to capture broad knowledge\nacross diverse item characteristics, while specific topics emphasize detailed\nattributes, providing a more comprehensive understanding of the semantic\nfeatures of items and the preferences of users. Experimental results\ndemonstrate significant improvements in recommendation performance across\ndiverse knowledge graphs."
                },
                "authors": [
                    {
                        "name": "Minhye Jeon"
                    },
                    {
                        "name": "Seokho Ahn"
                    },
                    {
                        "name": "Young-Duk Seo"
                    }
                ],
                "author_detail": {
                    "name": "Young-Duk Seo"
                },
                "author": "Young-Duk Seo",
                "arxiv_doi": "10.1145/3672608.3707958",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3672608.3707958",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in The 40th ACM/SIGAPP Symposium On Applied Computing(SAC)\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16185v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16185v3",
                "updated": "2025-01-13T06:10:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    6,
                    10,
                    24,
                    0,
                    13,
                    0
                ],
                "published": "2024-01-29T14:32:27Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    14,
                    32,
                    27,
                    0,
                    29,
                    0
                ],
                "title": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing\n  LLMs' Vulnerability Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing\n  LLMs' Vulnerability Reasoning"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in\nvarious tasks, including those requiring human-level intelligence, such as\nvulnerability detection. However, recent efforts to use LLMs for vulnerability\ndetection remain preliminary, as they lack a deep understanding of whether a\nsubject LLM's vulnerability reasoning capability stems from the model itself or\nfrom external aids such as knowledge retrieval and tooling support.\n  In this paper, we aim to decouple LLMs' vulnerability reasoning from other\ncapabilities, such as vulnerability knowledge adoption, context information\nretrieval, and advanced prompt schemes. We introduce LLM4Vuln, a unified\nevaluation framework that separates and assesses LLMs' vulnerability reasoning\ncapabilities and examines improvements when combined with other enhancements.\n  We conduct controlled experiments using 147 ground-truth vulnerabilities and\n147 non-vulnerable cases in Solidity, Java and C/C++, testing them in a total\nof 3,528 scenarios across four LLMs (GPT-3.5, GPT-4, Phi-3, and Llama 3). Our\nfindings reveal the varying impacts of knowledge enhancement, context\nsupplementation, and prompt schemes. We also identify 14 zero-day\nvulnerabilities in four pilot bug bounty programs, resulting in $3,576 in\nbounties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in\nvarious tasks, including those requiring human-level intelligence, such as\nvulnerability detection. However, recent efforts to use LLMs for vulnerability\ndetection remain preliminary, as they lack a deep understanding of whether a\nsubject LLM's vulnerability reasoning capability stems from the model itself or\nfrom external aids such as knowledge retrieval and tooling support.\n  In this paper, we aim to decouple LLMs' vulnerability reasoning from other\ncapabilities, such as vulnerability knowledge adoption, context information\nretrieval, and advanced prompt schemes. We introduce LLM4Vuln, a unified\nevaluation framework that separates and assesses LLMs' vulnerability reasoning\ncapabilities and examines improvements when combined with other enhancements.\n  We conduct controlled experiments using 147 ground-truth vulnerabilities and\n147 non-vulnerable cases in Solidity, Java and C/C++, testing them in a total\nof 3,528 scenarios across four LLMs (GPT-3.5, GPT-4, Phi-3, and Llama 3). Our\nfindings reveal the varying impacts of knowledge enhancement, context\nsupplementation, and prompt schemes. We also identify 14 zero-day\nvulnerabilities in four pilot bug bounty programs, resulting in $3,576 in\nbounties."
                },
                "authors": [
                    {
                        "name": "Yuqiang Sun"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Yue Xue"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Lyuye Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yingjiu Li"
                    }
                ],
                "author_detail": {
                    "name": "Yingjiu Li"
                },
                "author": "Yingjiu Li",
                "arxiv_comment": "This is a technical report by Nanyang Technological University.\n  Updated to support Solidity, Java and C/C++",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16185v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16185v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.19435v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.19435v3",
                "updated": "2025-01-13T06:06:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    6,
                    6,
                    14,
                    0,
                    13,
                    0
                ],
                "published": "2023-10-30T10:49:09Z",
                "published_parsed": [
                    2023,
                    10,
                    30,
                    10,
                    49,
                    9,
                    0,
                    303,
                    0
                ],
                "title": "A novel characterization of structures in smooth regression curves: from\n  a viewpoint of persistent homology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A novel characterization of structures in smooth regression curves: from\n  a viewpoint of persistent homology"
                },
                "summary": "We characterize structures such as monotonicity, convexity, and modality in\nsmooth regression curves using persistent homology. Persistent homology is a\nkey tool in topological data analysis that detects higher dimensional\ntopological features such as connected components and holes (cycles or loops)\nin the data. In other words, persistent homology is a multiscale version of\nhomology that characterizes sets based on the connected components and holes.\nWe use super-level sets of functions to extract geometric features via\npersistent homology. In particular, we explore structures in regression curves\nvia the persistent homology of super-level sets of a function, where the\nfunction of interest is - the first derivative of the regression function.\n  In the course of this study, we extend an existing procedure of estimating\nthe persistent homology for the first derivative of a regression function and\nestablish its consistency. Moreover, as an application of the proposed\nmethodology, we demonstrate that the persistent homology of the derivative of a\nfunction can reveal hidden structures in the function that are not visible from\nthe persistent homology of the function itself. In particular, we characterize\nstructures such as monotonicity, convexity, and modality, and propose a measure\nof statistical significance to infer these structures in practice. Finally, we\nconduct an empirical study to implement the proposed methodology on simulated\nand real data sets and compare the derived results with an existing\nmethodology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We characterize structures such as monotonicity, convexity, and modality in\nsmooth regression curves using persistent homology. Persistent homology is a\nkey tool in topological data analysis that detects higher dimensional\ntopological features such as connected components and holes (cycles or loops)\nin the data. In other words, persistent homology is a multiscale version of\nhomology that characterizes sets based on the connected components and holes.\nWe use super-level sets of functions to extract geometric features via\npersistent homology. In particular, we explore structures in regression curves\nvia the persistent homology of super-level sets of a function, where the\nfunction of interest is - the first derivative of the regression function.\n  In the course of this study, we extend an existing procedure of estimating\nthe persistent homology for the first derivative of a regression function and\nestablish its consistency. Moreover, as an application of the proposed\nmethodology, we demonstrate that the persistent homology of the derivative of a\nfunction can reveal hidden structures in the function that are not visible from\nthe persistent homology of the function itself. In particular, we characterize\nstructures such as monotonicity, convexity, and modality, and propose a measure\nof statistical significance to infer these structures in practice. Finally, we\nconduct an empirical study to implement the proposed methodology on simulated\nand real data sets and compare the derived results with an existing\nmethodology."
                },
                "authors": [
                    {
                        "name": "Satish Kumar"
                    },
                    {
                        "name": "Subhra Sankar Dhar"
                    }
                ],
                "author_detail": {
                    "name": "Subhra Sankar Dhar"
                },
                "author": "Subhra Sankar Dhar",
                "arxiv_comment": "This is a revised manuscript that contains the following major\n  modifications: 1) A quantitative measure of significance to observed local\n  structures is proposed. 2) Implementation of the methodology on simulated\n  data. 3) Implementation of the methodology on two real data sets. 4)\n  Comparison with the SiZer map on both simulated and real data",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.19435v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.19435v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07071v1",
                "updated": "2025-01-13T05:53:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    5,
                    53,
                    56,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T05:53:56Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    5,
                    53,
                    56,
                    0,
                    13,
                    0
                ],
                "title": "Value Compass Leaderboard: A Platform for Fundamental and Validated\n  Evaluation of LLMs Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Compass Leaderboard: A Platform for Fundamental and Validated\n  Evaluation of LLMs Values"
                },
                "summary": "As Large Language Models (LLMs) achieve remarkable breakthroughs, aligning\ntheir values with humans has become imperative for their responsible\ndevelopment and customized applications. However, there still lack evaluations\nof LLMs values that fulfill three desirable goals. (1) Value Clarification: We\nexpect to clarify the underlying values of LLMs precisely and comprehensively,\nwhile current evaluations focus narrowly on safety risks such as bias and\ntoxicity. (2) Evaluation Validity: Existing static, open-source benchmarks are\nprone to data contamination and quickly become obsolete as LLMs evolve.\nAdditionally, these discriminative evaluations uncover LLMs' knowledge about\nvalues, rather than valid assessments of LLMs' behavioral conformity to values.\n(3) Value Pluralism: The pluralistic nature of human values across individuals\nand cultures is largely ignored in measuring LLMs value alignment. To address\nthese challenges, we presents the Value Compass Leaderboard, with three\ncorrespondingly designed modules. It (i) grounds the evaluation on\nmotivationally distinct \\textit{basic values to clarify LLMs' underlying values\nfrom a holistic view; (ii) applies a \\textit{generative evolving evaluation\nframework with adaptive test items for evolving LLMs and direct value\nrecognition from behaviors in realistic scenarios; (iii) propose a metric that\nquantifies LLMs alignment with a specific value as a weighted sum over multiple\ndimensions, with weights determined by pluralistic values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) achieve remarkable breakthroughs, aligning\ntheir values with humans has become imperative for their responsible\ndevelopment and customized applications. However, there still lack evaluations\nof LLMs values that fulfill three desirable goals. (1) Value Clarification: We\nexpect to clarify the underlying values of LLMs precisely and comprehensively,\nwhile current evaluations focus narrowly on safety risks such as bias and\ntoxicity. (2) Evaluation Validity: Existing static, open-source benchmarks are\nprone to data contamination and quickly become obsolete as LLMs evolve.\nAdditionally, these discriminative evaluations uncover LLMs' knowledge about\nvalues, rather than valid assessments of LLMs' behavioral conformity to values.\n(3) Value Pluralism: The pluralistic nature of human values across individuals\nand cultures is largely ignored in measuring LLMs value alignment. To address\nthese challenges, we presents the Value Compass Leaderboard, with three\ncorrespondingly designed modules. It (i) grounds the evaluation on\nmotivationally distinct \\textit{basic values to clarify LLMs' underlying values\nfrom a holistic view; (ii) applies a \\textit{generative evolving evaluation\nframework with adaptive test items for evolving LLMs and direct value\nrecognition from behaviors in realistic scenarios; (iii) propose a metric that\nquantifies LLMs alignment with a specific value as a weighted sum over multiple\ndimensions, with weights determined by pluralistic values."
                },
                "authors": [
                    {
                        "name": "Jing Yao"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Shitong Duan"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Yuzhuo Bai"
                    },
                    {
                        "name": "Muhua Huang"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07070v1",
                "updated": "2025-01-13T05:48:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    5,
                    48,
                    32,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T05:48:32Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    5,
                    48,
                    32,
                    0,
                    13,
                    0
                ],
                "title": "Enhancing Image Generation Fidelity via Progressive Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Image Generation Fidelity via Progressive Prompts"
                },
                "summary": "The diffusion transformer (DiT) architecture has attracted significant\nattention in image generation, achieving better fidelity, performance, and\ndiversity. However, most existing DiT - based image generation methods focus on\nglobal - aware synthesis, and regional prompt control has been less explored.\nIn this paper, we propose a coarse - to - fine generation pipeline for regional\nprompt - following generation. Specifically, we first utilize the powerful\nlarge language model (LLM) to generate both high - level descriptions of the\nimage (such as content, topic, and objects) and low - level descriptions (such\nas details and style). Then, we explore the influence of cross - attention\nlayers at different depths. We find that deeper layers are always responsible\nfor high - level content control, while shallow layers handle low - level\ncontent control. Various prompts are injected into the proposed regional cross\n- attention control for coarse - to - fine generation. By using the proposed\npipeline, we enhance the controllability of DiT - based image generation.\nExtensive quantitative and qualitative results show that our pipeline can\nimprove the performance of the generated images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The diffusion transformer (DiT) architecture has attracted significant\nattention in image generation, achieving better fidelity, performance, and\ndiversity. However, most existing DiT - based image generation methods focus on\nglobal - aware synthesis, and regional prompt control has been less explored.\nIn this paper, we propose a coarse - to - fine generation pipeline for regional\nprompt - following generation. Specifically, we first utilize the powerful\nlarge language model (LLM) to generate both high - level descriptions of the\nimage (such as content, topic, and objects) and low - level descriptions (such\nas details and style). Then, we explore the influence of cross - attention\nlayers at different depths. We find that deeper layers are always responsible\nfor high - level content control, while shallow layers handle low - level\ncontent control. Various prompts are injected into the proposed regional cross\n- attention control for coarse - to - fine generation. By using the proposed\npipeline, we enhance the controllability of DiT - based image generation.\nExtensive quantitative and qualitative results show that our pipeline can\nimprove the performance of the generated images."
                },
                "authors": [
                    {
                        "name": "Zhen Xiong"
                    },
                    {
                        "name": "Yuqi Li"
                    },
                    {
                        "name": "Chuanguang Yang"
                    },
                    {
                        "name": "Tiao Tan"
                    },
                    {
                        "name": "Zhihong Zhu"
                    },
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Yue Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yue Ma"
                },
                "author": "Yue Ma",
                "arxiv_comment": "Accepted by ICASSP 2025, Github:\n  https://github.com/ZhenXiong-dl/ICASSP2025-RCAC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07063v1",
                "updated": "2025-01-13T05:16:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    5,
                    16,
                    14,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T05:16:14Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    5,
                    16,
                    14,
                    0,
                    13,
                    0
                ],
                "title": "Research on the Online Update Method for Retrieval-Augmented Generation\n  (RAG) Model with Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on the Online Update Method for Retrieval-Augmented Generation\n  (RAG) Model with Incremental Learning"
                },
                "summary": "In the contemporary context of rapid advancements in information technology\nand the exponential growth of data volume, language models are confronted with\nsignificant challenges in effectively navigating the dynamic and ever-evolving\ninformation landscape to update and adapt to novel knowledge in real time. In\nthis work, an online update method is proposed, which is based on the existing\nRetrieval Enhanced Generation (RAG) model with multiple innovation mechanisms.\nFirstly, the dynamic memory is used to capture the emerging data samples, and\nthen gradually integrate them into the core model through a tunable knowledge\ndistillation strategy. At the same time, hierarchical indexing and multi-layer\ngating mechanism are introduced into the retrieval module to ensure that the\nretrieved content is more targeted and accurate. Finally, a multi-stage network\nstructure is established for different types of inputs in the generation stage,\nand cross-attention matching and screening are carried out on the intermediate\nrepresentations of each stage to ensure the effective integration and iterative\nupdate of new and old knowledge. Experimental results show that the proposed\nmethod is better than the existing mainstream comparison models in terms of\nknowledge retention and inference accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the contemporary context of rapid advancements in information technology\nand the exponential growth of data volume, language models are confronted with\nsignificant challenges in effectively navigating the dynamic and ever-evolving\ninformation landscape to update and adapt to novel knowledge in real time. In\nthis work, an online update method is proposed, which is based on the existing\nRetrieval Enhanced Generation (RAG) model with multiple innovation mechanisms.\nFirstly, the dynamic memory is used to capture the emerging data samples, and\nthen gradually integrate them into the core model through a tunable knowledge\ndistillation strategy. At the same time, hierarchical indexing and multi-layer\ngating mechanism are introduced into the retrieval module to ensure that the\nretrieved content is more targeted and accurate. Finally, a multi-stage network\nstructure is established for different types of inputs in the generation stage,\nand cross-attention matching and screening are carried out on the intermediate\nrepresentations of each stage to ensure the effective integration and iterative\nupdate of new and old knowledge. Experimental results show that the proposed\nmethod is better than the existing mainstream comparison models in terms of\nknowledge retention and inference accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxin Fan"
                    },
                    {
                        "name": "Yuxiang Wang"
                    },
                    {
                        "name": "Lipeng Liu"
                    },
                    {
                        "name": "Xirui Tang"
                    },
                    {
                        "name": "Na Sun"
                    },
                    {
                        "name": "Zidong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Zidong Yu"
                },
                "author": "Zidong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04945v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04945v2",
                "updated": "2025-01-13T05:06:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    5,
                    6,
                    10,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-09T03:34:07Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    34,
                    7,
                    3,
                    9,
                    0
                ],
                "title": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of\n  Large Language Models"
                },
                "summary": "It is crucial for large language models (LLMs) to follow instructions that\ninvolve multiple constraints. However, soft constraints are semantically\nrelated and difficult to verify through automated methods. These constraints\nremain a significant challenge for LLMs. To enhance the ability of LLMs to\nfollow soft constraints, we initially design a pipeline to obtain high-quality\noutputs automatically. Additionally, to fully utilize the acquired data, we\nintroduce a training paradigm based on curriculum learning. We experimentally\nevaluate the effectiveness of our methods in improving LLMs' soft constraint\nfollowing ability and analyze the factors driving the improvements. The\ndatasets and code are publicly available at\nhttps://github.com/Rainier-rq/FollowSoftConstraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is crucial for large language models (LLMs) to follow instructions that\ninvolve multiple constraints. However, soft constraints are semantically\nrelated and difficult to verify through automated methods. These constraints\nremain a significant challenge for LLMs. To enhance the ability of LLMs to\nfollow soft constraints, we initially design a pipeline to obtain high-quality\noutputs automatically. Additionally, to fully utilize the acquired data, we\nintroduce a training paradigm based on curriculum learning. We experimentally\nevaluate the effectiveness of our methods in improving LLMs' soft constraint\nfollowing ability and analyze the factors driving the improvements. The\ndatasets and code are publicly available at\nhttps://github.com/Rainier-rq/FollowSoftConstraints."
                },
                "authors": [
                    {
                        "name": "Qingyu Ren"
                    },
                    {
                        "name": "Jie Zeng"
                    },
                    {
                        "name": "Qianyu He"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Weikang Zhou"
                    },
                    {
                        "name": "Zeye Sun"
                    },
                    {
                        "name": "Fei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Yu"
                },
                "author": "Fei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04945v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04945v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16837v2",
                "updated": "2025-01-13T05:04:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    5,
                    4,
                    59,
                    0,
                    13,
                    0
                ],
                "published": "2024-07-23T21:02:38Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    21,
                    2,
                    38,
                    1,
                    205,
                    0
                ],
                "title": "MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs"
                },
                "summary": "The ability to compare objects, scenes, or situations is crucial for\neffective decision-making and problem-solving in everyday life. For instance,\ncomparing the freshness of apples enables better choices during grocery\nshopping while comparing sofa designs helps optimize the aesthetics of our\nliving space. Despite its significance, the comparative capability is largely\nunexplored in artificial general intelligence (AGI). In this paper, we\nintroduce MLLM-CompBench, a benchmark designed to evaluate the comparative\nreasoning capability of multimodal large language models (MLLMs).\nMLLM-CompBench mines and pairs images through visually oriented questions\ncovering eight dimensions of relative comparison: visual attribute, existence,\nstate, emotion, temporality, spatiality, quantity, and quality. We curate a\ncollection of around 40K image pairs using metadata from diverse vision\ndatasets and CLIP similarity scores. These image pairs span a broad array of\nvisual domains, including animals, fashion, sports, and both outdoor and indoor\nscenes. The questions are carefully crafted to discern relative characteristics\nbetween two images and are labeled by human annotators for accuracy and\nrelevance. We use MLLM-CompBench to evaluate recent MLLMs, including\nGPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our results reveal notable\nshortcomings in their comparative abilities. We believe MLLM-COMPBENCH not only\nsheds light on these limitations but also establishes a solid foundation for\nfuture enhancements in the comparative capability of MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to compare objects, scenes, or situations is crucial for\neffective decision-making and problem-solving in everyday life. For instance,\ncomparing the freshness of apples enables better choices during grocery\nshopping while comparing sofa designs helps optimize the aesthetics of our\nliving space. Despite its significance, the comparative capability is largely\nunexplored in artificial general intelligence (AGI). In this paper, we\nintroduce MLLM-CompBench, a benchmark designed to evaluate the comparative\nreasoning capability of multimodal large language models (MLLMs).\nMLLM-CompBench mines and pairs images through visually oriented questions\ncovering eight dimensions of relative comparison: visual attribute, existence,\nstate, emotion, temporality, spatiality, quantity, and quality. We curate a\ncollection of around 40K image pairs using metadata from diverse vision\ndatasets and CLIP similarity scores. These image pairs span a broad array of\nvisual domains, including animals, fashion, sports, and both outdoor and indoor\nscenes. The questions are carefully crafted to discern relative characteristics\nbetween two images and are labeled by human annotators for accuracy and\nrelevance. We use MLLM-CompBench to evaluate recent MLLMs, including\nGPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our results reveal notable\nshortcomings in their comparative abilities. We believe MLLM-COMPBENCH not only\nsheds light on these limitations but also establishes a solid foundation for\nfuture enhancements in the comparative capability of MLLMs."
                },
                "authors": [
                    {
                        "name": "Jihyung Kil"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Justin Lee"
                    },
                    {
                        "name": "Zihe Wang"
                    },
                    {
                        "name": "Kerrie Cheng"
                    },
                    {
                        "name": "Lemeng Wang"
                    },
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Arpita Chowdhury"
                    },
                    {
                        "name": "Wei-Lun Chao"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Lun Chao"
                },
                "author": "Wei-Lun Chao",
                "arxiv_comment": "This paper has been accepted to NeurIPS 2024. The first two authors\n  contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.07572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07572v1",
                "updated": "2025-01-13T18:58:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    58,
                    7,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T18:58:07Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    58,
                    7,
                    0,
                    13,
                    0
                ],
                "title": "WebWalker: Benchmarking LLMs in Web Traversal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WebWalker: Benchmarking LLMs in Web Traversal"
                },
                "summary": "Retrieval-augmented generation (RAG) demonstrates remarkable performance\nacross tasks in open-domain question-answering. However, traditional search\nengines may retrieve shallow content, limiting the ability of LLMs to handle\ncomplex, multi-layered information. To address it, we introduce WebWalkerQA, a\nbenchmark designed to assess the ability of LLMs to perform web traversal. It\nevaluates the capacity of LLMs to traverse a website's subpages to extract\nhigh-quality data systematically. We propose WebWalker, which is a multi-agent\nframework that mimics human-like web navigation through an explore-critic\nparadigm. Extensive experimental results show that WebWalkerQA is challenging\nand demonstrates the effectiveness of RAG combined with WebWalker, through the\nhorizontal and vertical integration in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) demonstrates remarkable performance\nacross tasks in open-domain question-answering. However, traditional search\nengines may retrieve shallow content, limiting the ability of LLMs to handle\ncomplex, multi-layered information. To address it, we introduce WebWalkerQA, a\nbenchmark designed to assess the ability of LLMs to perform web traversal. It\nevaluates the capacity of LLMs to traverse a website's subpages to extract\nhigh-quality data systematically. We propose WebWalker, which is a multi-agent\nframework that mimics human-like web navigation through an explore-critic\nparadigm. Extensive experimental results show that WebWalkerQA is challenging\nand demonstrates the effectiveness of RAG combined with WebWalker, through the\nhorizontal and vertical integration in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Wenbiao Yin"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Zekun Xi"
                    },
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Deyu Zhou"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05451v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05451v2",
                "updated": "2025-01-13T18:45:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    45,
                    57,
                    0,
                    13,
                    0
                ],
                "published": "2024-10-07T19:34:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    19,
                    34,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "SecAlign: Defending Against Prompt Injection with Preference\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SecAlign: Defending Against Prompt Injection with Preference\n  Optimization"
                },
                "summary": "Large language models (LLMs) are becoming increasingly prevalent in modern\nsoftware systems, interfacing between the user and the Internet to assist with\ntasks that require advanced language understanding. To accomplish these tasks,\nthe LLM often uses external data sources such as user documents, web retrieval,\nresults from API calls, etc. This opens up new avenues for attackers to\nmanipulate the LLM via prompt injection. Adversarial prompts can be injected\ninto external data sources to override the system's intended instruction and\ninstead execute a malicious instruction.\n  To mitigate this vulnerability, we propose a new defense called SecAlign\nbased on the technique of preference optimization. Our defense first constructs\na preference dataset with prompt-injected inputs, secure outputs (ones that\nrespond to the legitimate instruction), and insecure outputs (ones that respond\nto the injection). We then perform preference optimization on this dataset to\nteach the LLM to prefer the secure output over the insecure one. This provides\nthe first known method that reduces the success rates of various prompt\ninjections to around 0%, even against attacks much more sophisticated than ones\nseen during training. This indicates our defense generalizes well against\nunknown and yet-to-come attacks. Also, our defended models are still practical\nwith similar utility to the one before our defensive training. Our code is at\nhttps://github.com/facebookresearch/SecAlign",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are becoming increasingly prevalent in modern\nsoftware systems, interfacing between the user and the Internet to assist with\ntasks that require advanced language understanding. To accomplish these tasks,\nthe LLM often uses external data sources such as user documents, web retrieval,\nresults from API calls, etc. This opens up new avenues for attackers to\nmanipulate the LLM via prompt injection. Adversarial prompts can be injected\ninto external data sources to override the system's intended instruction and\ninstead execute a malicious instruction.\n  To mitigate this vulnerability, we propose a new defense called SecAlign\nbased on the technique of preference optimization. Our defense first constructs\na preference dataset with prompt-injected inputs, secure outputs (ones that\nrespond to the legitimate instruction), and insecure outputs (ones that respond\nto the injection). We then perform preference optimization on this dataset to\nteach the LLM to prefer the secure output over the insecure one. This provides\nthe first known method that reduces the success rates of various prompt\ninjections to around 0%, even against attacks much more sophisticated than ones\nseen during training. This indicates our defense generalizes well against\nunknown and yet-to-come attacks. Also, our defended models are still practical\nwith similar utility to the one before our defensive training. Our code is at\nhttps://github.com/facebookresearch/SecAlign"
                },
                "authors": [
                    {
                        "name": "Sizhe Chen"
                    },
                    {
                        "name": "Arman Zharmagambetov"
                    },
                    {
                        "name": "Saeed Mahloujifar"
                    },
                    {
                        "name": "Kamalika Chaudhuri"
                    },
                    {
                        "name": "David Wagner"
                    },
                    {
                        "name": "Chuan Guo"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Guo"
                },
                "author": "Chuan Guo",
                "arxiv_comment": "Key words: prompt injection defense, LLM security, LLM-integrated\n  applications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05451v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05451v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07554v1",
                "updated": "2025-01-13T18:37:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    37,
                    8,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T18:37:08Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    37,
                    8,
                    0,
                    13,
                    0
                ],
                "title": "SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal\n  Aspects in Video Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SST-EM: Advanced Metrics for Evaluating Semantic, Spatial and Temporal\n  Aspects in Video Editing"
                },
                "summary": "Video editing models have advanced significantly, but evaluating their\nperformance remains challenging. Traditional metrics, such as CLIP text and\nimage scores, often fall short: text scores are limited by inadequate training\ndata and hierarchical dependencies, while image scores fail to assess temporal\nconsistency. We present SST-EM (Semantic, Spatial, and Temporal Evaluation\nMetric), a novel evaluation framework that leverages modern Vision-Language\nModels (VLMs), Object Detection, and Temporal Consistency checks. SST-EM\ncomprises four components: (1) semantic extraction from frames using a VLM, (2)\nprimary object tracking with Object Detection, (3) focused object refinement\nvia an LLM agent, and (4) temporal consistency assessment using a Vision\nTransformer (ViT). These components are integrated into a unified metric with\nweights derived from human evaluations and regression analysis. The name SST-EM\nreflects its focus on Semantic, Spatial, and Temporal aspects of video\nevaluation. SST-EM provides a comprehensive evaluation of semantic fidelity and\ntemporal smoothness in video editing. The source code is available in the\n\\textbf{\\href{https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub\nRepository}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video editing models have advanced significantly, but evaluating their\nperformance remains challenging. Traditional metrics, such as CLIP text and\nimage scores, often fall short: text scores are limited by inadequate training\ndata and hierarchical dependencies, while image scores fail to assess temporal\nconsistency. We present SST-EM (Semantic, Spatial, and Temporal Evaluation\nMetric), a novel evaluation framework that leverages modern Vision-Language\nModels (VLMs), Object Detection, and Temporal Consistency checks. SST-EM\ncomprises four components: (1) semantic extraction from frames using a VLM, (2)\nprimary object tracking with Object Detection, (3) focused object refinement\nvia an LLM agent, and (4) temporal consistency assessment using a Vision\nTransformer (ViT). These components are integrated into a unified metric with\nweights derived from human evaluations and regression analysis. The name SST-EM\nreflects its focus on Semantic, Spatial, and Temporal aspects of video\nevaluation. SST-EM provides a comprehensive evaluation of semantic fidelity and\ntemporal smoothness in video editing. The source code is available in the\n\\textbf{\\href{https://github.com/custommetrics-sst/SST_CustomEvaluationMetrics.git}{GitHub\nRepository}}."
                },
                "authors": [
                    {
                        "name": "Varun Biyyala"
                    },
                    {
                        "name": "Bharat Chanderprakash Kathuria"
                    },
                    {
                        "name": "Jialu Li"
                    },
                    {
                        "name": "Youshan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Youshan Zhang"
                },
                "author": "Youshan Zhang",
                "arxiv_comment": "WACV workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07542v1",
                "updated": "2025-01-13T18:23:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    23,
                    57,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T18:23:57Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    23,
                    57,
                    0,
                    13,
                    0
                ],
                "title": "Imagine while Reasoning in Space: Multimodal Visualization-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Imagine while Reasoning in Space: Multimodal Visualization-of-Thought"
                },
                "summary": "Chain-of-Thought (CoT) prompting has proven highly effective for enhancing\ncomplex reasoning in Large Language Models (LLMs) and Multimodal Large Language\nModels (MLLMs). Yet, it struggles in complex spatial reasoning tasks.\nNonetheless, human cognition extends beyond language alone, enabling the\nremarkable capability to think in both words and images. Inspired by this\nmechanism, we propose a new reasoning paradigm, Multimodal\nVisualization-of-Thought (MVoT). It enables visual thinking in MLLMs by\ngenerating image visualizations of their reasoning traces. To ensure\nhigh-quality visualization, we introduce token discrepancy loss into\nautoregressive MLLMs. This innovation significantly improves both visual\ncoherence and fidelity. We validate this approach through several dynamic\nspatial reasoning tasks. Experimental results reveal that MVoT demonstrates\ncompetitive performance across tasks. Moreover, it exhibits robust and reliable\nimprovements in the most challenging scenarios where CoT fails. Ultimately,\nMVoT establishes new possibilities for complex reasoning tasks where visual\nthinking can effectively complement verbal reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) prompting has proven highly effective for enhancing\ncomplex reasoning in Large Language Models (LLMs) and Multimodal Large Language\nModels (MLLMs). Yet, it struggles in complex spatial reasoning tasks.\nNonetheless, human cognition extends beyond language alone, enabling the\nremarkable capability to think in both words and images. Inspired by this\nmechanism, we propose a new reasoning paradigm, Multimodal\nVisualization-of-Thought (MVoT). It enables visual thinking in MLLMs by\ngenerating image visualizations of their reasoning traces. To ensure\nhigh-quality visualization, we introduce token discrepancy loss into\nautoregressive MLLMs. This innovation significantly improves both visual\ncoherence and fidelity. We validate this approach through several dynamic\nspatial reasoning tasks. Experimental results reveal that MVoT demonstrates\ncompetitive performance across tasks. Moreover, it exhibits robust and reliable\nimprovements in the most challenging scenarios where CoT fails. Ultimately,\nMVoT establishes new possibilities for complex reasoning tasks where visual\nthinking can effectively complement verbal reasoning."
                },
                "authors": [
                    {
                        "name": "Chengzu Li"
                    },
                    {
                        "name": "Wenshan Wu"
                    },
                    {
                        "name": "Huanyu Zhang"
                    },
                    {
                        "name": "Yan Xia"
                    },
                    {
                        "name": "Shaoguang Mao"
                    },
                    {
                        "name": "Li Dong"
                    },
                    {
                        "name": "Ivan VuliÄ"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "11 pages, 6 figures, 4 tables (27 pages, 10 figures, 16 tables\n  including references and appendices)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07532v1",
                "updated": "2025-01-13T18:09:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    9,
                    58,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T18:09:58Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    9,
                    58,
                    0,
                    13,
                    0
                ],
                "title": "Investigating Large Language Models in Inferring Personality Traits from\n  User Conversations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Large Language Models in Inferring Personality Traits from\n  User Conversations"
                },
                "summary": "Large Language Models (LLMs) are demonstrating remarkable human like\ncapabilities across diverse domains, including psychological assessment. This\nstudy evaluates whether LLMs, specifically GPT-4o and GPT-4o mini, can infer\nBig Five personality traits and generate Big Five Inventory-10 (BFI-10) item\nscores from user conversations under zero-shot prompting conditions. Our\nfindings reveal that incorporating an intermediate step--prompting for BFI-10\nitem scores before calculating traits--enhances accuracy and aligns more\nclosely with the gold standard than direct trait inference. This structured\napproach underscores the importance of leveraging psychological frameworks in\nimproving predictive precision. Additionally, a group comparison based on\ndepressive symptom presence revealed differential model performance.\nParticipants were categorized into two groups: those experiencing at least one\ndepressive symptom and those without symptoms. GPT-4o mini demonstrated\nheightened sensitivity to depression-related shifts in traits such as\nNeuroticism and Conscientiousness within the symptom-present group, whereas\nGPT-4o exhibited strengths in nuanced interpretation across groups. These\nfindings underscore the potential of LLMs to analyze real-world psychological\ndata effectively, offering a valuable foundation for interdisciplinary research\nat the intersection of artificial intelligence and psychology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are demonstrating remarkable human like\ncapabilities across diverse domains, including psychological assessment. This\nstudy evaluates whether LLMs, specifically GPT-4o and GPT-4o mini, can infer\nBig Five personality traits and generate Big Five Inventory-10 (BFI-10) item\nscores from user conversations under zero-shot prompting conditions. Our\nfindings reveal that incorporating an intermediate step--prompting for BFI-10\nitem scores before calculating traits--enhances accuracy and aligns more\nclosely with the gold standard than direct trait inference. This structured\napproach underscores the importance of leveraging psychological frameworks in\nimproving predictive precision. Additionally, a group comparison based on\ndepressive symptom presence revealed differential model performance.\nParticipants were categorized into two groups: those experiencing at least one\ndepressive symptom and those without symptoms. GPT-4o mini demonstrated\nheightened sensitivity to depression-related shifts in traits such as\nNeuroticism and Conscientiousness within the symptom-present group, whereas\nGPT-4o exhibited strengths in nuanced interpretation across groups. These\nfindings underscore the potential of LLMs to analyze real-world psychological\ndata effectively, offering a valuable foundation for interdisciplinary research\nat the intersection of artificial intelligence and psychology."
                },
                "authors": [
                    {
                        "name": "Jianfeng Zhu"
                    },
                    {
                        "name": "Ruoming Jin"
                    },
                    {
                        "name": "Karin G. Coifman"
                    }
                ],
                "author_detail": {
                    "name": "Karin G. Coifman"
                },
                "author": "Karin G. Coifman",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07531v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07531v1",
                "updated": "2025-01-13T18:09:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    9,
                    25,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T18:09:25Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    9,
                    25,
                    0,
                    13,
                    0
                ],
                "title": "Evaluating Agent-based Program Repair at Google",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Agent-based Program Repair at Google"
                },
                "summary": "Agent-based program repair offers to automatically resolve complex bugs\nend-to-end by combining the planning, tool use, and code generation abilities\nof modern LLMs. Recent work has explored the use of agent-based repair\napproaches on the popular open-source SWE-Bench, a collection of bugs from\nhighly-rated GitHub Python projects. In addition, various agentic approaches\nsuch as SWE-Agent have been proposed to solve bugs in this benchmark. This\npaper explores the viability of using an agentic approach to address bugs in an\nenterprise context. To investigate this, we curate an evaluation set of 178\nbugs drawn from Google's issue tracking system. This dataset spans both\nhuman-reported (78) and machine-reported bugs (100).\n  To establish a repair performance baseline on this benchmark, we implement\nPasserine, an agent similar in spirit to SWE-Agent that can work within\nGoogle's development environment. We show that with 20 trajectory samples and\nGemini 1.5 Pro, Passerine can produce a patch that passes bug tests (i.e.,\nplausible) for 73% of machine-reported and 25.6% of human-reported bugs in our\nevaluation set. After manual examination, we found that 43% of machine-reported\nbugs and 17.9% of human-reported bugs have at least one patch that is\nsemantically equivalent to the ground-truth patch.\n  These results establish a baseline on an industrially relevant benchmark,\nwhich as we show, contains bugs drawn from a different distribution -- in terms\nof language diversity, size, and spread of changes, etc. -- compared to those\nin the popular SWE-Bench dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent-based program repair offers to automatically resolve complex bugs\nend-to-end by combining the planning, tool use, and code generation abilities\nof modern LLMs. Recent work has explored the use of agent-based repair\napproaches on the popular open-source SWE-Bench, a collection of bugs from\nhighly-rated GitHub Python projects. In addition, various agentic approaches\nsuch as SWE-Agent have been proposed to solve bugs in this benchmark. This\npaper explores the viability of using an agentic approach to address bugs in an\nenterprise context. To investigate this, we curate an evaluation set of 178\nbugs drawn from Google's issue tracking system. This dataset spans both\nhuman-reported (78) and machine-reported bugs (100).\n  To establish a repair performance baseline on this benchmark, we implement\nPasserine, an agent similar in spirit to SWE-Agent that can work within\nGoogle's development environment. We show that with 20 trajectory samples and\nGemini 1.5 Pro, Passerine can produce a patch that passes bug tests (i.e.,\nplausible) for 73% of machine-reported and 25.6% of human-reported bugs in our\nevaluation set. After manual examination, we found that 43% of machine-reported\nbugs and 17.9% of human-reported bugs have at least one patch that is\nsemantically equivalent to the ground-truth patch.\n  These results establish a baseline on an industrially relevant benchmark,\nwhich as we show, contains bugs drawn from a different distribution -- in terms\nof language diversity, size, and spread of changes, etc. -- compared to those\nin the popular SWE-Bench dataset."
                },
                "authors": [
                    {
                        "name": "Pat Rondon"
                    },
                    {
                        "name": "Renyao Wei"
                    },
                    {
                        "name": "JosÃ© Cambronero"
                    },
                    {
                        "name": "JÃ¼rgen Cito"
                    },
                    {
                        "name": "Aaron Sun"
                    },
                    {
                        "name": "Siddhant Sanyam"
                    },
                    {
                        "name": "Michele Tufano"
                    },
                    {
                        "name": "Satish Chandra"
                    }
                ],
                "author_detail": {
                    "name": "Satish Chandra"
                },
                "author": "Satish Chandra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07531v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07531v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17752v2",
                "updated": "2025-01-13T18:03:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    18,
                    3,
                    46,
                    0,
                    13,
                    0
                ],
                "published": "2024-11-25T16:20:49Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    16,
                    20,
                    49,
                    0,
                    330,
                    0
                ],
                "title": "Path Loss Prediction Using Deep Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Path Loss Prediction Using Deep Learning"
                },
                "summary": "Radio deployments and spectrum planning benefit from path loss predictions.\nObstructions along a communications link are often considered implicitly or\nthrough derived metrics such as representative clutter height or total\nobstruction depth. In this paper, we propose a path-specific path loss\nprediction method that uses convolutional neural networks to automatically\nperform feature extraction from high-resolution obstruction height maps. Our\nmethods result in low prediction error in a variety of environments without\nrequiring derived metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio deployments and spectrum planning benefit from path loss predictions.\nObstructions along a communications link are often considered implicitly or\nthrough derived metrics such as representative clutter height or total\nobstruction depth. In this paper, we propose a path-specific path loss\nprediction method that uses convolutional neural networks to automatically\nperform feature extraction from high-resolution obstruction height maps. Our\nmethods result in low prediction error in a variety of environments without\nrequiring derived metrics."
                },
                "authors": [
                    {
                        "name": "Ryan G. Dempsey"
                    },
                    {
                        "name": "Jonathan Ethier"
                    },
                    {
                        "name": "Halim Yanikomeroglu"
                    }
                ],
                "author_detail": {
                    "name": "Halim Yanikomeroglu"
                },
                "author": "Halim Yanikomeroglu",
                "arxiv_comment": "5 pages, 3 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07525v1",
                "updated": "2025-01-13T17:55:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    55,
                    32,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T17:55:32Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    55,
                    32,
                    0,
                    13,
                    0
                ],
                "title": "RadAlign: Advancing Radiology Report Generation with Vision-Language\n  Concept Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RadAlign: Advancing Radiology Report Generation with Vision-Language\n  Concept Alignment"
                },
                "summary": "Automated chest radiographs interpretation requires both accurate disease\nclassification and detailed radiology report generation, presenting a\nsignificant challenge in the clinical workflow. Current approaches either focus\non classification accuracy at the expense of interpretability or generate\ndetailed but potentially unreliable reports through image captioning\ntechniques. In this study, we present RadAlign, a novel framework that combines\nthe predictive accuracy of vision-language models (VLMs) with the reasoning\ncapabilities of large language models (LLMs). Inspired by the radiologist's\nworkflow, RadAlign first employs a specialized VLM to align visual features\nwith key medical concepts, achieving superior disease classification with an\naverage AUC of 0.885 across multiple diseases. These recognized medical\nconditions, represented as text-based concepts in the aligned visual-language\nspace, are then used to prompt LLM-based report generation. Enhanced by a\nretrieval-augmented generation mechanism that grounds outputs in similar\nhistorical cases, RadAlign delivers superior report quality with a GREEN score\nof 0.678, outperforming state-of-the-art methods' 0.634. Our framework\nmaintains strong clinical interpretability while reducing hallucinations,\nadvancing automated medical imaging and report analysis through integrated\npredictive and generative AI. Code is available at\nhttps://github.com/difeigu/RadAlign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated chest radiographs interpretation requires both accurate disease\nclassification and detailed radiology report generation, presenting a\nsignificant challenge in the clinical workflow. Current approaches either focus\non classification accuracy at the expense of interpretability or generate\ndetailed but potentially unreliable reports through image captioning\ntechniques. In this study, we present RadAlign, a novel framework that combines\nthe predictive accuracy of vision-language models (VLMs) with the reasoning\ncapabilities of large language models (LLMs). Inspired by the radiologist's\nworkflow, RadAlign first employs a specialized VLM to align visual features\nwith key medical concepts, achieving superior disease classification with an\naverage AUC of 0.885 across multiple diseases. These recognized medical\nconditions, represented as text-based concepts in the aligned visual-language\nspace, are then used to prompt LLM-based report generation. Enhanced by a\nretrieval-augmented generation mechanism that grounds outputs in similar\nhistorical cases, RadAlign delivers superior report quality with a GREEN score\nof 0.678, outperforming state-of-the-art methods' 0.634. Our framework\nmaintains strong clinical interpretability while reducing hallucinations,\nadvancing automated medical imaging and report analysis through integrated\npredictive and generative AI. Code is available at\nhttps://github.com/difeigu/RadAlign."
                },
                "authors": [
                    {
                        "name": "Difei Gu"
                    },
                    {
                        "name": "Yunhe Gao"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Mu Zhou"
                    },
                    {
                        "name": "Dimitris Metaxas"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Metaxas"
                },
                "author": "Dimitris Metaxas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v1",
                "updated": "2025-01-13T17:50:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09698v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09698v5",
                "updated": "2025-01-13T17:48:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    48,
                    9,
                    0,
                    13,
                    0
                ],
                "published": "2024-08-19T04:44:32Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    4,
                    44,
                    32,
                    0,
                    232,
                    0
                ],
                "title": "Harnessing Multimodal Large Language Models for Multimodal Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Multimodal Large Language Models for Multimodal Sequential\n  Recommendation"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have demonstrated significant\npotential in the field of Recommendation Systems (RSs). Most existing studies\nhave focused on converting user behavior logs into textual prompts and\nleveraging techniques such as prompt tuning to enable LLMs for recommendation\ntasks. Meanwhile, research interest has recently grown in multimodal\nrecommendation systems that integrate data from images, text, and other sources\nusing modality fusion techniques. This introduces new challenges to the\nexisting LLM-based recommendation paradigm which relies solely on text modality\ninformation. Moreover, although Multimodal Large Language Models (MLLMs)\ncapable of processing multi-modal inputs have emerged, how to equip MLLMs with\nmulti-modal recommendation capabilities remains largely unexplored. To this\nend, in this paper, we propose the Multimodal Large Language Model-enhanced\nMultimodaln Sequential Recommendation (MLLM-MSR) model. To capture the dynamic\nuser preference, we design a two-stage user preference summarization method.\nSpecifically, we first utilize an MLLM-based item-summarizer to extract image\nfeature given an item and convert the image into text. Then, we employ a\nrecurrent user preference summarization generation paradigm to capture the\ndynamic changes in user preferences based on an LLM-based user-summarizer.\nFinally, to enable the MLLM for multi-modal recommendation task, we propose to\nfine-tune a MLLM-based recommender using Supervised Fine-Tuning (SFT)\ntechniques. Extensive evaluations across various datasets validate the\neffectiveness of MLLM-MSR, showcasing its superior ability to capture and adapt\nto the evolving dynamics of user preferences."
                },
                "authors": [
                    {
                        "name": "Yuyang Ye"
                    },
                    {
                        "name": "Zhi Zheng"
                    },
                    {
                        "name": "Yishan Shen"
                    },
                    {
                        "name": "Tianshu Wang"
                    },
                    {
                        "name": "Hengruo Zhang"
                    },
                    {
                        "name": "Peijun Zhu"
                    },
                    {
                        "name": "Runlong Yu"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09698v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09698v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07493v1",
                "updated": "2025-01-13T17:12:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    12,
                    38,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T17:12:38Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    12,
                    38,
                    0,
                    13,
                    0
                ],
                "title": "Exploring and Mitigating Adversarial Manipulation of Voting-Based\n  Leaderboards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring and Mitigating Adversarial Manipulation of Voting-Based\n  Leaderboards"
                },
                "summary": "It is now common to evaluate Large Language Models (LLMs) by having humans\nmanually vote to evaluate model outputs, in contrast to typical benchmarks that\nevaluate knowledge or skill at some particular task. Chatbot Arena, the most\npopular benchmark of this type, ranks models by asking users to select the\nbetter response between two randomly selected models (without revealing which\nmodel was responsible for the generations). These platforms are widely trusted\nas a fair and accurate measure of LLM capabilities. In this paper, we show that\nif bot protection and other defenses are not implemented, these voting-based\nbenchmarks are potentially vulnerable to adversarial manipulation.\nSpecifically, we show that an attacker can alter the leaderboard (to promote\ntheir favorite model or demote competitors) at the cost of roughly a thousand\nvotes (verified in a simulated, offline version of Chatbot Arena). Our attack\nconsists of two steps: first, we show how an attacker can determine which model\nwas used to generate a given reply with more than $95\\%$ accuracy; and then,\nthe attacker can use this information to consistently vote for (or against) a\ntarget model. Working with the Chatbot Arena developers, we identify, propose,\nand implement mitigations to improve the robustness of Chatbot Arena against\nadversarial manipulation, which, based on our analysis, substantially increases\nthe cost of such attacks. Some of these defenses were present before our\ncollaboration, such as bot protection with Cloudflare, malicious user\ndetection, and rate limiting. Others, including reCAPTCHA and login are being\nintegrated to strengthen the security in Chatbot Arena.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is now common to evaluate Large Language Models (LLMs) by having humans\nmanually vote to evaluate model outputs, in contrast to typical benchmarks that\nevaluate knowledge or skill at some particular task. Chatbot Arena, the most\npopular benchmark of this type, ranks models by asking users to select the\nbetter response between two randomly selected models (without revealing which\nmodel was responsible for the generations). These platforms are widely trusted\nas a fair and accurate measure of LLM capabilities. In this paper, we show that\nif bot protection and other defenses are not implemented, these voting-based\nbenchmarks are potentially vulnerable to adversarial manipulation.\nSpecifically, we show that an attacker can alter the leaderboard (to promote\ntheir favorite model or demote competitors) at the cost of roughly a thousand\nvotes (verified in a simulated, offline version of Chatbot Arena). Our attack\nconsists of two steps: first, we show how an attacker can determine which model\nwas used to generate a given reply with more than $95\\%$ accuracy; and then,\nthe attacker can use this information to consistently vote for (or against) a\ntarget model. Working with the Chatbot Arena developers, we identify, propose,\nand implement mitigations to improve the robustness of Chatbot Arena against\nadversarial manipulation, which, based on our analysis, substantially increases\nthe cost of such attacks. Some of these defenses were present before our\ncollaboration, such as bot protection with Cloudflare, malicious user\ndetection, and rate limiting. Others, including reCAPTCHA and login are being\nintegrated to strengthen the security in Chatbot Arena."
                },
                "authors": [
                    {
                        "name": "Yangsibo Huang"
                    },
                    {
                        "name": "Milad Nasr"
                    },
                    {
                        "name": "Anastasios Angelopoulos"
                    },
                    {
                        "name": "Nicholas Carlini"
                    },
                    {
                        "name": "Wei-Lin Chiang"
                    },
                    {
                        "name": "Christopher A. Choquette-Choo"
                    },
                    {
                        "name": "Daphne Ippolito"
                    },
                    {
                        "name": "Matthew Jagielski"
                    },
                    {
                        "name": "Katherine Lee"
                    },
                    {
                        "name": "Ken Ziyu Liu"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Florian Tramer"
                    },
                    {
                        "name": "Chiyuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chiyuan Zhang"
                },
                "author": "Chiyuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.09998v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.09998v5",
                "updated": "2025-01-13T17:01:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    1,
                    23,
                    0,
                    13,
                    0
                ],
                "published": "2023-07-19T14:13:02Z",
                "published_parsed": [
                    2023,
                    7,
                    19,
                    14,
                    13,
                    2,
                    2,
                    200,
                    0
                ],
                "title": "Controlling Equational Reasoning in Large Language Models with Prompt\n  Interventions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controlling Equational Reasoning in Large Language Models with Prompt\n  Interventions"
                },
                "summary": "This paper investigates how hallucination rates in Large Language Models\n(LLMs) may be controlled via a symbolic data generation framework, exploring a\nfundamental relationship between the rate of certain mathematical errors and\ntypes of input intervention. Specifically, we systematically generate data for\na derivation generation task using a symbolic engine, applying targeted\ninterventions to prompts to perturb features of mathematical derivations such\nas the surface forms of symbols, equational tree structures, and mathematical\ncontext. We then evaluate the effect of prompt interventions across a range of\nLLMs including fine-tuned T5 models, GPT, and LLaMa-based models. Our\nexperiments suggest that T5-Large can outperform the few-shot performance of\nGPT-4 on various evaluation sets generated via the framework. However, an\nextensive evaluation based on human analysis, template-based error detection,\nand text generation metrics reveals model weaknesses beyond what the\nreference-based metrics singularly describe. We use these results to tie\ncharacteristic distributional footprints of interventions to the human\nevaluation of LLM derivation quality, potentially leading to significant\ncontrol over fine-grained mathematical capabilities of language models with\nrespect to specific types of errors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates how hallucination rates in Large Language Models\n(LLMs) may be controlled via a symbolic data generation framework, exploring a\nfundamental relationship between the rate of certain mathematical errors and\ntypes of input intervention. Specifically, we systematically generate data for\na derivation generation task using a symbolic engine, applying targeted\ninterventions to prompts to perturb features of mathematical derivations such\nas the surface forms of symbols, equational tree structures, and mathematical\ncontext. We then evaluate the effect of prompt interventions across a range of\nLLMs including fine-tuned T5 models, GPT, and LLaMa-based models. Our\nexperiments suggest that T5-Large can outperform the few-shot performance of\nGPT-4 on various evaluation sets generated via the framework. However, an\nextensive evaluation based on human analysis, template-based error detection,\nand text generation metrics reveals model weaknesses beyond what the\nreference-based metrics singularly describe. We use these results to tie\ncharacteristic distributional footprints of interventions to the human\nevaluation of LLM derivation quality, potentially leading to significant\ncontrol over fine-grained mathematical capabilities of language models with\nrespect to specific types of errors."
                },
                "authors": [
                    {
                        "name": "Jordan Meadows"
                    },
                    {
                        "name": "Marco Valentino"
                    },
                    {
                        "name": "Andre Freitas"
                    }
                ],
                "author_detail": {
                    "name": "Andre Freitas"
                },
                "author": "Andre Freitas",
                "arxiv_comment": "AAAI 2025 (7 pages)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.09998v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.09998v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.HO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07482v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07482v1",
                "updated": "2025-01-13T16:58:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    16,
                    58,
                    32,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T16:58:32Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    16,
                    58,
                    32,
                    0,
                    13,
                    0
                ],
                "title": "TiEBe: A Benchmark for Assessing the Current Knowledge of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TiEBe: A Benchmark for Assessing the Current Knowledge of Large Language\n  Models"
                },
                "summary": "In a rapidly evolving knowledge landscape and the increasing adoption of\nlarge language models, a need has emerged to keep these models continuously\nupdated with current events. While existing benchmarks evaluate general factual\nrecall, they often overlook two critical aspects: the ability of models to\nintegrate evolving knowledge through continual learning and the significant\nregional disparities in their performance. To address these gaps, we introduce\nthe Timely Events Benchmark (TiEBe), a dataset containing over 11,000\nquestion-answer pairs focused on globally and regionally significant events.\nTiEBe leverages structured retrospective data from Wikipedia, enabling\ncontinuous updates to assess LLMs' knowledge of evolving global affairs and\ntheir understanding of events across different regions. Our benchmark\ndemonstrates that LLMs exhibit substantial geographic disparities in factual\nrecall, emphasizing the need for more balanced global knowledge representation.\nFurthermore, TiEBe serves as a tool for evaluating continual learning\nstrategies, providing insights into models' ability to acquire new information\nwithout forgetting past knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a rapidly evolving knowledge landscape and the increasing adoption of\nlarge language models, a need has emerged to keep these models continuously\nupdated with current events. While existing benchmarks evaluate general factual\nrecall, they often overlook two critical aspects: the ability of models to\nintegrate evolving knowledge through continual learning and the significant\nregional disparities in their performance. To address these gaps, we introduce\nthe Timely Events Benchmark (TiEBe), a dataset containing over 11,000\nquestion-answer pairs focused on globally and regionally significant events.\nTiEBe leverages structured retrospective data from Wikipedia, enabling\ncontinuous updates to assess LLMs' knowledge of evolving global affairs and\ntheir understanding of events across different regions. Our benchmark\ndemonstrates that LLMs exhibit substantial geographic disparities in factual\nrecall, emphasizing the need for more balanced global knowledge representation.\nFurthermore, TiEBe serves as a tool for evaluating continual learning\nstrategies, providing insights into models' ability to acquire new information\nwithout forgetting past knowledge."
                },
                "authors": [
                    {
                        "name": "Thales Sales Almeida"
                    },
                    {
                        "name": "Giovana Kerche BonÃ¡s"
                    },
                    {
                        "name": "JoÃ£o Guilherme Alves Santos"
                    },
                    {
                        "name": "Hugo Abonizio"
                    },
                    {
                        "name": "Rodrigo Nogueira"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Nogueira"
                },
                "author": "Rodrigo Nogueira",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07482v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16314v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16314v3",
                "updated": "2025-01-13T16:53:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    16,
                    53,
                    2,
                    0,
                    13,
                    0
                ],
                "published": "2024-10-09T10:09:37Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    10,
                    9,
                    37,
                    2,
                    283,
                    0
                ],
                "title": "Steering Large Language Models using Conceptors: Improving\n  Addition-Based Activation Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steering Large Language Models using Conceptors: Improving\n  Addition-Based Activation Engineering"
                },
                "summary": "Large language models have transformed AI, yet reliably controlling their\noutputs remains a challenge. This paper explores activation engineering, where\noutputs of pre-trained LLMs are controlled by manipulating their activations at\ninference time. Unlike traditional methods using a single steering vector, we\nintroduce conceptors - mathematical constructs that represent sets of\nactivation vectors as ellipsoidal regions. Conceptors act as soft projection\nmatrices and offer more precise control over complex activation patterns. Our\nexperiments demonstrate that conceptors outperform traditional methods across\nmultiple steering tasks. We further use Boolean operations on conceptors for\ncombined steering goals that empirically outperform additively combining\nsteering vectors on a set of tasks. These results highlight conceptors as a\npromising tool for more effective steering of LLMs. Our code is available on\ngithub.com/jorispos/conceptorsteering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have transformed AI, yet reliably controlling their\noutputs remains a challenge. This paper explores activation engineering, where\noutputs of pre-trained LLMs are controlled by manipulating their activations at\ninference time. Unlike traditional methods using a single steering vector, we\nintroduce conceptors - mathematical constructs that represent sets of\nactivation vectors as ellipsoidal regions. Conceptors act as soft projection\nmatrices and offer more precise control over complex activation patterns. Our\nexperiments demonstrate that conceptors outperform traditional methods across\nmultiple steering tasks. We further use Boolean operations on conceptors for\ncombined steering goals that empirically outperform additively combining\nsteering vectors on a set of tasks. These results highlight conceptors as a\npromising tool for more effective steering of LLMs. Our code is available on\ngithub.com/jorispos/conceptorsteering."
                },
                "authors": [
                    {
                        "name": "Joris Postmus"
                    },
                    {
                        "name": "Steven Abreu"
                    }
                ],
                "author_detail": {
                    "name": "Steven Abreu"
                },
                "author": "Steven Abreu",
                "arxiv_comment": "Presented at the MINT workshop at NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16314v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16314v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07458v1",
                "updated": "2025-01-13T16:28:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    16,
                    28,
                    1,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T16:28:01Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    16,
                    28,
                    1,
                    0,
                    13,
                    0
                ],
                "title": "Understanding and Benchmarking Artificial Intelligence: OpenAI's o3 Is\n  Not AGI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and Benchmarking Artificial Intelligence: OpenAI's o3 Is\n  Not AGI"
                },
                "summary": "OpenAI's o3 achieves a high score of 87.5 % on ARC-AGI, a benchmark proposed\nto measure intelligence. This raises the question whether systems based on\nLarge Language Models (LLMs), particularly o3, demonstrate intelligence and\nprogress towards artificial general intelligence (AGI). Building on the\ndistinction between skills and intelligence made by Fran\\c{c}ois Chollet, the\ncreator of ARC-AGI, a new understanding of intelligence is introduced: an agent\nis the more intelligent, the more efficiently it can achieve the more diverse\ngoals in the more diverse worlds with the less knowledge. An analysis of the\nARC-AGI benchmark shows that its tasks represent a very specific type of\nproblem that can be solved by massive trialling of combinations of predefined\noperations. This method is also applied by o3, achieving its high score through\nthe extensive use of computing power. However, for most problems in the\nphysical world and in the human domain, solutions cannot be tested in advance\nand predefined operations are not available. Consequently, massive trialling of\npredefined operations, as o3 does, cannot be a basis for AGI - instead, new\napproaches are required that can reliably solve a wide variety of problems\nwithout existing skills. To support this development, a new benchmark for\nintelligence is outlined that covers a much higher diversity of unknown tasks\nto be solved, thus enabling a comprehensive assessment of intelligence and of\nprogress towards AGI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenAI's o3 achieves a high score of 87.5 % on ARC-AGI, a benchmark proposed\nto measure intelligence. This raises the question whether systems based on\nLarge Language Models (LLMs), particularly o3, demonstrate intelligence and\nprogress towards artificial general intelligence (AGI). Building on the\ndistinction between skills and intelligence made by Fran\\c{c}ois Chollet, the\ncreator of ARC-AGI, a new understanding of intelligence is introduced: an agent\nis the more intelligent, the more efficiently it can achieve the more diverse\ngoals in the more diverse worlds with the less knowledge. An analysis of the\nARC-AGI benchmark shows that its tasks represent a very specific type of\nproblem that can be solved by massive trialling of combinations of predefined\noperations. This method is also applied by o3, achieving its high score through\nthe extensive use of computing power. However, for most problems in the\nphysical world and in the human domain, solutions cannot be tested in advance\nand predefined operations are not available. Consequently, massive trialling of\npredefined operations, as o3 does, cannot be a basis for AGI - instead, new\napproaches are required that can reliably solve a wide variety of problems\nwithout existing skills. To support this development, a new benchmark for\nintelligence is outlined that covers a much higher diversity of unknown tasks\nto be solved, thus enabling a comprehensive assessment of intelligence and of\nprogress towards AGI."
                },
                "authors": [
                    {
                        "name": "Rolf Pfister"
                    },
                    {
                        "name": "Hansueli Jud"
                    }
                ],
                "author_detail": {
                    "name": "Hansueli Jud"
                },
                "author": "Hansueli Jud",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07451v1",
                "updated": "2025-01-13T16:24:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    16,
                    24,
                    49,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T16:24:49Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    16,
                    24,
                    49,
                    0,
                    13,
                    0
                ],
                "title": "A Survey on Dynamic Neural Networks: from Computer Vision to Multi-modal\n  Sensor Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Dynamic Neural Networks: from Computer Vision to Multi-modal\n  Sensor Fusion"
                },
                "summary": "Model compression is essential in the deployment of large Computer Vision\nmodels on embedded devices. However, static optimization techniques (e.g.\npruning, quantization, etc.) neglect the fact that different inputs have\ndifferent complexities, thus requiring different amount of computations.\nDynamic Neural Networks allow to condition the number of computations to the\nspecific input. The current literature on the topic is very extensive and\nfragmented. We present a comprehensive survey that synthesizes and unifies\nexisting Dynamic Neural Networks research in the context of Computer Vision.\nAdditionally, we provide a logical taxonomy based on which component of the\nnetwork is adaptive: the output, the computation graph or the input.\nFurthermore, we argue that Dynamic Neural Networks are particularly beneficial\nin the context of Sensor Fusion for better adaptivity, noise reduction and\ninformation prioritization. We present preliminary works in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model compression is essential in the deployment of large Computer Vision\nmodels on embedded devices. However, static optimization techniques (e.g.\npruning, quantization, etc.) neglect the fact that different inputs have\ndifferent complexities, thus requiring different amount of computations.\nDynamic Neural Networks allow to condition the number of computations to the\nspecific input. The current literature on the topic is very extensive and\nfragmented. We present a comprehensive survey that synthesizes and unifies\nexisting Dynamic Neural Networks research in the context of Computer Vision.\nAdditionally, we provide a logical taxonomy based on which component of the\nnetwork is adaptive: the output, the computation graph or the input.\nFurthermore, we argue that Dynamic Neural Networks are particularly beneficial\nin the context of Sensor Fusion for better adaptivity, noise reduction and\ninformation prioritization. We present preliminary works in this direction."
                },
                "authors": [
                    {
                        "name": "Fabio Montello"
                    },
                    {
                        "name": "Ronja GÃ¼ldenring"
                    },
                    {
                        "name": "Simone Scardapane"
                    },
                    {
                        "name": "Lazaros Nalpantidis"
                    }
                ],
                "author_detail": {
                    "name": "Lazaros Nalpantidis"
                },
                "author": "Lazaros Nalpantidis",
                "arxiv_comment": "Under review at International Journal of Computer Vision",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.0; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02321v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02321v3",
                "updated": "2025-01-13T15:47:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    15,
                    47,
                    53,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-04T15:59:33Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    15,
                    59,
                    33,
                    5,
                    4,
                    0
                ],
                "title": "KD-MSLRT: Lightweight Sign Language Recognition Model Based on Mediapipe\n  and 3D to 1D Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KD-MSLRT: Lightweight Sign Language Recognition Model Based on Mediapipe\n  and 3D to 1D Knowledge Distillation"
                },
                "summary": "Artificial intelligence has achieved notable results in sign language\nrecognition and translation. However, relatively few efforts have been made to\nsignificantly improve the quality of life for the 72 million hearing-impaired\npeople worldwide. Sign language translation models, relying on video inputs,\ninvolves with large parameter sizes, making it time-consuming and\ncomputationally intensive to be deployed. This directly contributes to the\nscarcity of human-centered technology in this field. Additionally, the lack of\ndatasets in sign language translation hampers research progress in this area.\nTo address these, we first propose a cross-modal multi-knowledge distillation\ntechnique from 3D to 1D and a novel end-to-end pre-training text correction\nframework. Compared to other pre-trained models, our framework achieves\nsignificant advancements in correcting text output errors. Our model achieves a\ndecrease in Word Error Rate (WER) of at least 1.4% on PHOENIX14 and PHOENIX14T\ndatasets compared to the state-of-the-art CorrNet. Additionally, the TensorFlow\nLite (TFLite) quantized model size is reduced to 12.93 MB, making it the\nsmallest, fastest, and most accurate model to date. We have also collected and\nreleased extensive Chinese sign language datasets, and developed a specialized\ntraining vocabulary. To address the lack of research on data augmentation for\nlandmark data, we have designed comparative experiments on various augmentation\nmethods. Moreover, we performed a simulated deployment and prediction of our\nmodel on Intel platform CPUs and assessed the feasibility of deploying the\nmodel on other platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence has achieved notable results in sign language\nrecognition and translation. However, relatively few efforts have been made to\nsignificantly improve the quality of life for the 72 million hearing-impaired\npeople worldwide. Sign language translation models, relying on video inputs,\ninvolves with large parameter sizes, making it time-consuming and\ncomputationally intensive to be deployed. This directly contributes to the\nscarcity of human-centered technology in this field. Additionally, the lack of\ndatasets in sign language translation hampers research progress in this area.\nTo address these, we first propose a cross-modal multi-knowledge distillation\ntechnique from 3D to 1D and a novel end-to-end pre-training text correction\nframework. Compared to other pre-trained models, our framework achieves\nsignificant advancements in correcting text output errors. Our model achieves a\ndecrease in Word Error Rate (WER) of at least 1.4% on PHOENIX14 and PHOENIX14T\ndatasets compared to the state-of-the-art CorrNet. Additionally, the TensorFlow\nLite (TFLite) quantized model size is reduced to 12.93 MB, making it the\nsmallest, fastest, and most accurate model to date. We have also collected and\nreleased extensive Chinese sign language datasets, and developed a specialized\ntraining vocabulary. To address the lack of research on data augmentation for\nlandmark data, we have designed comparative experiments on various augmentation\nmethods. Moreover, we performed a simulated deployment and prediction of our\nmodel on Intel platform CPUs and assessed the feasibility of deploying the\nmodel on other platforms."
                },
                "authors": [
                    {
                        "name": "Yulong Li"
                    },
                    {
                        "name": "Bolin Ren"
                    },
                    {
                        "name": "Ke Hu"
                    },
                    {
                        "name": "Changyuan Liu"
                    },
                    {
                        "name": "Zhengyong Jiang"
                    },
                    {
                        "name": "Kang Dang"
                    },
                    {
                        "name": "Jionglong Su"
                    }
                ],
                "author_detail": {
                    "name": "Jionglong Su"
                },
                "author": "Jionglong Su",
                "arxiv_comment": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02321v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02321v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07425v1",
                "updated": "2025-01-13T15:43:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    15,
                    43,
                    36,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T15:43:36Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    15,
                    43,
                    36,
                    0,
                    13,
                    0
                ],
                "title": "Enhancing LLM's Ability to Generate More Repository-Aware Unit Tests\n  Through Precise Contextual Information Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM's Ability to Generate More Repository-Aware Unit Tests\n  Through Precise Contextual Information Injection"
                },
                "summary": "Though many learning-based approaches have been proposed for unit test\ngeneration and achieved remarkable performance, they still have limitations in\nrelying on task-specific datasets. Recently, Large Language Models (LLMs)\nguided by prompt engineering have gained attention for their ability to handle\na broad range of tasks, including unit test generation. Despite their success,\nLLMs may exhibit hallucinations when generating unit tests for focal methods or\nfunctions due to their lack of awareness regarding the project's global\ncontext. These hallucinations may manifest as calls to non-existent methods, as\nwell as incorrect parameters or return values, such as mismatched parameter\ntypes or numbers. While many studies have explored the role of context, they\noften extract fixed patterns of context for different models and focal methods,\nwhich may not be suitable for all generation processes (e.g., excessive\nirrelevant context could lead to redundancy, preventing the model from focusing\non essential information). To overcome this limitation, we propose RATester,\nwhich enhances the LLM's ability to generate more repository-aware unit tests\nthrough global contextual information injection. To equip LLMs with global\nknowledge similar to that of human testers, we integrate the language server\ngopls, which provides essential features (e.g., definition lookup) to assist\nthe LLM. When RATester encounters an unfamiliar identifier (e.g., an unfamiliar\nstruct name), it first leverages gopls to fetch relevant definitions and\ndocumentation comments, and then uses this global knowledge to guide the LLM.\nBy utilizing gopls, RATester enriches the LLM's knowledge of the project's\nglobal context, thereby reducing hallucinations during unit test generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Though many learning-based approaches have been proposed for unit test\ngeneration and achieved remarkable performance, they still have limitations in\nrelying on task-specific datasets. Recently, Large Language Models (LLMs)\nguided by prompt engineering have gained attention for their ability to handle\na broad range of tasks, including unit test generation. Despite their success,\nLLMs may exhibit hallucinations when generating unit tests for focal methods or\nfunctions due to their lack of awareness regarding the project's global\ncontext. These hallucinations may manifest as calls to non-existent methods, as\nwell as incorrect parameters or return values, such as mismatched parameter\ntypes or numbers. While many studies have explored the role of context, they\noften extract fixed patterns of context for different models and focal methods,\nwhich may not be suitable for all generation processes (e.g., excessive\nirrelevant context could lead to redundancy, preventing the model from focusing\non essential information). To overcome this limitation, we propose RATester,\nwhich enhances the LLM's ability to generate more repository-aware unit tests\nthrough global contextual information injection. To equip LLMs with global\nknowledge similar to that of human testers, we integrate the language server\ngopls, which provides essential features (e.g., definition lookup) to assist\nthe LLM. When RATester encounters an unfamiliar identifier (e.g., an unfamiliar\nstruct name), it first leverages gopls to fetch relevant definitions and\ndocumentation comments, and then uses this global knowledge to guide the LLM.\nBy utilizing gopls, RATester enriches the LLM's knowledge of the project's\nglobal context, thereby reducing hallucinations during unit test generation."
                },
                "authors": [
                    {
                        "name": "Xin Yin"
                    },
                    {
                        "name": "Chao Ni"
                    },
                    {
                        "name": "Xinrui Li"
                    },
                    {
                        "name": "Liushan Chen"
                    },
                    {
                        "name": "Guojun Ma"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.11547v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.11547v2",
                "updated": "2025-01-13T15:37:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    15,
                    37,
                    3,
                    0,
                    13,
                    0
                ],
                "published": "2024-09-17T20:40:02Z",
                "published_parsed": [
                    2024,
                    9,
                    17,
                    20,
                    40,
                    2,
                    1,
                    261,
                    0
                ],
                "title": "Small Language Models can Outperform Humans in Short Creative Writing: A\n  Study Comparing SLMs with Humans and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Language Models can Outperform Humans in Short Creative Writing: A\n  Study Comparing SLMs with Humans and LLMs"
                },
                "summary": "In this paper, we evaluate the creative fiction writing abilities of a\nfine-tuned small language model (SLM), BART-large, and compare its performance\nto human writers and two large language models (LLMs): GPT-3.5 and GPT-4o. Our\nevaluation consists of two experiments: (i) a human study in which 68\nparticipants rated short stories from humans and the SLM on grammaticality,\nrelevance, creativity, and attractiveness, and (ii) a qualitative linguistic\nanalysis examining the textual characteristics of stories produced by each\nmodel. In the first experiment, BART-large outscored average human writers\noverall (2.11 vs. 1.85), a 14% relative improvement, though the slight human\nadvantage in creativity was not statistically significant. In the second\nexperiment, qualitative analysis showed that while GPT-4o demonstrated\nnear-perfect coherence and used less cliche phrases, it tended to produce more\npredictable language, with only 3% of its synopses featuring surprising\nassociations (compared to 15% for BART). These findings highlight how model\nsize and fine-tuning influence the balance between creativity, fluency, and\ncoherence in creative writing tasks, and demonstrate that smaller models can,\nin certain contexts, rival both humans and larger models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we evaluate the creative fiction writing abilities of a\nfine-tuned small language model (SLM), BART-large, and compare its performance\nto human writers and two large language models (LLMs): GPT-3.5 and GPT-4o. Our\nevaluation consists of two experiments: (i) a human study in which 68\nparticipants rated short stories from humans and the SLM on grammaticality,\nrelevance, creativity, and attractiveness, and (ii) a qualitative linguistic\nanalysis examining the textual characteristics of stories produced by each\nmodel. In the first experiment, BART-large outscored average human writers\noverall (2.11 vs. 1.85), a 14% relative improvement, though the slight human\nadvantage in creativity was not statistically significant. In the second\nexperiment, qualitative analysis showed that while GPT-4o demonstrated\nnear-perfect coherence and used less cliche phrases, it tended to produce more\npredictable language, with only 3% of its synopses featuring surprising\nassociations (compared to 15% for BART). These findings highlight how model\nsize and fine-tuning influence the balance between creativity, fluency, and\ncoherence in creative writing tasks, and demonstrate that smaller models can,\nin certain contexts, rival both humans and larger models."
                },
                "authors": [
                    {
                        "name": "Guillermo Marco"
                    },
                    {
                        "name": "Luz Rello"
                    },
                    {
                        "name": "Julio Gonzalo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Gonzalo"
                },
                "author": "Julio Gonzalo",
                "arxiv_comment": "Accepted as Main Conference Paper at COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.11547v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.11547v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09536v2",
                "updated": "2025-01-13T15:25:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    15,
                    25,
                    37,
                    0,
                    13,
                    0
                ],
                "published": "2024-08-18T16:44:01Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    16,
                    44,
                    1,
                    6,
                    231,
                    0
                ],
                "title": "Galapagos: Automated N-Version Programming with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galapagos: Automated N-Version Programming with LLMs"
                },
                "summary": "N-Version Programming is a well-known methodology for developing\nfault-tolerant systems. It achieves fault detection and correction at runtime\nby adding diverse redundancy into programs, minimizing fault mode overlap\nbetween redundant program variants. In this work, we propose the automated\ngeneration of program variants using large language models. We design, develop\nand evaluate Gal\\'apagos: a tool for generating program variants using LLMs,\nvalidating their correctness and equivalence, and using them to assemble\nN-Version binaries. We evaluate Gal\\'apagos by creating N-Version components of\nreal-world C code. Our original results show that Gal\\'apagos can produce\nprogram variants that are proven to be functionally equivalent, even when the\nvariants are written in a different programming language. Our systematic\ndiversity measurement indicates that functionally equivalent variants produced\nby Gal\\'apagos, are statically different after compilation, and present\ndiverging internal behavior at runtime. We demonstrate that the variants\nproduced by Gal\\'apagos can protect C code against real miscompilation bugs\nwhich affect the Clang compiler. Overall, our paper shows that producing\nN-Version software can be drastically automated by advanced usage of practical\nformal verification and generative language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "N-Version Programming is a well-known methodology for developing\nfault-tolerant systems. It achieves fault detection and correction at runtime\nby adding diverse redundancy into programs, minimizing fault mode overlap\nbetween redundant program variants. In this work, we propose the automated\ngeneration of program variants using large language models. We design, develop\nand evaluate Gal\\'apagos: a tool for generating program variants using LLMs,\nvalidating their correctness and equivalence, and using them to assemble\nN-Version binaries. We evaluate Gal\\'apagos by creating N-Version components of\nreal-world C code. Our original results show that Gal\\'apagos can produce\nprogram variants that are proven to be functionally equivalent, even when the\nvariants are written in a different programming language. Our systematic\ndiversity measurement indicates that functionally equivalent variants produced\nby Gal\\'apagos, are statically different after compilation, and present\ndiverging internal behavior at runtime. We demonstrate that the variants\nproduced by Gal\\'apagos can protect C code against real miscompilation bugs\nwhich affect the Clang compiler. Overall, our paper shows that producing\nN-Version software can be drastically automated by advanced usage of practical\nformal verification and generative language models."
                },
                "authors": [
                    {
                        "name": "Javier Ron"
                    },
                    {
                        "name": "Diogo Gaspar"
                    },
                    {
                        "name": "Javier Cabrera-Arteaga"
                    },
                    {
                        "name": "Benoit Baudry"
                    },
                    {
                        "name": "Martin Monperrus"
                    }
                ],
                "author_detail": {
                    "name": "Martin Monperrus"
                },
                "author": "Martin Monperrus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07408v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07408v1",
                "updated": "2025-01-13T15:24:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    15,
                    24,
                    10,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T15:24:10Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    15,
                    24,
                    10,
                    0,
                    13,
                    0
                ],
                "title": "Initial Findings on Sensor based Open Vocabulary Activity Recognition\n  via Text Embedding Inversion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Initial Findings on Sensor based Open Vocabulary Activity Recognition\n  via Text Embedding Inversion"
                },
                "summary": "Conventional human activity recognition (HAR) relies on classifiers trained\nto predict discrete activity classes, inherently limiting recognition to\nactivities explicitly present in the training set. Such classifiers would\ninvariably fail, putting zero likelihood, when encountering unseen activities.\nWe propose Open Vocabulary HAR (OV-HAR), a framework that overcomes this\nlimitation by first converting each activity into natural language and breaking\nit into a sequence of elementary motions. This descriptive text is then encoded\ninto a fixed-size embedding. The model is trained to regress this embedding,\nwhich is subsequently decoded back into natural language using a pre-trained\nembedding inversion model. Unlike other works that rely on auto-regressive\nlarge language models (LLMs) at their core, OV-HAR achieves open vocabulary\nrecognition without the computational overhead of such models. The generated\ntext can be transformed into a single activity class using LLM prompt\nengineering. We have evaluated our approach on different modalities, including\nvision (pose), IMU, and pressure sensors, demonstrating robust generalization\nacross unseen activities and modalities, offering a fundamentally different\nparadigm from contemporary classifiers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional human activity recognition (HAR) relies on classifiers trained\nto predict discrete activity classes, inherently limiting recognition to\nactivities explicitly present in the training set. Such classifiers would\ninvariably fail, putting zero likelihood, when encountering unseen activities.\nWe propose Open Vocabulary HAR (OV-HAR), a framework that overcomes this\nlimitation by first converting each activity into natural language and breaking\nit into a sequence of elementary motions. This descriptive text is then encoded\ninto a fixed-size embedding. The model is trained to regress this embedding,\nwhich is subsequently decoded back into natural language using a pre-trained\nembedding inversion model. Unlike other works that rely on auto-regressive\nlarge language models (LLMs) at their core, OV-HAR achieves open vocabulary\nrecognition without the computational overhead of such models. The generated\ntext can be transformed into a single activity class using LLM prompt\nengineering. We have evaluated our approach on different modalities, including\nvision (pose), IMU, and pressure sensors, demonstrating robust generalization\nacross unseen activities and modalities, offering a fundamentally different\nparadigm from contemporary classifiers."
                },
                "authors": [
                    {
                        "name": "Lala Shakti Swarup Ray"
                    },
                    {
                        "name": "Bo Zhou"
                    },
                    {
                        "name": "Sungho Suh"
                    },
                    {
                        "name": "Paul Lukowicz"
                    }
                ],
                "author_detail": {
                    "name": "Paul Lukowicz"
                },
                "author": "Paul Lukowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07408v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07408v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08926v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08926v3",
                "updated": "2025-01-13T15:19:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    15,
                    19,
                    14,
                    0,
                    13,
                    0
                ],
                "published": "2024-10-11T15:50:53Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    15,
                    50,
                    53,
                    4,
                    285,
                    0
                ],
                "title": "Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million\n  Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Shot Pupil Segmentation with SAM 2: A Case Study of Over 14 Million\n  Images"
                },
                "summary": "We explore the transformative potential of SAM 2, a vision foundation model,\nin advancing gaze estimation and eye tracking technologies. By significantly\nreducing annotation time, lowering technical barriers through its ease of\ndeployment, and enhancing segmentation accuracy, SAM 2 addresses critical\nchallenges faced by researchers and practitioners. Utilizing its zero-shot\nsegmentation capabilities with minimal user input-a single click per video-we\ntested SAM 2 on over 14 million eye images from diverse datasets, including\nvirtual reality setups and the world's largest unified dataset recorded using\nwearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matches\nthe performance of domain-specific models trained solely on eye images,\nachieving competitive mean Intersection over Union (mIoU) scores of up to 93%\nwithout fine-tuning. Additionally, we provide our code and segmentation masks\nfor these widely used datasets to promote further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the transformative potential of SAM 2, a vision foundation model,\nin advancing gaze estimation and eye tracking technologies. By significantly\nreducing annotation time, lowering technical barriers through its ease of\ndeployment, and enhancing segmentation accuracy, SAM 2 addresses critical\nchallenges faced by researchers and practitioners. Utilizing its zero-shot\nsegmentation capabilities with minimal user input-a single click per video-we\ntested SAM 2 on over 14 million eye images from diverse datasets, including\nvirtual reality setups and the world's largest unified dataset recorded using\nwearable eye trackers. Remarkably, in pupil segmentation tasks, SAM 2 matches\nthe performance of domain-specific models trained solely on eye images,\nachieving competitive mean Intersection over Union (mIoU) scores of up to 93%\nwithout fine-tuning. Additionally, we provide our code and segmentation masks\nfor these widely used datasets to promote further research."
                },
                "authors": [
                    {
                        "name": "Virmarie Maquiling"
                    },
                    {
                        "name": "Sean Anthony Byrne"
                    },
                    {
                        "name": "Diederick C. Niehorster"
                    },
                    {
                        "name": "Marco Carminati"
                    },
                    {
                        "name": "Enkelejda Kasneci"
                    }
                ],
                "author_detail": {
                    "name": "Enkelejda Kasneci"
                },
                "author": "Enkelejda Kasneci",
                "arxiv_comment": "Virmarie Maquiling and Sean Anthony Byrne contributed equally to this\n  paper, 8 pages, 3 figures, ETRA 2025, pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08926v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08926v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07392v1",
                "updated": "2025-01-13T15:08:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    15,
                    8,
                    32,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T15:08:32Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    15,
                    8,
                    32,
                    0,
                    13,
                    0
                ],
                "title": "The Essentials of AI for Life and Society: An AI Literacy Course for the\n  University Community",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Essentials of AI for Life and Society: An AI Literacy Course for the\n  University Community"
                },
                "summary": "We describe the development of a one-credit course to promote AI literacy at\nThe University of Texas at Austin. In response to a call for the rapid\ndeployment of class to serve a broad audience in Fall of 2023, we designed a\n14-week seminar-style course that incorporated an interdisciplinary group of\nspeakers who lectured on topics ranging from the fundamentals of AI to societal\nconcerns including disinformation and employment. University students, faculty,\nand staff, and even community members outside of the University, were invited\nto enroll in this online offering: The Essentials of AI for Life and Society.\nWe collected feedback from course participants through weekly reflections and a\nfinal survey. Satisfyingly, we found that attendees reported gains in their AI\nliteracy. We sought critical feedback through quantitative and qualitative\nanalysis, which uncovered challenges in designing a course for this general\naudience. We utilized the course feedback to design a three-credit version of\nthe course that is being offered in Fall of 2024. The lessons we learned and\nour plans for this new iteration may serve as a guide to instructors designing\nAI courses for a broad audience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe the development of a one-credit course to promote AI literacy at\nThe University of Texas at Austin. In response to a call for the rapid\ndeployment of class to serve a broad audience in Fall of 2023, we designed a\n14-week seminar-style course that incorporated an interdisciplinary group of\nspeakers who lectured on topics ranging from the fundamentals of AI to societal\nconcerns including disinformation and employment. University students, faculty,\nand staff, and even community members outside of the University, were invited\nto enroll in this online offering: The Essentials of AI for Life and Society.\nWe collected feedback from course participants through weekly reflections and a\nfinal survey. Satisfyingly, we found that attendees reported gains in their AI\nliteracy. We sought critical feedback through quantitative and qualitative\nanalysis, which uncovered challenges in designing a course for this general\naudience. We utilized the course feedback to design a three-credit version of\nthe course that is being offered in Fall of 2024. The lessons we learned and\nour plans for this new iteration may serve as a guide to instructors designing\nAI courses for a broad audience."
                },
                "authors": [
                    {
                        "name": "Joydeep Biswas"
                    },
                    {
                        "name": "Don Fussell"
                    },
                    {
                        "name": "Peter Stone"
                    },
                    {
                        "name": "Kristin Patterson"
                    },
                    {
                        "name": "Kristen Procko"
                    },
                    {
                        "name": "Lea Sabatini"
                    },
                    {
                        "name": "Zifan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Zifan Xu"
                },
                "author": "Zifan Xu",
                "arxiv_comment": "Accepted to EAAI-25: The 15th Symposium on Educational Advances in\n  Artificial Intelligence, collocated with AAAI-25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09718v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09718v2",
                "updated": "2025-01-13T14:37:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    14,
                    37,
                    52,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-12T20:48:06Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    20,
                    48,
                    6,
                    3,
                    347,
                    0
                ],
                "title": "BayesAdapter: enhanced uncertainty estimation in CLIP few-shot\n  adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BayesAdapter: enhanced uncertainty estimation in CLIP few-shot\n  adaptation"
                },
                "summary": "The emergence of large pre-trained vision-language models (VLMs) represents a\nparadigm shift in machine learning, with unprecedented results in a broad span\nof visual recognition tasks. CLIP, one of the most popular VLMs, has exhibited\nremarkable zero-shot and transfer learning capabilities in classification. To\ntransfer CLIP to downstream tasks, adapters constitute a parameter-efficient\napproach that avoids backpropagation through the large model (unlike related\nprompt learning methods). However, CLIP adapters have been developed to target\ndiscriminative performance, and the quality of their uncertainty estimates has\nbeen overlooked. In this work we show that the discriminative performance of\nstate-of-the-art CLIP adapters does not always correlate with their uncertainty\nestimation capabilities, which are essential for a safe deployment in\nreal-world scenarios. We also demonstrate that one of such adapters is obtained\nthrough MAP inference from a more general probabilistic framework. Based on\nthis observation we introduce BayesAdapter, which leverages Bayesian inference\nto estimate a full probability distribution instead of a single point, better\ncapturing the variability inherent in the parameter space. In a comprehensive\nempirical evaluation we show that our approach obtains high quality uncertainty\nestimates in the predictions, standing out in calibration and selective\nclassification. Our code will be publicly available upon acceptance of the\npaper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large pre-trained vision-language models (VLMs) represents a\nparadigm shift in machine learning, with unprecedented results in a broad span\nof visual recognition tasks. CLIP, one of the most popular VLMs, has exhibited\nremarkable zero-shot and transfer learning capabilities in classification. To\ntransfer CLIP to downstream tasks, adapters constitute a parameter-efficient\napproach that avoids backpropagation through the large model (unlike related\nprompt learning methods). However, CLIP adapters have been developed to target\ndiscriminative performance, and the quality of their uncertainty estimates has\nbeen overlooked. In this work we show that the discriminative performance of\nstate-of-the-art CLIP adapters does not always correlate with their uncertainty\nestimation capabilities, which are essential for a safe deployment in\nreal-world scenarios. We also demonstrate that one of such adapters is obtained\nthrough MAP inference from a more general probabilistic framework. Based on\nthis observation we introduce BayesAdapter, which leverages Bayesian inference\nto estimate a full probability distribution instead of a single point, better\ncapturing the variability inherent in the parameter space. In a comprehensive\nempirical evaluation we show that our approach obtains high quality uncertainty\nestimates in the predictions, standing out in calibration and selective\nclassification. Our code will be publicly available upon acceptance of the\npaper."
                },
                "authors": [
                    {
                        "name": "Pablo Morales-Ãlvarez"
                    },
                    {
                        "name": "Stergios Christodoulidis"
                    },
                    {
                        "name": "Maria Vakalopoulou"
                    },
                    {
                        "name": "Pablo Piantanida"
                    },
                    {
                        "name": "Jose Dolz"
                    }
                ],
                "author_detail": {
                    "name": "Jose Dolz"
                },
                "author": "Jose Dolz",
                "arxiv_comment": "30 pages, 5 figures, 23 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09718v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16531v2",
                "updated": "2025-01-13T14:34:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    14,
                    34,
                    40,
                    0,
                    13,
                    0
                ],
                "published": "2024-06-24T11:10:41Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    11,
                    10,
                    41,
                    0,
                    176,
                    0
                ],
                "title": "GIM: A Million-scale Benchmark for Generative Image Manipulation\n  Detection and Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GIM: A Million-scale Benchmark for Generative Image Manipulation\n  Detection and Localization"
                },
                "summary": "The extraordinary ability of generative models emerges as a new trend in\nimage editing and generating realistic images, posing a serious threat to the\ntrustworthiness of multimedia data and driving the research of image\nmanipulation detection and location (IMDL). However, the lack of a large-scale\ndata foundation makes the IMDL task unattainable. In this paper, we build a\nlocal manipulation data generation pipeline that integrates the powerful\ncapabilities of SAM, LLM, and generative models. Upon this basis, we propose\nthe GIM dataset, which has the following advantages: 1) Large scale, GIM\nincludes over one million pairs of AI-manipulated images and real images. 2)\nRich image content, GIM encompasses a broad range of image classes. 3) Diverse\ngenerative manipulation, the images are manipulated images with\nstate-of-the-art generators and various manipulation tasks. The aforementioned\nadvantages allow for a more comprehensive evaluation of IMDL methods, extending\ntheir applicability to diverse images. We introduce the GIM benchmark with two\nsettings to evaluate existing IMDL methods. In addition, we propose a novel\nIMDL framework, termed GIMFormer, which consists of a ShadowTracer,\nFrequency-Spatial block (FSB), and a Multi-Window Anomalous Modeling (MWAM)\nmodule. Extensive experiments on the GIM demonstrate that GIMFormer surpasses\nthe previous state-of-the-art approach on two different benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The extraordinary ability of generative models emerges as a new trend in\nimage editing and generating realistic images, posing a serious threat to the\ntrustworthiness of multimedia data and driving the research of image\nmanipulation detection and location (IMDL). However, the lack of a large-scale\ndata foundation makes the IMDL task unattainable. In this paper, we build a\nlocal manipulation data generation pipeline that integrates the powerful\ncapabilities of SAM, LLM, and generative models. Upon this basis, we propose\nthe GIM dataset, which has the following advantages: 1) Large scale, GIM\nincludes over one million pairs of AI-manipulated images and real images. 2)\nRich image content, GIM encompasses a broad range of image classes. 3) Diverse\ngenerative manipulation, the images are manipulated images with\nstate-of-the-art generators and various manipulation tasks. The aforementioned\nadvantages allow for a more comprehensive evaluation of IMDL methods, extending\ntheir applicability to diverse images. We introduce the GIM benchmark with two\nsettings to evaluate existing IMDL methods. In addition, we propose a novel\nIMDL framework, termed GIMFormer, which consists of a ShadowTracer,\nFrequency-Spatial block (FSB), and a Multi-Window Anomalous Modeling (MWAM)\nmodule. Extensive experiments on the GIM demonstrate that GIMFormer surpasses\nthe previous state-of-the-art approach on two different benchmarks."
                },
                "authors": [
                    {
                        "name": "Yirui Chen"
                    },
                    {
                        "name": "Xudong Huang"
                    },
                    {
                        "name": "Quan Zhang"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Qiangyu Yan"
                    },
                    {
                        "name": "Simiao Li"
                    },
                    {
                        "name": "Hanting Chen"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jie Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jie Hu"
                },
                "author": "Jie Hu",
                "arxiv_comment": "Code page: https://github.com/chenyirui/GIM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07359v1",
                "updated": "2025-01-13T14:27:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    14,
                    27,
                    39,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T14:27:39Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    14,
                    27,
                    39,
                    0,
                    13,
                    0
                ],
                "title": "Emergent effects of scaling on the functional hierarchies within large\n  language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent effects of scaling on the functional hierarchies within large\n  language models"
                },
                "summary": "Large language model (LLM) architectures are often described as functionally\nhierarchical: Early layers process syntax, middle layers begin to parse\nsemantics, and late layers integrate information. The present work revisits\nthese ideas. This research submits simple texts to an LLM (e.g., \"A church and\norgan\") and extracts the resulting activations. Then, for each layer, support\nvector machines and ridge regressions are fit to predict a text's label and\nthus examine whether a given layer encodes some information. Analyses using a\nsmall model (Llama-3.2-3b; 28 layers) partly bolster the common hierarchical\nperspective: Item-level semantics are most strongly represented early (layers\n2-7), then two-item relations (layers 8-12), and then four-item analogies\n(layers 10-15). Afterward, the representation of items and simple relations\ngradually decreases in deeper layers that focus on more global information.\nHowever, several findings run counter to a steady hierarchy view: First,\nalthough deep layers can represent document-wide abstractions, deep layers also\ncompress information from early portions of the context window without\nmeaningful abstraction. Second, when examining a larger model\n(Llama-3.3-70b-Instruct), stark fluctuations in abstraction level appear: As\ndepth increases, two-item relations and four-item analogies initially increase\nin their representation, then markedly decrease, and afterward increase again\nmomentarily. This peculiar pattern consistently emerges across several\nexperiments. Third, another emergent effect of scaling is coordination between\nthe attention mechanisms of adjacent layers. Across multiple experiments using\nthe larger model, adjacent layers fluctuate between what information they each\nspecialize in representing. In sum, an abstraction hierarchy often manifests\nacross layers, but large models also deviate from this structure in curious\nways.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) architectures are often described as functionally\nhierarchical: Early layers process syntax, middle layers begin to parse\nsemantics, and late layers integrate information. The present work revisits\nthese ideas. This research submits simple texts to an LLM (e.g., \"A church and\norgan\") and extracts the resulting activations. Then, for each layer, support\nvector machines and ridge regressions are fit to predict a text's label and\nthus examine whether a given layer encodes some information. Analyses using a\nsmall model (Llama-3.2-3b; 28 layers) partly bolster the common hierarchical\nperspective: Item-level semantics are most strongly represented early (layers\n2-7), then two-item relations (layers 8-12), and then four-item analogies\n(layers 10-15). Afterward, the representation of items and simple relations\ngradually decreases in deeper layers that focus on more global information.\nHowever, several findings run counter to a steady hierarchy view: First,\nalthough deep layers can represent document-wide abstractions, deep layers also\ncompress information from early portions of the context window without\nmeaningful abstraction. Second, when examining a larger model\n(Llama-3.3-70b-Instruct), stark fluctuations in abstraction level appear: As\ndepth increases, two-item relations and four-item analogies initially increase\nin their representation, then markedly decrease, and afterward increase again\nmomentarily. This peculiar pattern consistently emerges across several\nexperiments. Third, another emergent effect of scaling is coordination between\nthe attention mechanisms of adjacent layers. Across multiple experiments using\nthe larger model, adjacent layers fluctuate between what information they each\nspecialize in representing. In sum, an abstraction hierarchy often manifests\nacross layers, but large models also deviate from this structure in curious\nways."
                },
                "authors": [
                    {
                        "name": "Paul C. Bogdan"
                    }
                ],
                "author_detail": {
                    "name": "Paul C. Bogdan"
                },
                "author": "Paul C. Bogdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07330v1",
                "updated": "2025-01-13T13:44:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    44,
                    6,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T13:44:06Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    44,
                    6,
                    0,
                    13,
                    0
                ],
                "title": "Occamy: A 432-Core Dual-Chiplet Dual-HBM2E 768-DP-GFLOP/s RISC-V System\n  for 8-to-64-bit Dense and Sparse Computing in 12nm FinFET",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Occamy: A 432-Core Dual-Chiplet Dual-HBM2E 768-DP-GFLOP/s RISC-V System\n  for 8-to-64-bit Dense and Sparse Computing in 12nm FinFET"
                },
                "summary": "ML and HPC applications increasingly combine dense and sparse memory access\ncomputations to maximize storage efficiency. However, existing CPUs and GPUs\nstruggle to flexibly handle these heterogeneous workloads with consistently\nhigh compute efficiency. We present Occamy, a 432-Core, 768-DP-GFLOP/s,\ndual-HBM2E, dual-chiplet RISC-V system with a latency-tolerant hierarchical\ninterconnect and in-core streaming units (SUs) designed to accelerate dense and\nsparse FP8-to-FP64 ML and HPC workloads. We implement Occamy's compute chiplets\nin 12 nm FinFET, and its passive interposer, Hedwig, in a 65 nm node. On dense\nlinear algebra (LA), Occamy achieves a competitive FPU utilization of 89%. On\nstencil codes, Occamy reaches an FPU utilization of 83% and a\ntechnology-node-normalized compute density of 11.1 DP-GFLOP/s/mm2,leading\nstate-of-the-art (SoA) processors by 1.7x and 1.2x, respectively. On\nsparse-dense linear algebra (LA), it achieves 42% FPU utilization and a\nnormalized compute density of 5.95 DP-GFLOP/s/mm2, surpassing the SoA by 5.2x\nand 11x, respectively. On, sparse-sparse LA, Occamy reaches a throughput of up\nto 187 GCOMP/s at 17.4 GCOMP/s/W and a compute density of 3.63 GCOMP/s/mm2.\nFinally, we reach up to 75% and 54% FPU utilization on and dense (LLM) and\ngraph-sparse (GCN) ML inference workloads. Occamy's RTL is freely available\nunder a permissive open-source license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ML and HPC applications increasingly combine dense and sparse memory access\ncomputations to maximize storage efficiency. However, existing CPUs and GPUs\nstruggle to flexibly handle these heterogeneous workloads with consistently\nhigh compute efficiency. We present Occamy, a 432-Core, 768-DP-GFLOP/s,\ndual-HBM2E, dual-chiplet RISC-V system with a latency-tolerant hierarchical\ninterconnect and in-core streaming units (SUs) designed to accelerate dense and\nsparse FP8-to-FP64 ML and HPC workloads. We implement Occamy's compute chiplets\nin 12 nm FinFET, and its passive interposer, Hedwig, in a 65 nm node. On dense\nlinear algebra (LA), Occamy achieves a competitive FPU utilization of 89%. On\nstencil codes, Occamy reaches an FPU utilization of 83% and a\ntechnology-node-normalized compute density of 11.1 DP-GFLOP/s/mm2,leading\nstate-of-the-art (SoA) processors by 1.7x and 1.2x, respectively. On\nsparse-dense linear algebra (LA), it achieves 42% FPU utilization and a\nnormalized compute density of 5.95 DP-GFLOP/s/mm2, surpassing the SoA by 5.2x\nand 11x, respectively. On, sparse-sparse LA, Occamy reaches a throughput of up\nto 187 GCOMP/s at 17.4 GCOMP/s/W and a compute density of 3.63 GCOMP/s/mm2.\nFinally, we reach up to 75% and 54% FPU utilization on and dense (LLM) and\ngraph-sparse (GCN) ML inference workloads. Occamy's RTL is freely available\nunder a permissive open-source license."
                },
                "authors": [
                    {
                        "name": "Paul Scheffler"
                    },
                    {
                        "name": "Thomas Benz"
                    },
                    {
                        "name": "Viviane Potocnik"
                    },
                    {
                        "name": "Tim Fischer"
                    },
                    {
                        "name": "Luca Colagrande"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Luca Bertaccini"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Manuel Eggimann"
                    },
                    {
                        "name": "Matheus Cavalcante"
                    },
                    {
                        "name": "Gianna Paulin"
                    },
                    {
                        "name": "Frank K. GÃ¼rkaynak"
                    },
                    {
                        "name": "Davide Rossi"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_doi": "10.1109/JSSC.2025.3529249",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/JSSC.2025.3529249",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.07330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 13 figures, 1 table. Accepted for publication in IEEE JSSC",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07314v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07314v1",
                "updated": "2025-01-13T13:26:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    26,
                    50,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T13:26:50Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    26,
                    50,
                    0,
                    13,
                    0
                ],
                "title": "FinerWeb-10BT: Refining Web Data with LLM-Based Line-Level Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinerWeb-10BT: Refining Web Data with LLM-Based Line-Level Filtering"
                },
                "summary": "Data quality is crucial for training Large Language Models (LLMs).\nTraditional heuristic filters often miss low-quality text or mistakenly remove\nvaluable content. In this paper, we introduce an LLM-based line-level filtering\nmethod to enhance training data quality. We use GPT-4o mini to label a\n20,000-document sample from FineWeb at the line level, allowing the model to\ncreate descriptive labels for low-quality lines. These labels are grouped into\nnine main categories, and we train a DeBERTa-v3 classifier to scale the\nfiltering to a 10B-token subset of FineWeb. To test the impact of our\nfiltering, we train GPT-2 models on both the original and the filtered\ndatasets. The results show that models trained on the filtered data achieve\nhigher accuracy on the HellaSwag benchmark and reach their performance targets\nfaster, even with up to 25\\% less data. This demonstrates that LLM-based\nline-level filtering can significantly improve data quality and training\nefficiency for LLMs. We release our quality-annotated dataset, FinerWeb-10BT,\nand the codebase to support further work in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data quality is crucial for training Large Language Models (LLMs).\nTraditional heuristic filters often miss low-quality text or mistakenly remove\nvaluable content. In this paper, we introduce an LLM-based line-level filtering\nmethod to enhance training data quality. We use GPT-4o mini to label a\n20,000-document sample from FineWeb at the line level, allowing the model to\ncreate descriptive labels for low-quality lines. These labels are grouped into\nnine main categories, and we train a DeBERTa-v3 classifier to scale the\nfiltering to a 10B-token subset of FineWeb. To test the impact of our\nfiltering, we train GPT-2 models on both the original and the filtered\ndatasets. The results show that models trained on the filtered data achieve\nhigher accuracy on the HellaSwag benchmark and reach their performance targets\nfaster, even with up to 25\\% less data. This demonstrates that LLM-based\nline-level filtering can significantly improve data quality and training\nefficiency for LLMs. We release our quality-annotated dataset, FinerWeb-10BT,\nand the codebase to support further work in this area."
                },
                "authors": [
                    {
                        "name": "Erik Henriksson"
                    },
                    {
                        "name": "Otto Tarkka"
                    },
                    {
                        "name": "Filip Ginter"
                    }
                ],
                "author_detail": {
                    "name": "Filip Ginter"
                },
                "author": "Filip Ginter",
                "arxiv_comment": "11 pages, 4 figures, 4 tables. To be published in NoDaLiDa/Baltic-HLT\n  2025 proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07314v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07314v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10561v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10561v3",
                "updated": "2025-01-13T13:12:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    12,
                    9,
                    0,
                    13,
                    0
                ],
                "published": "2024-09-11T14:41:44Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    14,
                    41,
                    44,
                    2,
                    255,
                    0
                ],
                "title": "DrLLM: Prompt-Enhanced Distributed Denial-of-Service Resistance Method\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DrLLM: Prompt-Enhanced Distributed Denial-of-Service Resistance Method\n  with Large Language Models"
                },
                "summary": "The increasing number of Distributed Denial of Service (DDoS) attacks poses a\nmajor threat to the Internet, highlighting the importance of DDoS mitigation.\nMost existing approaches require complex training methods to learn data\nfeatures, which increases the complexity and generality of the application. In\nthis paper, we propose DrLLM, which aims to mine anomalous traffic information\nin zero-shot scenarios through Large Language Models (LLMs). To bridge the gap\nbetween DrLLM and existing approaches, we embed the global and local\ninformation of the traffic data into the reasoning paradigm and design three\nmodules, namely Knowledge Embedding, Token Embedding, and Progressive Role\nReasoning, for data representation and reasoning. In addition we explore the\ngeneralization of prompt engineering in the cybersecurity domain to improve the\nclassification capability of DrLLM. Our ablation experiments demonstrate the\napplicability of DrLLM in zero-shot scenarios and further demonstrate the\npotential of LLMs in the network domains. DrLLM implementation code has been\nopen-sourced at https://github.com/liuup/DrLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing number of Distributed Denial of Service (DDoS) attacks poses a\nmajor threat to the Internet, highlighting the importance of DDoS mitigation.\nMost existing approaches require complex training methods to learn data\nfeatures, which increases the complexity and generality of the application. In\nthis paper, we propose DrLLM, which aims to mine anomalous traffic information\nin zero-shot scenarios through Large Language Models (LLMs). To bridge the gap\nbetween DrLLM and existing approaches, we embed the global and local\ninformation of the traffic data into the reasoning paradigm and design three\nmodules, namely Knowledge Embedding, Token Embedding, and Progressive Role\nReasoning, for data representation and reasoning. In addition we explore the\ngeneralization of prompt engineering in the cybersecurity domain to improve the\nclassification capability of DrLLM. Our ablation experiments demonstrate the\napplicability of DrLLM in zero-shot scenarios and further demonstrate the\npotential of LLMs in the network domains. DrLLM implementation code has been\nopen-sourced at https://github.com/liuup/DrLLM."
                },
                "authors": [
                    {
                        "name": "Zhenyu Yin"
                    },
                    {
                        "name": "Shang Liu"
                    },
                    {
                        "name": "Guangyuan Xu"
                    }
                ],
                "author_detail": {
                    "name": "Guangyuan Xu"
                },
                "author": "Guangyuan Xu",
                "arxiv_comment": "Accepted by ICASSP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10561v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10561v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07301v1",
                "updated": "2025-01-13T13:10:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    10,
                    16,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T13:10:16Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    10,
                    16,
                    0,
                    13,
                    0
                ],
                "title": "The Lessons of Developing Process Reward Models in Mathematical\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lessons of Developing Process Reward Models in Mathematical\n  Reasoning"
                },
                "summary": "Process Reward Models (PRMs) emerge as a promising approach for process\nsupervision in mathematical reasoning of Large Language Models (LLMs), which\naim to identify and mitigate intermediate errors in the reasoning processes.\nHowever, the development of effective PRMs faces significant challenges,\nparticularly in data annotation and evaluation methodologies. In this paper,\nthrough extensive experiments, we demonstrate that commonly used Monte Carlo\n(MC) estimation-based data synthesis for PRMs typically yields inferior\nperformance and generalization compared to LLM-as-a-judge and human annotation\nmethods. MC estimation relies on completion models to evaluate current-step\ncorrectness, leading to inaccurate step verification. Furthermore, we identify\npotential biases in conventional Best-of-N (BoN) evaluation strategies for\nPRMs: (1) The unreliable policy models generate responses with correct answers\nbut flawed processes, leading to a misalignment between the evaluation criteria\nof BoN and the PRM objectives of process verification. (2) The tolerance of\nPRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a\nsignificant proportion of minimum scores concentrated on the final answer\nsteps, revealing the shift from process to outcome-based assessment in BoN\nOptimized PRMs. To address these challenges, we develop a consensus filtering\nmechanism that effectively integrates MC estimation with LLM-as-a-judge and\nadvocates a more comprehensive evaluation framework that combines\nresponse-level and step-level metrics. Based on the mechanisms, we\nsignificantly improve both model performance and data efficiency in the BoN\nevaluation and the step-wise error identification task. Finally, we release a\nnew state-of-the-art PRM that outperforms existing open-source alternatives and\nprovides practical guidelines for future research in building process\nsupervision models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process Reward Models (PRMs) emerge as a promising approach for process\nsupervision in mathematical reasoning of Large Language Models (LLMs), which\naim to identify and mitigate intermediate errors in the reasoning processes.\nHowever, the development of effective PRMs faces significant challenges,\nparticularly in data annotation and evaluation methodologies. In this paper,\nthrough extensive experiments, we demonstrate that commonly used Monte Carlo\n(MC) estimation-based data synthesis for PRMs typically yields inferior\nperformance and generalization compared to LLM-as-a-judge and human annotation\nmethods. MC estimation relies on completion models to evaluate current-step\ncorrectness, leading to inaccurate step verification. Furthermore, we identify\npotential biases in conventional Best-of-N (BoN) evaluation strategies for\nPRMs: (1) The unreliable policy models generate responses with correct answers\nbut flawed processes, leading to a misalignment between the evaluation criteria\nof BoN and the PRM objectives of process verification. (2) The tolerance of\nPRMs of such responses leads to inflated BoN scores. (3) Existing PRMs have a\nsignificant proportion of minimum scores concentrated on the final answer\nsteps, revealing the shift from process to outcome-based assessment in BoN\nOptimized PRMs. To address these challenges, we develop a consensus filtering\nmechanism that effectively integrates MC estimation with LLM-as-a-judge and\nadvocates a more comprehensive evaluation framework that combines\nresponse-level and step-level metrics. Based on the mechanisms, we\nsignificantly improve both model performance and data efficiency in the BoN\nevaluation and the step-wise error identification task. Finally, we release a\nnew state-of-the-art PRM that outperforms existing open-source alternatives and\nprovides practical guidelines for future research in building process\nsupervision models."
                },
                "authors": [
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Yangzhen Wu"
                    },
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Runji Lin"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07292v1",
                "updated": "2025-01-13T13:00:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    0,
                    24,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T13:00:24Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    13,
                    0,
                    24,
                    0,
                    13,
                    0
                ],
                "title": "Estimating quantum relative entropies on quantum computers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating quantum relative entropies on quantum computers"
                },
                "summary": "Quantum relative entropy, a quantum generalization of the well-known\nKullback-Leibler divergence, serves as a fundamental measure of the\ndistinguishability between quantum states and plays a pivotal role in quantum\ninformation science. Despite its importance, efficiently estimating quantum\nrelative entropy between two quantum states on quantum computers remains a\nsignificant challenge. In this work, we propose the first quantum algorithm for\nestimating quantum relative entropy and Petz R\\'{e}nyi divergence from two\nunknown quantum states on quantum computers, addressing open problems\nhighlighted in [Phys. Rev. A 109, 032431 (2024)] and [IEEE Trans. Inf. Theory\n70, 5653-5680 (2024)]. This is achieved by combining quadrature approximations\nof relative entropies, the variational representation of quantum f-divergences,\nand a new technique for parameterizing Hermitian polynomial operators to\nestimate their traces with quantum states. Notably, the circuit size of our\nalgorithm is at most 2n+1 with n being the number of qubits in the quantum\nstates and it is directly applicable to distributed scenarios, where quantum\nstates to be compared are hosted on cross-platform quantum computers. We\nvalidate our algorithm through numerical simulations, laying the groundwork for\nits future deployment on quantum hardware devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum relative entropy, a quantum generalization of the well-known\nKullback-Leibler divergence, serves as a fundamental measure of the\ndistinguishability between quantum states and plays a pivotal role in quantum\ninformation science. Despite its importance, efficiently estimating quantum\nrelative entropy between two quantum states on quantum computers remains a\nsignificant challenge. In this work, we propose the first quantum algorithm for\nestimating quantum relative entropy and Petz R\\'{e}nyi divergence from two\nunknown quantum states on quantum computers, addressing open problems\nhighlighted in [Phys. Rev. A 109, 032431 (2024)] and [IEEE Trans. Inf. Theory\n70, 5653-5680 (2024)]. This is achieved by combining quadrature approximations\nof relative entropies, the variational representation of quantum f-divergences,\nand a new technique for parameterizing Hermitian polynomial operators to\nestimate their traces with quantum states. Notably, the circuit size of our\nalgorithm is at most 2n+1 with n being the number of qubits in the quantum\nstates and it is directly applicable to distributed scenarios, where quantum\nstates to be compared are hosted on cross-platform quantum computers. We\nvalidate our algorithm through numerical simulations, laying the groundwork for\nits future deployment on quantum hardware devices."
                },
                "authors": [
                    {
                        "name": "Yuchen Lu"
                    },
                    {
                        "name": "Kun Fang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Fang"
                },
                "author": "Kun Fang",
                "arxiv_comment": "24 pages, 10 figures; comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07290v1",
                "updated": "2025-01-13T12:59:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    59,
                    53,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T12:59:53Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    59,
                    53,
                    0,
                    13,
                    0
                ],
                "title": "Principles for Responsible AI Consciousness Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Principles for Responsible AI Consciousness Research"
                },
                "summary": "Recent research suggests that it may be possible to build conscious AI\nsystems now or in the near future. Conscious AI systems would arguably deserve\nmoral consideration, and it may be the case that large numbers of conscious\nsystems could be created and caused to suffer. Furthermore, AI systems or\nAI-generated characters may increasingly give the impression of being\nconscious, leading to debate about their moral status. Organisations involved\nin AI research must establish principles and policies to guide research and\ndeployment choices and public communication concerning consciousness. Even if\nan organisation chooses not to study AI consciousness as such, it will still\nneed policies in place, as those developing advanced AI systems risk\ninadvertently creating conscious entities. Responsible research and deployment\npractices are essential to address this possibility. We propose five principles\nfor responsible research and argue that research organisations should make\nvoluntary, public commitments to principles on these lines. Our principles\nconcern research objectives and procedures, knowledge sharing and public\ncommunications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research suggests that it may be possible to build conscious AI\nsystems now or in the near future. Conscious AI systems would arguably deserve\nmoral consideration, and it may be the case that large numbers of conscious\nsystems could be created and caused to suffer. Furthermore, AI systems or\nAI-generated characters may increasingly give the impression of being\nconscious, leading to debate about their moral status. Organisations involved\nin AI research must establish principles and policies to guide research and\ndeployment choices and public communication concerning consciousness. Even if\nan organisation chooses not to study AI consciousness as such, it will still\nneed policies in place, as those developing advanced AI systems risk\ninadvertently creating conscious entities. Responsible research and deployment\npractices are essential to address this possibility. We propose five principles\nfor responsible research and argue that research organisations should make\nvoluntary, public commitments to principles on these lines. Our principles\nconcern research objectives and procedures, knowledge sharing and public\ncommunications."
                },
                "authors": [
                    {
                        "name": "Patrick Butlin"
                    },
                    {
                        "name": "Theodoros Lappas"
                    }
                ],
                "author_detail": {
                    "name": "Theodoros Lappas"
                },
                "author": "Theodoros Lappas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07288v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07288v1",
                "updated": "2025-01-13T12:56:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    56,
                    5,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T12:56:05Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    56,
                    5,
                    0,
                    13,
                    0
                ],
                "title": "LLM-Net: Democratizing LLMs-as-a-Service through Blockchain-based Expert\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Net: Democratizing LLMs-as-a-Service through Blockchain-based Expert\n  Networks"
                },
                "summary": "The centralization of Large Language Models (LLMs) development has created\nsignificant barriers to AI advancement, limiting the democratization of these\npowerful technologies. This centralization, coupled with the scarcity of\nhigh-quality training data and mounting complexity of maintaining comprehensive\nexpertise across rapidly expanding knowledge domains, poses critical challenges\nto the continued growth of LLMs. While solutions like Retrieval-Augmented\nGeneration (RAG) offer potential remedies, maintaining up-to-date expert\nknowledge across diverse domains remains a significant challenge, particularly\ngiven the exponential growth of specialized information. This paper introduces\nLLMs Networks (LLM-Net), a blockchain-based framework that democratizes\nLLMs-as-a-Service through a decentralized network of specialized LLM providers.\nBy leveraging collective computational resources and distributed domain\nexpertise, LLM-Net incorporates fine-tuned expert models for various specific\ndomains, ensuring sustained knowledge growth while maintaining service quality\nthrough collaborative prompting mechanisms. The framework's robust design\nincludes blockchain technology for transparent transaction and performance\nvalidation, establishing an immutable record of service delivery. Our\nsimulation, built on top of state-of-the-art LLMs such as Claude 3.5 Sonnet,\nLlama 3.1, Grok-2, and GPT-4o, validates the effectiveness of the\nreputation-based mechanism in maintaining service quality by selecting\nhigh-performing respondents (LLM providers). Thereby it demonstrates the\npotential of LLM-Net to sustain AI advancement through the integration of\ndecentralized expertise and blockchain-based accountability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The centralization of Large Language Models (LLMs) development has created\nsignificant barriers to AI advancement, limiting the democratization of these\npowerful technologies. This centralization, coupled with the scarcity of\nhigh-quality training data and mounting complexity of maintaining comprehensive\nexpertise across rapidly expanding knowledge domains, poses critical challenges\nto the continued growth of LLMs. While solutions like Retrieval-Augmented\nGeneration (RAG) offer potential remedies, maintaining up-to-date expert\nknowledge across diverse domains remains a significant challenge, particularly\ngiven the exponential growth of specialized information. This paper introduces\nLLMs Networks (LLM-Net), a blockchain-based framework that democratizes\nLLMs-as-a-Service through a decentralized network of specialized LLM providers.\nBy leveraging collective computational resources and distributed domain\nexpertise, LLM-Net incorporates fine-tuned expert models for various specific\ndomains, ensuring sustained knowledge growth while maintaining service quality\nthrough collaborative prompting mechanisms. The framework's robust design\nincludes blockchain technology for transparent transaction and performance\nvalidation, establishing an immutable record of service delivery. Our\nsimulation, built on top of state-of-the-art LLMs such as Claude 3.5 Sonnet,\nLlama 3.1, Grok-2, and GPT-4o, validates the effectiveness of the\nreputation-based mechanism in maintaining service quality by selecting\nhigh-performing respondents (LLM providers). Thereby it demonstrates the\npotential of LLM-Net to sustain AI advancement through the integration of\ndecentralized expertise and blockchain-based accountability."
                },
                "authors": [
                    {
                        "name": "Zan-Kai Chong"
                    },
                    {
                        "name": "Hiroyuki Ohsaki"
                    },
                    {
                        "name": "Bryan Ng"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Ng"
                },
                "author": "Bryan Ng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07288v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07288v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07278v1",
                "updated": "2025-01-13T12:42:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    42,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T12:42:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    42,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "Lifelong Learning of Large Language Model based Agents: A Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifelong Learning of Large Language Model based Agents: A Roadmap"
                },
                "summary": "Lifelong learning, also known as continual or incremental learning, is a\ncrucial component for advancing Artificial General Intelligence (AGI) by\nenabling systems to continuously adapt in dynamic environments. While large\nlanguage models (LLMs) have demonstrated impressive capabilities in natural\nlanguage processing, existing LLM agents are typically designed for static\nsystems and lack the ability to adapt over time in response to new challenges.\nThis survey is the first to systematically summarize the potential techniques\nfor incorporating lifelong learning into LLM-based agents. We categorize the\ncore components of these agents into three modules: the perception module for\nmultimodal input integration, the memory module for storing and retrieving\nevolving knowledge, and the action module for grounded interactions with the\ndynamic environment. We highlight how these pillars collectively enable\ncontinuous adaptation, mitigate catastrophic forgetting, and improve long-term\nperformance. This survey provides a roadmap for researchers and practitioners\nworking to develop lifelong learning capabilities in LLM agents, offering\ninsights into emerging trends, evaluation metrics, and application scenarios.\nRelevant literature and resources are available at \\href{this\nurl}{https://github.com/qianlima-lab/awesome-lifelong-llm-agent}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifelong learning, also known as continual or incremental learning, is a\ncrucial component for advancing Artificial General Intelligence (AGI) by\nenabling systems to continuously adapt in dynamic environments. While large\nlanguage models (LLMs) have demonstrated impressive capabilities in natural\nlanguage processing, existing LLM agents are typically designed for static\nsystems and lack the ability to adapt over time in response to new challenges.\nThis survey is the first to systematically summarize the potential techniques\nfor incorporating lifelong learning into LLM-based agents. We categorize the\ncore components of these agents into three modules: the perception module for\nmultimodal input integration, the memory module for storing and retrieving\nevolving knowledge, and the action module for grounded interactions with the\ndynamic environment. We highlight how these pillars collectively enable\ncontinuous adaptation, mitigate catastrophic forgetting, and improve long-term\nperformance. This survey provides a roadmap for researchers and practitioners\nworking to develop lifelong learning capabilities in LLM agents, offering\ninsights into emerging trends, evaluation metrics, and application scenarios.\nRelevant literature and resources are available at \\href{this\nurl}{https://github.com/qianlima-lab/awesome-lifelong-llm-agent}."
                },
                "authors": [
                    {
                        "name": "Junhao Zheng"
                    },
                    {
                        "name": "Chengming Shi"
                    },
                    {
                        "name": "Xidi Cai"
                    },
                    {
                        "name": "Qiuke Li"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Chenxing Li"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Qianli Ma"
                    }
                ],
                "author_detail": {
                    "name": "Qianli Ma"
                },
                "author": "Qianli Ma",
                "arxiv_comment": "46 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07267v1",
                "updated": "2025-01-13T12:30:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    30,
                    8,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T12:30:08Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    30,
                    8,
                    0,
                    13,
                    0
                ],
                "title": "Transforming Role Classification in Scientific Teams Using LLMs and\n  Advanced Predictive Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming Role Classification in Scientific Teams Using LLMs and\n  Advanced Predictive Analytics"
                },
                "summary": "Scientific team dynamics are critical in determining the nature and impact of\nresearch outputs. However, existing methods for classifying author roles based\non self-reports and clustering lack comprehensive contextual analysis of\ncontributions. Thus, we present a transformative approach to classifying author\nroles in scientific teams using advanced large language models (LLMs), which\noffers a more refined analysis compared to traditional clustering methods.\nSpecifically, we seek to complement and enhance these traditional methods by\nutilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2\n70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting,\nwe categorize author roles and demonstrate that GPT-4 outperforms other models\nacross multiple categories, surpassing traditional approaches such as XGBoost\nand BERT. Our methodology also includes building a predictive deep learning\nmodel using 10 features. By training this model on a dataset derived from the\nOpenAlex database, which provides detailed metadata on academic publications --\nsuch as author-publication history, author affiliation, research topics, and\ncitation counts -- we achieve an F1 score of 0.76, demonstrating robust\nclassification of author roles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific team dynamics are critical in determining the nature and impact of\nresearch outputs. However, existing methods for classifying author roles based\non self-reports and clustering lack comprehensive contextual analysis of\ncontributions. Thus, we present a transformative approach to classifying author\nroles in scientific teams using advanced large language models (LLMs), which\noffers a more refined analysis compared to traditional clustering methods.\nSpecifically, we seek to complement and enhance these traditional methods by\nutilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2\n70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting,\nwe categorize author roles and demonstrate that GPT-4 outperforms other models\nacross multiple categories, surpassing traditional approaches such as XGBoost\nand BERT. Our methodology also includes building a predictive deep learning\nmodel using 10 features. By training this model on a dataset derived from the\nOpenAlex database, which provides detailed metadata on academic publications --\nsuch as author-publication history, author affiliation, research topics, and\ncitation counts -- we achieve an F1 score of 0.76, demonstrating robust\nclassification of author roles."
                },
                "authors": [
                    {
                        "name": "Wonduk Seo"
                    },
                    {
                        "name": "Yi Bu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Bu"
                },
                "author": "Yi Bu",
                "arxiv_comment": "14 pages, 4 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07262v1",
                "updated": "2025-01-13T12:23:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    23,
                    23,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T12:23:23Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    23,
                    23,
                    0,
                    13,
                    0
                ],
                "title": "OblivCDN: A Practical Privacy-preserving CDN with Oblivious Content\n  Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OblivCDN: A Practical Privacy-preserving CDN with Oblivious Content\n  Access"
                },
                "summary": "Content providers increasingly utilise Content Delivery Networks (CDNs) to\nenhance users' content download experience. However, this deployment scenario\nraises significant security concerns regarding content confidentiality and user\nprivacy due to the involvement of third-party providers. Prior proposals using\nprivate information retrieval (PIR) and oblivious RAM (ORAM) have proven\nimpractical due to high computation and communication costs, as well as\nintegration challenges within distributed CDN architectures. In response, we\npresent \\textsf{OblivCDN}, a practical privacy-preserving system meticulously\ndesigned for seamless integration with the existing real-world Internet-CDN\ninfrastructure. Our design strategically adapts Range ORAM primitives to\noptimise memory and disk seeks when accessing contiguous blocks of CDN content,\nboth at the origin and edge servers, while preserving both content\nconfidentiality and user access pattern hiding features. Also, we carefully\ncustomise several oblivious building blocks that integrate the distributed\ntrust model into the ORAM client, thereby eliminating the computational\nbottleneck in the origin server and reducing communication costs between the\norigin server and edge servers. Moreover, the newly-designed ORAM client also\neliminates the need for trusted hardware on edge servers, and thus\nsignificantly ameliorates the compatibility towards networks with massive\nlegacy devices.In real-world streaming evaluations, OblivCDN} demonstrates\nremarkable performance, downloading a $256$ MB video in just $5.6$ seconds.\nThis achievement represents a speedup of $90\\times$ compared to a strawman\napproach (direct ORAM adoption) and a $366\\times$ improvement over the prior\nart, OblivP2P.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content providers increasingly utilise Content Delivery Networks (CDNs) to\nenhance users' content download experience. However, this deployment scenario\nraises significant security concerns regarding content confidentiality and user\nprivacy due to the involvement of third-party providers. Prior proposals using\nprivate information retrieval (PIR) and oblivious RAM (ORAM) have proven\nimpractical due to high computation and communication costs, as well as\nintegration challenges within distributed CDN architectures. In response, we\npresent \\textsf{OblivCDN}, a practical privacy-preserving system meticulously\ndesigned for seamless integration with the existing real-world Internet-CDN\ninfrastructure. Our design strategically adapts Range ORAM primitives to\noptimise memory and disk seeks when accessing contiguous blocks of CDN content,\nboth at the origin and edge servers, while preserving both content\nconfidentiality and user access pattern hiding features. Also, we carefully\ncustomise several oblivious building blocks that integrate the distributed\ntrust model into the ORAM client, thereby eliminating the computational\nbottleneck in the origin server and reducing communication costs between the\norigin server and edge servers. Moreover, the newly-designed ORAM client also\neliminates the need for trusted hardware on edge servers, and thus\nsignificantly ameliorates the compatibility towards networks with massive\nlegacy devices.In real-world streaming evaluations, OblivCDN} demonstrates\nremarkable performance, downloading a $256$ MB video in just $5.6$ seconds.\nThis achievement represents a speedup of $90\\times$ compared to a strawman\napproach (direct ORAM adoption) and a $366\\times$ improvement over the prior\nart, OblivP2P."
                },
                "authors": [
                    {
                        "name": "Viet Vo"
                    },
                    {
                        "name": "Shangqi Lai"
                    },
                    {
                        "name": "Xingliang Yuan"
                    },
                    {
                        "name": "Surya Nepal"
                    },
                    {
                        "name": "Qi Li"
                    }
                ],
                "author_detail": {
                    "name": "Qi Li"
                },
                "author": "Qi Li",
                "arxiv_doi": "10.1145/3708821.3710826",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3708821.3710826",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.07262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "The 20th ACM ASIA Conference on Computer and Communications Security\n  (ACM ASIACCS 2025)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12094v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12094v2",
                "updated": "2025-01-13T11:46:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    46,
                    59,
                    0,
                    13,
                    0
                ],
                "published": "2024-03-15T06:57:08Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    6,
                    57,
                    8,
                    4,
                    75,
                    0
                ],
                "title": "Are LLMs Good Cryptic Crossword Solvers?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLMs Good Cryptic Crossword Solvers?"
                },
                "summary": "Cryptic crosswords are puzzles that rely not only on general knowledge but\nalso on the solver's ability to manipulate language on different levels and\ndeal with various types of wordplay. Previous research suggests that solving\nsuch puzzles is a challenge even for modern NLP models. However, the abilities\nof large language models (LLMs) have not yet been tested on this task. In this\npaper, we establish the benchmark results for three popular LLMs -- LLaMA2,\nMistral, and ChatGPT -- showing that their performance on this task is still\nfar from that of humans.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryptic crosswords are puzzles that rely not only on general knowledge but\nalso on the solver's ability to manipulate language on different levels and\ndeal with various types of wordplay. Previous research suggests that solving\nsuch puzzles is a challenge even for modern NLP models. However, the abilities\nof large language models (LLMs) have not yet been tested on this task. In this\npaper, we establish the benchmark results for three popular LLMs -- LLaMA2,\nMistral, and ChatGPT -- showing that their performance on this task is still\nfar from that of humans."
                },
                "authors": [
                    {
                        "name": "Abdelrahman Sadallah"
                    },
                    {
                        "name": "Daria Kotova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12094v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12094v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07238v1",
                "updated": "2025-01-13T11:36:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    36,
                    33,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T11:36:33Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    36,
                    33,
                    0,
                    13,
                    0
                ],
                "title": "Lessons From Red Teaming 100 Generative AI Products",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lessons From Red Teaming 100 Generative AI Products"
                },
                "summary": "In recent years, AI red teaming has emerged as a practice for probing the\nsafety and security of generative AI systems. Due to the nascency of the field,\nthere are many open questions about how red teaming operations should be\nconducted. Based on our experience red teaming over 100 generative AI products\nat Microsoft, we present our internal threat model ontology and eight main\nlessons we have learned:\n  1. Understand what the system can do and where it is applied\n  2. You don't have to compute gradients to break an AI system\n  3. AI red teaming is not safety benchmarking\n  4. Automation can help cover more of the risk landscape\n  5. The human element of AI red teaming is crucial\n  6. Responsible AI harms are pervasive but difficult to measure\n  7. LLMs amplify existing security risks and introduce new ones\n  8. The work of securing AI systems will never be complete\n  By sharing these insights alongside case studies from our operations, we\noffer practical recommendations aimed at aligning red teaming efforts with real\nworld risks. We also highlight aspects of AI red teaming that we believe are\noften misunderstood and discuss open questions for the field to consider.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, AI red teaming has emerged as a practice for probing the\nsafety and security of generative AI systems. Due to the nascency of the field,\nthere are many open questions about how red teaming operations should be\nconducted. Based on our experience red teaming over 100 generative AI products\nat Microsoft, we present our internal threat model ontology and eight main\nlessons we have learned:\n  1. Understand what the system can do and where it is applied\n  2. You don't have to compute gradients to break an AI system\n  3. AI red teaming is not safety benchmarking\n  4. Automation can help cover more of the risk landscape\n  5. The human element of AI red teaming is crucial\n  6. Responsible AI harms are pervasive but difficult to measure\n  7. LLMs amplify existing security risks and introduce new ones\n  8. The work of securing AI systems will never be complete\n  By sharing these insights alongside case studies from our operations, we\noffer practical recommendations aimed at aligning red teaming efforts with real\nworld risks. We also highlight aspects of AI red teaming that we believe are\noften misunderstood and discuss open questions for the field to consider."
                },
                "authors": [
                    {
                        "name": "Blake Bullwinkel"
                    },
                    {
                        "name": "Amanda Minnich"
                    },
                    {
                        "name": "Shiven Chawla"
                    },
                    {
                        "name": "Gary Lopez"
                    },
                    {
                        "name": "Martin Pouliot"
                    },
                    {
                        "name": "Whitney Maxwell"
                    },
                    {
                        "name": "Joris de Gruyter"
                    },
                    {
                        "name": "Katherine Pratt"
                    },
                    {
                        "name": "Saphir Qi"
                    },
                    {
                        "name": "Nina Chikanov"
                    },
                    {
                        "name": "Roman Lutz"
                    },
                    {
                        "name": "Raja Sekhar Rao Dheekonda"
                    },
                    {
                        "name": "Bolor-Erdene Jagdagdorj"
                    },
                    {
                        "name": "Eugenia Kim"
                    },
                    {
                        "name": "Justin Song"
                    },
                    {
                        "name": "Keegan Hines"
                    },
                    {
                        "name": "Daniel Jones"
                    },
                    {
                        "name": "Giorgio Severi"
                    },
                    {
                        "name": "Richard Lundeen"
                    },
                    {
                        "name": "Sam Vaughan"
                    },
                    {
                        "name": "Victoria Westerhoff"
                    },
                    {
                        "name": "Pete Bryan"
                    },
                    {
                        "name": "Ram Shankar Siva Kumar"
                    },
                    {
                        "name": "Yonatan Zunger"
                    },
                    {
                        "name": "Chang Kawaguchi"
                    },
                    {
                        "name": "Mark Russinovich"
                    }
                ],
                "author_detail": {
                    "name": "Mark Russinovich"
                },
                "author": "Mark Russinovich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07237v1",
                "updated": "2025-01-13T11:35:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    35,
                    9,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T11:35:09Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    35,
                    9,
                    0,
                    13,
                    0
                ],
                "title": "Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Memory Limits: Gradient Wavelet Transform Enhances LLMs\n  Training"
                },
                "summary": "Large language models (LLMs) have shown impressive performance across a range\nof natural language processing tasks. However, their vast number of parameters\nintroduces significant memory challenges during training, particularly when\nusing memory-intensive optimizers like Adam. Existing memory-efficient\nalgorithms often rely on techniques such as singular value decomposition\nprojection or weight freezing. While these approaches help alleviate memory\nconstraints, they generally produce suboptimal results compared to full-rank\nupdates. In this paper, we investigate the memory-efficient method beyond\nlow-rank training, proposing a novel solution called Gradient Wavelet Transform\n(GWT), which applies wavelet transforms to gradients in order to significantly\nreduce the memory requirements for maintaining optimizer states. We demonstrate\nthat GWT can be seamlessly integrated with memory-intensive optimizers,\nenabling efficient training without sacrificing performance. Through extensive\nexperiments on both pre-training and fine-tuning tasks, we show that GWT\nachieves state-of-the-art performance compared with advanced memory-efficient\noptimizers and full-rank approaches in terms of both memory usage and training\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown impressive performance across a range\nof natural language processing tasks. However, their vast number of parameters\nintroduces significant memory challenges during training, particularly when\nusing memory-intensive optimizers like Adam. Existing memory-efficient\nalgorithms often rely on techniques such as singular value decomposition\nprojection or weight freezing. While these approaches help alleviate memory\nconstraints, they generally produce suboptimal results compared to full-rank\nupdates. In this paper, we investigate the memory-efficient method beyond\nlow-rank training, proposing a novel solution called Gradient Wavelet Transform\n(GWT), which applies wavelet transforms to gradients in order to significantly\nreduce the memory requirements for maintaining optimizer states. We demonstrate\nthat GWT can be seamlessly integrated with memory-intensive optimizers,\nenabling efficient training without sacrificing performance. Through extensive\nexperiments on both pre-training and fine-tuning tasks, we show that GWT\nachieves state-of-the-art performance compared with advanced memory-efficient\noptimizers and full-rank approaches in terms of both memory usage and training\nperformance."
                },
                "authors": [
                    {
                        "name": "Ziqing Wen"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Jiahuan Wang"
                    },
                    {
                        "name": "Xiaoge Deng"
                    },
                    {
                        "name": "Jinping Zou"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Tao Sun"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07224v1",
                "updated": "2025-01-13T11:22:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    22,
                    57,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T11:22:57Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    22,
                    57,
                    0,
                    13,
                    0
                ],
                "title": "Touched by ChatGPT: Using an LLM to Drive Affective Tactile Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Touched by ChatGPT: Using an LLM to Drive Affective Tactile Interaction"
                },
                "summary": "Touch is a fundamental aspect of emotion-rich communication, playing a vital\nrole in human interaction and offering significant potential in human-robot\ninteraction. Previous research has demonstrated that a sparse representation of\nhuman touch can effectively convey social tactile signals. However, advances in\nhuman-robot tactile interaction remain limited, as many humanoid robots possess\nsimplistic capabilities, such as only opening and closing their hands,\nrestricting nuanced tactile expressions. In this study, we explore how a robot\ncan use sparse representations of tactile vibrations to convey emotions to a\nperson. To achieve this, we developed a wearable sleeve integrated with a 5x5\ngrid of vibration motors, enabling the robot to communicate diverse tactile\nemotions and gestures. Using chain prompts within a Large Language Model (LLM),\nwe generated distinct 10-second vibration patterns corresponding to 10 emotions\n(e.g., happiness, sadness, fear) and 6 touch gestures (e.g., pat, rub, tap).\nParticipants (N = 32) then rated each vibration stimulus based on perceived\nvalence and arousal. People are accurate at recognising intended emotions, a\nresult which aligns with earlier findings. These results highlight the LLM's\nability to generate emotional haptic data and effectively convey emotions\nthrough tactile signals. By translating complex emotional and tactile\nexpressions into vibratory patterns, this research demonstrates how LLMs can\nenhance physical interaction between humans and robots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Touch is a fundamental aspect of emotion-rich communication, playing a vital\nrole in human interaction and offering significant potential in human-robot\ninteraction. Previous research has demonstrated that a sparse representation of\nhuman touch can effectively convey social tactile signals. However, advances in\nhuman-robot tactile interaction remain limited, as many humanoid robots possess\nsimplistic capabilities, such as only opening and closing their hands,\nrestricting nuanced tactile expressions. In this study, we explore how a robot\ncan use sparse representations of tactile vibrations to convey emotions to a\nperson. To achieve this, we developed a wearable sleeve integrated with a 5x5\ngrid of vibration motors, enabling the robot to communicate diverse tactile\nemotions and gestures. Using chain prompts within a Large Language Model (LLM),\nwe generated distinct 10-second vibration patterns corresponding to 10 emotions\n(e.g., happiness, sadness, fear) and 6 touch gestures (e.g., pat, rub, tap).\nParticipants (N = 32) then rated each vibration stimulus based on perceived\nvalence and arousal. People are accurate at recognising intended emotions, a\nresult which aligns with earlier findings. These results highlight the LLM's\nability to generate emotional haptic data and effectively convey emotions\nthrough tactile signals. By translating complex emotional and tactile\nexpressions into vibratory patterns, this research demonstrates how LLMs can\nenhance physical interaction between humans and robots."
                },
                "authors": [
                    {
                        "name": "Qiaoqiao Ren"
                    },
                    {
                        "name": "Tony Belpaeme"
                    }
                ],
                "author_detail": {
                    "name": "Tony Belpaeme"
                },
                "author": "Tony Belpaeme",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07209v1",
                "updated": "2025-01-13T11:04:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    4,
                    5,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T11:04:05Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    4,
                    5,
                    0,
                    13,
                    0
                ],
                "title": "Privacy-Preserving Authentication: Theory vs. Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy-Preserving Authentication: Theory vs. Practice"
                },
                "summary": "With the increasing use of online services, the protection of the privacy of\nusers becomes more and more important. This is particularly critical as\nauthentication and authorization as realized on the Internet nowadays,\ntypically relies on centralized identity management solutions. Although those\nare very convenient from a user's perspective, they are quite intrusive from a\nprivacy perspective and are currently far from implementing the concept of data\nminimization. Fortunately, cryptography offers exciting primitives such as\nzero-knowledge proofs and advanced signature schemes to realize various forms\nof so-called anonymous credentials. Such primitives allow to realize online\nauthentication and authorization with a high level of built-in privacy\nprotection (what we call privacy-preserving authentication). Though these\nprimitives have already been researched for various decades and are well\nunderstood in the research community, unfortunately, they lack widespread\nadoption. In this paper, we look at the problems, what cryptography can do,\nsome deployment examples, and barriers to widespread adoption. Latter using the\nexample of the EU Digital Identity Wallet (EUDIW) and the recent discussion and\nfeedback from cryptography experts around this topic. We also briefly comment\non the transition to post-quantum cryptography.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing use of online services, the protection of the privacy of\nusers becomes more and more important. This is particularly critical as\nauthentication and authorization as realized on the Internet nowadays,\ntypically relies on centralized identity management solutions. Although those\nare very convenient from a user's perspective, they are quite intrusive from a\nprivacy perspective and are currently far from implementing the concept of data\nminimization. Fortunately, cryptography offers exciting primitives such as\nzero-knowledge proofs and advanced signature schemes to realize various forms\nof so-called anonymous credentials. Such primitives allow to realize online\nauthentication and authorization with a high level of built-in privacy\nprotection (what we call privacy-preserving authentication). Though these\nprimitives have already been researched for various decades and are well\nunderstood in the research community, unfortunately, they lack widespread\nadoption. In this paper, we look at the problems, what cryptography can do,\nsome deployment examples, and barriers to widespread adoption. Latter using the\nexample of the EU Digital Identity Wallet (EUDIW) and the recent discussion and\nfeedback from cryptography experts around this topic. We also briefly comment\non the transition to post-quantum cryptography."
                },
                "authors": [
                    {
                        "name": "Daniel Slamanig"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Slamanig"
                },
                "arxiv_affiliation": "Research Institute CODE, UniversitÃ¤t der Bundeswehr MÃ¼nchen",
                "author": "Daniel Slamanig",
                "arxiv_comment": "This paper is based on a keynote with the same title given at the\n  19th IFIP Summer School on Privacy and Identity Management held between 10th\n  and 13th September 2024 in Madrid, Spain and appears in the Proceedings of\n  the Summer School",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06007v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06007v2",
                "updated": "2025-01-13T11:02:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    11,
                    2,
                    23,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-10T14:42:08Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    14,
                    42,
                    8,
                    4,
                    10,
                    0
                ],
                "title": "CoNOAir: A Neural Operator for Forecasting Carbon Monoxide Evolution in\n  Cities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoNOAir: A Neural Operator for Forecasting Carbon Monoxide Evolution in\n  Cities"
                },
                "summary": "Carbon Monoxide (CO) is a dominant pollutant in urban areas due to the energy\ngeneration from fossil fuels for industry, automobile, and domestic\nrequirements. Forecasting the evolution of CO in real-time can enable the\ndeployment of effective early warning systems and intervention strategies.\nHowever, the computational cost associated with the physics and chemistry-based\nsimulation makes it prohibitive to implement such a model at the city and\ncountry scale. To address this challenge, here, we present a machine learning\nmodel based on neural operator, namely, Complex Neural Operator for Air Quality\n(CoNOAir), that can effectively forecast CO concentrations. We demonstrate this\nby developing a country-level model for short-term (hourly) and long-term\n(72-hour) forecasts of CO concentrations. Our model outperforms\nstate-of-the-art models such as Fourier neural operators (FNO) and provides\nreliable predictions for both short and long-term forecasts. We further analyse\nthe capability of the model to capture extreme events and generate forecasts in\nurban cities in India. Interestingly, we observe that the model predicts the\nnext hour CO concentrations with R2 values greater than 0.95 for all the cities\nconsidered. The deployment of such a model can greatly assist the governing\nbodies to provide early warning, plan intervention strategies, and develop\neffective strategies by considering several what-if scenarios. Altogether, the\npresent approach could provide a fillip to real-time predictions of CO\npollution in urban cities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Carbon Monoxide (CO) is a dominant pollutant in urban areas due to the energy\ngeneration from fossil fuels for industry, automobile, and domestic\nrequirements. Forecasting the evolution of CO in real-time can enable the\ndeployment of effective early warning systems and intervention strategies.\nHowever, the computational cost associated with the physics and chemistry-based\nsimulation makes it prohibitive to implement such a model at the city and\ncountry scale. To address this challenge, here, we present a machine learning\nmodel based on neural operator, namely, Complex Neural Operator for Air Quality\n(CoNOAir), that can effectively forecast CO concentrations. We demonstrate this\nby developing a country-level model for short-term (hourly) and long-term\n(72-hour) forecasts of CO concentrations. Our model outperforms\nstate-of-the-art models such as Fourier neural operators (FNO) and provides\nreliable predictions for both short and long-term forecasts. We further analyse\nthe capability of the model to capture extreme events and generate forecasts in\nurban cities in India. Interestingly, we observe that the model predicts the\nnext hour CO concentrations with R2 values greater than 0.95 for all the cities\nconsidered. The deployment of such a model can greatly assist the governing\nbodies to provide early warning, plan intervention strategies, and develop\neffective strategies by considering several what-if scenarios. Altogether, the\npresent approach could provide a fillip to real-time predictions of CO\npollution in urban cities."
                },
                "authors": [
                    {
                        "name": "Sanchit Bedi"
                    },
                    {
                        "name": "Karn Tiwari"
                    },
                    {
                        "name": "Prathosh A. P."
                    },
                    {
                        "name": "Sri Harsha Kota"
                    },
                    {
                        "name": "N. M. Anoop Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "N. M. Anoop Krishnan"
                },
                "arxiv_affiliation": "Yardi School of Artificial Intelligence, Indian Institute of Technology Delhi, New Delhi, India",
                "author": "N. M. Anoop Krishnan",
                "arxiv_comment": "28 pages, 14 figures, under submission process",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06007v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06007v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07203v1",
                "updated": "2025-01-13T10:56:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    56,
                    23,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T10:56:23Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    56,
                    23,
                    0,
                    13,
                    0
                ],
                "title": "Integrated Wind Farm Design: Optimizing Turbine Placement and Cable\n  Routing with Wake Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated Wind Farm Design: Optimizing Turbine Placement and Cable\n  Routing with Wake Effects"
                },
                "summary": "An accelerated deployment of renewable energy sources is crucial for a\nsuccessful transformation of the current energy system, with wind energy\nplaying a key role in this transition. This study addresses the integrated wind\nfarm layout and cable routing problem, a challenging nonlinear optimization\nproblem. We model this problem as an extended version of the Quota Steiner Tree\nProblem (QSTP), optimizing turbine placement and network connectivity\nsimultaneously to meet specified expansion targets. Our proposed approach\naccounts for the wake effect - a region of reduced wind speed induced by each\ninstalled turbine - and enforces minimum spacing between turbines. We introduce\nan exact solution framework in terms of the novel Quota Steiner Tree Problem\nwith interference (QSTPI). By leveraging an interference-based splitting\nstrategy, we develop an advanced solver capable of tackling large-scale problem\ninstances. The presented approach outperforms generic state-of-the-art mixed\ninteger programming solvers on our dataset by up to two orders of magnitude.\nMoreover, we demonstrate that our integrated method significantly reduces the\ncosts in contrast to a sequential approach. Thus, we provide a planning tool\nthat enhances existing planning methodologies for supporting a faster and\ncost-efficient expansion of wind energy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An accelerated deployment of renewable energy sources is crucial for a\nsuccessful transformation of the current energy system, with wind energy\nplaying a key role in this transition. This study addresses the integrated wind\nfarm layout and cable routing problem, a challenging nonlinear optimization\nproblem. We model this problem as an extended version of the Quota Steiner Tree\nProblem (QSTP), optimizing turbine placement and network connectivity\nsimultaneously to meet specified expansion targets. Our proposed approach\naccounts for the wake effect - a region of reduced wind speed induced by each\ninstalled turbine - and enforces minimum spacing between turbines. We introduce\nan exact solution framework in terms of the novel Quota Steiner Tree Problem\nwith interference (QSTPI). By leveraging an interference-based splitting\nstrategy, we develop an advanced solver capable of tackling large-scale problem\ninstances. The presented approach outperforms generic state-of-the-art mixed\ninteger programming solvers on our dataset by up to two orders of magnitude.\nMoreover, we demonstrate that our integrated method significantly reduces the\ncosts in contrast to a sequential approach. Thus, we provide a planning tool\nthat enhances existing planning methodologies for supporting a faster and\ncost-efficient expansion of wind energy."
                },
                "authors": [
                    {
                        "name": "Jaap Pedersen"
                    },
                    {
                        "name": "Niels Lindner"
                    },
                    {
                        "name": "Daniel Rehfeldt"
                    },
                    {
                        "name": "Thorsten Koch"
                    }
                ],
                "author_detail": {
                    "name": "Thorsten Koch"
                },
                "author": "Thorsten Koch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90-10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07202v1",
                "updated": "2025-01-13T10:53:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    53,
                    48,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T10:53:48Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    53,
                    48,
                    0,
                    13,
                    0
                ],
                "title": "FaceOracle: Chat with a Face Image Oracle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaceOracle: Chat with a Face Image Oracle"
                },
                "summary": "A face image is a mandatory part of ID and travel documents. Obtaining\nhigh-quality face images when issuing such documents is crucial for both human\nexaminers and automated face recognition systems. In several international\nstandards, face image quality requirements are intricate and defined in detail.\nIdentifying and understanding non-compliance or defects in the submitted face\nimages is crucial for both issuing authorities and applicants. In this work, we\nintroduce FaceOracle, an LLM-powered AI assistant that helps its users analyze\na face image in a natural conversational manner using standard compliant\nalgorithms. Leveraging the power of LLMs, users can get explanations of various\nface image quality concepts as well as interpret the outcome of face image\nquality assessment (FIQA) algorithms. We implement a proof-of-concept that\ndemonstrates how experts at an issuing authority could integrate FaceOracle\ninto their workflow to analyze, understand, and communicate their decisions\nmore efficiently, resulting in enhanced productivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A face image is a mandatory part of ID and travel documents. Obtaining\nhigh-quality face images when issuing such documents is crucial for both human\nexaminers and automated face recognition systems. In several international\nstandards, face image quality requirements are intricate and defined in detail.\nIdentifying and understanding non-compliance or defects in the submitted face\nimages is crucial for both issuing authorities and applicants. In this work, we\nintroduce FaceOracle, an LLM-powered AI assistant that helps its users analyze\na face image in a natural conversational manner using standard compliant\nalgorithms. Leveraging the power of LLMs, users can get explanations of various\nface image quality concepts as well as interpret the outcome of face image\nquality assessment (FIQA) algorithms. We implement a proof-of-concept that\ndemonstrates how experts at an issuing authority could integrate FaceOracle\ninto their workflow to analyze, understand, and communicate their decisions\nmore efficiently, resulting in enhanced productivity."
                },
                "authors": [
                    {
                        "name": "Wassim Kabbani"
                    },
                    {
                        "name": "Kiran Raja"
                    },
                    {
                        "name": "Raghavendra Ramachandra"
                    },
                    {
                        "name": "Christoph Busch"
                    }
                ],
                "author_detail": {
                    "name": "Christoph Busch"
                },
                "author": "Christoph Busch",
                "arxiv_journal_ref": "ECCV 2024 Workshops",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00393v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00393v3",
                "updated": "2025-01-13T10:50:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    50,
                    53,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-01T08:46:36Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    8,
                    46,
                    36,
                    2,
                    122,
                    0
                ],
                "title": "Inferring State Machine from the Protocol Implementation via Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring State Machine from the Protocol Implementation via Large\n  Language Model"
                },
                "summary": "State machines play a pivotal role in augmenting the efficacy of protocol\nanalyzing to unveil more vulnerabilities. However, inferring state machines\nfrom network protocol implementations presents significant challenges, mainly\nbecause of the complicated code syntax and semantics. Traditional methods based\non dynamic analysis often overlook crucial state transitions due to limited\ncoverage, while static analysis suffers from path explosion facing to protocol\nimplementations. To address these limitations, we propose an innovative state\nmachine inference approach powered by Large Language Models (LLMs) named\nProtocolGPT. Utilizing retrieval augmented generation technology, this method\naugments pre-trained model with specific knowledge drawn from protocol\nimplementations. Through targeted prompt engineering, we systematically\nidentify and infer the underlying state machines. Our evaluation across six\nprotocol implementations demonstrates the method's high efficacy, achieving\nprecision exceeding 90% and successfully delineating differences on state\nmachines among various implementations of the same protocol. Integrating our\napproach with protocol fuzzing significantly improves fuzzers by more than 20%\nin terms of coverage and detects two zero-day vulnerabilities compared to the\nbaseline. Our proposed method represents a major advancement in accurate state\nmachine inference and highlights the substantial potential of LLMs in enhancing\nnetwork protocol security analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State machines play a pivotal role in augmenting the efficacy of protocol\nanalyzing to unveil more vulnerabilities. However, inferring state machines\nfrom network protocol implementations presents significant challenges, mainly\nbecause of the complicated code syntax and semantics. Traditional methods based\non dynamic analysis often overlook crucial state transitions due to limited\ncoverage, while static analysis suffers from path explosion facing to protocol\nimplementations. To address these limitations, we propose an innovative state\nmachine inference approach powered by Large Language Models (LLMs) named\nProtocolGPT. Utilizing retrieval augmented generation technology, this method\naugments pre-trained model with specific knowledge drawn from protocol\nimplementations. Through targeted prompt engineering, we systematically\nidentify and infer the underlying state machines. Our evaluation across six\nprotocol implementations demonstrates the method's high efficacy, achieving\nprecision exceeding 90% and successfully delineating differences on state\nmachines among various implementations of the same protocol. Integrating our\napproach with protocol fuzzing significantly improves fuzzers by more than 20%\nin terms of coverage and detects two zero-day vulnerabilities compared to the\nbaseline. Our proposed method represents a major advancement in accurate state\nmachine inference and highlights the substantial potential of LLMs in enhancing\nnetwork protocol security analysis."
                },
                "authors": [
                    {
                        "name": "Haiyang Wei"
                    },
                    {
                        "name": "Zhengjie Du"
                    },
                    {
                        "name": "Haohui Huang"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Guang Cheng"
                    },
                    {
                        "name": "Linzhang Wang"
                    },
                    {
                        "name": "Bing Mao"
                    }
                ],
                "author_detail": {
                    "name": "Bing Mao"
                },
                "author": "Bing Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00393v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00393v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05855v2",
                "updated": "2025-01-13T10:39:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    39,
                    54,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-10T10:53:48Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    53,
                    48,
                    4,
                    10,
                    0
                ],
                "title": "ConSim: Measuring Concept-Based Explanations' Effectiveness with\n  Automated Simulatability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConSim: Measuring Concept-Based Explanations' Effectiveness with\n  Automated Simulatability"
                },
                "summary": "Concept-based explanations work by mapping complex model computations to\nhuman-understandable concepts. Evaluating such explanations is very difficult,\nas it includes not only the quality of the induced space of possible concepts\nbut also how effectively the chosen concepts are communicated to users.\nExisting evaluation metrics often focus solely on the former, neglecting the\nlatter. We introduce an evaluation framework for measuring concept explanations\nvia automated simulatability: a simulator's ability to predict the explained\nmodel's outputs based on the provided explanations. This approach accounts for\nboth the concept space and its interpretation in an end-to-end evaluation.\nHuman studies for simulatability are notoriously difficult to enact,\nparticularly at the scale of a wide, comprehensive empirical evaluation (which\nis the subject of this work). We propose using large language models (LLMs) as\nsimulators to approximate the evaluation and report various analyses to make\nsuch approximations reliable. Our method allows for scalable and consistent\nevaluation across various models and datasets. We report a comprehensive\nempirical evaluation using this framework and show that LLMs provide consistent\nrankings of explanation methods. Code available at\nhttps://github.com/AnonymousConSim/ConSim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept-based explanations work by mapping complex model computations to\nhuman-understandable concepts. Evaluating such explanations is very difficult,\nas it includes not only the quality of the induced space of possible concepts\nbut also how effectively the chosen concepts are communicated to users.\nExisting evaluation metrics often focus solely on the former, neglecting the\nlatter. We introduce an evaluation framework for measuring concept explanations\nvia automated simulatability: a simulator's ability to predict the explained\nmodel's outputs based on the provided explanations. This approach accounts for\nboth the concept space and its interpretation in an end-to-end evaluation.\nHuman studies for simulatability are notoriously difficult to enact,\nparticularly at the scale of a wide, comprehensive empirical evaluation (which\nis the subject of this work). We propose using large language models (LLMs) as\nsimulators to approximate the evaluation and report various analyses to make\nsuch approximations reliable. Our method allows for scalable and consistent\nevaluation across various models and datasets. We report a comprehensive\nempirical evaluation using this framework and show that LLMs provide consistent\nrankings of explanation methods. Code available at\nhttps://github.com/AnonymousConSim/ConSim."
                },
                "authors": [
                    {
                        "name": "Antonin PochÃ©"
                    },
                    {
                        "name": "Alon Jacovi"
                    },
                    {
                        "name": "Agustin Martin Picard"
                    },
                    {
                        "name": "Victor Boutin"
                    },
                    {
                        "name": "Fanny Jourdan"
                    }
                ],
                "author_detail": {
                    "name": "Fanny Jourdan"
                },
                "arxiv_affiliation": "CERCO UMR5549, ANITI",
                "author": "Fanny Jourdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11543v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11543v4",
                "updated": "2025-01-13T10:39:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    39,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2024-11-18T13:01:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    1,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "PSA-VLM: Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PSA-VLM: Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment"
                },
                "summary": "Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark."
                },
                "authors": [
                    {
                        "name": "Zhendong Liu"
                    },
                    {
                        "name": "Yuanbi Nie"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xiangyu Yue"
                    },
                    {
                        "name": "Qiushi Cui"
                    },
                    {
                        "name": "Chongjun Wang"
                    },
                    {
                        "name": "Xiaoyong Zhu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2405.13581",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11543v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11543v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15911v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15911v2",
                "updated": "2025-01-13T10:23:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    23,
                    35,
                    0,
                    13,
                    0
                ],
                "published": "2024-10-21T11:33:18Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    11,
                    33,
                    18,
                    0,
                    295,
                    0
                ],
                "title": "DefVerify: Do Hate Speech Models Reflect Their Dataset's Definition?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DefVerify: Do Hate Speech Models Reflect Their Dataset's Definition?"
                },
                "summary": "When building a predictive model, it is often difficult to ensure that\napplication-specific requirements are encoded by the model that will eventually\nbe deployed. Consider researchers working on hate speech detection. They will\nhave an idea of what is considered hate speech, but building a model that\nreflects their view accurately requires preserving those ideals throughout the\nworkflow of data set construction and model training. Complications such as\nsampling bias, annotation bias, and model misspecification almost always arise,\npossibly resulting in a gap between the application specification and the\nmodel's actual behavior upon deployment. To address this issue for hate speech\ndetection, we propose DefVerify: a 3-step procedure that (i) encodes a\nuser-specified definition of hate speech, (ii) quantifies to what extent the\nmodel reflects the intended definition, and (iii) tries to identify the point\nof failure in the workflow. We use DefVerify to find gaps between definition\nand model behavior when applied to six popular hate speech benchmark datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When building a predictive model, it is often difficult to ensure that\napplication-specific requirements are encoded by the model that will eventually\nbe deployed. Consider researchers working on hate speech detection. They will\nhave an idea of what is considered hate speech, but building a model that\nreflects their view accurately requires preserving those ideals throughout the\nworkflow of data set construction and model training. Complications such as\nsampling bias, annotation bias, and model misspecification almost always arise,\npossibly resulting in a gap between the application specification and the\nmodel's actual behavior upon deployment. To address this issue for hate speech\ndetection, we propose DefVerify: a 3-step procedure that (i) encodes a\nuser-specified definition of hate speech, (ii) quantifies to what extent the\nmodel reflects the intended definition, and (iii) tries to identify the point\nof failure in the workflow. We use DefVerify to find gaps between definition\nand model behavior when applied to six popular hate speech benchmark datasets."
                },
                "authors": [
                    {
                        "name": "Urja Khurana"
                    },
                    {
                        "name": "Eric Nalisnick"
                    },
                    {
                        "name": "Antske Fokkens"
                    }
                ],
                "author_detail": {
                    "name": "Antske Fokkens"
                },
                "author": "Antske Fokkens",
                "arxiv_comment": "Camera-ready COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.15911v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15911v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.20971v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.20971v2",
                "updated": "2025-01-13T10:14:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    14,
                    27,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-31T16:18:46Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    16,
                    18,
                    46,
                    4,
                    152,
                    0
                ],
                "title": "Amortizing intractable inference in diffusion models for vision,\n  language, and control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amortizing intractable inference in diffusion models for vision,\n  language, and control"
                },
                "summary": "Diffusion models have emerged as effective distribution estimators in vision,\nlanguage, and reinforcement learning, but their use as priors in downstream\ntasks poses an intractable posterior inference problem. This paper studies\namortized sampling of the posterior over data, $\\mathbf{x}\\sim p^{\\rm\npost}(\\mathbf{x})\\propto p(\\mathbf{x})r(\\mathbf{x})$, in a model that consists\nof a diffusion generative model prior $p(\\mathbf{x})$ and a black-box\nconstraint or likelihood function $r(\\mathbf{x})$. We state and prove the\nasymptotic correctness of a data-free learning objective, relative trajectory\nbalance, for training a diffusion model that samples from this posterior, a\nproblem that existing methods solve only approximately or in restricted cases.\nRelative trajectory balance arises from the generative flow network perspective\non diffusion models, which allows the use of deep reinforcement learning\ntechniques to improve mode coverage. Experiments illustrate the broad potential\nof unbiased inference of arbitrary posteriors under diffusion priors: in vision\n(classifier guidance), language (infilling under a discrete diffusion LLM), and\nmultimodal data (text-to-image generation). Beyond generative modeling, we\napply relative trajectory balance to the problem of continuous control with a\nscore-based behavior prior, achieving state-of-the-art results on benchmarks in\noffline reinforcement learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as effective distribution estimators in vision,\nlanguage, and reinforcement learning, but their use as priors in downstream\ntasks poses an intractable posterior inference problem. This paper studies\namortized sampling of the posterior over data, $\\mathbf{x}\\sim p^{\\rm\npost}(\\mathbf{x})\\propto p(\\mathbf{x})r(\\mathbf{x})$, in a model that consists\nof a diffusion generative model prior $p(\\mathbf{x})$ and a black-box\nconstraint or likelihood function $r(\\mathbf{x})$. We state and prove the\nasymptotic correctness of a data-free learning objective, relative trajectory\nbalance, for training a diffusion model that samples from this posterior, a\nproblem that existing methods solve only approximately or in restricted cases.\nRelative trajectory balance arises from the generative flow network perspective\non diffusion models, which allows the use of deep reinforcement learning\ntechniques to improve mode coverage. Experiments illustrate the broad potential\nof unbiased inference of arbitrary posteriors under diffusion priors: in vision\n(classifier guidance), language (infilling under a discrete diffusion LLM), and\nmultimodal data (text-to-image generation). Beyond generative modeling, we\napply relative trajectory balance to the problem of continuous control with a\nscore-based behavior prior, achieving state-of-the-art results on benchmarks in\noffline reinforcement learning."
                },
                "authors": [
                    {
                        "name": "Siddarth Venkatraman"
                    },
                    {
                        "name": "Moksh Jain"
                    },
                    {
                        "name": "Luca Scimeca"
                    },
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Marcin Sendera"
                    },
                    {
                        "name": "Mohsin Hasan"
                    },
                    {
                        "name": "Luke Rowe"
                    },
                    {
                        "name": "Sarthak Mittal"
                    },
                    {
                        "name": "Pablo Lemos"
                    },
                    {
                        "name": "Emmanuel Bengio"
                    },
                    {
                        "name": "Alexandre Adam"
                    },
                    {
                        "name": "Jarrid Rector-Brooks"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Glen Berseth"
                    },
                    {
                        "name": "Nikolay Malkin"
                    }
                ],
                "author_detail": {
                    "name": "Nikolay Malkin"
                },
                "author": "Nikolay Malkin",
                "arxiv_comment": "NeurIPS 2024; code: https://github.com/GFNOrg/diffusion-finetuning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.20971v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.20971v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v3",
                "updated": "2025-01-13T10:02:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    10,
                    2,
                    27,
                    0,
                    13,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Efficient Large Foundation Models Design: A Perspective From Model and\n  System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Large Foundation Models Design: A Perspective From Model and\n  System Co-Design"
                },
                "summary": "This paper focuses on modern efficient training and inference technologies on\nfoundation models and illustrates them from two perspectives: model and system\ndesign. Model and System Design optimize LLM training and inference from\ndifferent aspects to save computational resources, making LLMs more efficient,\naffordable, and more accessible. The paper list repository is available at\n\\url{https://github.com/NoakLiu/Efficient-Foundation-Models-Survey}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper focuses on modern efficient training and inference technologies on\nfoundation models and illustrates them from two perspectives: model and system\ndesign. Model and System Design optimize LLM training and inference from\ndifferent aspects to save computational resources, making LLMs more efficient,\naffordable, and more accessible. The paper list repository is available at\n\\url{https://github.com/NoakLiu/Efficient-Foundation-Models-Survey}"
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Zhixin Lai"
                    },
                    {
                        "name": "Yite Wang"
                    },
                    {
                        "name": "Jing Wu"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Sina Alinejad"
                    },
                    {
                        "name": "Benjamin Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07165v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07165v1",
                "updated": "2025-01-13T09:51:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    51,
                    23,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T09:51:23Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    51,
                    23,
                    0,
                    13,
                    0
                ],
                "title": "Unveiling Code Clone Patterns in Open Source VR Software: An Empirical\n  Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Code Clone Patterns in Open Source VR Software: An Empirical\n  Study"
                },
                "summary": "Code cloning is frequently observed in software development, often leading to\na variety of maintenance and security issues. While substantial research has\nbeen conducted on code cloning in traditional software, to the best of my\nknowledge, there is a lack of studies on cloning in VR software that consider\nits unique nature, particularly the presence of numerous serialized files in\nconjunction with the source code. In this paper, we conduct the first\nlarge-scale quantitative empirical analysis of software clones in 345\nopen-source VR projects, using the NiCad detector for source code clone\ndetection and large language models (LLMs) for identifying serialized file\nclones. Our study leads to a number of insights into cloning phenomena in VR\nsoftware, guided by seven carefully formulated research questions. These\nfindings, along with their implications, are anticipated to provide useful\nguidance for both researchers and software developers within the VR field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code cloning is frequently observed in software development, often leading to\na variety of maintenance and security issues. While substantial research has\nbeen conducted on code cloning in traditional software, to the best of my\nknowledge, there is a lack of studies on cloning in VR software that consider\nits unique nature, particularly the presence of numerous serialized files in\nconjunction with the source code. In this paper, we conduct the first\nlarge-scale quantitative empirical analysis of software clones in 345\nopen-source VR projects, using the NiCad detector for source code clone\ndetection and large language models (LLMs) for identifying serialized file\nclones. Our study leads to a number of insights into cloning phenomena in VR\nsoftware, guided by seven carefully formulated research questions. These\nfindings, along with their implications, are anticipated to provide useful\nguidance for both researchers and software developers within the VR field."
                },
                "authors": [
                    {
                        "name": "Huashan Chen"
                    },
                    {
                        "name": "Zisheng Huang"
                    },
                    {
                        "name": "Yifan Xu"
                    },
                    {
                        "name": "Wenjie Huang"
                    },
                    {
                        "name": "Jinfu Chen"
                    },
                    {
                        "name": "Haotang Li"
                    },
                    {
                        "name": "Kebin Peng"
                    },
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Sen He"
                    }
                ],
                "author_detail": {
                    "name": "Sen He"
                },
                "author": "Sen He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07165v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07161v1",
                "updated": "2025-01-13T09:41:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    41,
                    54,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T09:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    41,
                    54,
                    0,
                    13,
                    0
                ],
                "title": "QuantuneV2: Compiler-Based Local Metric-Driven Mixed Precision\n  Quantization for Practical Embedded AI Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuantuneV2: Compiler-Based Local Metric-Driven Mixed Precision\n  Quantization for Practical Embedded AI Applications"
                },
                "summary": "Mixed-precision quantization methods have been proposed to reduce model size\nwhile minimizing accuracy degradation. However, existing studies require\nretraining and do not consider the computational overhead and intermediate\nrepresentations (IR) generated during the compilation process, limiting their\napplication at the compiler level. This computational overhead refers to the\nruntime latency caused by frequent quantization and dequantization operations\nduring inference. Performing these operations at the individual operator level\ncauses significant runtime delays. To address these issues, we propose\nQuantuneV2, a compiler-based mixed-precision quantization method designed for\npractical embedded AI applications. QuantuneV2 performs inference only twice,\nonce before quantization and once after quantization, and operates with a\ncomputational complexity of O(n) that increases linearly with the number of\nmodel parameters. We also made the sensitivity analysis more stable by using\nlocal metrics like weights, activation values, the Signal to Quantization Noise\nRatio, and the Mean Squared Error. We also cut down on computational overhead\nby choosing the best IR and using operator fusion. Experimental results show\nthat QuantuneV2 achieved up to a 10.28 percent improvement in accuracy and a\n12.52 percent increase in speed compared to existing methods across five\nmodels: ResNet18v1, ResNet50v1, SqueezeNetv1, VGGNet, and MobileNetv2. This\ndemonstrates that QuantuneV2 enhances model performance while maintaining\ncomputational efficiency, making it suitable for deployment in embedded AI\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-precision quantization methods have been proposed to reduce model size\nwhile minimizing accuracy degradation. However, existing studies require\nretraining and do not consider the computational overhead and intermediate\nrepresentations (IR) generated during the compilation process, limiting their\napplication at the compiler level. This computational overhead refers to the\nruntime latency caused by frequent quantization and dequantization operations\nduring inference. Performing these operations at the individual operator level\ncauses significant runtime delays. To address these issues, we propose\nQuantuneV2, a compiler-based mixed-precision quantization method designed for\npractical embedded AI applications. QuantuneV2 performs inference only twice,\nonce before quantization and once after quantization, and operates with a\ncomputational complexity of O(n) that increases linearly with the number of\nmodel parameters. We also made the sensitivity analysis more stable by using\nlocal metrics like weights, activation values, the Signal to Quantization Noise\nRatio, and the Mean Squared Error. We also cut down on computational overhead\nby choosing the best IR and using operator fusion. Experimental results show\nthat QuantuneV2 achieved up to a 10.28 percent improvement in accuracy and a\n12.52 percent increase in speed compared to existing methods across five\nmodels: ResNet18v1, ResNet50v1, SqueezeNetv1, VGGNet, and MobileNetv2. This\ndemonstrates that QuantuneV2 enhances model performance while maintaining\ncomputational efficiency, making it suitable for deployment in embedded AI\nenvironments."
                },
                "authors": [
                    {
                        "name": "Jeongseok Kim"
                    },
                    {
                        "name": "Jemin Lee"
                    },
                    {
                        "name": "Yongin Kwon"
                    },
                    {
                        "name": "Daeyoung Kim"
                    }
                ],
                "author_detail": {
                    "name": "Daeyoung Kim"
                },
                "author": "Daeyoung Kim",
                "arxiv_comment": "18 pages, 10 figures, Accepted in Future Generation Computer Systems\n  Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.18873v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.18873v3",
                "updated": "2025-01-13T09:11:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    11,
                    12,
                    0,
                    13,
                    0
                ],
                "published": "2024-06-27T03:57:12Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    3,
                    57,
                    12,
                    3,
                    179,
                    0
                ],
                "title": "LayoutCopilot: An LLM-powered Multi-agent Collaborative Framework for\n  Interactive Analog Layout Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LayoutCopilot: An LLM-powered Multi-agent Collaborative Framework for\n  Interactive Analog Layout Design"
                },
                "summary": "Analog layout design heavily involves interactive processes between humans\nand design tools. Electronic Design Automation (EDA) tools for this task are\nusually designed to use scripting commands or visualized buttons for\nmanipulation, especially for interactive automation functionalities, which have\na steep learning curve and cumbersome user experience, making a notable barrier\nto designers' adoption. Aiming to address such a usability issue, this paper\nintroduces LayoutCopilot, a pioneering multi-agent collaborative framework\npowered by Large Language Models (LLMs) for interactive analog layout design.\nLayoutCopilot simplifies human-tool interaction by converting natural language\ninstructions into executable script commands, and it interprets high-level\ndesign intents into actionable suggestions, significantly streamlining the\ndesign process. Experimental results demonstrate the flexibility, efficiency,\nand accessibility of LayoutCopilot in handling real-world analog designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog layout design heavily involves interactive processes between humans\nand design tools. Electronic Design Automation (EDA) tools for this task are\nusually designed to use scripting commands or visualized buttons for\nmanipulation, especially for interactive automation functionalities, which have\na steep learning curve and cumbersome user experience, making a notable barrier\nto designers' adoption. Aiming to address such a usability issue, this paper\nintroduces LayoutCopilot, a pioneering multi-agent collaborative framework\npowered by Large Language Models (LLMs) for interactive analog layout design.\nLayoutCopilot simplifies human-tool interaction by converting natural language\ninstructions into executable script commands, and it interprets high-level\ndesign intents into actionable suggestions, significantly streamlining the\ndesign process. Experimental results demonstrate the flexibility, efficiency,\nand accessibility of LayoutCopilot in handling real-world analog designs."
                },
                "authors": [
                    {
                        "name": "Bingyang Liu"
                    },
                    {
                        "name": "Haoyi Zhang"
                    },
                    {
                        "name": "Xiaohan Gao"
                    },
                    {
                        "name": "Zichen Kong"
                    },
                    {
                        "name": "Xiyuan Tang"
                    },
                    {
                        "name": "Yibo Lin"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "arxiv_comment": "8pages, 8figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.18873v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.18873v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14047v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14047v3",
                "updated": "2025-01-13T09:01:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    1,
                    13,
                    0,
                    13,
                    0
                ],
                "published": "2024-04-22T10:03:03Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    10,
                    3,
                    3,
                    0,
                    113,
                    0
                ],
                "title": "An empirical study of LLaMA3 quantization: from LLMs to MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An empirical study of LLaMA3 quantization: from LLMs to MLLMs"
                },
                "summary": "The LLaMA family, a collection of foundation language models ranging from 7B\nto 65B parameters, has become one of the most powerful open-source large\nlanguage models (LLMs) and the popular LLM backbone of multi-modal large\nlanguage models (MLLMs), widely used in computer vision and natural language\nunderstanding tasks. In particular, LLaMA3 models have recently been released\nand have achieved impressive performance in various domains with super-large\nscale pre-training on over 15T tokens of data. Given the wide application of\nlow-bit quantization for LLMs in resource-constrained scenarios, we explore\nLLaMA3's capabilities when quantized to low bit-width. This exploration can\npotentially provide new insights and challenges for the low-bit quantization of\nLLaMA3 and other future LLMs, especially in addressing performance degradation\nissues that suffer in LLM compression. Specifically, we comprehensively\nevaluate the 10 existing post-training quantization and LoRA fine-tuning\n(LoRA-FT) methods of LLaMA3 on 1-8 bits and various datasets to reveal the\nlow-bit quantization performance of LLaMA3. To uncover the capabilities of\nlow-bit quantized MLLM, we assessed the performance of the LLaMA3-based\nLLaVA-Next-8B model under 2-4 ultra-low bits with post-training quantization\nmethods. Our experimental results indicate that LLaMA3 still suffers from\nnon-negligible degradation in linguistic and visual contexts, particularly\nunder ultra-low bit widths. This highlights the significant performance gap at\nlow bit-width that needs to be addressed in future developments. We expect that\nthis empirical study will prove valuable in advancing future models, driving\nLLMs and MLLMs to achieve higher accuracy at lower bit to enhance practicality.\nOur project is released on https://github.com/Macaronlin/LLaMA3-Quantization ,\nand quantized models are released at https://huggingface.co/Efficient-ML .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LLaMA family, a collection of foundation language models ranging from 7B\nto 65B parameters, has become one of the most powerful open-source large\nlanguage models (LLMs) and the popular LLM backbone of multi-modal large\nlanguage models (MLLMs), widely used in computer vision and natural language\nunderstanding tasks. In particular, LLaMA3 models have recently been released\nand have achieved impressive performance in various domains with super-large\nscale pre-training on over 15T tokens of data. Given the wide application of\nlow-bit quantization for LLMs in resource-constrained scenarios, we explore\nLLaMA3's capabilities when quantized to low bit-width. This exploration can\npotentially provide new insights and challenges for the low-bit quantization of\nLLaMA3 and other future LLMs, especially in addressing performance degradation\nissues that suffer in LLM compression. Specifically, we comprehensively\nevaluate the 10 existing post-training quantization and LoRA fine-tuning\n(LoRA-FT) methods of LLaMA3 on 1-8 bits and various datasets to reveal the\nlow-bit quantization performance of LLaMA3. To uncover the capabilities of\nlow-bit quantized MLLM, we assessed the performance of the LLaMA3-based\nLLaVA-Next-8B model under 2-4 ultra-low bits with post-training quantization\nmethods. Our experimental results indicate that LLaMA3 still suffers from\nnon-negligible degradation in linguistic and visual contexts, particularly\nunder ultra-low bit widths. This highlights the significant performance gap at\nlow bit-width that needs to be addressed in future developments. We expect that\nthis empirical study will prove valuable in advancing future models, driving\nLLMs and MLLMs to achieve higher accuracy at lower bit to enhance practicality.\nOur project is released on https://github.com/Macaronlin/LLaMA3-Quantization ,\nand quantized models are released at https://huggingface.co/Efficient-ML ."
                },
                "authors": [
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Xingyu Zheng"
                    },
                    {
                        "name": "Xudong Ma"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Chengtao Lv"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Jie Luo"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Michele Magno"
                    }
                ],
                "author_detail": {
                    "name": "Michele Magno"
                },
                "author": "Michele Magno",
                "arxiv_doi": "10.1007/s44267-024-00070-x",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s44267-024-00070-x",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.14047v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14047v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07139v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07139v1",
                "updated": "2025-01-13T08:58:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    8,
                    58,
                    0,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T08:58:00Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    8,
                    58,
                    0,
                    0,
                    13,
                    0
                ],
                "title": "FlexQuant: Elastic Quantization Framework for Locally Hosted LLM on Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexQuant: Elastic Quantization Framework for Locally Hosted LLM on Edge\n  Devices"
                },
                "summary": "Deploying LLMs on edge devices presents serious technical challenges. Memory\nelasticity is crucial for edge devices with unified memory, where memory is\nshared and fluctuates dynamically. Existing solutions suffer from either poor\ntransition granularity or high storage costs. We propose FlexQuant, a novel\nelasticity framework that generates an ensemble of quantized models, providing\nan elastic hosting solution with 15x granularity improvement and 10x storage\nreduction compared to SoTA methods. FlexQuant works with most quantization\nmethods and creates a family of trade-off options under various storage limits\nthrough our pruning method. It brings great performance and flexibility to the\nedge deployment of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying LLMs on edge devices presents serious technical challenges. Memory\nelasticity is crucial for edge devices with unified memory, where memory is\nshared and fluctuates dynamically. Existing solutions suffer from either poor\ntransition granularity or high storage costs. We propose FlexQuant, a novel\nelasticity framework that generates an ensemble of quantized models, providing\nan elastic hosting solution with 15x granularity improvement and 10x storage\nreduction compared to SoTA methods. FlexQuant works with most quantization\nmethods and creates a family of trade-off options under various storage limits\nthrough our pruning method. It brings great performance and flexibility to the\nedge deployment of LLMs."
                },
                "authors": [
                    {
                        "name": "Yuji Chai"
                    },
                    {
                        "name": "Mujin Kwen"
                    },
                    {
                        "name": "David Brooks"
                    },
                    {
                        "name": "Gu-Yeon Wei"
                    }
                ],
                "author_detail": {
                    "name": "Gu-Yeon Wei"
                },
                "author": "Gu-Yeon Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07139v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07139v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07130v1",
                "updated": "2025-01-13T08:34:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    8,
                    34,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T08:34:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    8,
                    34,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "KubeDSM: A Kubernetes-based Dynamic Scheduling and Migration Framework\n  for Cloud-Assisted Edge Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KubeDSM: A Kubernetes-based Dynamic Scheduling and Migration Framework\n  for Cloud-Assisted Edge Clusters"
                },
                "summary": "Edge computing has become critical for enabling latency-sensitive\napplications, especially when paired with cloud resources to form\ncloud-assisted edge clusters. However, efficient resource management remains\nchallenging due to edge nodes' limited capacity and unreliable connectivity.\nThis paper introduces KubeDSM, a Kubernetes-based dynamic scheduling and\nmigration framework tailored for cloud-assisted edge environments. KubeDSM\naddresses the challenges of resource fragmentation, dynamic scheduling, and\nlive migration while ensuring Quality of Service (QoS) for latency-sensitive\napplications. Unlike Kubernetes' default scheduler, KubeDSM adopts batch\nscheduling to minimize resource fragmentation and incorporates a live migration\nmechanism to optimize edge resource utilization. Specifically, KubeDSM\nfacilitates three key operations: intra-edge migration to reduce fragmentation,\nedge-to-cloud migration during resource shortages, and cloud-to-edge migration\nwhen resources become available, thereby increasing the number of pods\nallocated to the edge. Our results demonstrate that KubeDSM consistently\nachieves a higher average edge ratio and a lower standard deviation in edge\nratios, highlighting its ability to provide more effective and stable\nscheduling across different deployments. We also explore the impact of\nmigration strategies and Quality of Service (QoS) configurations on the edge\nratios achieved by KubeDSM. The findings reveal that enabling migrations\nsignificantly enhances the edge ratio by reducing fragmentation. Additionally,\nKubeDSM's adaptability in respecting QoS requirements while maximizing overall\nedge ratios is confirmed through different QoS scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge computing has become critical for enabling latency-sensitive\napplications, especially when paired with cloud resources to form\ncloud-assisted edge clusters. However, efficient resource management remains\nchallenging due to edge nodes' limited capacity and unreliable connectivity.\nThis paper introduces KubeDSM, a Kubernetes-based dynamic scheduling and\nmigration framework tailored for cloud-assisted edge environments. KubeDSM\naddresses the challenges of resource fragmentation, dynamic scheduling, and\nlive migration while ensuring Quality of Service (QoS) for latency-sensitive\napplications. Unlike Kubernetes' default scheduler, KubeDSM adopts batch\nscheduling to minimize resource fragmentation and incorporates a live migration\nmechanism to optimize edge resource utilization. Specifically, KubeDSM\nfacilitates three key operations: intra-edge migration to reduce fragmentation,\nedge-to-cloud migration during resource shortages, and cloud-to-edge migration\nwhen resources become available, thereby increasing the number of pods\nallocated to the edge. Our results demonstrate that KubeDSM consistently\nachieves a higher average edge ratio and a lower standard deviation in edge\nratios, highlighting its ability to provide more effective and stable\nscheduling across different deployments. We also explore the impact of\nmigration strategies and Quality of Service (QoS) configurations on the edge\nratios achieved by KubeDSM. The findings reveal that enabling migrations\nsignificantly enhances the edge ratio by reducing fragmentation. Additionally,\nKubeDSM's adaptability in respecting QoS requirements while maximizing overall\nedge ratios is confirmed through different QoS scenarios."
                },
                "authors": [
                    {
                        "name": "Amirhossein Pashaeehir"
                    },
                    {
                        "name": "Sina Shariati"
                    },
                    {
                        "name": "Shayan Shafaghi"
                    },
                    {
                        "name": "Manni Moghimi"
                    },
                    {
                        "name": "Mahmoud Momtazpour"
                    }
                ],
                "author_detail": {
                    "name": "Mahmoud Momtazpour"
                },
                "author": "Mahmoud Momtazpour",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07124v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07124v1",
                "updated": "2025-01-13T08:26:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    8,
                    26,
                    43,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T08:26:43Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    8,
                    26,
                    43,
                    0,
                    13,
                    0
                ],
                "title": "LLM360 K2: Scaling Up 360-Open-Source Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM360 K2: Scaling Up 360-Open-Source Large Language Models"
                },
                "summary": "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree\nOPEN SOURCE approach to the largest and most powerful models under project\nLLM360. While open-source LLMs continue to advance, the answer to \"How are the\nlargest LLMs trained?\" remains unclear within the community. The implementation\ndetails for such high-capacity models are often protected due to business\nconsiderations associated with their high cost. This lack of transparency\nprevents LLM researchers from leveraging valuable insights from prior\nexperience, e.g., \"What are the best practices for addressing loss spikes?\" The\nLLM360 K2 project addresses this gap by providing full transparency and access\nto resources accumulated during the training of LLMs at the largest scale. This\nreport highlights key elements of the K2 project, including our first model, K2\nDIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals\nLLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the\nimplementation steps and present a longitudinal analysis of K2 DIAMOND's\ncapabilities throughout its training process. We also outline ongoing projects\nsuch as TXT360, setting the stage for future models in the series. By offering\npreviously unavailable resources, the K2 project also resonates with the\n360-degree OPEN SOURCE principles of transparency, reproducibility, and\naccessibility, which we believe are vital in the era of resource-intensive AI\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We detail the training of the LLM360 K2-65B model, scaling up our 360-degree\nOPEN SOURCE approach to the largest and most powerful models under project\nLLM360. While open-source LLMs continue to advance, the answer to \"How are the\nlargest LLMs trained?\" remains unclear within the community. The implementation\ndetails for such high-capacity models are often protected due to business\nconsiderations associated with their high cost. This lack of transparency\nprevents LLM researchers from leveraging valuable insights from prior\nexperience, e.g., \"What are the best practices for addressing loss spikes?\" The\nLLM360 K2 project addresses this gap by providing full transparency and access\nto resources accumulated during the training of LLMs at the largest scale. This\nreport highlights key elements of the K2 project, including our first model, K2\nDIAMOND, a 65 billion-parameter LLM that surpasses LLaMA-65B and rivals\nLLaMA2-70B, while requiring fewer FLOPs and tokens. We detail the\nimplementation steps and present a longitudinal analysis of K2 DIAMOND's\ncapabilities throughout its training process. We also outline ongoing projects\nsuch as TXT360, setting the stage for future models in the series. By offering\npreviously unavailable resources, the K2 project also resonates with the\n360-degree OPEN SOURCE principles of transparency, reproducibility, and\naccessibility, which we believe are vital in the era of resource-intensive AI\nresearch."
                },
                "authors": [
                    {
                        "name": "Zhengzhong Liu"
                    },
                    {
                        "name": "Bowen Tan"
                    },
                    {
                        "name": "Hongyi Wang"
                    },
                    {
                        "name": "Willie Neiswanger"
                    },
                    {
                        "name": "Tianhua Tao"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Fajri Koto"
                    },
                    {
                        "name": "Yuqi Wang"
                    },
                    {
                        "name": "Suqi Sun"
                    },
                    {
                        "name": "Omkar Pangarkar"
                    },
                    {
                        "name": "Richard Fan"
                    },
                    {
                        "name": "Yi Gu"
                    },
                    {
                        "name": "Victor Miller"
                    },
                    {
                        "name": "Liqun Ma"
                    },
                    {
                        "name": "Liping Tang"
                    },
                    {
                        "name": "Nikhil Ranjan"
                    },
                    {
                        "name": "Yonghao Zhuang"
                    },
                    {
                        "name": "Guowei He"
                    },
                    {
                        "name": "Renxi Wang"
                    },
                    {
                        "name": "Mingkai Deng"
                    },
                    {
                        "name": "Robin Algayres"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Eric Xing"
                    }
                ],
                "author_detail": {
                    "name": "Eric Xing"
                },
                "author": "Eric Xing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07124v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07124v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07108v1",
                "updated": "2025-01-13T07:42:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    7,
                    42,
                    55,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T07:42:55Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    7,
                    42,
                    55,
                    0,
                    13,
                    0
                ],
                "title": "How GPT learns layer by layer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How GPT learns layer by layer"
                },
                "summary": "Large Language Models (LLMs) excel at tasks like language processing,\nstrategy games, and reasoning but struggle to build generalizable internal\nrepresentations essential for adaptive decision-making in agents. For agents to\neffectively navigate complex environments, they must construct reliable world\nmodels. While LLMs perform well on specific benchmarks, they often fail to\ngeneralize, leading to brittle representations that limit their real-world\neffectiveness. Understanding how LLMs build internal world models is key to\ndeveloping agents capable of consistent, adaptive behavior across tasks. We\nanalyze OthelloGPT, a GPT-based model trained on Othello gameplay, as a\ncontrolled testbed for studying representation learning. Despite being trained\nsolely on next-token prediction with random valid moves, OthelloGPT shows\nmeaningful layer-wise progression in understanding board state and gameplay.\nEarly layers capture static attributes like board edges, while deeper layers\nreflect dynamic tile changes. To interpret these representations, we compare\nSparse Autoencoders (SAEs) with linear probes, finding that SAEs offer more\nrobust, disentangled insights into compositional features, whereas linear\nprobes mainly detect features useful for classification. We use SAEs to decode\nfeatures related to tile color and tile stability, a previously unexamined\nfeature that reflects complex gameplay concepts like board control and\nlong-term planning. We study the progression of linear probe accuracy and tile\ncolor using both SAE's and linear probes to compare their effectiveness at\ncapturing what the model is learning. Although we begin with a smaller language\nmodel, OthelloGPT, this study establishes a framework for understanding the\ninternal representations learned by GPT models, transformers, and LLMs more\nbroadly. Our code is publicly available: https://github.com/ALT-JS/OthelloSAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at tasks like language processing,\nstrategy games, and reasoning but struggle to build generalizable internal\nrepresentations essential for adaptive decision-making in agents. For agents to\neffectively navigate complex environments, they must construct reliable world\nmodels. While LLMs perform well on specific benchmarks, they often fail to\ngeneralize, leading to brittle representations that limit their real-world\neffectiveness. Understanding how LLMs build internal world models is key to\ndeveloping agents capable of consistent, adaptive behavior across tasks. We\nanalyze OthelloGPT, a GPT-based model trained on Othello gameplay, as a\ncontrolled testbed for studying representation learning. Despite being trained\nsolely on next-token prediction with random valid moves, OthelloGPT shows\nmeaningful layer-wise progression in understanding board state and gameplay.\nEarly layers capture static attributes like board edges, while deeper layers\nreflect dynamic tile changes. To interpret these representations, we compare\nSparse Autoencoders (SAEs) with linear probes, finding that SAEs offer more\nrobust, disentangled insights into compositional features, whereas linear\nprobes mainly detect features useful for classification. We use SAEs to decode\nfeatures related to tile color and tile stability, a previously unexamined\nfeature that reflects complex gameplay concepts like board control and\nlong-term planning. We study the progression of linear probe accuracy and tile\ncolor using both SAE's and linear probes to compare their effectiveness at\ncapturing what the model is learning. Although we begin with a smaller language\nmodel, OthelloGPT, this study establishes a framework for understanding the\ninternal representations learned by GPT models, transformers, and LLMs more\nbroadly. Our code is publicly available: https://github.com/ALT-JS/OthelloSAE."
                },
                "authors": [
                    {
                        "name": "Jason Du"
                    },
                    {
                        "name": "Kelly Hong"
                    },
                    {
                        "name": "Alishba Imran"
                    },
                    {
                        "name": "Erfan Jahanparast"
                    },
                    {
                        "name": "Mehdi Khfifi"
                    },
                    {
                        "name": "Kaichun Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Kaichun Qiao"
                },
                "author": "Kaichun Qiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17692v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17692v3",
                "updated": "2025-01-13T07:41:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    7,
                    41,
                    44,
                    0,
                    13,
                    0
                ],
                "published": "2024-09-26T09:57:16Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    9,
                    57,
                    16,
                    3,
                    270,
                    0
                ],
                "title": "MIO: A Foundation Model on Multimodal Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIO: A Foundation Model on Multimodal Tokens"
                },
                "summary": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce MIO, a novel foundation model built on multimodal\ntokens, capable of understanding and generating speech, text, images, and\nvideos in an end-to-end, autoregressive manner. While the emergence of large\nlanguage models (LLMs) and multimodal large language models (MM-LLMs) propels\nadvancements in artificial general intelligence through their versatile\ncapabilities, they still lack true any-to-any understanding and generation.\nRecently, the release of GPT-4o has showcased the remarkable potential of\nany-to-any LLMs for complex real-world tasks, enabling omnidirectional input\nand output across images, speech, and text. However, it is closed-source and\ndoes not support the generation of multimodal interleaved sequences. To address\nthis gap, we present MIO, which is trained on a mixture of discrete tokens\nacross four modalities using causal multimodal modeling. MIO undergoes a\nfour-stage training process: (1) alignment pre-training, (2) interleaved\npre-training, (3) speech-enhanced pre-training, and (4) comprehensive\nsupervised fine-tuning on diverse textual, visual, and speech tasks. Our\nexperimental results indicate that MIO exhibits competitive, and in some cases\nsuperior, performance compared to previous dual-modal baselines, any-to-any\nmodel baselines, and even modality-specific baselines. Moreover, MIO\ndemonstrates advanced capabilities inherent to its any-to-any feature, such as\ninterleaved video-text generation, chain-of-visual-thought reasoning, visual\nguideline generation, instructional image editing, etc."
                },
                "authors": [
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "King Zhu"
                    },
                    {
                        "name": "Chunpu Xu"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Yibo Zhang"
                    },
                    {
                        "name": "Jiashuo Wang"
                    },
                    {
                        "name": "Ning Shi"
                    },
                    {
                        "name": "Siyu Li"
                    },
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Haoran Que"
                    },
                    {
                        "name": "Zhaoxiang Zhang"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Ke Xu"
                    },
                    {
                        "name": "Jie Fu"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "arxiv_comment": "Technical Report. Codes and models are available in\n  https://github.com/MIO-Team/MIO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17692v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17692v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14789v2",
                "updated": "2025-01-13T07:29:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    7,
                    29,
                    53,
                    0,
                    13,
                    0
                ],
                "published": "2024-11-22T08:17:46Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    8,
                    17,
                    46,
                    4,
                    327,
                    0
                ],
                "title": "Simplifying CLIP: Unleashing the Power of Large-Scale Models on\n  Consumer-level Computers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simplifying CLIP: Unleashing the Power of Large-Scale Models on\n  Consumer-level Computers"
                },
                "summary": "Contrastive Language-Image Pre-training (CLIP) has attracted a surge of\nattention for its superior zero-shot performance and excellent transferability\nto downstream tasks. However, training such large-scale models usually requires\nsubstantial computation and storage, which poses barriers for general users\nwith consumer-level computers. Motivated by this observation, in this paper we\ninvestigate how to achieve competitive performance on only one Nvidia RTX3090\nGPU and with one terabyte for storing dataset. On one hand, we simplify the\ntransformer block structure and combine Weight Inheritance with multi-stage\nKnowledge Distillation (WIKD), thereby reducing the parameters and improving\nthe inference speed during training along with deployment. On the other hand,\nconfronted with the convergence challenge posed by small dataset, we generate\nsynthetic captions for each sample as data augmentation, and devise a novel\nPair Matching (PM) loss to fully exploit the distinguishment among positive and\nnegative image-text pairs. Extensive experiments demonstrate that our model can\nachieve a new state-of-the-art datascale-parameter-accuracy tradeoff, which\ncould further popularize the CLIP model in the related research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrastive Language-Image Pre-training (CLIP) has attracted a surge of\nattention for its superior zero-shot performance and excellent transferability\nto downstream tasks. However, training such large-scale models usually requires\nsubstantial computation and storage, which poses barriers for general users\nwith consumer-level computers. Motivated by this observation, in this paper we\ninvestigate how to achieve competitive performance on only one Nvidia RTX3090\nGPU and with one terabyte for storing dataset. On one hand, we simplify the\ntransformer block structure and combine Weight Inheritance with multi-stage\nKnowledge Distillation (WIKD), thereby reducing the parameters and improving\nthe inference speed during training along with deployment. On the other hand,\nconfronted with the convergence challenge posed by small dataset, we generate\nsynthetic captions for each sample as data augmentation, and devise a novel\nPair Matching (PM) loss to fully exploit the distinguishment among positive and\nnegative image-text pairs. Extensive experiments demonstrate that our model can\nachieve a new state-of-the-art datascale-parameter-accuracy tradeoff, which\ncould further popularize the CLIP model in the related research community."
                },
                "authors": [
                    {
                        "name": "Hongbo Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hongbo Liu"
                },
                "author": "Hongbo Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.01933v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.01933v4",
                "updated": "2025-01-13T07:13:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    7,
                    13,
                    56,
                    0,
                    13,
                    0
                ],
                "published": "2024-08-04T05:15:02Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    5,
                    15,
                    2,
                    6,
                    217,
                    0
                ],
                "title": "DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 511 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 511 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios."
                },
                "authors": [
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Jiuyang Chang"
                    },
                    {
                        "name": "Yiming Qian"
                    },
                    {
                        "name": "Guoxin Chen"
                    },
                    {
                        "name": "Junhao Chen"
                    },
                    {
                        "name": "Zhouqiang Jiang"
                    },
                    {
                        "name": "Jiahao Zhang"
                    },
                    {
                        "name": "Yuta Nakashima"
                    },
                    {
                        "name": "Hajime Nagahara"
                    }
                ],
                "author_detail": {
                    "name": "Hajime Nagahara"
                },
                "author": "Hajime Nagahara",
                "arxiv_comment": "9 pages,6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.01933v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.01933v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19943v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19943v3",
                "updated": "2025-01-13T06:53:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    6,
                    53,
                    56,
                    0,
                    13,
                    0
                ],
                "published": "2024-11-29T18:58:22Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    18,
                    58,
                    22,
                    4,
                    334,
                    0
                ],
                "title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhances\n  LLM's Reasoning Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhances\n  LLM's Reasoning Capability"
                },
                "summary": "Mathematical reasoning tasks pose significant challenges for large language\nmodels (LLMs) because they require precise logical deduction and sequence\nanalysis. In this work, we introduce the concept of critical tokens -- elements\nwithin reasoning trajectories that significantly influence incorrect outcomes.\nWe present a novel framework for identifying these tokens through rollout\nsampling and demonstrate their substantial divergence from traditional error\ntokens. Through extensive experiments on datasets such as GSM8K and MATH500, we\nshow that identifying and replacing critical tokens significantly improves\nmodel accuracy. We propose an efficient methodology for pinpointing these\ntokens in large-scale datasets using contrastive estimation and extend this\nframework to enhance model training processes with direct preference\noptimization (DPO). Experimental results on GSM8K and MATH500 benchmarks with\nthe widely used models Llama-3 (8B and 70B) and Deepseek-math (7B) demonstrate\nthe effectiveness of the proposed approach, cDPO. Our results underscore the\npotential of leveraging critical tokens to reduce errors in reasoning tasks,\nadvancing the development of AI systems capable of robust logical deduction.\nOur code, annotated datasets, and trained models are available at\nhttps://github.com/chenzhiling9954/Critical-Tokens-Matter to support and\nencourage future research in this promising field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning tasks pose significant challenges for large language\nmodels (LLMs) because they require precise logical deduction and sequence\nanalysis. In this work, we introduce the concept of critical tokens -- elements\nwithin reasoning trajectories that significantly influence incorrect outcomes.\nWe present a novel framework for identifying these tokens through rollout\nsampling and demonstrate their substantial divergence from traditional error\ntokens. Through extensive experiments on datasets such as GSM8K and MATH500, we\nshow that identifying and replacing critical tokens significantly improves\nmodel accuracy. We propose an efficient methodology for pinpointing these\ntokens in large-scale datasets using contrastive estimation and extend this\nframework to enhance model training processes with direct preference\noptimization (DPO). Experimental results on GSM8K and MATH500 benchmarks with\nthe widely used models Llama-3 (8B and 70B) and Deepseek-math (7B) demonstrate\nthe effectiveness of the proposed approach, cDPO. Our results underscore the\npotential of leveraging critical tokens to reduce errors in reasoning tasks,\nadvancing the development of AI systems capable of robust logical deduction.\nOur code, annotated datasets, and trained models are available at\nhttps://github.com/chenzhiling9954/Critical-Tokens-Matter to support and\nencourage future research in this promising field."
                },
                "authors": [
                    {
                        "name": "Zicheng Lin"
                    },
                    {
                        "name": "Tian Liang"
                    },
                    {
                        "name": "Jiahao Xu"
                    },
                    {
                        "name": "Qiuzhi Lin"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Ruilin Luo"
                    },
                    {
                        "name": "Chufan Shi"
                    },
                    {
                        "name": "Siheng Li"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Zhaopeng Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhaopeng Tu"
                },
                "author": "Zhaopeng Tu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19943v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19943v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.10182v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.10182v5",
                "updated": "2025-01-13T06:51:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    6,
                    51,
                    13,
                    0,
                    13,
                    0
                ],
                "published": "2024-03-15T10:38:48Z",
                "published_parsed": [
                    2024,
                    3,
                    15,
                    10,
                    38,
                    48,
                    4,
                    75,
                    0
                ],
                "title": "Fast and reliable uncertainty quantification with neural network\n  ensembles for industrial image classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and reliable uncertainty quantification with neural network\n  ensembles for industrial image classification"
                },
                "summary": "Image classification with neural networks (NNs) is widely used in industrial\nprocesses, situations where the model likely encounters unknown objects during\ndeployment, i.e., out-of-distribution (OOD) data. Worryingly, NNs tend to make\nconfident yet incorrect predictions when confronted with OOD data. To increase\nthe models' reliability, they should quantify the uncertainty in their own\npredictions, communicating when the output should (not) be trusted. Deep\nensembles, composed of multiple independent NNs, have been shown to perform\nstrongly but are computationally expensive. Recent research has proposed more\nefficient NN ensembles, namely the snapshot, batch, and multi-input\nmulti-output ensemble. This study investigates the predictive and uncertainty\nperformance of efficient NN ensembles in the context of image classification\nfor industrial processes. It is the first to provide a comprehensive comparison\nand it proposes a novel Diversity Quality metric to quantify the ensembles'\nperformance on the in-distribution and OOD sets in one single metric. The\nresults highlight the batch ensemble as a cost-effective and competitive\nalternative to the deep ensemble. It matches the deep ensemble in both\nuncertainty and accuracy while exhibiting considerable savings in training\ntime, test time, and memory storage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image classification with neural networks (NNs) is widely used in industrial\nprocesses, situations where the model likely encounters unknown objects during\ndeployment, i.e., out-of-distribution (OOD) data. Worryingly, NNs tend to make\nconfident yet incorrect predictions when confronted with OOD data. To increase\nthe models' reliability, they should quantify the uncertainty in their own\npredictions, communicating when the output should (not) be trusted. Deep\nensembles, composed of multiple independent NNs, have been shown to perform\nstrongly but are computationally expensive. Recent research has proposed more\nefficient NN ensembles, namely the snapshot, batch, and multi-input\nmulti-output ensemble. This study investigates the predictive and uncertainty\nperformance of efficient NN ensembles in the context of image classification\nfor industrial processes. It is the first to provide a comprehensive comparison\nand it proposes a novel Diversity Quality metric to quantify the ensembles'\nperformance on the in-distribution and OOD sets in one single metric. The\nresults highlight the batch ensemble as a cost-effective and competitive\nalternative to the deep ensemble. It matches the deep ensemble in both\nuncertainty and accuracy while exhibiting considerable savings in training\ntime, test time, and memory storage."
                },
                "authors": [
                    {
                        "name": "Arthur Thuy"
                    },
                    {
                        "name": "Dries F. Benoit"
                    }
                ],
                "author_detail": {
                    "name": "Dries F. Benoit"
                },
                "author": "Dries F. Benoit",
                "arxiv_doi": "10.1007/s10479-024-06440-4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10479-024-06440-4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.10182v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.10182v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted Manuscript version of an article published in Annals of\n  Operations Research",
                "arxiv_journal_ref": "Ann Oper Res (2024)",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10047v2",
                "updated": "2025-01-13T06:47:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    6,
                    47,
                    21,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-13T11:19:56Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    11,
                    19,
                    56,
                    4,
                    348,
                    0
                ],
                "title": "Large Action Models: From Inception to Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Action Models: From Inception to Implementation"
                },
                "summary": "As AI continues to advance, there is a growing demand for systems that go\nbeyond language-based assistance and move toward intelligent agents capable of\nperforming real-world actions. This evolution requires the transition from\ntraditional Large Language Models (LLMs), which excel at generating textual\nresponses, to Large Action Models (LAMs), designed for action generation and\nexecution within dynamic environments. Enabled by agent systems, LAMs hold the\npotential to transform AI from passive language understanding to active task\ncompletion, marking a significant milestone in the progression toward\nartificial general intelligence.\n  In this paper, we present a comprehensive framework for developing LAMs,\noffering a systematic approach to their creation, from inception to deployment.\nWe begin with an overview of LAMs, highlighting their unique characteristics\nand delineating their differences from LLMs. Using a Windows OS-based agent as\na case study, we provide a detailed, step-by-step guide on the key stages of\nLAM development, including data collection, model training, environment\nintegration, grounding, and evaluation. This generalizable workflow can serve\nas a blueprint for creating functional LAMs in various application domains. We\nconclude by identifying the current limitations of LAMs and discussing\ndirections for future research and industrial deployment, emphasizing the\nchallenges and opportunities that lie ahead in realizing the full potential of\nLAMs in real-world applications.\n  The code for the data collection process utilized in this paper is publicly\navailable at: https://github.com/microsoft/UFO/tree/main/dataflow, and\ncomprehensive documentation can be found at\nhttps://microsoft.github.io/UFO/dataflow/overview/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI continues to advance, there is a growing demand for systems that go\nbeyond language-based assistance and move toward intelligent agents capable of\nperforming real-world actions. This evolution requires the transition from\ntraditional Large Language Models (LLMs), which excel at generating textual\nresponses, to Large Action Models (LAMs), designed for action generation and\nexecution within dynamic environments. Enabled by agent systems, LAMs hold the\npotential to transform AI from passive language understanding to active task\ncompletion, marking a significant milestone in the progression toward\nartificial general intelligence.\n  In this paper, we present a comprehensive framework for developing LAMs,\noffering a systematic approach to their creation, from inception to deployment.\nWe begin with an overview of LAMs, highlighting their unique characteristics\nand delineating their differences from LLMs. Using a Windows OS-based agent as\na case study, we provide a detailed, step-by-step guide on the key stages of\nLAM development, including data collection, model training, environment\nintegration, grounding, and evaluation. This generalizable workflow can serve\nas a blueprint for creating functional LAMs in various application domains. We\nconclude by identifying the current limitations of LAMs and discussing\ndirections for future research and industrial deployment, emphasizing the\nchallenges and opportunities that lie ahead in realizing the full potential of\nLAMs in real-world applications.\n  The code for the data collection process utilized in this paper is publicly\navailable at: https://github.com/microsoft/UFO/tree/main/dataflow, and\ncomprehensive documentation can be found at\nhttps://microsoft.github.io/UFO/dataflow/overview/."
                },
                "authors": [
                    {
                        "name": "Lu Wang"
                    },
                    {
                        "name": "Fangkai Yang"
                    },
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Junting Lu"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Bo Qiao"
                    },
                    {
                        "name": "Ray Huang"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Qisheng Su"
                    },
                    {
                        "name": "Jiayi Ye"
                    },
                    {
                        "name": "Yudi Zhang"
                    },
                    {
                        "name": "Jian-Guang Lou"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "25pages,12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07078v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07078v1",
                "updated": "2025-01-13T06:22:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    6,
                    22,
                    52,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T06:22:52Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    6,
                    22,
                    52,
                    0,
                    13,
                    0
                ],
                "title": "ADKGD: Anomaly Detection in Knowledge Graphs with Dual-Channel Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ADKGD: Anomaly Detection in Knowledge Graphs with Dual-Channel Training"
                },
                "summary": "In the current development of large language models (LLMs), it is important\nto ensure the accuracy and reliability of the underlying data sources. LLMs are\ncritical for various applications, but they often suffer from hallucinations\nand inaccuracies due to knowledge gaps in the training data. Knowledge graphs\n(KGs), as a powerful structural tool, could serve as a vital external\ninformation source to mitigate the aforementioned issues. By providing a\nstructured and comprehensive understanding of real-world data, KGs enhance the\nperformance and reliability of LLMs. However, it is common that errors exist in\nKGs while extracting triplets from unstructured data to construct KGs. This\ncould lead to degraded performance in downstream tasks such as\nquestion-answering and recommender systems. Therefore, anomaly detection in KGs\nis essential to identify and correct these errors. This paper presents an\nanomaly detection algorithm in knowledge graphs with dual-channel learning\n(ADKGD). ADKGD leverages a dual-channel learning approach to enhance\nrepresentation learning from both the entity-view and triplet-view\nperspectives. Furthermore, using a cross-layer approach, our framework\nintegrates internal information aggregation and context information\naggregation. We introduce a kullback-leibler (KL)-loss component to improve the\naccuracy of the scoring function between the dual channels. To evaluate ADKGD's\nperformance, we conduct empirical studies on three real-world KGs: WN18RR,\nFB15K, and NELL-995. Experimental results demonstrate that ADKGD outperforms\nthe state-of-the-art anomaly detection algorithms. The source code and datasets\nare publicly available at https://github.com/csjywu1/ADKGD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the current development of large language models (LLMs), it is important\nto ensure the accuracy and reliability of the underlying data sources. LLMs are\ncritical for various applications, but they often suffer from hallucinations\nand inaccuracies due to knowledge gaps in the training data. Knowledge graphs\n(KGs), as a powerful structural tool, could serve as a vital external\ninformation source to mitigate the aforementioned issues. By providing a\nstructured and comprehensive understanding of real-world data, KGs enhance the\nperformance and reliability of LLMs. However, it is common that errors exist in\nKGs while extracting triplets from unstructured data to construct KGs. This\ncould lead to degraded performance in downstream tasks such as\nquestion-answering and recommender systems. Therefore, anomaly detection in KGs\nis essential to identify and correct these errors. This paper presents an\nanomaly detection algorithm in knowledge graphs with dual-channel learning\n(ADKGD). ADKGD leverages a dual-channel learning approach to enhance\nrepresentation learning from both the entity-view and triplet-view\nperspectives. Furthermore, using a cross-layer approach, our framework\nintegrates internal information aggregation and context information\naggregation. We introduce a kullback-leibler (KL)-loss component to improve the\naccuracy of the scoring function between the dual channels. To evaluate ADKGD's\nperformance, we conduct empirical studies on three real-world KGs: WN18RR,\nFB15K, and NELL-995. Experimental results demonstrate that ADKGD outperforms\nthe state-of-the-art anomaly detection algorithms. The source code and datasets\nare publicly available at https://github.com/csjywu1/ADKGD."
                },
                "authors": [
                    {
                        "name": "Jiayang Wu"
                    },
                    {
                        "name": "Wensheng Gan"
                    },
                    {
                        "name": "Jiahao Zhang"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "Preprint. 11 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07078v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07078v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20163v2",
                "updated": "2025-01-13T06:17:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    6,
                    17,
                    38,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-28T14:27:45Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    27,
                    45,
                    5,
                    363,
                    0
                ],
                "title": "Topic-Aware Knowledge Graph with Large Language Models for\n  Interoperability in Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topic-Aware Knowledge Graph with Large Language Models for\n  Interoperability in Recommender Systems"
                },
                "summary": "The use of knowledge graphs in recommender systems has become one of the\ncommon approaches to addressing data sparsity and cold start problems. Recent\nadvances in large language models (LLMs) offer new possibilities for processing\nside and context information within knowledge graphs. However, consistent\nintegration across various systems remains challenging due to the need for\ndomain expert intervention and differences in system characteristics. To\naddress these issues, we propose a consistent approach that extracts both\ngeneral and specific topics from both side and context information using LLMs.\nFirst, general topics are iteratively extracted and updated from side\ninformation. Then, specific topics are extracted using context information.\nFinally, to address synonymous topics generated during the specific topic\nextraction process, a refining algorithm processes and resolves these issues\neffectively. This approach allows general topics to capture broad knowledge\nacross diverse item characteristics, while specific topics emphasize detailed\nattributes, providing a more comprehensive understanding of the semantic\nfeatures of items and the preferences of users. Experimental results\ndemonstrate significant improvements in recommendation performance across\ndiverse knowledge graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of knowledge graphs in recommender systems has become one of the\ncommon approaches to addressing data sparsity and cold start problems. Recent\nadvances in large language models (LLMs) offer new possibilities for processing\nside and context information within knowledge graphs. However, consistent\nintegration across various systems remains challenging due to the need for\ndomain expert intervention and differences in system characteristics. To\naddress these issues, we propose a consistent approach that extracts both\ngeneral and specific topics from both side and context information using LLMs.\nFirst, general topics are iteratively extracted and updated from side\ninformation. Then, specific topics are extracted using context information.\nFinally, to address synonymous topics generated during the specific topic\nextraction process, a refining algorithm processes and resolves these issues\neffectively. This approach allows general topics to capture broad knowledge\nacross diverse item characteristics, while specific topics emphasize detailed\nattributes, providing a more comprehensive understanding of the semantic\nfeatures of items and the preferences of users. Experimental results\ndemonstrate significant improvements in recommendation performance across\ndiverse knowledge graphs."
                },
                "authors": [
                    {
                        "name": "Minhye Jeon"
                    },
                    {
                        "name": "Seokho Ahn"
                    },
                    {
                        "name": "Young-Duk Seo"
                    }
                ],
                "author_detail": {
                    "name": "Young-Duk Seo"
                },
                "author": "Young-Duk Seo",
                "arxiv_doi": "10.1145/3672608.3707958",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3672608.3707958",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted in The 40th ACM/SIGAPP Symposium On Applied Computing(SAC)\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16185v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16185v3",
                "updated": "2025-01-13T06:10:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    6,
                    10,
                    24,
                    0,
                    13,
                    0
                ],
                "published": "2024-01-29T14:32:27Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    14,
                    32,
                    27,
                    0,
                    29,
                    0
                ],
                "title": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing\n  LLMs' Vulnerability Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing\n  LLMs' Vulnerability Reasoning"
                },
                "summary": "Large language models (LLMs) have demonstrated significant potential in\nvarious tasks, including those requiring human-level intelligence, such as\nvulnerability detection. However, recent efforts to use LLMs for vulnerability\ndetection remain preliminary, as they lack a deep understanding of whether a\nsubject LLM's vulnerability reasoning capability stems from the model itself or\nfrom external aids such as knowledge retrieval and tooling support.\n  In this paper, we aim to decouple LLMs' vulnerability reasoning from other\ncapabilities, such as vulnerability knowledge adoption, context information\nretrieval, and advanced prompt schemes. We introduce LLM4Vuln, a unified\nevaluation framework that separates and assesses LLMs' vulnerability reasoning\ncapabilities and examines improvements when combined with other enhancements.\n  We conduct controlled experiments using 147 ground-truth vulnerabilities and\n147 non-vulnerable cases in Solidity, Java and C/C++, testing them in a total\nof 3,528 scenarios across four LLMs (GPT-3.5, GPT-4, Phi-3, and Llama 3). Our\nfindings reveal the varying impacts of knowledge enhancement, context\nsupplementation, and prompt schemes. We also identify 14 zero-day\nvulnerabilities in four pilot bug bounty programs, resulting in $3,576 in\nbounties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated significant potential in\nvarious tasks, including those requiring human-level intelligence, such as\nvulnerability detection. However, recent efforts to use LLMs for vulnerability\ndetection remain preliminary, as they lack a deep understanding of whether a\nsubject LLM's vulnerability reasoning capability stems from the model itself or\nfrom external aids such as knowledge retrieval and tooling support.\n  In this paper, we aim to decouple LLMs' vulnerability reasoning from other\ncapabilities, such as vulnerability knowledge adoption, context information\nretrieval, and advanced prompt schemes. We introduce LLM4Vuln, a unified\nevaluation framework that separates and assesses LLMs' vulnerability reasoning\ncapabilities and examines improvements when combined with other enhancements.\n  We conduct controlled experiments using 147 ground-truth vulnerabilities and\n147 non-vulnerable cases in Solidity, Java and C/C++, testing them in a total\nof 3,528 scenarios across four LLMs (GPT-3.5, GPT-4, Phi-3, and Llama 3). Our\nfindings reveal the varying impacts of knowledge enhancement, context\nsupplementation, and prompt schemes. We also identify 14 zero-day\nvulnerabilities in four pilot bug bounty programs, resulting in $3,576 in\nbounties."
                },
                "authors": [
                    {
                        "name": "Yuqiang Sun"
                    },
                    {
                        "name": "Daoyuan Wu"
                    },
                    {
                        "name": "Yue Xue"
                    },
                    {
                        "name": "Han Liu"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Lyuye Zhang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yingjiu Li"
                    }
                ],
                "author_detail": {
                    "name": "Yingjiu Li"
                },
                "author": "Yingjiu Li",
                "arxiv_comment": "This is a technical report by Nanyang Technological University.\n  Updated to support Solidity, Java and C/C++",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.16185v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16185v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07071v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07071v1",
                "updated": "2025-01-13T05:53:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    5,
                    53,
                    56,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T05:53:56Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    5,
                    53,
                    56,
                    0,
                    13,
                    0
                ],
                "title": "Value Compass Leaderboard: A Platform for Fundamental and Validated\n  Evaluation of LLMs Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Compass Leaderboard: A Platform for Fundamental and Validated\n  Evaluation of LLMs Values"
                },
                "summary": "As Large Language Models (LLMs) achieve remarkable breakthroughs, aligning\ntheir values with humans has become imperative for their responsible\ndevelopment and customized applications. However, there still lack evaluations\nof LLMs values that fulfill three desirable goals. (1) Value Clarification: We\nexpect to clarify the underlying values of LLMs precisely and comprehensively,\nwhile current evaluations focus narrowly on safety risks such as bias and\ntoxicity. (2) Evaluation Validity: Existing static, open-source benchmarks are\nprone to data contamination and quickly become obsolete as LLMs evolve.\nAdditionally, these discriminative evaluations uncover LLMs' knowledge about\nvalues, rather than valid assessments of LLMs' behavioral conformity to values.\n(3) Value Pluralism: The pluralistic nature of human values across individuals\nand cultures is largely ignored in measuring LLMs value alignment. To address\nthese challenges, we presents the Value Compass Leaderboard, with three\ncorrespondingly designed modules. It (i) grounds the evaluation on\nmotivationally distinct \\textit{basic values to clarify LLMs' underlying values\nfrom a holistic view; (ii) applies a \\textit{generative evolving evaluation\nframework with adaptive test items for evolving LLMs and direct value\nrecognition from behaviors in realistic scenarios; (iii) propose a metric that\nquantifies LLMs alignment with a specific value as a weighted sum over multiple\ndimensions, with weights determined by pluralistic values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) achieve remarkable breakthroughs, aligning\ntheir values with humans has become imperative for their responsible\ndevelopment and customized applications. However, there still lack evaluations\nof LLMs values that fulfill three desirable goals. (1) Value Clarification: We\nexpect to clarify the underlying values of LLMs precisely and comprehensively,\nwhile current evaluations focus narrowly on safety risks such as bias and\ntoxicity. (2) Evaluation Validity: Existing static, open-source benchmarks are\nprone to data contamination and quickly become obsolete as LLMs evolve.\nAdditionally, these discriminative evaluations uncover LLMs' knowledge about\nvalues, rather than valid assessments of LLMs' behavioral conformity to values.\n(3) Value Pluralism: The pluralistic nature of human values across individuals\nand cultures is largely ignored in measuring LLMs value alignment. To address\nthese challenges, we presents the Value Compass Leaderboard, with three\ncorrespondingly designed modules. It (i) grounds the evaluation on\nmotivationally distinct \\textit{basic values to clarify LLMs' underlying values\nfrom a holistic view; (ii) applies a \\textit{generative evolving evaluation\nframework with adaptive test items for evolving LLMs and direct value\nrecognition from behaviors in realistic scenarios; (iii) propose a metric that\nquantifies LLMs alignment with a specific value as a weighted sum over multiple\ndimensions, with weights determined by pluralistic values."
                },
                "authors": [
                    {
                        "name": "Jing Yao"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Shitong Duan"
                    },
                    {
                        "name": "Jindong Wang"
                    },
                    {
                        "name": "Yuzhuo Bai"
                    },
                    {
                        "name": "Muhua Huang"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Tun Lu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Maosong Sun"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07071v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07071v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07070v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07070v1",
                "updated": "2025-01-13T05:48:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    5,
                    48,
                    32,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T05:48:32Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    5,
                    48,
                    32,
                    0,
                    13,
                    0
                ],
                "title": "Enhancing Image Generation Fidelity via Progressive Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Image Generation Fidelity via Progressive Prompts"
                },
                "summary": "The diffusion transformer (DiT) architecture has attracted significant\nattention in image generation, achieving better fidelity, performance, and\ndiversity. However, most existing DiT - based image generation methods focus on\nglobal - aware synthesis, and regional prompt control has been less explored.\nIn this paper, we propose a coarse - to - fine generation pipeline for regional\nprompt - following generation. Specifically, we first utilize the powerful\nlarge language model (LLM) to generate both high - level descriptions of the\nimage (such as content, topic, and objects) and low - level descriptions (such\nas details and style). Then, we explore the influence of cross - attention\nlayers at different depths. We find that deeper layers are always responsible\nfor high - level content control, while shallow layers handle low - level\ncontent control. Various prompts are injected into the proposed regional cross\n- attention control for coarse - to - fine generation. By using the proposed\npipeline, we enhance the controllability of DiT - based image generation.\nExtensive quantitative and qualitative results show that our pipeline can\nimprove the performance of the generated images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The diffusion transformer (DiT) architecture has attracted significant\nattention in image generation, achieving better fidelity, performance, and\ndiversity. However, most existing DiT - based image generation methods focus on\nglobal - aware synthesis, and regional prompt control has been less explored.\nIn this paper, we propose a coarse - to - fine generation pipeline for regional\nprompt - following generation. Specifically, we first utilize the powerful\nlarge language model (LLM) to generate both high - level descriptions of the\nimage (such as content, topic, and objects) and low - level descriptions (such\nas details and style). Then, we explore the influence of cross - attention\nlayers at different depths. We find that deeper layers are always responsible\nfor high - level content control, while shallow layers handle low - level\ncontent control. Various prompts are injected into the proposed regional cross\n- attention control for coarse - to - fine generation. By using the proposed\npipeline, we enhance the controllability of DiT - based image generation.\nExtensive quantitative and qualitative results show that our pipeline can\nimprove the performance of the generated images."
                },
                "authors": [
                    {
                        "name": "Zhen Xiong"
                    },
                    {
                        "name": "Yuqi Li"
                    },
                    {
                        "name": "Chuanguang Yang"
                    },
                    {
                        "name": "Tiao Tan"
                    },
                    {
                        "name": "Zhihong Zhu"
                    },
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Yue Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yue Ma"
                },
                "author": "Yue Ma",
                "arxiv_comment": "Accepted by ICASSP 2025, Github:\n  https://github.com/ZhenXiong-dl/ICASSP2025-RCAC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07070v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07070v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04945v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04945v2",
                "updated": "2025-01-13T05:06:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    5,
                    6,
                    10,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-09T03:34:07Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    3,
                    34,
                    7,
                    3,
                    9,
                    0
                ],
                "title": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of\n  Large Language Models"
                },
                "summary": "It is crucial for large language models (LLMs) to follow instructions that\ninvolve multiple constraints. However, soft constraints are semantically\nrelated and difficult to verify through automated methods. These constraints\nremain a significant challenge for LLMs. To enhance the ability of LLMs to\nfollow soft constraints, we initially design a pipeline to obtain high-quality\noutputs automatically. Additionally, to fully utilize the acquired data, we\nintroduce a training paradigm based on curriculum learning. We experimentally\nevaluate the effectiveness of our methods in improving LLMs' soft constraint\nfollowing ability and analyze the factors driving the improvements. The\ndatasets and code are publicly available at\nhttps://github.com/Rainier-rq/FollowSoftConstraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is crucial for large language models (LLMs) to follow instructions that\ninvolve multiple constraints. However, soft constraints are semantically\nrelated and difficult to verify through automated methods. These constraints\nremain a significant challenge for LLMs. To enhance the ability of LLMs to\nfollow soft constraints, we initially design a pipeline to obtain high-quality\noutputs automatically. Additionally, to fully utilize the acquired data, we\nintroduce a training paradigm based on curriculum learning. We experimentally\nevaluate the effectiveness of our methods in improving LLMs' soft constraint\nfollowing ability and analyze the factors driving the improvements. The\ndatasets and code are publicly available at\nhttps://github.com/Rainier-rq/FollowSoftConstraints."
                },
                "authors": [
                    {
                        "name": "Qingyu Ren"
                    },
                    {
                        "name": "Jie Zeng"
                    },
                    {
                        "name": "Qianyu He"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Yanghua Xiao"
                    },
                    {
                        "name": "Weikang Zhou"
                    },
                    {
                        "name": "Zeye Sun"
                    },
                    {
                        "name": "Fei Yu"
                    }
                ],
                "author_detail": {
                    "name": "Fei Yu"
                },
                "author": "Fei Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04945v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04945v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16837v2",
                "updated": "2025-01-13T05:04:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    5,
                    4,
                    59,
                    0,
                    13,
                    0
                ],
                "published": "2024-07-23T21:02:38Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    21,
                    2,
                    38,
                    1,
                    205,
                    0
                ],
                "title": "MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs"
                },
                "summary": "The ability to compare objects, scenes, or situations is crucial for\neffective decision-making and problem-solving in everyday life. For instance,\ncomparing the freshness of apples enables better choices during grocery\nshopping while comparing sofa designs helps optimize the aesthetics of our\nliving space. Despite its significance, the comparative capability is largely\nunexplored in artificial general intelligence (AGI). In this paper, we\nintroduce MLLM-CompBench, a benchmark designed to evaluate the comparative\nreasoning capability of multimodal large language models (MLLMs).\nMLLM-CompBench mines and pairs images through visually oriented questions\ncovering eight dimensions of relative comparison: visual attribute, existence,\nstate, emotion, temporality, spatiality, quantity, and quality. We curate a\ncollection of around 40K image pairs using metadata from diverse vision\ndatasets and CLIP similarity scores. These image pairs span a broad array of\nvisual domains, including animals, fashion, sports, and both outdoor and indoor\nscenes. The questions are carefully crafted to discern relative characteristics\nbetween two images and are labeled by human annotators for accuracy and\nrelevance. We use MLLM-CompBench to evaluate recent MLLMs, including\nGPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our results reveal notable\nshortcomings in their comparative abilities. We believe MLLM-COMPBENCH not only\nsheds light on these limitations but also establishes a solid foundation for\nfuture enhancements in the comparative capability of MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability to compare objects, scenes, or situations is crucial for\neffective decision-making and problem-solving in everyday life. For instance,\ncomparing the freshness of apples enables better choices during grocery\nshopping while comparing sofa designs helps optimize the aesthetics of our\nliving space. Despite its significance, the comparative capability is largely\nunexplored in artificial general intelligence (AGI). In this paper, we\nintroduce MLLM-CompBench, a benchmark designed to evaluate the comparative\nreasoning capability of multimodal large language models (MLLMs).\nMLLM-CompBench mines and pairs images through visually oriented questions\ncovering eight dimensions of relative comparison: visual attribute, existence,\nstate, emotion, temporality, spatiality, quantity, and quality. We curate a\ncollection of around 40K image pairs using metadata from diverse vision\ndatasets and CLIP similarity scores. These image pairs span a broad array of\nvisual domains, including animals, fashion, sports, and both outdoor and indoor\nscenes. The questions are carefully crafted to discern relative characteristics\nbetween two images and are labeled by human annotators for accuracy and\nrelevance. We use MLLM-CompBench to evaluate recent MLLMs, including\nGPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our results reveal notable\nshortcomings in their comparative abilities. We believe MLLM-COMPBENCH not only\nsheds light on these limitations but also establishes a solid foundation for\nfuture enhancements in the comparative capability of MLLMs."
                },
                "authors": [
                    {
                        "name": "Jihyung Kil"
                    },
                    {
                        "name": "Zheda Mai"
                    },
                    {
                        "name": "Justin Lee"
                    },
                    {
                        "name": "Zihe Wang"
                    },
                    {
                        "name": "Kerrie Cheng"
                    },
                    {
                        "name": "Lemeng Wang"
                    },
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Arpita Chowdhury"
                    },
                    {
                        "name": "Wei-Lun Chao"
                    }
                ],
                "author_detail": {
                    "name": "Wei-Lun Chao"
                },
                "author": "Wei-Lun Chao",
                "arxiv_comment": "This paper has been accepted to NeurIPS 2024. The first two authors\n  contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07058v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07058v1",
                "updated": "2025-01-13T04:42:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    42,
                    45,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T04:42:45Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    42,
                    45,
                    0,
                    13,
                    0
                ],
                "title": "Logic Meets Magic: LLMs Cracking Smart Contract Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Logic Meets Magic: LLMs Cracking Smart Contract Vulnerabilities"
                },
                "summary": "Smart contract vulnerabilities caused significant economic losses in\nblockchain applications. Large Language Models (LLMs) provide new possibilities\nfor addressing this time-consuming task. However, state-of-the-art LLM-based\ndetection solutions are often plagued by high false-positive rates.\n  In this paper, we push the boundaries of existing research in two key ways.\nFirst, our evaluation is based on Solidity v0.8, offering the most up-to-date\ninsights compared to prior studies that focus on older versions (v0.4). Second,\nwe leverage the latest five LLM models (across companies), ensuring\ncomprehensive coverage across the most advanced capabilities in the field.\n  We conducted a series of rigorous evaluations. Our experiments demonstrate\nthat a well-designed prompt can reduce the false-positive rate by over 60%.\nSurprisingly, we also discovered that the recall rate for detecting some\nspecific vulnerabilities in Solidity v0.8 has dropped to just 13% compared to\nearlier versions (i.e., v0.4). Further analysis reveals the root cause of this\ndecline: the reliance of LLMs on identifying changes in newly introduced\nlibraries and frameworks during detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contract vulnerabilities caused significant economic losses in\nblockchain applications. Large Language Models (LLMs) provide new possibilities\nfor addressing this time-consuming task. However, state-of-the-art LLM-based\ndetection solutions are often plagued by high false-positive rates.\n  In this paper, we push the boundaries of existing research in two key ways.\nFirst, our evaluation is based on Solidity v0.8, offering the most up-to-date\ninsights compared to prior studies that focus on older versions (v0.4). Second,\nwe leverage the latest five LLM models (across companies), ensuring\ncomprehensive coverage across the most advanced capabilities in the field.\n  We conducted a series of rigorous evaluations. Our experiments demonstrate\nthat a well-designed prompt can reduce the false-positive rate by over 60%.\nSurprisingly, we also discovered that the recall rate for detecting some\nspecific vulnerabilities in Solidity v0.8 has dropped to just 13% compared to\nearlier versions (i.e., v0.4). Further analysis reveals the root cause of this\ndecline: the reliance of LLMs on identifying changes in newly introduced\nlibraries and frameworks during detection."
                },
                "authors": [
                    {
                        "name": "ZeKe Xiao"
                    },
                    {
                        "name": "Qin Wang"
                    },
                    {
                        "name": "Hammond Pearce"
                    },
                    {
                        "name": "Shiping Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shiping Chen"
                },
                "author": "Shiping Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07058v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07058v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19925v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19925v2",
                "updated": "2025-01-13T04:33:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    33,
                    1,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-27T21:19:01Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    21,
                    19,
                    1,
                    4,
                    362,
                    0
                ],
                "title": "HADES: Hardware Accelerated Decoding for Efficient Speculation in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HADES: Hardware Accelerated Decoding for Efficient Speculation in Large\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\nby understanding and generating human-like text. However, the increasing demand\nfor more sophisticated LLMs presents significant computational challenges due\nto their scale and complexity. This paper introduces Hardware Accelerated\nDecoding (HADES), a novel approach to enhance the performance and energy\nefficiency of LLMs. We address the design of an LLM accelerator with\nhardware-level speculative decoding support, a concept not previously explored\nin existing literature. Our work demonstrates how speculative decoding can\nsignificantly improve the efficiency of LLM operations, paving the way for more\nadvanced and practical applications of these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\nby understanding and generating human-like text. However, the increasing demand\nfor more sophisticated LLMs presents significant computational challenges due\nto their scale and complexity. This paper introduces Hardware Accelerated\nDecoding (HADES), a novel approach to enhance the performance and energy\nefficiency of LLMs. We address the design of an LLM accelerator with\nhardware-level speculative decoding support, a concept not previously explored\nin existing literature. Our work demonstrates how speculative decoding can\nsignificantly improve the efficiency of LLM operations, paving the way for more\nadvanced and practical applications of these models."
                },
                "authors": [
                    {
                        "name": "Ze Yang"
                    },
                    {
                        "name": "Yihong Jin"
                    },
                    {
                        "name": "Xinhe Xu"
                    }
                ],
                "author_detail": {
                    "name": "Xinhe Xu"
                },
                "author": "Xinhe Xu",
                "arxiv_comment": "Accepted to ICCEA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19925v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19925v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07054v1",
                "updated": "2025-01-13T04:28:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    28,
                    40,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T04:28:40Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    28,
                    40,
                    0,
                    13,
                    0
                ],
                "title": "PoAct: Policy and Action Dual-Control Agent for Generalized Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoAct: Policy and Action Dual-Control Agent for Generalized Applications"
                },
                "summary": "Based on their superior comprehension and reasoning capabilities, Large\nLanguage Model (LLM) driven agent frameworks have achieved significant success\nin numerous complex reasoning tasks. ReAct-like agents can solve various\nintricate problems step-by-step through progressive planning and tool calls,\niteratively optimizing new steps based on environmental feedback. However, as\nthe planning capabilities of LLMs improve, the actions invoked by tool calls in\nReAct-like frameworks often misalign with complex planning and challenging data\norganization. Code Action addresses these issues while also introducing the\nchallenges of a more complex action space and more difficult action\norganization. To leverage Code Action and tackle the challenges of its\ncomplexity, this paper proposes Policy and Action Dual-Control Agent (PoAct)\nfor generalized applications. The aim is to achieve higher-quality code actions\nand more accurate reasoning paths by dynamically switching reasoning policies\nand modifying the action space. Experimental results on the Agent Benchmark for\nboth legal and generic scenarios demonstrate the superior reasoning\ncapabilities and reduced token consumption of our approach in complex tasks. On\nthe LegalAgentBench, our method shows a 20 percent improvement over the\nbaseline while requiring fewer tokens. We conducted experiments and analyses on\nthe GPT-4o and GLM-4 series models, demonstrating the significant potential and\nscalability of our approach to solve complex problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on their superior comprehension and reasoning capabilities, Large\nLanguage Model (LLM) driven agent frameworks have achieved significant success\nin numerous complex reasoning tasks. ReAct-like agents can solve various\nintricate problems step-by-step through progressive planning and tool calls,\niteratively optimizing new steps based on environmental feedback. However, as\nthe planning capabilities of LLMs improve, the actions invoked by tool calls in\nReAct-like frameworks often misalign with complex planning and challenging data\norganization. Code Action addresses these issues while also introducing the\nchallenges of a more complex action space and more difficult action\norganization. To leverage Code Action and tackle the challenges of its\ncomplexity, this paper proposes Policy and Action Dual-Control Agent (PoAct)\nfor generalized applications. The aim is to achieve higher-quality code actions\nand more accurate reasoning paths by dynamically switching reasoning policies\nand modifying the action space. Experimental results on the Agent Benchmark for\nboth legal and generic scenarios demonstrate the superior reasoning\ncapabilities and reduced token consumption of our approach in complex tasks. On\nthe LegalAgentBench, our method shows a 20 percent improvement over the\nbaseline while requiring fewer tokens. We conducted experiments and analyses on\nthe GPT-4o and GLM-4 series models, demonstrating the significant potential and\nscalability of our approach to solve complex problems."
                },
                "authors": [
                    {
                        "name": "Guozhi Yuan"
                    },
                    {
                        "name": "Youfeng Liu"
                    },
                    {
                        "name": "Jingli Yang"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Kai Lin"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Shan He"
                    },
                    {
                        "name": "Zilin Ding"
                    },
                    {
                        "name": "Haitao Li"
                    }
                ],
                "author_detail": {
                    "name": "Haitao Li"
                },
                "author": "Haitao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07051v1",
                "updated": "2025-01-13T04:18:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    18,
                    52,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T04:18:52Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    18,
                    52,
                    0,
                    13,
                    0
                ],
                "title": "ROSAnnotator: A Web Application for ROSBag Data Analysis in Human-Robot\n  Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROSAnnotator: A Web Application for ROSBag Data Analysis in Human-Robot\n  Interaction"
                },
                "summary": "Human-robot interaction (HRI) is an interdisciplinary field that utilises\nboth quantitative and qualitative methods. While ROSBags, a file format within\nthe Robot Operating System (ROS), offer an efficient means of collecting\ntemporally synched multimodal data in empirical studies with real robots, there\nis a lack of tools specifically designed to integrate qualitative coding and\nanalysis functions with ROSBags. To address this gap, we developed\nROSAnnotator, a web-based application that incorporates a multimodal Large\nLanguage Model (LLM) to support both manual and automated annotation of ROSBag\ndata. ROSAnnotator currently facilitates video, audio, and transcription\nannotations and provides an open interface for custom ROS messages and tools.\nBy using ROSAnnotator, researchers can streamline the qualitative analysis\nprocess, create a more cohesive analysis pipeline, and quickly access\nstatistical summaries of annotations, thereby enhancing the overall efficiency\nof HRI data analysis. https://github.com/CHRI-Lab/ROSAnnotator",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-robot interaction (HRI) is an interdisciplinary field that utilises\nboth quantitative and qualitative methods. While ROSBags, a file format within\nthe Robot Operating System (ROS), offer an efficient means of collecting\ntemporally synched multimodal data in empirical studies with real robots, there\nis a lack of tools specifically designed to integrate qualitative coding and\nanalysis functions with ROSBags. To address this gap, we developed\nROSAnnotator, a web-based application that incorporates a multimodal Large\nLanguage Model (LLM) to support both manual and automated annotation of ROSBag\ndata. ROSAnnotator currently facilitates video, audio, and transcription\nannotations and provides an open interface for custom ROS messages and tools.\nBy using ROSAnnotator, researchers can streamline the qualitative analysis\nprocess, create a more cohesive analysis pipeline, and quickly access\nstatistical summaries of annotations, thereby enhancing the overall efficiency\nof HRI data analysis. https://github.com/CHRI-Lab/ROSAnnotator"
                },
                "authors": [
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Haoqi Li"
                    },
                    {
                        "name": "Ramtin Tabatabaei"
                    },
                    {
                        "name": "Wafa Johal"
                    }
                ],
                "author_detail": {
                    "name": "Wafa Johal"
                },
                "author": "Wafa Johal",
                "arxiv_comment": "Accepted to HRI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01312v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01312v3",
                "updated": "2025-01-13T03:53:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    3,
                    53,
                    34,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-02T15:53:25Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    53,
                    25,
                    3,
                    2,
                    0
                ],
                "title": "Learning Spectral Methods by Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Spectral Methods by Transformers"
                },
                "summary": "Transformers demonstrate significant advantages as the building block of\nmodern LLMs. In this work, we study the capacities of Transformers in\nperforming unsupervised learning. We show that multi-layered Transformers,\ngiven a sufficiently large set of pre-training instances, are able to learn the\nalgorithms themselves and perform statistical estimation tasks given new\ninstances. This learning paradigm is distinct from the in-context learning\nsetup and is similar to the learning procedure of human brains where skills are\nlearned through past experience. Theoretically, we prove that pre-trained\nTransformers can learn the spectral methods and use the classification of\nbi-class Gaussian mixture model as an example. Our proof is constructive using\nalgorithmic design techniques. Our results are built upon the similarities of\nmulti-layered Transformer architecture with the iterative recovery algorithms\nused in practice. Empirically, we verify the strong capacity of the\nmulti-layered (pre-trained) Transformer on unsupervised learning through the\nlens of both the PCA and the Clustering tasks performed on the synthetic and\nreal-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers demonstrate significant advantages as the building block of\nmodern LLMs. In this work, we study the capacities of Transformers in\nperforming unsupervised learning. We show that multi-layered Transformers,\ngiven a sufficiently large set of pre-training instances, are able to learn the\nalgorithms themselves and perform statistical estimation tasks given new\ninstances. This learning paradigm is distinct from the in-context learning\nsetup and is similar to the learning procedure of human brains where skills are\nlearned through past experience. Theoretically, we prove that pre-trained\nTransformers can learn the spectral methods and use the classification of\nbi-class Gaussian mixture model as an example. Our proof is constructive using\nalgorithmic design techniques. Our results are built upon the similarities of\nmulti-layered Transformer architecture with the iterative recovery algorithms\nused in practice. Empirically, we verify the strong capacity of the\nmulti-layered (pre-trained) Transformer on unsupervised learning through the\nlens of both the PCA and the Clustering tasks performed on the synthetic and\nreal-world datasets."
                },
                "authors": [
                    {
                        "name": "Yihan He"
                    },
                    {
                        "name": "Yuan Cao"
                    },
                    {
                        "name": "Hong-Yu Chen"
                    },
                    {
                        "name": "Dennis Wu"
                    },
                    {
                        "name": "Jianqing Fan"
                    },
                    {
                        "name": "Han Liu"
                    }
                ],
                "author_detail": {
                    "name": "Han Liu"
                },
                "author": "Han Liu",
                "arxiv_comment": "77 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01312v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01312v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07024v1",
                "updated": "2025-01-13T02:53:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    2,
                    53,
                    7,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T02:53:07Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    2,
                    53,
                    7,
                    0,
                    13,
                    0
                ],
                "title": "A Proposed Large Language Model-Based Smart Search for Archive System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Proposed Large Language Model-Based Smart Search for Archive System"
                },
                "summary": "This study presents a novel framework for smart search in digital archival\nsystems, leveraging the capabilities of Large Language Models (LLMs) to enhance\ninformation retrieval. By employing a Retrieval-Augmented Generation (RAG)\napproach, the framework enables the processing of natural language queries and\ntransforming non-textual data into meaningful textual representations. The\nsystem integrates advanced metadata generation techniques, a hybrid retrieval\nmechanism, a router query engine, and robust response synthesis, the results\nproved search precision and relevance. We present the architecture and\nimplementation of the system and evaluate its performance in four experiments\nconcerning LLM efficiency, hybrid retrieval optimizations, multilingual query\nhandling, and the impacts of individual components. Obtained results show\nsignificant improvements over conventional approaches and have demonstrated the\npotential of AI-powered systems to transform modern archival practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents a novel framework for smart search in digital archival\nsystems, leveraging the capabilities of Large Language Models (LLMs) to enhance\ninformation retrieval. By employing a Retrieval-Augmented Generation (RAG)\napproach, the framework enables the processing of natural language queries and\ntransforming non-textual data into meaningful textual representations. The\nsystem integrates advanced metadata generation techniques, a hybrid retrieval\nmechanism, a router query engine, and robust response synthesis, the results\nproved search precision and relevance. We present the architecture and\nimplementation of the system and evaluate its performance in four experiments\nconcerning LLM efficiency, hybrid retrieval optimizations, multilingual query\nhandling, and the impacts of individual components. Obtained results show\nsignificant improvements over conventional approaches and have demonstrated the\npotential of AI-powered systems to transform modern archival practices."
                },
                "authors": [
                    {
                        "name": "Ha Dung Nguyen"
                    },
                    {
                        "name": "Thi-Hoang Anh Nguyen"
                    },
                    {
                        "name": "Thanh Binh Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thanh Binh Nguyen"
                },
                "author": "Thanh Binh Nguyen",
                "arxiv_comment": "The 13th International Symposium on Information and Communication\n  Technology (SOICT 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19714v2",
                "updated": "2025-01-13T02:43:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    2,
                    43,
                    47,
                    0,
                    13,
                    0
                ],
                "published": "2024-11-29T14:02:00Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    2,
                    0,
                    4,
                    334,
                    0
                ],
                "title": "The Streetscape Application Services Stack (SASS): Towards a Distributed\n  Sensing Architecture for Urban Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Streetscape Application Services Stack (SASS): Towards a Distributed\n  Sensing Architecture for Urban Applications"
                },
                "summary": "As urban populations grow, cities are becoming more complex, driving the\ndeployment of interconnected sensing systems to realize the vision of smart\ncities. These systems aim to improve safety, mobility, and quality of life\nthrough applications that integrate diverse sensors with real-time\ndecision-making. Streetscape applications-focusing on challenges like\npedestrian safety and adaptive traffic management-depend on managing\ndistributed, heterogeneous sensor data, aligning information across time and\nspace, and enabling real-time processing. These tasks are inherently complex\nand often difficult to scale. The Streetscape Application Services Stack (SASS)\naddresses these challenges with three core services: multimodal data\nsynchronization, spatiotemporal data fusion, and distributed edge computing. By\nstructuring these capabilities as clear, composable abstractions with clear\nsemantics, SASS allows developers to scale streetscape applications efficiently\nwhile minimizing the complexity of multimodal integration.\n  We evaluated SASS in two real-world testbed environments: a controlled\nparking lot and an urban intersection in a major U.S. city. These testbeds\nallowed us to test SASS under diverse conditions, demonstrating its practical\napplicability. The Multimodal Data Synchronization service reduced temporal\nmisalignment errors by 88%, achieving synchronization accuracy within 50\nmilliseconds. Spatiotemporal Data Fusion service improved detection accuracy\nfor pedestrians and vehicles by over 10%, leveraging multicamera integration.\nThe Distributed Edge Computing service increased system throughput by more than\nan order of magnitude. Together, these results show how SASS provides the\nabstractions and performance needed to support real-time, scalable urban\napplications, bridging the gap between sensing infrastructure and actionable\nstreetscape intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As urban populations grow, cities are becoming more complex, driving the\ndeployment of interconnected sensing systems to realize the vision of smart\ncities. These systems aim to improve safety, mobility, and quality of life\nthrough applications that integrate diverse sensors with real-time\ndecision-making. Streetscape applications-focusing on challenges like\npedestrian safety and adaptive traffic management-depend on managing\ndistributed, heterogeneous sensor data, aligning information across time and\nspace, and enabling real-time processing. These tasks are inherently complex\nand often difficult to scale. The Streetscape Application Services Stack (SASS)\naddresses these challenges with three core services: multimodal data\nsynchronization, spatiotemporal data fusion, and distributed edge computing. By\nstructuring these capabilities as clear, composable abstractions with clear\nsemantics, SASS allows developers to scale streetscape applications efficiently\nwhile minimizing the complexity of multimodal integration.\n  We evaluated SASS in two real-world testbed environments: a controlled\nparking lot and an urban intersection in a major U.S. city. These testbeds\nallowed us to test SASS under diverse conditions, demonstrating its practical\napplicability. The Multimodal Data Synchronization service reduced temporal\nmisalignment errors by 88%, achieving synchronization accuracy within 50\nmilliseconds. Spatiotemporal Data Fusion service improved detection accuracy\nfor pedestrians and vehicles by over 10%, leveraging multicamera integration.\nThe Distributed Edge Computing service increased system throughput by more than\nan order of magnitude. Together, these results show how SASS provides the\nabstractions and performance needed to support real-time, scalable urban\napplications, bridging the gap between sensing infrastructure and actionable\nstreetscape intelligence."
                },
                "authors": [
                    {
                        "name": "Navid Salami Pargoo"
                    },
                    {
                        "name": "Mahshid Ghasemi"
                    },
                    {
                        "name": "Shuren Xia"
                    },
                    {
                        "name": "Mehmet Kerem Turkcan"
                    },
                    {
                        "name": "Taqiya Ehsan"
                    },
                    {
                        "name": "Chengbo Zang"
                    },
                    {
                        "name": "Yuan Sun"
                    },
                    {
                        "name": "Javad Ghaderi"
                    },
                    {
                        "name": "Gil Zussman"
                    },
                    {
                        "name": "Zoran Kostic"
                    },
                    {
                        "name": "Jorge Ortiz"
                    }
                ],
                "author_detail": {
                    "name": "Jorge Ortiz"
                },
                "author": "Jorge Ortiz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11159v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11159v2",
                "updated": "2025-01-13T00:47:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    0,
                    47,
                    31,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-15T11:47:39Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    11,
                    47,
                    39,
                    6,
                    350,
                    0
                ],
                "title": "A Report on Financial Regulations Challenge at COLING 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Report on Financial Regulations Challenge at COLING 2025"
                },
                "summary": "Financial large language models (FinLLMs) have been applied to various tasks\nin business, finance, accounting, and auditing. Complex financial regulations\nand standards are critical to financial services, which LLMs must comply with.\nHowever, FinLLMs' performance in understanding and interpreting financial\nregulations has rarely been studied. Therefore, we organize the Regulations\nChallenge, a shared task at COLING 2025. It encourages the academic community\nto explore the strengths and limitations of popular LLMs. We create 9 novel\ntasks and corresponding question sets. In this paper, we provide an overview of\nthese tasks and summarize participants' approaches and results. We aim to raise\nawareness of FinLLMs' professional capability in financial regulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial large language models (FinLLMs) have been applied to various tasks\nin business, finance, accounting, and auditing. Complex financial regulations\nand standards are critical to financial services, which LLMs must comply with.\nHowever, FinLLMs' performance in understanding and interpreting financial\nregulations has rarely been studied. Therefore, we organize the Regulations\nChallenge, a shared task at COLING 2025. It encourages the academic community\nto explore the strengths and limitations of popular LLMs. We create 9 novel\ntasks and corresponding question sets. In this paper, we provide an overview of\nthese tasks and summarize participants' approaches and results. We aim to raise\nawareness of FinLLMs' professional capability in financial regulations."
                },
                "authors": [
                    {
                        "name": "Keyi Wang"
                    },
                    {
                        "name": "Jaisal Patel"
                    },
                    {
                        "name": "Charlie Shen"
                    },
                    {
                        "name": "Daniel Kim"
                    },
                    {
                        "name": "Andy Zhu"
                    },
                    {
                        "name": "Alex Lin"
                    },
                    {
                        "name": "Luca Borella"
                    },
                    {
                        "name": "Cailean Osborne"
                    },
                    {
                        "name": "Matt White"
                    },
                    {
                        "name": "Steve Yang"
                    },
                    {
                        "name": "Kairong Xiao"
                    },
                    {
                        "name": "Xiao-Yang Liu Yanglet"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Yang Liu Yanglet"
                },
                "author": "Xiao-Yang Liu Yanglet",
                "arxiv_comment": "8 pages, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11159v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11159v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06980v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06980v1",
                "updated": "2025-01-13T00:03:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    0,
                    3,
                    20,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T00:03:20Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    0,
                    3,
                    20,
                    0,
                    13,
                    0
                ],
                "title": "Combining LLM decision and RL action selection to improve RL policy for\n  adaptive interventions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining LLM decision and RL action selection to improve RL policy for\n  adaptive interventions"
                },
                "summary": "Reinforcement learning (RL) is increasingly being used in the healthcare\ndomain, particularly for the development of personalized health adaptive\ninterventions. Inspired by the success of Large Language Models (LLMs), we are\ninterested in using LLMs to update the RL policy in real time, with the goal of\naccelerating personalization. We use the text-based user preference to\ninfluence the action selection on the fly, in order to immediately incorporate\nthe user preference. We use the term \"user preference\" as a broad term to refer\nto a user personal preference, constraint, health status, or a statement\nexpressing like or dislike, etc. Our novel approach is a hybrid method that\ncombines the LLM response and the RL action selection to improve the RL policy.\nGiven an LLM prompt that incorporates the user preference, the LLM acts as a\nfilter in the typical RL action selection. We investigate different prompting\nstrategies and action selection strategies. To evaluate our approach, we\nimplement a simulation environment that generates the text-based user\npreferences and models the constraints that impact behavioral dynamics. We show\nthat our approach is able to take into account the text-based user preferences,\nwhile improving the RL policy, thus improving personalization in adaptive\nintervention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) is increasingly being used in the healthcare\ndomain, particularly for the development of personalized health adaptive\ninterventions. Inspired by the success of Large Language Models (LLMs), we are\ninterested in using LLMs to update the RL policy in real time, with the goal of\naccelerating personalization. We use the text-based user preference to\ninfluence the action selection on the fly, in order to immediately incorporate\nthe user preference. We use the term \"user preference\" as a broad term to refer\nto a user personal preference, constraint, health status, or a statement\nexpressing like or dislike, etc. Our novel approach is a hybrid method that\ncombines the LLM response and the RL action selection to improve the RL policy.\nGiven an LLM prompt that incorporates the user preference, the LLM acts as a\nfilter in the typical RL action selection. We investigate different prompting\nstrategies and action selection strategies. To evaluate our approach, we\nimplement a simulation environment that generates the text-based user\npreferences and models the constraints that impact behavioral dynamics. We show\nthat our approach is able to take into account the text-based user preferences,\nwhile improving the RL policy, thus improving personalization in adaptive\nintervention."
                },
                "authors": [
                    {
                        "name": "Karine Karine"
                    },
                    {
                        "name": "Benjamin M. Marlin"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin M. Marlin"
                },
                "author": "Benjamin M. Marlin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06980v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06980v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06972v1",
                "updated": "2025-01-12T23:06:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    23,
                    6,
                    25,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T23:06:25Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    23,
                    6,
                    25,
                    6,
                    12,
                    0
                ],
                "title": "How is Google using AI for internal code migrations?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How is Google using AI for internal code migrations?"
                },
                "summary": "In recent years, there has been a tremendous interest in using generative AI,\nand particularly large language models (LLMs) in software engineering; indeed\nthere are now several commercially available tools, and many large companies\nalso have created proprietary ML-based tools for their own software engineers.\nWhile the use of ML for common tasks such as code completion is available in\ncommodity tools, there is a growing interest in application of LLMs for more\nbespoke purposes. One such purpose is code migration.\n  This article is an experience report on using LLMs for code migrations at\nGoogle. It is not a research study, in the sense that we do not carry out\ncomparisons against other approaches or evaluate research questions/hypotheses.\nRather, we share our experiences in applying LLM-based code migration in an\nenterprise context across a range of migration cases, in the hope that other\nindustry practitioners will find our insights useful. Many of these learnings\napply to any application of ML in software engineering. We see evidence that\nthe use of LLMs can reduce the time needed for migrations significantly, and\ncan reduce barriers to get started and complete migration programs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, there has been a tremendous interest in using generative AI,\nand particularly large language models (LLMs) in software engineering; indeed\nthere are now several commercially available tools, and many large companies\nalso have created proprietary ML-based tools for their own software engineers.\nWhile the use of ML for common tasks such as code completion is available in\ncommodity tools, there is a growing interest in application of LLMs for more\nbespoke purposes. One such purpose is code migration.\n  This article is an experience report on using LLMs for code migrations at\nGoogle. It is not a research study, in the sense that we do not carry out\ncomparisons against other approaches or evaluate research questions/hypotheses.\nRather, we share our experiences in applying LLM-based code migration in an\nenterprise context across a range of migration cases, in the hope that other\nindustry practitioners will find our insights useful. Many of these learnings\napply to any application of ML in software engineering. We see evidence that\nthe use of LLMs can reduce the time needed for migrations significantly, and\ncan reduce barriers to get started and complete migration programs."
                },
                "authors": [
                    {
                        "name": "Stoyan Nikolov"
                    },
                    {
                        "name": "Daniele Codecasa"
                    },
                    {
                        "name": "Anna Sjovall"
                    },
                    {
                        "name": "Maxim Tabachnyk"
                    },
                    {
                        "name": "Satish Chandra"
                    },
                    {
                        "name": "Siddharth Taneja"
                    },
                    {
                        "name": "Celal Ziftci"
                    }
                ],
                "author_detail": {
                    "name": "Celal Ziftci"
                },
                "author": "Celal Ziftci",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.03093v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.03093v2",
                "updated": "2025-01-12T22:56:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    22,
                    56,
                    52,
                    6,
                    12,
                    0
                ],
                "published": "2024-09-04T21:46:18Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    21,
                    46,
                    18,
                    2,
                    248,
                    0
                ],
                "title": "ASTER: Natural and Multi-language Unit Test Generation with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASTER: Natural and Multi-language Unit Test Generation with LLMs"
                },
                "summary": "Implementing automated unit tests is an important but time-consuming activity\nin software development. To assist developers in this task, many techniques for\nautomating unit test generation have been developed. However, despite this\neffort, usable tools exist for very few programming languages. Moreover,\nstudies have found that automatically generated tests suffer poor readability\nand do not resemble developer-written tests. In this work, we present a\nrigorous investigation of how large language models (LLMs) can help bridge the\ngap. We describe a generic pipeline that incorporates static analysis to guide\nLLMs in generating compilable and high-coverage test cases. We illustrate how\nthe pipeline can be applied to different programming languages, specifically\nJava and Python, and to complex software requiring environment mocking. We\nconducted an empirical study to assess the quality of the generated tests in\nterms of code coverage and test naturalness -- evaluating them on standard as\nwell as enterprise Java applications and a large Python benchmark. Our results\ndemonstrate that LLM-based test generation, when guided by static analysis, can\nbe competitive with, and even outperform, state-of-the-art test-generation\ntechniques in coverage achieved while also producing considerably more natural\ntest cases that developers find easy to understand. We also present the results\nof a user study, conducted with 161 professional developers, that highlights\nthe naturalness characteristics of the tests generated by our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementing automated unit tests is an important but time-consuming activity\nin software development. To assist developers in this task, many techniques for\nautomating unit test generation have been developed. However, despite this\neffort, usable tools exist for very few programming languages. Moreover,\nstudies have found that automatically generated tests suffer poor readability\nand do not resemble developer-written tests. In this work, we present a\nrigorous investigation of how large language models (LLMs) can help bridge the\ngap. We describe a generic pipeline that incorporates static analysis to guide\nLLMs in generating compilable and high-coverage test cases. We illustrate how\nthe pipeline can be applied to different programming languages, specifically\nJava and Python, and to complex software requiring environment mocking. We\nconducted an empirical study to assess the quality of the generated tests in\nterms of code coverage and test naturalness -- evaluating them on standard as\nwell as enterprise Java applications and a large Python benchmark. Our results\ndemonstrate that LLM-based test generation, when guided by static analysis, can\nbe competitive with, and even outperform, state-of-the-art test-generation\ntechniques in coverage achieved while also producing considerably more natural\ntest cases that developers find easy to understand. We also present the results\nof a user study, conducted with 161 professional developers, that highlights\nthe naturalness characteristics of the tests generated by our approach."
                },
                "authors": [
                    {
                        "name": "Rangeet Pan"
                    },
                    {
                        "name": "Myeongsoo Kim"
                    },
                    {
                        "name": "Rahul Krishna"
                    },
                    {
                        "name": "Raju Pavuluri"
                    },
                    {
                        "name": "Saurabh Sinha"
                    }
                ],
                "author_detail": {
                    "name": "Saurabh Sinha"
                },
                "author": "Saurabh Sinha",
                "arxiv_comment": "Accepted at ICSE-SEIP, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.03093v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.03093v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06964v1",
                "updated": "2025-01-12T22:49:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    22,
                    49,
                    32,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T22:49:32Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    22,
                    49,
                    32,
                    6,
                    12,
                    0
                ],
                "title": "Enhancing Patient-Centric Communication: Leveraging LLMs to Simulate\n  Patient Perspectives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Patient-Centric Communication: Leveraging LLMs to Simulate\n  Patient Perspectives"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nrole-playing scenarios, particularly in simulating domain-specific experts\nusing tailored prompts. This ability enables LLMs to adopt the persona of\nindividuals with specific backgrounds, offering a cost-effective and efficient\nalternative to traditional, resource-intensive user studies. By mimicking human\nbehavior, LLMs can anticipate responses based on concrete demographic or\nprofessional profiles. In this paper, we evaluate the effectiveness of LLMs in\nsimulating individuals with diverse backgrounds and analyze the consistency of\nthese simulated behaviors compared to real-world outcomes. In particular, we\nexplore the potential of LLMs to interpret and respond to discharge summaries\nprovided to patients leaving the Intensive Care Unit (ICU). We evaluate and\ncompare with human responses the comprehensibility of discharge summaries among\nindividuals with varying educational backgrounds, using this analysis to assess\nthe strengths and limitations of LLM-driven simulations. Notably, when LLMs are\nprimed with educational background information, they deliver accurate and\nactionable medical guidance 88% of the time. However, when other information is\nprovided, performance significantly drops, falling below random chance levels.\nThis preliminary study shows the potential benefits and pitfalls of\nautomatically generating patient-specific health information from diverse\npopulations. While LLMs show promise in simulating health personas, our results\nhighlight critical gaps that must be addressed before they can be reliably used\nin clinical settings. Our findings suggest that a straightforward\nquery-response model could outperform a more tailored approach in delivering\nhealth information. This is a crucial first step in understanding how LLMs can\nbe optimized for personalized health communication while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nrole-playing scenarios, particularly in simulating domain-specific experts\nusing tailored prompts. This ability enables LLMs to adopt the persona of\nindividuals with specific backgrounds, offering a cost-effective and efficient\nalternative to traditional, resource-intensive user studies. By mimicking human\nbehavior, LLMs can anticipate responses based on concrete demographic or\nprofessional profiles. In this paper, we evaluate the effectiveness of LLMs in\nsimulating individuals with diverse backgrounds and analyze the consistency of\nthese simulated behaviors compared to real-world outcomes. In particular, we\nexplore the potential of LLMs to interpret and respond to discharge summaries\nprovided to patients leaving the Intensive Care Unit (ICU). We evaluate and\ncompare with human responses the comprehensibility of discharge summaries among\nindividuals with varying educational backgrounds, using this analysis to assess\nthe strengths and limitations of LLM-driven simulations. Notably, when LLMs are\nprimed with educational background information, they deliver accurate and\nactionable medical guidance 88% of the time. However, when other information is\nprovided, performance significantly drops, falling below random chance levels.\nThis preliminary study shows the potential benefits and pitfalls of\nautomatically generating patient-specific health information from diverse\npopulations. While LLMs show promise in simulating health personas, our results\nhighlight critical gaps that must be addressed before they can be reliably used\nin clinical settings. Our findings suggest that a straightforward\nquery-response model could outperform a more tailored approach in delivering\nhealth information. This is a crucial first step in understanding how LLMs can\nbe optimized for personalized health communication while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Xinyao Ma"
                    },
                    {
                        "name": "Rui Zhu"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Jingwei Xiong"
                    },
                    {
                        "name": "Qingyu Chen"
                    },
                    {
                        "name": "Haixu Tang"
                    },
                    {
                        "name": "L. Jean Camp"
                    },
                    {
                        "name": "Lucila Ohno-Machado"
                    }
                ],
                "author_detail": {
                    "name": "Lucila Ohno-Machado"
                },
                "author": "Lucila Ohno-Machado",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06942v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06942v1",
                "updated": "2025-01-12T21:39:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    21,
                    39,
                    6,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T21:39:06Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    21,
                    39,
                    6,
                    6,
                    12,
                    0
                ],
                "title": "Comparison of Autoencoders for tokenization of ASL datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparison of Autoencoders for tokenization of ASL datasets"
                },
                "summary": "Generative AI, powered by large language models (LLMs), has revolutionized\napplications across text, audio, images, and video. This study focuses on\ndeveloping and evaluating encoder-decoder architectures for the American Sign\nLanguage (ASL) image dataset, consisting of 87,000 images across 29 hand sign\nclasses. Three approaches were compared: Feedforward Autoencoders,\nConvolutional Autoencoders, and Diffusion Autoencoders. The Diffusion\nAutoencoder outperformed the others, achieving the lowest mean squared error\n(MSE) and highest Mean Opinion Score (MOS) due to its probabilistic noise\nmodeling and iterative denoising capabilities. The Convolutional Autoencoder\ndemonstrated effective spatial feature extraction but lacked the robustness of\nthe diffusion process, while the Feedforward Autoencoder served as a baseline\nwith limitations in handling complex image data. Objective and subjective\nevaluations confirmed the superiority of the Diffusion Autoencoder for\nhigh-fidelity image reconstruction, emphasizing its potential in multimodal AI\napplications such as sign language recognition and generation. This work\nprovides critical insights into designing robust encoder-decoder systems to\nadvance multimodal AI capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI, powered by large language models (LLMs), has revolutionized\napplications across text, audio, images, and video. This study focuses on\ndeveloping and evaluating encoder-decoder architectures for the American Sign\nLanguage (ASL) image dataset, consisting of 87,000 images across 29 hand sign\nclasses. Three approaches were compared: Feedforward Autoencoders,\nConvolutional Autoencoders, and Diffusion Autoencoders. The Diffusion\nAutoencoder outperformed the others, achieving the lowest mean squared error\n(MSE) and highest Mean Opinion Score (MOS) due to its probabilistic noise\nmodeling and iterative denoising capabilities. The Convolutional Autoencoder\ndemonstrated effective spatial feature extraction but lacked the robustness of\nthe diffusion process, while the Feedforward Autoencoder served as a baseline\nwith limitations in handling complex image data. Objective and subjective\nevaluations confirmed the superiority of the Diffusion Autoencoder for\nhigh-fidelity image reconstruction, emphasizing its potential in multimodal AI\napplications such as sign language recognition and generation. This work\nprovides critical insights into designing robust encoder-decoder systems to\nadvance multimodal AI capabilities."
                },
                "authors": [
                    {
                        "name": "Vouk Praun-Petrovic"
                    },
                    {
                        "name": "Aadhvika Koundinya"
                    },
                    {
                        "name": "Lavanya Prahallad"
                    }
                ],
                "author_detail": {
                    "name": "Lavanya Prahallad"
                },
                "author": "Lavanya Prahallad",
                "arxiv_comment": "9 pages, 2 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06942v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06942v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.03123v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.03123v4",
                "updated": "2025-01-12T21:07:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    21,
                    7,
                    54,
                    6,
                    12,
                    0
                ],
                "published": "2023-04-13T16:01:28Z",
                "published_parsed": [
                    2023,
                    4,
                    13,
                    16,
                    1,
                    28,
                    3,
                    103,
                    0
                ],
                "title": "ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review"
                },
                "summary": "ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability"
                },
                "authors": [
                    {
                        "name": "Sunder Ali Khowaja"
                    },
                    {
                        "name": "Parus Khuwaja"
                    },
                    {
                        "name": "Kapal Dev"
                    },
                    {
                        "name": "Weizheng Wang"
                    },
                    {
                        "name": "Lewis Nkenyereye"
                    }
                ],
                "author_detail": {
                    "name": "Lewis Nkenyereye"
                },
                "author": "Lewis Nkenyereye",
                "arxiv_doi": "10.1007/s12559-024-10285-1",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s12559-024-10285-1",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2305.03123v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.03123v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "29 pages, 8 figures, 4 tables",
                "arxiv_journal_ref": "Cognitive Computation, 2024",
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06932v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06932v1",
                "updated": "2025-01-12T21:00:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    21,
                    0,
                    50,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T21:00:50Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    21,
                    0,
                    50,
                    6,
                    12,
                    0
                ],
                "title": "Harnessing Large Language Models for Disaster Management: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Large Language Models for Disaster Management: A Survey"
                },
                "summary": "Large language models (LLMs) have revolutionized scientific research with\ntheir exceptional capabilities and transformed various fields. Among their\npractical applications, LLMs have been playing a crucial role in mitigating\nthreats to human life, infrastructure, and the environment. Despite growing\nresearch in disaster LLMs, there remains a lack of systematic review and\nin-depth analysis of LLMs for natural disaster management. To address the gap,\nthis paper presents a comprehensive survey of existing LLMs in natural disaster\nmanagement, along with a taxonomy that categorizes existing works based on\ndisaster phases and application scenarios. By collecting public datasets and\nidentifying key challenges and opportunities, this study aims to guide the\nprofessional community in developing advanced LLMs for disaster management to\nenhance the resilience against natural disasters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have revolutionized scientific research with\ntheir exceptional capabilities and transformed various fields. Among their\npractical applications, LLMs have been playing a crucial role in mitigating\nthreats to human life, infrastructure, and the environment. Despite growing\nresearch in disaster LLMs, there remains a lack of systematic review and\nin-depth analysis of LLMs for natural disaster management. To address the gap,\nthis paper presents a comprehensive survey of existing LLMs in natural disaster\nmanagement, along with a taxonomy that categorizes existing works based on\ndisaster phases and application scenarios. By collecting public datasets and\nidentifying key challenges and opportunities, this study aims to guide the\nprofessional community in developing advanced LLMs for disaster management to\nenhance the resilience against natural disasters."
                },
                "authors": [
                    {
                        "name": "Zhenyu Lei"
                    },
                    {
                        "name": "Yushun Dong"
                    },
                    {
                        "name": "Weiyu Li"
                    },
                    {
                        "name": "Rong Ding"
                    },
                    {
                        "name": "Qi Wang"
                    },
                    {
                        "name": "Jundong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jundong Li"
                },
                "author": "Jundong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06932v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06932v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06929v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06929v1",
                "updated": "2025-01-12T20:50:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    20,
                    50,
                    24,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T20:50:24Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    20,
                    50,
                    24,
                    6,
                    12,
                    0
                ],
                "title": "Why are we living the age of AI applications right now? The long\n  innovation path from AI's birth to a child's bedtime magic",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why are we living the age of AI applications right now? The long\n  innovation path from AI's birth to a child's bedtime magic"
                },
                "summary": "Today a four-year-old child who does not know how to read or write can now\ncreate bedtime stories with graphical illustrations and narrated audio, using\nAI tools that seamlessly transform speech into text, generate visuals, and\nconvert text back into speech in a natural and engaging manner. This remarkable\nexample demonstrates why we are living in the age of AI applications. This\npaper examines contemporary leading AI applications and traces their historical\ndevelopment, highlighting the major advancements that have enabled their\nrealization. Five key factors are identified: 1) The evolution of computational\nhardware (CPUs and GPUs), enabling the training of complex AI models 2) The\nvast digital archives provided by the World Wide Web, which serve as a\nfoundational data resource for AI systems 3) The ubiquity of mobile computing,\nwith smartphones acting as powerful, accessible small computers in the hands of\nbillions 4) The rise of industrial-scale cloud infrastructures, offering\nelastic computational power for AI training and deployment 5) Breakthroughs in\nAI research, including neural networks, backpropagation, and the \"Attention is\nAll You Need\" framework, which underpin modern AI capabilities. These\ninnovations have elevated AI from solving narrow tasks to enabling applications\nlike ChatGPT that are adaptable for numerous use cases, redefining\nhuman-computer interaction. By situating these developments within a historical\ncontext, the paper highlights the critical milestones that have made AI's\ncurrent capabilities both possible and widely accessible, offering profound\nimplications for society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today a four-year-old child who does not know how to read or write can now\ncreate bedtime stories with graphical illustrations and narrated audio, using\nAI tools that seamlessly transform speech into text, generate visuals, and\nconvert text back into speech in a natural and engaging manner. This remarkable\nexample demonstrates why we are living in the age of AI applications. This\npaper examines contemporary leading AI applications and traces their historical\ndevelopment, highlighting the major advancements that have enabled their\nrealization. Five key factors are identified: 1) The evolution of computational\nhardware (CPUs and GPUs), enabling the training of complex AI models 2) The\nvast digital archives provided by the World Wide Web, which serve as a\nfoundational data resource for AI systems 3) The ubiquity of mobile computing,\nwith smartphones acting as powerful, accessible small computers in the hands of\nbillions 4) The rise of industrial-scale cloud infrastructures, offering\nelastic computational power for AI training and deployment 5) Breakthroughs in\nAI research, including neural networks, backpropagation, and the \"Attention is\nAll You Need\" framework, which underpin modern AI capabilities. These\ninnovations have elevated AI from solving narrow tasks to enabling applications\nlike ChatGPT that are adaptable for numerous use cases, redefining\nhuman-computer interaction. By situating these developments within a historical\ncontext, the paper highlights the critical milestones that have made AI's\ncurrent capabilities both possible and widely accessible, offering profound\nimplications for society."
                },
                "authors": [
                    {
                        "name": "Tapio PitkÃ¤ranta"
                    }
                ],
                "author_detail": {
                    "name": "Tapio PitkÃ¤ranta"
                },
                "author": "Tapio PitkÃ¤ranta",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06929v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06929v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06921v1",
                "updated": "2025-01-12T20:15:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    20,
                    15,
                    36,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T20:15:36Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    20,
                    15,
                    36,
                    6,
                    12,
                    0
                ],
                "title": "Monolithic 3D FPGAs Utilizing Back-End-of-Line Configuration Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monolithic 3D FPGAs Utilizing Back-End-of-Line Configuration Memories"
                },
                "summary": "This work presents a novel monolithic 3D (M3D) FPGA architecture that\nleverages stackable back-end-of-line (BEOL) transistors to implement\nconfiguration memory and pass gates, significantly improving area, latency, and\npower efficiency. By integrating n-type (W-doped In_2O_3) and p-type (SnO)\namorphous oxide semiconductor (AOS) transistors in the BEOL, Si SRAM\nconfiguration bits are substituted with a less leaky equivalent that can be\nprogrammed at logic-compatible voltages. BEOL-compatible AOS transistors are\ncurrently under extensive research and development in the device community,\nwith investment by leading foundries, from which reported data is used to\ndevelop robust physics-based models in TCAD that enable circuit design. The use\nof AOS pass gates reduces the overhead of reconfigurable circuits by mapping\nFPGA switch block (SB) and connection block (CB) matrices above configurable\nlogic blocks (CLBs), thereby increasing the proximity of logic elements and\nreducing latency. By interfacing with the latest Verilog-to-Routing (VTR)\nsuite, an AOS-based M3D FPGA design implemented in 7 nm technology is\ndemonstrated with 3.4x lower area-time squared product (AT^2), 27% lower\ncritical path latency, and 26% lower reconfigurable routing block power on\nbenchmarks including hyperdimensional computing and large language models\n(LLMs).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a novel monolithic 3D (M3D) FPGA architecture that\nleverages stackable back-end-of-line (BEOL) transistors to implement\nconfiguration memory and pass gates, significantly improving area, latency, and\npower efficiency. By integrating n-type (W-doped In_2O_3) and p-type (SnO)\namorphous oxide semiconductor (AOS) transistors in the BEOL, Si SRAM\nconfiguration bits are substituted with a less leaky equivalent that can be\nprogrammed at logic-compatible voltages. BEOL-compatible AOS transistors are\ncurrently under extensive research and development in the device community,\nwith investment by leading foundries, from which reported data is used to\ndevelop robust physics-based models in TCAD that enable circuit design. The use\nof AOS pass gates reduces the overhead of reconfigurable circuits by mapping\nFPGA switch block (SB) and connection block (CB) matrices above configurable\nlogic blocks (CLBs), thereby increasing the proximity of logic elements and\nreducing latency. By interfacing with the latest Verilog-to-Routing (VTR)\nsuite, an AOS-based M3D FPGA design implemented in 7 nm technology is\ndemonstrated with 3.4x lower area-time squared product (AT^2), 27% lower\ncritical path latency, and 26% lower reconfigurable routing block power on\nbenchmarks including hyperdimensional computing and large language models\n(LLMs)."
                },
                "authors": [
                    {
                        "name": "Faaiq Waqar"
                    },
                    {
                        "name": "Jiahao Zhang"
                    },
                    {
                        "name": "Anni Lu"
                    },
                    {
                        "name": "Zifan He"
                    },
                    {
                        "name": "Jason Cong"
                    },
                    {
                        "name": "Shimeng Yu"
                    }
                ],
                "author_detail": {
                    "name": "Shimeng Yu"
                },
                "author": "Shimeng Yu",
                "arxiv_comment": "8 Pages, 9 Figures, 3 Tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.1; B.7.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17298v2",
                "updated": "2025-01-12T20:13:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    20,
                    13,
                    35,
                    6,
                    12,
                    0
                ],
                "published": "2024-12-23T05:41:01Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    5,
                    41,
                    1,
                    0,
                    358,
                    0
                ],
                "title": "Prompting in the Wild: An Empirical Study of Prompt Evolution in\n  Software Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting in the Wild: An Empirical Study of Prompt Evolution in\n  Software Repositories"
                },
                "summary": "The adoption of Large Language Models (LLMs) is reshaping software\ndevelopment as developers integrate these LLMs into their applications. In such\napplications, prompts serve as the primary means of interacting with LLMs.\nDespite the widespread use of LLM-integrated applications, there is limited\nunderstanding of how developers manage and evolve prompts. This study presents\nthe first empirical analysis of prompt evolution in LLM-integrated software\ndevelopment. We analyzed 1,262 prompt changes across 243 GitHub repositories to\ninvestigate the patterns and frequencies of prompt changes, their relationship\nwith code changes, documentation practices, and their impact on system\nbehavior. Our findings show that developers primarily evolve prompts through\nadditions and modifications, with most changes occurring during feature\ndevelopment. We identified key challenges in prompt engineering: only 21.9% of\nprompt changes are documented in commit messages, changes can introduce logical\ninconsistencies, and misalignment often occurs between prompt changes and LLM\nresponses. These insights emphasize the need for specialized testing\nframeworks, automated validation tools, and improved documentation practices to\nenhance the reliability of LLM-integrated applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of Large Language Models (LLMs) is reshaping software\ndevelopment as developers integrate these LLMs into their applications. In such\napplications, prompts serve as the primary means of interacting with LLMs.\nDespite the widespread use of LLM-integrated applications, there is limited\nunderstanding of how developers manage and evolve prompts. This study presents\nthe first empirical analysis of prompt evolution in LLM-integrated software\ndevelopment. We analyzed 1,262 prompt changes across 243 GitHub repositories to\ninvestigate the patterns and frequencies of prompt changes, their relationship\nwith code changes, documentation practices, and their impact on system\nbehavior. Our findings show that developers primarily evolve prompts through\nadditions and modifications, with most changes occurring during feature\ndevelopment. We identified key challenges in prompt engineering: only 21.9% of\nprompt changes are documented in commit messages, changes can introduce logical\ninconsistencies, and misalignment often occurs between prompt changes and LLM\nresponses. These insights emphasize the need for specialized testing\nframeworks, automated validation tools, and improved documentation practices to\nenhance the reliability of LLM-integrated applications."
                },
                "authors": [
                    {
                        "name": "Mahan Tafreshipour"
                    },
                    {
                        "name": "Aaron Imani"
                    },
                    {
                        "name": "Eric Huang"
                    },
                    {
                        "name": "Eduardo Almeida"
                    },
                    {
                        "name": "Thomas Zimmermann"
                    },
                    {
                        "name": "Iftekhar Ahmed"
                    }
                ],
                "author_detail": {
                    "name": "Iftekhar Ahmed"
                },
                "author": "Iftekhar Ahmed",
                "arxiv_comment": "Accepted at MSR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06913v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06913v1",
                "updated": "2025-01-12T19:49:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    19,
                    49,
                    28,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T19:49:28Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    19,
                    49,
                    28,
                    6,
                    12,
                    0
                ],
                "title": "Towards Fair and Privacy-Aware Transfer Learning for Educational\n  Predictive Modeling: A Case Study on Retention Prediction in Community\n  Colleges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Fair and Privacy-Aware Transfer Learning for Educational\n  Predictive Modeling: A Case Study on Retention Prediction in Community\n  Colleges"
                },
                "summary": "Predictive analytics is widely used in learning analytics, but many\nresource-constrained institutions lack the capacity to develop their own models\nor rely on proprietary ones trained in different contexts with little\ntransparency. Transfer learning holds promise for expanding equitable access to\npredictive analytics but remains underexplored due to legal and technical\nconstraints. This paper examines transfer learning strategies for retention\nprediction at U.S. two-year community colleges. We envision a scenario where\ncommunity colleges collaborate with each other and four-year universities to\ndevelop retention prediction models under privacy constraints and evaluate\nrisks and improvement strategies of cross-institutional model transfer. Using\nadministrative records from 4 research universities and 23 community colleges\ncovering over 800,000 students across 7 cohorts, we identify performance and\nfairness degradation when external models are deployed locally without\nadaptation. Publicly available contextual information can forecast these\nperformance drops and offer early guidance for model portability. For\ndevelopers under privacy regulations, sequential training selecting\ninstitutions based on demographic similarities enhances fairness without\ncompromising performance. For institutions lacking local data to fine-tune\nsource models, customizing evaluation thresholds for sensitive groups\noutperforms standard transfer techniques in improving performance and fairness.\nOur findings suggest the value of transfer learning for more accessible\neducational predictive modeling and call for judicious use of contextual\ninformation in model training, selection, and deployment to achieve reliable\nand equitable model transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive analytics is widely used in learning analytics, but many\nresource-constrained institutions lack the capacity to develop their own models\nor rely on proprietary ones trained in different contexts with little\ntransparency. Transfer learning holds promise for expanding equitable access to\npredictive analytics but remains underexplored due to legal and technical\nconstraints. This paper examines transfer learning strategies for retention\nprediction at U.S. two-year community colleges. We envision a scenario where\ncommunity colleges collaborate with each other and four-year universities to\ndevelop retention prediction models under privacy constraints and evaluate\nrisks and improvement strategies of cross-institutional model transfer. Using\nadministrative records from 4 research universities and 23 community colleges\ncovering over 800,000 students across 7 cohorts, we identify performance and\nfairness degradation when external models are deployed locally without\nadaptation. Publicly available contextual information can forecast these\nperformance drops and offer early guidance for model portability. For\ndevelopers under privacy regulations, sequential training selecting\ninstitutions based on demographic similarities enhances fairness without\ncompromising performance. For institutions lacking local data to fine-tune\nsource models, customizing evaluation thresholds for sensitive groups\noutperforms standard transfer techniques in improving performance and fairness.\nOur findings suggest the value of transfer learning for more accessible\neducational predictive modeling and call for judicious use of contextual\ninformation in model training, selection, and deployment to achieve reliable\nand equitable model transfer."
                },
                "authors": [
                    {
                        "name": "Chengyuan Yao"
                    },
                    {
                        "name": "Carmen Cortez"
                    },
                    {
                        "name": "Renzhe Yu"
                    }
                ],
                "author_detail": {
                    "name": "Renzhe Yu"
                },
                "author": "Renzhe Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06913v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06913v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06911v1",
                "updated": "2025-01-12T19:48:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    19,
                    48,
                    21,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T19:48:21Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    19,
                    48,
                    21,
                    6,
                    12,
                    0
                ],
                "title": "Risk-Averse Finetuning of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk-Averse Finetuning of Large Language Models"
                },
                "summary": "We consider the challenge of mitigating the generation of negative or toxic\ncontent by the Large Language Models (LLMs) in response to certain prompts. We\npropose integrating risk-averse principles into LLM fine-tuning to minimize the\noccurrence of harmful outputs, particularly rare but significant events. By\noptimizing the risk measure of Conditional Value at Risk (CVaR), our\nmethodology trains LLMs to exhibit superior performance in avoiding toxic\noutputs while maintaining effectiveness in generative tasks. Empirical\nevaluations on sentiment modification and toxicity mitigation tasks demonstrate\nthe efficacy of risk-averse reinforcement learning with human feedback (RLHF)\nin promoting a safer and more constructive online discourse environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the challenge of mitigating the generation of negative or toxic\ncontent by the Large Language Models (LLMs) in response to certain prompts. We\npropose integrating risk-averse principles into LLM fine-tuning to minimize the\noccurrence of harmful outputs, particularly rare but significant events. By\noptimizing the risk measure of Conditional Value at Risk (CVaR), our\nmethodology trains LLMs to exhibit superior performance in avoiding toxic\noutputs while maintaining effectiveness in generative tasks. Empirical\nevaluations on sentiment modification and toxicity mitigation tasks demonstrate\nthe efficacy of risk-averse reinforcement learning with human feedback (RLHF)\nin promoting a safer and more constructive online discourse environment."
                },
                "authors": [
                    {
                        "name": "Sapana Chaudhary"
                    },
                    {
                        "name": "Ujwal Dinesha"
                    },
                    {
                        "name": "Dileep Kalathil"
                    },
                    {
                        "name": "Srinivas Shakkottai"
                    }
                ],
                "author_detail": {
                    "name": "Srinivas Shakkottai"
                },
                "author": "Srinivas Shakkottai",
                "arxiv_comment": "Neurips 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06904v1",
                "updated": "2025-01-12T19:05:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    19,
                    5,
                    44,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T19:05:44Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    19,
                    5,
                    44,
                    6,
                    12,
                    0
                ],
                "title": "From Simulation to Field: Learning Terrain Traversability for Real-World\n  Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Simulation to Field: Learning Terrain Traversability for Real-World\n  Deployment"
                },
                "summary": "The challenge of traversability estimation is a crucial aspect of autonomous\nnavigation in unstructured outdoor environments such as forests. It involves\ndetermining whether certain areas are passable or risky for robots, taking into\naccount factors like terrain irregularities, slopes, and potential obstacles.\nThe majority of current methods for traversability estimation operate on the\nassumption of an offline computation, overlooking the significant influence of\nthe robot's heading direction on accurate traversability estimates. In this\nwork, we introduce a deep neural network that uses detailed geometric\nenvironmental data together with the robot's recent movement characteristics.\nThis fusion enables the generation of robot direction awareness and continuous\ntraversability estimates, essential for enhancing robot autonomy in challenging\nterrains like dense forests. The efficacy and significance of our approach are\nunderscored by experiments conducted on both simulated and real robotic\nplatforms in various environments, yielding quantitatively superior performance\nresults compared to existing methods. Moreover, we demonstrate that our method,\ntrained exclusively in a high-fidelity simulated setting, can accurately\npredict traversability in real-world applications without any real data\ncollection. Our experiments showcase the advantages of our method for\noptimizing path-planning and exploration tasks within difficult outdoor\nenvironments, underscoring its practicality for effective, real-world robotic\nnavigation. In the spirit of collaborative advancement, we have made the code\nimplementation available to the public.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The challenge of traversability estimation is a crucial aspect of autonomous\nnavigation in unstructured outdoor environments such as forests. It involves\ndetermining whether certain areas are passable or risky for robots, taking into\naccount factors like terrain irregularities, slopes, and potential obstacles.\nThe majority of current methods for traversability estimation operate on the\nassumption of an offline computation, overlooking the significant influence of\nthe robot's heading direction on accurate traversability estimates. In this\nwork, we introduce a deep neural network that uses detailed geometric\nenvironmental data together with the robot's recent movement characteristics.\nThis fusion enables the generation of robot direction awareness and continuous\ntraversability estimates, essential for enhancing robot autonomy in challenging\nterrains like dense forests. The efficacy and significance of our approach are\nunderscored by experiments conducted on both simulated and real robotic\nplatforms in various environments, yielding quantitatively superior performance\nresults compared to existing methods. Moreover, we demonstrate that our method,\ntrained exclusively in a high-fidelity simulated setting, can accurately\npredict traversability in real-world applications without any real data\ncollection. Our experiments showcase the advantages of our method for\noptimizing path-planning and exploration tasks within difficult outdoor\nenvironments, underscoring its practicality for effective, real-world robotic\nnavigation. In the spirit of collaborative advancement, we have made the code\nimplementation available to the public."
                },
                "authors": [
                    {
                        "name": "Fetullah Atas"
                    },
                    {
                        "name": "Grzegorz Cielniak"
                    },
                    {
                        "name": "Lars Grimstad"
                    }
                ],
                "author_detail": {
                    "name": "Lars Grimstad"
                },
                "author": "Lars Grimstad",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06880v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06880v1",
                "updated": "2025-01-12T17:28:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    28,
                    9,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T17:28:09Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    28,
                    9,
                    6,
                    12,
                    0
                ],
                "title": "Real-Time Neural-Enhancement for Online Cloud Gaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Neural-Enhancement for Online Cloud Gaming"
                },
                "summary": "Online Cloud gaming demands real-time, high-quality video transmission across\nvariable wide-area networks (WANs). Neural-enhanced video transmission\nalgorithms employing super-resolution (SR) for video quality enhancement have\neffectively challenged WAN environments. However, these SR-based methods\nrequire intensive fine-tuning for the whole video, making it infeasible in\ndiverse online cloud gaming. To address this, we introduce River, a cloud\ngaming delivery framework designed based on the observation that video segment\nfeatures in cloud gaming are typically repetitive and redundant. This permits a\nsignificant opportunity to reuse fine-tuned SR models, reducing the fine-tuning\nlatency of minutes to query latency of milliseconds. To enable the idea, we\ndesign a practical system that addresses several challenges, such as model\norganization, online model scheduler, and transfer strategy. River first builds\na content-aware encoder that fine-tunes SR models for diverse video segments\nand stores them in a lookup table. When delivering cloud gaming video streams\nonline, River checks the video features and retrieves the most relevant SR\nmodels to enhance the frame quality. Meanwhile, if no existing SR model\nperforms well enough for some video segments, River will further fine-tune new\nmodels and update the lookup table. Finally, to avoid the overhead of streaming\nmodel weight to the clients, River designs a prefetching strategy that predicts\nthe models with the highest possibility of being retrieved. Our evaluation\nbased on real video game streaming demonstrates River can reduce redundant\ntraining overhead by 44% and improve the Peak-Signal-to-Noise-Ratio by 1.81dB\ncompared to the SOTA solutions. Practical deployment shows River meets\nreal-time requirements, achieving approximately 720p 20fps on mobile devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Cloud gaming demands real-time, high-quality video transmission across\nvariable wide-area networks (WANs). Neural-enhanced video transmission\nalgorithms employing super-resolution (SR) for video quality enhancement have\neffectively challenged WAN environments. However, these SR-based methods\nrequire intensive fine-tuning for the whole video, making it infeasible in\ndiverse online cloud gaming. To address this, we introduce River, a cloud\ngaming delivery framework designed based on the observation that video segment\nfeatures in cloud gaming are typically repetitive and redundant. This permits a\nsignificant opportunity to reuse fine-tuned SR models, reducing the fine-tuning\nlatency of minutes to query latency of milliseconds. To enable the idea, we\ndesign a practical system that addresses several challenges, such as model\norganization, online model scheduler, and transfer strategy. River first builds\na content-aware encoder that fine-tunes SR models for diverse video segments\nand stores them in a lookup table. When delivering cloud gaming video streams\nonline, River checks the video features and retrieves the most relevant SR\nmodels to enhance the frame quality. Meanwhile, if no existing SR model\nperforms well enough for some video segments, River will further fine-tune new\nmodels and update the lookup table. Finally, to avoid the overhead of streaming\nmodel weight to the clients, River designs a prefetching strategy that predicts\nthe models with the highest possibility of being retrieved. Our evaluation\nbased on real video game streaming demonstrates River can reduce redundant\ntraining overhead by 44% and improve the Peak-Signal-to-Noise-Ratio by 1.81dB\ncompared to the SOTA solutions. Practical deployment shows River meets\nreal-time requirements, achieving approximately 720p 20fps on mobile devices."
                },
                "authors": [
                    {
                        "name": "Shan Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Haisheng Tan"
                    },
                    {
                        "name": "Xinyang Jiang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xiaoxi Zhang"
                    },
                    {
                        "name": "Hongqiu Ni"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Xiang-Yang Li"
                    }
                ],
                "author_detail": {
                    "name": "Xiang-Yang Li"
                },
                "author": "Xiang-Yang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06880v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06880v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00879v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00879v2",
                "updated": "2025-01-12T17:03:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    17,
                    3,
                    12,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-01T15:57:34Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    15,
                    57,
                    34,
                    2,
                    1,
                    0
                ],
                "title": "TrustRAG: Enhancing Robustness and Trustworthiness in RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrustRAG: Enhancing Robustness and Trustworthiness in RAG"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user queries. However, these\nsystems remain vulnerable to corpus poisoning attacks that can significantly\ndegrade LLM performance through the injection of malicious content. To address\nthese challenges, we propose TrustRAG, a robust framework that systematically\nfilters compromised and irrelevant contents before they are retrieved for\ngeneration. Our approach implements a two-stage defense mechanism: At the first\nstage, it employs K-means clustering to identify potential attack patterns in\nretrieved documents using cosine similarity and ROUGE metrics as guidance,\neffectively isolating suspicious content. Secondly, it performs a\nself-assessment which detects malicious documents and resolves discrepancies\nbetween the model's internal knowledge and external information. TrustRAG\nfunctions as a plug-and-play, training-free module that integrates seamlessly\nwith any language model, whether open or closed-source. In addition, TrustRAG\nmaintains high contextual relevance while strengthening defenses against corpus\npoisoning attacks. Through extensive experimental validation, we demonstrate\nthat TrustRAG delivers substantial improvements in retrieval accuracy,\nefficiency, and attack resistance compared to existing approaches across\nmultiple model architectures and datasets. We have made TrustRAG available as\nopen-source software at \\url{https://github.com/HuichiZhou/TrustRAG}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems enhance large language models\n(LLMs) by integrating external knowledge sources, enabling more accurate and\ncontextually relevant responses tailored to user queries. However, these\nsystems remain vulnerable to corpus poisoning attacks that can significantly\ndegrade LLM performance through the injection of malicious content. To address\nthese challenges, we propose TrustRAG, a robust framework that systematically\nfilters compromised and irrelevant contents before they are retrieved for\ngeneration. Our approach implements a two-stage defense mechanism: At the first\nstage, it employs K-means clustering to identify potential attack patterns in\nretrieved documents using cosine similarity and ROUGE metrics as guidance,\neffectively isolating suspicious content. Secondly, it performs a\nself-assessment which detects malicious documents and resolves discrepancies\nbetween the model's internal knowledge and external information. TrustRAG\nfunctions as a plug-and-play, training-free module that integrates seamlessly\nwith any language model, whether open or closed-source. In addition, TrustRAG\nmaintains high contextual relevance while strengthening defenses against corpus\npoisoning attacks. Through extensive experimental validation, we demonstrate\nthat TrustRAG delivers substantial improvements in retrieval accuracy,\nefficiency, and attack resistance compared to existing approaches across\nmultiple model architectures and datasets. We have made TrustRAG available as\nopen-source software at \\url{https://github.com/HuichiZhou/TrustRAG}."
                },
                "authors": [
                    {
                        "name": "Huichi Zhou"
                    },
                    {
                        "name": "Kin-Hei Lee"
                    },
                    {
                        "name": "Zhonghao Zhan"
                    },
                    {
                        "name": "Yue Chen"
                    },
                    {
                        "name": "Zhenhao Li"
                    },
                    {
                        "name": "Zhaoyang Wang"
                    },
                    {
                        "name": "Hamed Haddadi"
                    },
                    {
                        "name": "Emine Yilmaz"
                    }
                ],
                "author_detail": {
                    "name": "Emine Yilmaz"
                },
                "author": "Emine Yilmaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00879v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00879v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09172v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09172v4",
                "updated": "2025-01-12T16:31:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    16,
                    31,
                    19,
                    6,
                    12,
                    0
                ],
                "published": "2024-08-17T11:33:23Z",
                "published_parsed": [
                    2024,
                    8,
                    17,
                    11,
                    33,
                    23,
                    5,
                    230,
                    0
                ],
                "title": "Unlocking the Power of LLM Uncertainty for Active In-Context Example\n  Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking the Power of LLM Uncertainty for Active In-Context Example\n  Selection"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable performance across a wide\nrange of downstream tasks. However, it is challenging for users to discern\nwhether the responses of LLM are generated with certainty or are fabricated to\nmeet user expectations. In this paper, we introduce Uncertainty Tripartite\nTesting Paradigm (Unc-TTP), a novel method for classifying LLM uncertainty by\nleveraging output inconsistency. Specifically, Unc-TTP performs three rounds of\nsampling under varying label injection interference, enumerating all possible\noutcomes, and uses the degree of output inconsistency as the indicator of the\nLLM's intrinsic uncertainty. To validate the effectiveness of this\ninconsistency-defined uncertainty, we draw inspiration from Active Learning,\ncomparing the informativeness of actively selected in-context examples. Our\nexperiments show that uncertainty examples selected via Unc-TTP are more\ninformative than certainty examples. Furthermore, the Unc-TTP-guided\nuncertainty-based active example selection strategy outperforms existing\nmethods, highlighting its effectiveness in classifying LLM uncertainty and\nenhancing in-context learning. This work not only underscores the potential of\ninconsistency-based uncertainty classification for both open- and closed-source\nLLMs but also presents a practical approach for leveraging uncertainty to\nimprove LLM performance in real-world tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable performance across a wide\nrange of downstream tasks. However, it is challenging for users to discern\nwhether the responses of LLM are generated with certainty or are fabricated to\nmeet user expectations. In this paper, we introduce Uncertainty Tripartite\nTesting Paradigm (Unc-TTP), a novel method for classifying LLM uncertainty by\nleveraging output inconsistency. Specifically, Unc-TTP performs three rounds of\nsampling under varying label injection interference, enumerating all possible\noutcomes, and uses the degree of output inconsistency as the indicator of the\nLLM's intrinsic uncertainty. To validate the effectiveness of this\ninconsistency-defined uncertainty, we draw inspiration from Active Learning,\ncomparing the informativeness of actively selected in-context examples. Our\nexperiments show that uncertainty examples selected via Unc-TTP are more\ninformative than certainty examples. Furthermore, the Unc-TTP-guided\nuncertainty-based active example selection strategy outperforms existing\nmethods, highlighting its effectiveness in classifying LLM uncertainty and\nenhancing in-context learning. This work not only underscores the potential of\ninconsistency-based uncertainty classification for both open- and closed-source\nLLMs but also presents a practical approach for leveraging uncertainty to\nimprove LLM performance in real-world tasks."
                },
                "authors": [
                    {
                        "name": "Hsiu-Yuan Huang"
                    },
                    {
                        "name": "Zichen Wu"
                    },
                    {
                        "name": "Yutong Yang"
                    },
                    {
                        "name": "Junzhao Zhang"
                    },
                    {
                        "name": "Yunfang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yunfang Wu"
                },
                "author": "Yunfang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09172v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09172v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06863v1",
                "updated": "2025-01-12T16:23:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    16,
                    23,
                    18,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T16:23:18Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    16,
                    23,
                    18,
                    6,
                    12,
                    0
                ],
                "title": "Transfer Learning of Tabular Data by Finetuning Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer Learning of Tabular Data by Finetuning Large Language Models"
                },
                "summary": "Despite the artificial intelligence (AI) revolution, deep learning has yet to\nachieve much success with tabular data due to heterogeneous feature space and\nlimited sample sizes without viable transfer learning. The new era of\ngenerative AI, powered by large language models (LLM), brings unprecedented\nlearning opportunities to diverse data and domains. This paper investigates the\neffectiveness of an LLM application programming interface (API) and transfer\nlearning of LLM in tabular data classification. LLM APIs respond to input text\nprompts with tokenized data and instructions, whereas transfer learning\nfinetunes an LLM for a target classification task. This paper proposes an\nend-to-end finetuning of LLM to demonstrate cross-data transfer learning on ten\nbenchmark data sets when large pre-trained tabular data models do not exist to\nfacilitate transfer learning. The proposed LLM finetuning method outperforms\nstate-of-the-art machine and deep learning methods on tabular data with less\nthan ten features - a standard feature size for tabular data sets. The transfer\nlearning approach uses a fraction of the computational cost of other deep\nlearning or API-based solutions while ensuring competitive or superior\nclassification performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the artificial intelligence (AI) revolution, deep learning has yet to\nachieve much success with tabular data due to heterogeneous feature space and\nlimited sample sizes without viable transfer learning. The new era of\ngenerative AI, powered by large language models (LLM), brings unprecedented\nlearning opportunities to diverse data and domains. This paper investigates the\neffectiveness of an LLM application programming interface (API) and transfer\nlearning of LLM in tabular data classification. LLM APIs respond to input text\nprompts with tokenized data and instructions, whereas transfer learning\nfinetunes an LLM for a target classification task. This paper proposes an\nend-to-end finetuning of LLM to demonstrate cross-data transfer learning on ten\nbenchmark data sets when large pre-trained tabular data models do not exist to\nfacilitate transfer learning. The proposed LLM finetuning method outperforms\nstate-of-the-art machine and deep learning methods on tabular data with less\nthan ten features - a standard feature size for tabular data sets. The transfer\nlearning approach uses a fraction of the computational cost of other deep\nlearning or API-based solutions while ensuring competitive or superior\nclassification performance."
                },
                "authors": [
                    {
                        "name": "Shourav B. Rabbani"
                    },
                    {
                        "name": "Ibna Kowsar"
                    },
                    {
                        "name": "Manar D. Samad"
                    }
                ],
                "author_detail": {
                    "name": "Manar D. Samad"
                },
                "author": "Manar D. Samad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02960v2",
                "updated": "2025-01-12T16:22:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    16,
                    22,
                    4,
                    6,
                    12,
                    0
                ],
                "published": "2024-07-03T09:54:08Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    9,
                    54,
                    8,
                    2,
                    185,
                    0
                ],
                "title": "ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary\n  LLMs on Private Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ObfuscaTune: Obfuscated Offsite Fine-tuning and Inference of Proprietary\n  LLMs on Private Datasets"
                },
                "summary": "This work addresses the timely yet underexplored problem of performing\ninference and finetuning of a proprietary LLM owned by a model provider entity\non the confidential/private data of another data owner entity, in a way that\nensures the confidentiality of both the model and the data. Hereby, the\nfinetuning is conducted offsite, i.e., on the computation infrastructure of a\nthird-party cloud provider. We tackle this problem by proposing ObfuscaTune, a\nnovel, efficient and fully utility-preserving approach that combines a simple\nyet effective obfuscation technique with an efficient usage of confidential\ncomputing (only 5% of the model parameters are placed on TEE). We empirically\ndemonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models\nwith different sizes on four NLP benchmark datasets. Finally, we compare to a\nna\\\"ive version of our approach to highlight the necessity of using random\nmatrices with low condition numbers in our approach to reduce errors induced by\nthe obfuscation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work addresses the timely yet underexplored problem of performing\ninference and finetuning of a proprietary LLM owned by a model provider entity\non the confidential/private data of another data owner entity, in a way that\nensures the confidentiality of both the model and the data. Hereby, the\nfinetuning is conducted offsite, i.e., on the computation infrastructure of a\nthird-party cloud provider. We tackle this problem by proposing ObfuscaTune, a\nnovel, efficient and fully utility-preserving approach that combines a simple\nyet effective obfuscation technique with an efficient usage of confidential\ncomputing (only 5% of the model parameters are placed on TEE). We empirically\ndemonstrate the effectiveness of ObfuscaTune by validating it on GPT-2 models\nwith different sizes on four NLP benchmark datasets. Finally, we compare to a\nna\\\"ive version of our approach to highlight the necessity of using random\nmatrices with low condition numbers in our approach to reduce errors induced by\nthe obfuscation."
                },
                "authors": [
                    {
                        "name": "Ahmed Frikha"
                    },
                    {
                        "name": "Nassim Walha"
                    },
                    {
                        "name": "Ricardo Mendes"
                    },
                    {
                        "name": "Krishna Kanth Nakka"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Xuebing Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xuebing Zhou"
                },
                "author": "Xuebing Zhou",
                "arxiv_comment": "Accepted at AAAI 2025 (PPAI Workshop)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06859v1",
                "updated": "2025-01-12T16:17:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    16,
                    17,
                    25,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T16:17:25Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    16,
                    17,
                    25,
                    6,
                    12,
                    0
                ],
                "title": "A Comprehensive Evaluation of Large Language Models on Mental Illnesses\n  in Arabic Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Evaluation of Large Language Models on Mental Illnesses\n  in Arabic Context"
                },
                "summary": "Mental health disorders pose a growing public health concern in the Arab\nworld, emphasizing the need for accessible diagnostic and intervention tools.\nLarge language models (LLMs) offer a promising approach, but their application\nin Arabic contexts faces challenges including limited labeled datasets,\nlinguistic complexity, and translation biases. This study comprehensively\nevaluates 8 LLMs, including general multi-lingual models, as well as bi-lingual\nones, on diverse mental health datasets (such as AraDepSu, Dreaddit, MedMCQA),\ninvestigating the impact of prompt design, language configuration (native\nArabic vs. translated English, and vice versa), and few-shot prompting on\ndiagnostic performance. We find that prompt engineering significantly\ninfluences LLM scores mainly due to reduced instruction following, with our\nstructured prompt outperforming a less structured variant on multi-class\ndatasets, with an average difference of 14.5\\%. While language influence on\nperformance was modest, model selection proved crucial: Phi-3.5 MoE excelled in\nbalanced accuracy, particularly for binary classification, while Mistral NeMo\nshowed superior performance in mean absolute error for severity prediction\ntasks. Few-shot prompting consistently improved performance, with particularly\nsubstantial gains observed for GPT-4o Mini on multi-class classification,\nboosting accuracy by an average factor of 1.58. These findings underscore the\nimportance of prompt optimization, multilingual analysis, and few-shot learning\nfor developing culturally sensitive and effective LLM-based mental health tools\nfor Arabic-speaking populations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental health disorders pose a growing public health concern in the Arab\nworld, emphasizing the need for accessible diagnostic and intervention tools.\nLarge language models (LLMs) offer a promising approach, but their application\nin Arabic contexts faces challenges including limited labeled datasets,\nlinguistic complexity, and translation biases. This study comprehensively\nevaluates 8 LLMs, including general multi-lingual models, as well as bi-lingual\nones, on diverse mental health datasets (such as AraDepSu, Dreaddit, MedMCQA),\ninvestigating the impact of prompt design, language configuration (native\nArabic vs. translated English, and vice versa), and few-shot prompting on\ndiagnostic performance. We find that prompt engineering significantly\ninfluences LLM scores mainly due to reduced instruction following, with our\nstructured prompt outperforming a less structured variant on multi-class\ndatasets, with an average difference of 14.5\\%. While language influence on\nperformance was modest, model selection proved crucial: Phi-3.5 MoE excelled in\nbalanced accuracy, particularly for binary classification, while Mistral NeMo\nshowed superior performance in mean absolute error for severity prediction\ntasks. Few-shot prompting consistently improved performance, with particularly\nsubstantial gains observed for GPT-4o Mini on multi-class classification,\nboosting accuracy by an average factor of 1.58. These findings underscore the\nimportance of prompt optimization, multilingual analysis, and few-shot learning\nfor developing culturally sensitive and effective LLM-based mental health tools\nfor Arabic-speaking populations."
                },
                "authors": [
                    {
                        "name": "Noureldin Zahran"
                    },
                    {
                        "name": "Aya E. Fouda"
                    },
                    {
                        "name": "Radwa J. Hanafy"
                    },
                    {
                        "name": "Mohammed E. Fouda"
                    }
                ],
                "author_detail": {
                    "name": "Mohammed E. Fouda"
                },
                "author": "Mohammed E. Fouda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06842v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06842v1",
                "updated": "2025-01-12T15:21:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    15,
                    21,
                    22,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T15:21:22Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    15,
                    21,
                    22,
                    6,
                    12,
                    0
                ],
                "title": "SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training"
                },
                "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\ndiverse tasks, yet their training remains highly resource-intensive and\nsusceptible to critical challenges such as training instability. A predominant\nsource of this instability stems from gradient and loss spikes, which disrupt\nthe learning process, often leading to costly interventions like checkpoint\nrecovery and experiment restarts, further amplifying inefficiencies. This paper\npresents a comprehensive investigation into gradient spikes observed during LLM\ntraining, revealing their prevalence across multiple architectures and\ndatasets. Our analysis shows that these spikes can be up to $1000\\times$ larger\nthan typical gradients, substantially deteriorating model performance. To\naddress this issue, we propose Spike-Aware Adam with Momentum Reset SPAM, a\nnovel optimizer designed to counteract gradient spikes through momentum reset\nand spike-aware gradient clipping. Extensive experiments, including both\npre-training and fine-tuning, demonstrate that SPAM consistently surpasses Adam\nand its variants across various tasks, including (1) LLM pre-training from 60M\nto 1B, (2) 4-bit LLM pre-training,(3) reinforcement learning, and (4) Time\nSeries Forecasting. Additionally, SPAM facilitates memory-efficient training by\nenabling sparse momentum, where only a subset of momentum terms are maintained\nand updated. When operating under memory constraints, SPAM outperforms\nstate-of-the-art memory-efficient optimizers such as GaLore and Adam-Mini. Our\nwork underscores the importance of mitigating gradient spikes in LLM training\nand introduces an effective optimization strategy that enhances both training\nstability and resource efficiency at scale. Code is available at\nhttps://github.com/TianjinYellow/SPAM-Optimizer.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated exceptional performance across\ndiverse tasks, yet their training remains highly resource-intensive and\nsusceptible to critical challenges such as training instability. A predominant\nsource of this instability stems from gradient and loss spikes, which disrupt\nthe learning process, often leading to costly interventions like checkpoint\nrecovery and experiment restarts, further amplifying inefficiencies. This paper\npresents a comprehensive investigation into gradient spikes observed during LLM\ntraining, revealing their prevalence across multiple architectures and\ndatasets. Our analysis shows that these spikes can be up to $1000\\times$ larger\nthan typical gradients, substantially deteriorating model performance. To\naddress this issue, we propose Spike-Aware Adam with Momentum Reset SPAM, a\nnovel optimizer designed to counteract gradient spikes through momentum reset\nand spike-aware gradient clipping. Extensive experiments, including both\npre-training and fine-tuning, demonstrate that SPAM consistently surpasses Adam\nand its variants across various tasks, including (1) LLM pre-training from 60M\nto 1B, (2) 4-bit LLM pre-training,(3) reinforcement learning, and (4) Time\nSeries Forecasting. Additionally, SPAM facilitates memory-efficient training by\nenabling sparse momentum, where only a subset of momentum terms are maintained\nand updated. When operating under memory constraints, SPAM outperforms\nstate-of-the-art memory-efficient optimizers such as GaLore and Adam-Mini. Our\nwork underscores the importance of mitigating gradient spikes in LLM training\nand introduces an effective optimization strategy that enhances both training\nstability and resource efficiency at scale. Code is available at\nhttps://github.com/TianjinYellow/SPAM-Optimizer.git"
                },
                "authors": [
                    {
                        "name": "Tianjin Huang"
                    },
                    {
                        "name": "Ziquan Zhu"
                    },
                    {
                        "name": "Gaojie Jin"
                    },
                    {
                        "name": "Lu Liu"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Shiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Liu"
                },
                "author": "Shiwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06842v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06842v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06837v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06837v1",
                "updated": "2025-01-12T15:10:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    12,
                    15,
                    10,
                    57,
                    6,
                    12,
                    0
                ],
                "published": "2025-01-12T15:10:57Z",
                "published_parsed": [
                    2025,
                    1,
                    12,
                    15,
                    10,
                    57,
                    6,
                    12,
                    0
                ],
                "title": "An efficient approach to represent enterprise web application structure\n  using Large Language Model in the service of Intelligent Quality Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient approach to represent enterprise web application structure\n  using Large Language Model in the service of Intelligent Quality Engineering"
                },
                "summary": "This paper presents a novel approach to represent enterprise web application\nstructures using Large Language Models (LLMs) to enable intelligent quality\nengineering at scale. We introduce a hierarchical representation methodology\nthat optimizes the few-shot learning capabilities of LLMs while preserving the\ncomplex relationships and interactions within web applications. The approach\nencompasses five key phases: comprehensive DOM analysis, multi-page synthesis,\ntest suite generation, execution, and result analysis. Our methodology\naddresses existing challenges around usage of Generative AI techniques in\nautomated software testing by developing a structured format that enables LLMs\nto understand web application architecture through in-context learning. We\nevaluated our approach using two distinct web applications: an e-commerce\nplatform (Swag Labs) and a healthcare application (MediBox) which is deployed\nwithin Atalgo engineering environment. The results demonstrate success rates of\n90\\% and 70\\%, respectively, in achieving automated testing, with high\nrelevance scores for test cases across multiple evaluation criteria. The\nfindings suggest that our representation approach significantly enhances LLMs'\nability to generate contextually relevant test cases and provide better quality\nassurance overall, while reducing the time and effort required for testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel approach to represent enterprise web application\nstructures using Large Language Models (LLMs) to enable intelligent quality\nengineering at scale. We introduce a hierarchical representation methodology\nthat optimizes the few-shot learning capabilities of LLMs while preserving the\ncomplex relationships and interactions within web applications. The approach\nencompasses five key phases: comprehensive DOM analysis, multi-page synthesis,\ntest suite generation, execution, and result analysis. Our methodology\naddresses existing challenges around usage of Generative AI techniques in\nautomated software testing by developing a structured format that enables LLMs\nto understand web application architecture through in-context learning. We\nevaluated our approach using two distinct web applications: an e-commerce\nplatform (Swag Labs) and a healthcare application (MediBox) which is deployed\nwithin Atalgo engineering environment. The results demonstrate success rates of\n90\\% and 70\\%, respectively, in achieving automated testing, with high\nrelevance scores for test cases across multiple evaluation criteria. The\nfindings suggest that our representation approach significantly enhances LLMs'\nability to generate contextually relevant test cases and provide better quality\nassurance overall, while reducing the time and effort required for testing."
                },
                "authors": [
                    {
                        "name": "Zaber Al Hassan Ayon"
                    },
                    {
                        "name": "Gulam Husain"
                    },
                    {
                        "name": "Roshankumar Bisoi"
                    },
                    {
                        "name": "Waliur Rahman"
                    },
                    {
                        "name": "Dr Tom Osborn"
                    }
                ],
                "author_detail": {
                    "name": "Dr Tom Osborn"
                },
                "author": "Dr Tom Osborn",
                "arxiv_comment": "16 pages, 1 figure and 4 tables, relevant for Gen AI and enterprise\n  AI use cases",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06837v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06837v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]